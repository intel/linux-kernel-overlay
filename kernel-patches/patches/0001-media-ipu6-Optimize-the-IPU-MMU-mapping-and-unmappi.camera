From 065375efe1b92edff12ffe4bd413f6f2e1b7fdec Mon Sep 17 00:00:00 2001
From: zouxiaoh <xiaohong.zou@intel.com>
Date: Tue, 16 Apr 2024 16:19:22 +0800
Subject: [PATCH 01/12] media/ipu6: Optimize the IPU MMU mapping and unmapping
 flow

Previously, ipu_mmu_map() and ipu_mmu_unmap() operated on a per-page
basis, leading to frequent calls to spin_locks/unlocks and
clflush_cache_range for each page. This caused inefficiencies,
especially when handling large dma-bufs with hundreds of pages.

This CL enhances ipu_mmu_map()/ipu_mmu_unmap() with batching process
multiple contiguous pages. This significantly reduces calls for
spin_lock/unlock and clflush_cache_range() and improve the
performance.

Test Platform:
rex

Signed-off-by: Bingbu Cao <bingbu.cao@intel.com>
Signed-off-by: Jianhui Dai <jianhui.j.dai@intel.com>
Signed-off-by: zouxiaoh <xiaohong.zou@intel.com>
---
 drivers/media/pci/intel/ipu-mmu.c | 255 +++++++++++++-----------------
 1 file changed, 106 insertions(+), 149 deletions(-)

diff --git a/drivers/media/pci/intel/ipu-mmu.c b/drivers/media/pci/intel/ipu-mmu.c
index a54a41e5fe5f..6a9c1cfc8bc9 100644
--- a/drivers/media/pci/intel/ipu-mmu.c
+++ b/drivers/media/pci/intel/ipu-mmu.c
@@ -243,75 +243,99 @@ static u32 *alloc_l2_pt(struct ipu_mmu_info *mmu_info)
 	return pt;
 }
 
+static size_t l2_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		       phys_addr_t dummy, size_t size);
 static int l2_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 		  phys_addr_t paddr, size_t size)
 {
-	u32 l1_idx = iova >> ISP_L1PT_SHIFT;
+	struct device *dev = mmu_info->dev;
+	u32 l1_idx;
 	u32 l1_entry;
 	u32 *l2_pt, *l2_virt;
-	u32 iova_start = iova;
 	unsigned int l2_idx;
 	unsigned long flags;
 	dma_addr_t dma;
-
-	dev_dbg(mmu_info->dev,
-		"mapping l2 page table for l1 index %u (iova %8.8x)\n",
-		l1_idx, (u32)iova);
+	unsigned int l2_entries;
+	size_t mapped_size = 0;
+	int err = 0;
 
 	spin_lock_irqsave(&mmu_info->lock, flags);
-	l1_entry = mmu_info->l1_pt[l1_idx];
-	if (l1_entry == mmu_info->dummy_l2_pteval) {
-		l2_virt = mmu_info->l2_pts[l1_idx];
-		if (likely(!l2_virt)) {
-			l2_virt = alloc_l2_pt(mmu_info);
-			if (!l2_virt) {
+
+	paddr = ALIGN(paddr, ISP_PAGE_SIZE);
+	for (l1_idx = iova >> ISP_L1PT_SHIFT;
+	     size > 0 && l1_idx < ISP_L1PT_PTES; l1_idx++) {
+		dev_dbg(dev,
+			"mapping l2 page table for l1 index %u (iova %8.8x)\n",
+			l1_idx, (u32)iova);
+
+		l1_entry = mmu_info->l1_pt[l1_idx];
+		if (l1_entry == mmu_info->dummy_l2_pteval) {
+			l2_virt = mmu_info->l2_pts[l1_idx];
+			if (likely(!l2_virt)) {
+				l2_virt = alloc_l2_pt(mmu_info);
+				if (!l2_virt) {
+					spin_unlock_irqrestore(&mmu_info->lock,
+							       flags);
+					err = -ENOMEM;
+					goto error;
+				}
+			}
+
+			dma = map_single(mmu_info, l2_virt);
+			if (!dma) {
+				dev_err(dev, "Failed to map l2pt page\n");
+				free_page((unsigned long)l2_virt);
+
 				spin_unlock_irqrestore(&mmu_info->lock, flags);
-				return -ENOMEM;
+				err = -EINVAL;
+				goto error;
 			}
-		}
 
-		dma = map_single(mmu_info, l2_virt);
-		if (!dma) {
-			dev_err(mmu_info->dev, "Failed to map l2pt page\n");
-			free_page((unsigned long)l2_virt);
-			spin_unlock_irqrestore(&mmu_info->lock, flags);
-			return -EINVAL;
-		}
+			l1_entry = dma >> ISP_PADDR_SHIFT;
 
-		l1_entry = dma >> ISP_PADDR_SHIFT;
+			dev_dbg(dev, "page for l1_idx %u %p allocated\n",
+				l1_idx, l2_virt);
+			mmu_info->l1_pt[l1_idx] = l1_entry;
+			mmu_info->l2_pts[l1_idx] = l2_virt;
 
-		dev_dbg(mmu_info->dev, "page for l1_idx %u %p allocated\n",
-			l1_idx, l2_virt);
-		mmu_info->l1_pt[l1_idx] = l1_entry;
-		mmu_info->l2_pts[l1_idx] = l2_virt;
-		clflush_cache_range(&mmu_info->l1_pt[l1_idx],
-				    sizeof(mmu_info->l1_pt[l1_idx]));
-	}
+			clflush_cache_range(&mmu_info->l1_pt[l1_idx],
+					    sizeof(mmu_info->l1_pt[l1_idx]));
+		}
 
-	l2_pt = mmu_info->l2_pts[l1_idx];
+		l2_pt = mmu_info->l2_pts[l1_idx];
+		l2_entries = 0;
 
-	dev_dbg(mmu_info->dev, "l2_pt at %p with dma 0x%x\n", l2_pt, l1_entry);
+		for (l2_idx = (iova & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
+		     size > 0 && l2_idx < ISP_L2PT_PTES; l2_idx++) {
+			l2_pt[l2_idx] = paddr >> ISP_PADDR_SHIFT;
 
-	paddr = ALIGN(paddr, ISP_PAGE_SIZE);
+			dev_dbg(dev, "l2 index %u mapped as 0x%8.8x\n", l2_idx,
+				l2_pt[l2_idx]);
 
-	l2_idx = (iova_start & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
+			iova += ISP_PAGE_SIZE;
+			paddr += ISP_PAGE_SIZE;
+			mapped_size += ISP_PAGE_SIZE;
+			size -= ISP_PAGE_SIZE;
 
-	dev_dbg(mmu_info->dev, "l2_idx %u, phys 0x%8.8x\n", l2_idx,
-		l2_pt[l2_idx]);
-	if (l2_pt[l2_idx] != mmu_info->dummy_page_pteval) {
-		spin_unlock_irqrestore(&mmu_info->lock, flags);
-		return -EINVAL;
-	}
+			l2_entries++;
+		}
 
-	l2_pt[l2_idx] = paddr >> ISP_PADDR_SHIFT;
+		if (l2_entries)
+			clflush_cache_range(&l2_pt[l2_idx - l2_entries],
+					    sizeof(l2_pt[0]) * l2_entries);
+	}
 
-	clflush_cache_range(&l2_pt[l2_idx], sizeof(l2_pt[l2_idx]));
 	spin_unlock_irqrestore(&mmu_info->lock, flags);
 
-	dev_dbg(mmu_info->dev, "l2 index %u mapped as 0x%8.8x\n", l2_idx,
-		l2_pt[l2_idx]);
-
 	return 0;
+
+error:
+	/* unroll mapping in case something went wrong */
+	if (size && mapped_size)
+		l2_unmap(mmu_info, iova - mapped_size, paddr - mapped_size,
+			 mapped_size);
+
+	return err;
 }
 
 static int __ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
@@ -330,37 +354,49 @@ static int __ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 static size_t l2_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
 		       phys_addr_t dummy, size_t size)
 {
-	u32 l1_idx = iova >> ISP_L1PT_SHIFT;
+	u32 l1_idx;
 	u32 *l2_pt;
-	u32 iova_start = iova;
 	unsigned int l2_idx;
+	unsigned int l2_entries;
 	size_t unmapped = 0;
 	unsigned long flags;
 
-	dev_dbg(mmu_info->dev, "unmapping l2 page table for l1 index %u (iova 0x%8.8lx)\n",
-		l1_idx, iova);
-
 	spin_lock_irqsave(&mmu_info->lock, flags);
-	if (mmu_info->l1_pt[l1_idx] == mmu_info->dummy_l2_pteval) {
-		spin_unlock_irqrestore(&mmu_info->lock, flags);
-		dev_err(mmu_info->dev,
-			"unmap iova 0x%8.8lx l1 idx %u which was not mapped\n",
-			iova, l1_idx);
-		return 0;
-	}
+	for (l1_idx = iova >> ISP_L1PT_SHIFT;
+	     size > 0 && l1_idx < ISP_L1PT_PTES; l1_idx++) {
+		dev_dbg(mmu_info->dev,
+			"unmapping l2 page table for l1 index %u (iova 0x%8.8lx)\n",
+			l1_idx, iova);
 
-	for (l2_idx = (iova_start & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
-	     (iova_start & ISP_L1PT_MASK) + (l2_idx << ISP_PAGE_SHIFT)
-	     < iova_start + size && l2_idx < ISP_L2PT_PTES; l2_idx++) {
+		if (mmu_info->l1_pt[l1_idx] == mmu_info->dummy_l2_pteval) {
+			spin_unlock_irqrestore(&mmu_info->lock, flags);
+			dev_err(mmu_info->dev,
+				"unmap iova 0x%8.8lx l1 idx %u which was not mapped\n",
+				iova, l1_idx);
+			return unmapped << ISP_PAGE_SHIFT;
+		}
 		l2_pt = mmu_info->l2_pts[l1_idx];
-		dev_dbg(mmu_info->dev,
-			"unmap l2 index %u with pteval 0x%10.10llx\n",
-			l2_idx, TBL_PHYS_ADDR(l2_pt[l2_idx]));
-		l2_pt[l2_idx] = mmu_info->dummy_page_pteval;
 
-		clflush_cache_range(&l2_pt[l2_idx], sizeof(l2_pt[l2_idx]));
-		unmapped++;
+		l2_entries = 0;
+		for (l2_idx = (iova & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
+		     size > 0 && l2_idx < ISP_L2PT_PTES; l2_idx++) {
+			dev_dbg(mmu_info->dev,
+				"unmap l2 index %u with pteval 0x%10.10llx\n",
+				l2_idx, TBL_PHYS_ADDR(l2_pt[l2_idx]));
+			l2_pt[l2_idx] = mmu_info->dummy_page_pteval;
+
+			iova += ISP_PAGE_SIZE;
+			size -= ISP_PAGE_SIZE;
+			l2_entries++;
+		}
+
+		if (l2_entries) {
+			clflush_cache_range(&l2_pt[l2_idx - l2_entries],
+					    sizeof(l2_pt[0]) * l2_entries);
+			unmapped += l2_entries;
+		}
 	}
+
 	spin_unlock_irqrestore(&mmu_info->lock, flags);
 
 	return unmapped << ISP_PAGE_SHIFT;
@@ -376,7 +412,7 @@ static int allocate_trash_buffer(struct ipu_mmu *mmu)
 {
 	unsigned int n_pages = PAGE_ALIGN(IPU_MMUV2_TRASH_RANGE) >> PAGE_SHIFT;
 	struct iova *iova;
-	unsigned long iova_addr;
+	u32 iova_addr;
 	unsigned int i;
 	dma_addr_t dma;
 	int ret;
@@ -614,50 +650,14 @@ phys_addr_t ipu_mmu_iova_to_phys(struct ipu_mmu_info *mmu_info,
 	return phy_addr;
 }
 
-/*
- * The following four functions are implemented based on iommu.c
- * drivers/iommu/iommu.c:iommu_pgsize().
- */
-static size_t ipu_mmu_pgsize(unsigned long pgsize_bitmap,
-			     unsigned long addr_merge, size_t size)
-{
-	unsigned int pgsize_idx;
-	size_t pgsize;
-
-	/* Max page size that still fits into 'size' */
-	pgsize_idx = __fls(size);
-
-	/* need to consider alignment requirements ? */
-	if (likely(addr_merge)) {
-		/* Max page size allowed by address */
-		unsigned int align_pgsize_idx = __ffs(addr_merge);
-
-		pgsize_idx = min(pgsize_idx, align_pgsize_idx);
-	}
-
-	/* build a mask of acceptable page sizes */
-	pgsize = (1UL << (pgsize_idx + 1)) - 1;
-
-	/* throw away page sizes not supported by the hardware */
-	pgsize &= pgsize_bitmap;
-
-	/* make sure we're still sane */
-	WARN_ON(!pgsize);
-
-	/* pick the biggest page */
-	pgsize_idx = __fls(pgsize);
-	pgsize = 1UL << pgsize_idx;
-
-	return pgsize;
-}
-
 /* drivers/iommu/iommu.c:iommu_unmap() */
 size_t ipu_mmu_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
 		     size_t size)
 {
-	size_t unmapped_page, unmapped = 0;
 	unsigned int min_pagesz;
 
+	dev_dbg(mmu_info->dev, "unmapping iova 0x%lx size 0x%zx\n", iova, size);
+
 	/* find out the minimum page size supported */
 	min_pagesz = 1 << __ffs(mmu_info->pgsize_bitmap);
 
@@ -672,36 +672,14 @@ size_t ipu_mmu_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
 		return -EINVAL;
 	}
 
-	/*
-	 * Keep iterating until we either unmap 'size' bytes (or more)
-	 * or we hit an area that isn't mapped.
-	 */
-	while (unmapped < size) {
-		size_t pgsize = ipu_mmu_pgsize(mmu_info->pgsize_bitmap,
-						iova, size - unmapped);
-
-		unmapped_page = __ipu_mmu_unmap(mmu_info, iova, pgsize);
-		if (!unmapped_page)
-			break;
-
-		dev_dbg(mmu_info->dev, "unmapped: iova 0x%lx size 0x%zx\n",
-			iova, unmapped_page);
-
-		iova += unmapped_page;
-		unmapped += unmapped_page;
-	}
-
-	return unmapped;
+	return __ipu_mmu_unmap(mmu_info, iova, size);
 }
 
 /* drivers/iommu/iommu.c:iommu_map() */
 int ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 		phys_addr_t paddr, size_t size)
 {
-	unsigned long orig_iova = iova;
 	unsigned int min_pagesz;
-	size_t orig_size = size;
-	int ret = 0;
 
 	if (mmu_info->pgsize_bitmap == 0UL)
 		return -ENODEV;
@@ -724,28 +702,7 @@ int ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 	dev_dbg(mmu_info->dev, "map: iova 0x%lx pa %pa size 0x%zx\n",
 		iova, &paddr, size);
 
-	while (size) {
-		size_t pgsize = ipu_mmu_pgsize(mmu_info->pgsize_bitmap,
-					       iova | paddr, size);
-
-		dev_dbg(mmu_info->dev,
-			"mapping: iova 0x%lx pa %pa pgsize 0x%zx\n",
-			iova, &paddr, pgsize);
-
-		ret = __ipu_mmu_map(mmu_info, iova, paddr, pgsize);
-		if (ret)
-			break;
-
-		iova += pgsize;
-		paddr += pgsize;
-		size -= pgsize;
-	}
-
-	/* unroll mapping in case something went wrong */
-	if (ret)
-		ipu_mmu_unmap(mmu_info, orig_iova, orig_size - size);
-
-	return ret;
+	return  __ipu_mmu_map(mmu_info, iova, paddr, size);
 }
 
 static void ipu_mmu_destroy(struct ipu_mmu *mmu)
-- 
2.25.1

