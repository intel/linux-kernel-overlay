From 28f72667fffb64d777e6f3f3017995d7d4f9e558 Mon Sep 17 00:00:00 2001
From: Srikanth Thokala <srikanth.thokala@intel.com>
Date: Thu, 16 Sep 2021 15:35:04 +0530
Subject: [PATCH 28/29] PCI: thunderbay: Add support for Thunderbay Root Port

Update driver to support Intel Thunder Bay PCIe Root Port.
The controller is based on DesignWare PCIe core.

In Root Complex mode, internal reference clock is used for
Thunder Bay.

Signed-off-by: Srikanth Thokala <srikanth.thokala@intel.com>
---
 drivers/pci/controller/dwc/Kconfig        |   6 +-
 drivers/pci/controller/dwc/pcie-keembay.c | 391 +++++++++++++++++++++-
 2 files changed, 391 insertions(+), 6 deletions(-)

diff --git a/drivers/pci/controller/dwc/Kconfig b/drivers/pci/controller/dwc/Kconfig
index 9a3f47927ebd..72752a063608 100644
--- a/drivers/pci/controller/dwc/Kconfig
+++ b/drivers/pci/controller/dwc/Kconfig
@@ -240,14 +240,14 @@ config PCIE_KEEMBAY
 	bool
 
 config PCIE_KEEMBAY_HOST
-	bool "Intel Keem Bay PCIe controller - Host mode"
-	depends on ARCH_KEEMBAY || COMPILE_TEST
+	bool "Intel Keem Bay or Thunder Bay PCIe controller - Host mode"
+	depends on ARCH_KEEMBAY || ARCH_THUNDERBAY || COMPILE_TEST
 	depends on PCI && PCI_MSI_IRQ_DOMAIN
 	select PCIE_DW_HOST
 	select PCIE_KEEMBAY
 	help
 	  Say 'Y' here to enable support for the PCIe controller in Keem Bay
-	  to work in host mode.
+	  or Thunder Bay to work in host mode.
 	  The PCIe controller is based on DesignWare Hardware and uses
 	  DesignWare core functions.
 
diff --git a/drivers/pci/controller/dwc/pcie-keembay.c b/drivers/pci/controller/dwc/pcie-keembay.c
index 6cbbab8383f3..ebe14f8802b9 100644
--- a/drivers/pci/controller/dwc/pcie-keembay.c
+++ b/drivers/pci/controller/dwc/pcie-keembay.c
@@ -8,6 +8,7 @@
 #include <linux/bits.h>
 #include <linux/clk.h>
 #include <linux/delay.h>
+#include <linux/dmaengine.h>
 #include <linux/err.h>
 #include <linux/gpio/consumer.h>
 #include <linux/init.h>
@@ -59,8 +60,36 @@
 #define THB_PCIE_CTRL0_CNF0_LTSSM_EN	BIT(4)
 
 #define THB_PCIE_CTRL0_STS		0x1000
+#define THB_PCIE_CTRL1_STS		0x1040
 #define THB_PCIE_CTRL_STS_LINK_UP	(BIT(10) | BIT(0))
 
+#define THB_PCIE_CTRL1_CNF0		0x28
+#define THB_PCIE_CTRL1_CNF0_LTSSM_EN	BIT(4)
+
+#define THB_PCIE_CTRL1_CNF1		0x2c
+#define THB_PCIE_CTRL1_CNF1_XFER_PEND	BIT(3)
+
+#define THB_PCIE_CTRL1_SPARE1		0x34
+#define THB_PCIE_CTRL1_SPARE1_PERST_PAD	(BIT(2) | BIT(1))
+
+#define THB_PCIE_PHY1_CNF0		0x54
+#define THB_PCIE_PHY1_CNF0_EXT_LD	BIT(29)
+
+#define THB_PCIE_PHY1_CNF2		0x5C
+#define THB_PCIE_PHY1_CNF2_PWR_STABLE	(BIT(1) | BIT(0))
+
+#define THB_PCIE_PHY_CMN_CNF1		0x64
+#define THB_PCIE_PHY_CMN_CNF1_UPCS_CFG	(BIT(16) | BIT(1) | BIT(0))
+
+#define THB_PCIE_PHY1_STS1		0x107c
+#define THB_PCIE_PHY1_STS1_SRAM_INIT	BIT(5)
+
+#define THB_PCIE_CPR_RST_EN		0x0
+#define THB_PCIE_CPR_RST_EN_CTRL1	BIT(4)
+
+#define THB_PCIE_EP_BUS_POS		BIT(24)
+#define THB_PCIE_LINK_UP_TO		10000
+
 #define THB_PCIE_FN_OFFSET		BIT(16)
 
 enum hw_plt_type {
@@ -71,12 +100,19 @@ enum hw_plt_type {
 struct keembay_pcie {
 	struct dw_pcie		pci;
 	void __iomem		*apb_base;
+	void __iomem		*cpr_base;
 	enum dw_pcie_device_mode mode;
 	enum hw_plt_type	plt_type;
 
 	struct clk		*clk_master;
 	struct clk		*clk_aux;
 	struct gpio_desc	*reset;
+	struct dma_pool		*rc_dma_pool;
+
+	dma_addr_t		rc_dma_mem_pa;
+	void			*rc_dma_mem_va;
+	struct dma_chan		*rd_dma_chan;
+	bool			rc_dma;
 };
 
 struct keembay_pcie_of_data {
@@ -84,6 +120,200 @@ struct keembay_pcie_of_data {
 	enum hw_plt_type	plt_type;
 };
 
+static void rc_dma_complete_cb(void *completion)
+{
+	complete(completion);
+}
+
+static int thunderbay_pcie_rc_dma_rd(struct pcie_port *pp,
+				     struct dma_chan *chan,
+				     dma_addr_t dst,
+				     dma_addr_t src,
+				     int size)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct dma_device *dma_dev = chan->device;
+	struct dma_async_tx_descriptor *tx = NULL;
+	enum dma_status status = DMA_COMPLETE;
+	struct device *dev = pci->dev;
+	unsigned long start_jiffies;
+	enum dma_ctrl_flags flags;
+	struct completion cmp;
+	dma_cookie_t cookie;
+
+	flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+	tx = dma_dev->device_prep_dma_memcpy(chan,
+					     dst,
+					     src,
+					     size,
+					     flags);
+	if (!tx) {
+		dev_err(dev, "failed to get dma async tx desc\n");
+		return -1;
+	}
+
+	init_completion(&cmp);
+	tx->callback = rc_dma_complete_cb;
+	tx->callback_param = &cmp;
+
+	cookie = tx->tx_submit(tx);
+	if (dma_submit_error(cookie)) {
+		dev_err(dev, "failed to submit dma desc\n");
+		return -1;
+	}
+
+	dma_async_issue_pending(chan);
+	start_jiffies = jiffies;
+	while (!completion_done(&cmp) &&
+	       !time_after(jiffies, start_jiffies + HZ))
+		;
+
+	status = dma_async_is_tx_complete(chan, cookie, NULL, NULL);
+	if (status != DMA_COMPLETE) {
+		dev_err(dev, "failed to complete dma transaction\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+static void
+__iomem *thunderbay_pcie_conf_addr_map_bus(struct pci_bus *bus,
+					   unsigned int devfn, int where)
+{
+	struct pcie_port *pp = bus->sysdata;
+	struct dw_pcie *pci;
+	u32 busdev;
+	int type;
+
+	pci = to_dw_pcie_from_pp(pp);
+
+	/*
+	 * Checking whether the link is up here is a last line of defense
+	 * against platforms that forward errors on the system bus as
+	 * SError upon PCI configuration transactions issued when the link
+	 * is down. This check is racy by definition and does not stop
+	 * the system from triggering an SError if the link goes down
+	 * after this check is performed.
+	 */
+	if (!dw_pcie_link_up(pci))
+		return NULL;
+
+	busdev = PCIE_ATU_BUS(bus->number) | PCIE_ATU_DEV(PCI_SLOT(devfn)) |
+		 PCIE_ATU_FUNC(PCI_FUNC(devfn));
+
+	if (pci_is_root_bus(bus->parent))
+		type = PCIE_ATU_TYPE_CFG0;
+	else
+		type = PCIE_ATU_TYPE_CFG1;
+
+	/* EP stays on the same bus as RC */
+	busdev -= THB_PCIE_EP_BUS_POS;
+
+	dw_pcie_prog_outbound_atu(pci, 0x1,
+				  type, pp->cfg0_base,
+				  busdev, pp->cfg0_size);
+
+	return pp->va_cfg0_base + where;
+}
+
+static int
+thunderbay_pcie_host_rd_other_conf(struct pci_bus *bus, unsigned int devfn,
+				   int where, int size, u32 *val)
+{
+	struct pcie_port *pp = bus->sysdata;
+	struct keembay_pcie *pcie;
+	int ret, where_align;
+	struct dw_pcie *pci;
+
+	pci = to_dw_pcie_from_pp(pp);
+	pcie = dev_get_drvdata(pci->dev);
+
+	if (pcie->rc_dma) {
+		where_align = where & ~((typeof(where))(4) - 1);
+		if (thunderbay_pcie_rc_dma_rd(pp,
+					      pcie->rd_dma_chan,
+					      pcie->rc_dma_mem_pa + where_align,
+					      pp->cfg0_base + where_align,
+					      4))
+			ret = PCIBIOS_BAD_REGISTER_NUMBER;
+
+		*val = *((int *)(pcie->rc_dma_mem_va + where));
+
+		if (size == 4)
+			*val &= 0xFFFFFFFF;
+		else if (size == 2)
+			*val &= 0xFFFF;
+		else if (size == 1)
+			*val &= 0xFF;
+		else
+			ret = PCIBIOS_BAD_REGISTER_NUMBER;
+	} else {
+		ret = dw_pcie_read(pp->va_cfg0_base + where, size, val);
+	}
+
+	return ret;
+}
+
+static struct pci_ops thunderbay_child_pci_ops = {
+	.map_bus = thunderbay_pcie_conf_addr_map_bus,
+	.read = thunderbay_pcie_host_rd_other_conf,
+	.write = pci_generic_config_write,
+};
+
+static int thunderbay_pcie_rc_dma_cfg(struct keembay_pcie *pcie,
+				      struct platform_device *pdev)
+{
+	struct dw_pcie *pci = &pcie->pci;
+	struct device *dev = &pdev->dev;
+	struct dma_chan *rd_chan;
+	dma_addr_t phys;
+	void *segment;
+	int ret;
+
+	ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
+	if (ret)
+		dev_warn(dev, "failed to set dma mask\n");
+
+	rd_chan = dma_request_chan(&pdev->dev, "rx");
+	if (IS_ERR(rd_chan)) {
+		ret = PTR_ERR(rd_chan);
+		if (ret != -EPROBE_DEFER)
+			dev_err(dev, "No RC AXI DMA Rd channel available\n");
+		return ret;
+	}
+	pcie->rd_dma_chan = rd_chan;
+
+	pcie->rc_dma_pool = dma_pool_create("thb_pcie_rc_dma_pool",
+					    pci->dev,
+					    PAGE_SIZE,
+					    PAGE_SIZE,
+					    0);
+	if (!pcie->rc_dma_pool) {
+		dev_err(dev, "unable to allocate RC AXI DMA pool\n");
+		ret = PTR_ERR(pcie->rc_dma_pool);
+		goto free_dma_channel;
+	}
+
+	segment = dma_pool_zalloc(pcie->rc_dma_pool, GFP_ATOMIC, &phys);
+	if (!segment) {
+		dev_err(dev, "unable to get mem from RC AXI DMA pool\n");
+		ret = PTR_ERR(segment);
+		goto free_dma_pool;
+	}
+	pcie->rc_dma_mem_pa = phys;
+	pcie->rc_dma_mem_va = segment;
+
+	return 0;
+
+free_dma_channel:
+	dma_release_channel(pcie->rd_dma_chan);
+free_dma_pool:
+	dma_pool_destroy(pcie->rc_dma_pool);
+
+	return ret;
+}
+
 static int thunderbay_pcie_link_up(struct dw_pcie *pci)
 {
 	struct keembay_pcie *pcie = dev_get_drvdata(pci->dev);
@@ -92,6 +322,8 @@ static int thunderbay_pcie_link_up(struct dw_pcie *pci)
 
 	if (pcie->mode == DW_PCIE_EP_TYPE)
 		val = readl(pcie->apb_base + THB_PCIE_CTRL0_STS);
+	if (pcie->mode == DW_PCIE_RC_TYPE)
+		val = readl(pcie->apb_base + THB_PCIE_CTRL1_STS);
 
 	if ((val & mask) == mask)
 		return 1;
@@ -99,6 +331,122 @@ static int thunderbay_pcie_link_up(struct dw_pcie *pci)
 	return 0;
 }
 
+static int thunderbay_pcie_rc_phy_init(struct dw_pcie *pci)
+{
+	struct keembay_pcie *pcie = dev_get_drvdata(pci->dev);
+	u32 timeout = THB_PCIE_LINK_UP_TO, val;
+
+	/* PCIe_RST_EN, POR, SubSysRst */
+	val = readl(pcie->cpr_base + THB_PCIE_CPR_RST_EN);
+	val |= THB_PCIE_CPR_RST_EN_CTRL1;
+	writel(val, pcie->cpr_base + THB_PCIE_CPR_RST_EN);
+
+	/* PHY1_CNF0, ref_use_pad */
+	writel(0x0, pcie->apb_base + THB_PCIE_PHY1_CNF0);
+
+	/* phy_cmn_conf_1 upcs_pwrstable */
+	val = THB_PCIE_PHY_CMN_CNF1_UPCS_CFG;
+	writel(val, pcie->apb_base + THB_PCIE_PHY_CMN_CNF1);
+
+	/* Phy1 Cnf2, pcs/pma pwr_stable */
+	val = THB_PCIE_PHY1_CNF2_PWR_STABLE;
+	writel(val, pcie->apb_base + THB_PCIE_PHY1_CNF2);
+
+	/* SRAM init done? */
+	val = readl(pcie->apb_base + THB_PCIE_PHY1_STS1);
+	val &= THB_PCIE_PHY1_STS1_SRAM_INIT;
+	while (!val && timeout-- > 0) {
+		val = readl(pcie->apb_base + THB_PCIE_PHY1_STS1);
+		val &= THB_PCIE_PHY1_STS1_SRAM_INIT;
+	}
+	if (!timeout)
+		return -EBUSY;
+
+	/* pciess_ctrl1_cmn_cnf_1 xfer_pending disable */
+	val = THB_PCIE_CTRL1_CNF1_XFER_PEND;
+	writel(val, pcie->apb_base + THB_PCIE_CTRL1_CNF1);
+
+	/* PHY1_CNF0, set ext_ld_done */
+	val = THB_PCIE_PHY1_CNF0_EXT_LD;
+	writel(val, pcie->apb_base + THB_PCIE_PHY1_CNF0);
+
+	/* PERST Pad configuration */
+	val = THB_PCIE_CTRL1_SPARE1_PERST_PAD;
+	writel(val, pcie->apb_base + THB_PCIE_CTRL1_SPARE1);
+
+	/* LTSSM enable */
+	val = readl(pcie->apb_base + THB_PCIE_CTRL1_CNF0);
+	val |= THB_PCIE_CTRL1_CNF0_LTSSM_EN;
+	writel(val, pcie->apb_base + THB_PCIE_CTRL1_CNF0);
+
+	/* Wait for link up */
+	timeout = THB_PCIE_LINK_UP_TO;
+	while (!thunderbay_pcie_link_up(pci) && timeout-- > 0)
+		;
+	if (!timeout)
+		return -EBUSY;
+
+	return 0;
+}
+
+static int __init thunderbay_pcie_host_init(struct pcie_port *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+
+	pp->bridge->child_ops = &thunderbay_child_pci_ops;
+
+	dw_pcie_setup_rc(pp);
+
+	dw_pcie_prog_outbound_atu(pci, 0x1,
+				  PCIE_ATU_TYPE_MEM, pp->io_base,
+				  pp->io_bus_addr, pp->io_size);
+
+	return 0;
+}
+
+static const struct dw_pcie_host_ops thunderbay_pcie_host_ops = {
+	.host_init	= thunderbay_pcie_host_init,
+};
+
+static int thunderbay_pcie_add_pcie_port(struct keembay_pcie *pcie,
+					 struct platform_device *pdev)
+{
+	struct dw_pcie *pci = &pcie->pci;
+	struct pcie_port *pp = &pci->pp;
+	struct device *dev = &pdev->dev;
+	int ret;
+
+	pp->ops = &thunderbay_pcie_host_ops;
+
+	pcie->cpr_base = devm_platform_ioremap_resource_byname(pdev, "cpr");
+	if (IS_ERR(pcie->cpr_base))
+		return PTR_ERR(pcie->cpr_base);
+
+	if (IS_ENABLED(CONFIG_PCI_MSI)) {
+		pp->msi_irq = platform_get_irq(pdev, 0);
+		if (pp->msi_irq < 0) {
+			dev_err(dev, "failed to get msi IRQ: %d\n",
+				pp->msi_irq);
+			return pp->msi_irq;
+		}
+	}
+
+	/* Initialize Root Port PHY */
+	ret = thunderbay_pcie_rc_phy_init(pci);
+	if (ret) {
+		dev_err(dev, "Failed to initialize RC PHY: %d\n", ret);
+		return ret;
+	}
+
+	ret = dw_pcie_host_init(pp);
+	if (ret) {
+		dev_err(dev, "Failed to initialize host: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
 static void thunderbay_pcie_ltssm_enable(struct keembay_pcie *pcie, bool enable)
 {
 	u32 val;
@@ -400,9 +748,10 @@ static int keembay_pcie_setup_msi_irq(struct keembay_pcie *pcie)
 {
 	struct dw_pcie *pci = &pcie->pci;
 	struct device *dev = pci->dev;
-	struct platform_device *pdev = to_platform_device(dev);
+	struct platform_device *pdev;
 	int irq;
 
+	pdev = to_platform_device(dev);
 	irq = platform_get_irq_byname(pdev, "pcie");
 	if (irq < 0)
 		return irq;
@@ -416,8 +765,9 @@ static int keembay_pcie_setup_msi_irq(struct keembay_pcie *pcie)
 static void keembay_pcie_ep_init(struct dw_pcie_ep *ep)
 {
 	struct dw_pcie *pci = to_dw_pcie_from_ep(ep);
-	struct keembay_pcie *pcie = dev_get_drvdata(pci->dev);
+	struct keembay_pcie *pcie;
 
+	pcie = dev_get_drvdata(pci->dev);
 	writel(EDMA_INT_EN, pcie->apb_base + PCIE_REGS_INTERRUPT_ENABLE);
 }
 
@@ -505,6 +855,7 @@ static int keembay_pcie_probe(struct platform_device *pdev)
 	enum dw_pcie_device_mode mode;
 	struct keembay_pcie *pcie;
 	struct dw_pcie *pci;
+	int ret;
 
 	data = device_get_match_data(dev);
 	if (!data)
@@ -529,6 +880,25 @@ static int keembay_pcie_probe(struct platform_device *pdev)
 
 		/* Create 32 inbound and 64 outbound windows */
 		pci->atu_size = SZ_32K;
+
+		/*
+		 * Due to a silicon bug in THB PCIe RC mode, all CfgRd access
+		 * should go through DMA. This is a SW Workaround.
+		 * So, configure the DMA for THB PCIe RC here.
+		 */
+		if (pcie->mode == DW_PCIE_RC_TYPE) {
+			ret = thunderbay_pcie_rc_dma_cfg(pcie, pdev);
+			if (ret) {
+				if (ret == -EPROBE_DEFER)
+					dev_err(dev,
+						"deferring probe as no RC AXI DMA\n");
+				else
+					dev_err(dev,
+						"failed to configure RC AXI DMA\n");
+				return ret;
+			}
+			pcie->rc_dma = true;
+		}
 	}
 
 	pcie->apb_base = devm_platform_ioremap_resource_byname(pdev, "apb");
@@ -542,7 +912,13 @@ static int keembay_pcie_probe(struct platform_device *pdev)
 		if (!IS_ENABLED(CONFIG_PCIE_KEEMBAY_HOST))
 			return -ENODEV;
 
-		return keembay_pcie_add_pcie_port(pcie, pdev);
+		if (pcie->plt_type == PLF_HW_KEEMBAY)
+			ret = keembay_pcie_add_pcie_port(pcie, pdev);
+		else
+			ret = thunderbay_pcie_add_pcie_port(pcie, pdev);
+
+		return ret;
+
 	case DW_PCIE_EP_TYPE:
 		if (!IS_ENABLED(CONFIG_PCIE_KEEMBAY_EP))
 			return -ENODEV;
@@ -574,6 +950,11 @@ static const struct keembay_pcie_of_data thunderbay_pcie_ep_of_data = {
 	.plt_type = PLF_HW_THUNDERBAY,
 };
 
+static const struct keembay_pcie_of_data thunderbay_pcie_rc_of_data = {
+	.mode = DW_PCIE_RC_TYPE,
+	.plt_type = PLF_HW_THUNDERBAY,
+};
+
 static const struct of_device_id keembay_pcie_of_match[] = {
 	{
 		.compatible = "intel,keembay-pcie",
@@ -587,6 +968,10 @@ static const struct of_device_id keembay_pcie_of_match[] = {
 		.compatible = "intel,thunderbay-pcie-ep",
 		.data = &thunderbay_pcie_ep_of_data,
 	},
+	{
+		.compatible = "intel,thunderbay-pcie",
+		.data = &thunderbay_pcie_rc_of_data,
+	},
 	{}
 };
 
-- 
2.25.1

