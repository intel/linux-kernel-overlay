From d34428c9acff78062ad8f2a9e781e441713c3a61 Mon Sep 17 00:00:00 2001
From: "Zawawi, Muhammad Zul Husni" <muhammad.zul.husni.zawawi@intel.com>
Date: Fri, 17 Mar 2023 14:37:24 +0800
Subject: [PATCH 2220/2236] drm/i915/gt: SR-IOV feature enablement

This SR-IOV feature based on DII-Client

Signed-off-by: Zawawi, Muhammad Zul Husni <muhammad.zul.husni.zawawi@intel.com>
---
 Documentation/gpu/i915.rst                    |   16 +
 Documentation/gpu/rfc/sysfs-i915-iov-obsolete |  106 +
 Documentation/gpu/rfc/sysfs-i915-iov-testing  |  383 +++
 drivers/gpu/drm/i915/Kconfig.debug            |   12 +
 drivers/gpu/drm/i915/Makefile                 |   16 +
 drivers/gpu/drm/i915/README.sriov             |    6 +
 .../drm/i915/display/intel_display_power.c    |   16 +-
 .../i915/display/intel_display_power_map.c    |    4 +-
 drivers/gpu/drm/i915/gt/intel_engine_cs.c     |   41 +
 drivers/gpu/drm/i915/gt/intel_engine_regs.h   |    3 +
 drivers/gpu/drm/i915/gt/intel_engine_types.h  |    2 +
 drivers/gpu/drm/i915/gt/intel_ggtt.c          |  157 +-
 drivers/gpu/drm/i915/gt/intel_gsc.c           |    4 +-
 drivers/gpu/drm/i915/gt/intel_gt.c            |   52 +-
 .../gpu/drm/i915/gt/intel_gt_clock_utils.c    |    9 +
 .../gpu/drm/i915/gt/intel_gt_clock_utils.h    |    1 +
 drivers/gpu/drm/i915/gt/intel_gt_debugfs.c    |    2 +
 drivers/gpu/drm/i915/gt/intel_gt_pm_debugfs.c |    3 +
 drivers/gpu/drm/i915/gt/intel_gt_sysfs_pm.c   |    3 +
 drivers/gpu/drm/i915/gt/intel_gt_types.h      |    2 +
 drivers/gpu/drm/i915/gt/intel_gtt.c           |    6 +
 drivers/gpu/drm/i915/gt/intel_gtt.h           |   27 +
 drivers/gpu/drm/i915/gt/intel_llc.c           |    3 +
 drivers/gpu/drm/i915/gt/intel_lrc.c           |   27 +
 drivers/gpu/drm/i915/gt/intel_lrc_reg.h       |    8 +
 drivers/gpu/drm/i915/gt/intel_mocs.c          |    3 +
 drivers/gpu/drm/i915/gt/intel_rc6.c           |    3 +
 drivers/gpu/drm/i915/gt/intel_reset.c         |   48 +-
 drivers/gpu/drm/i915/gt/intel_rps.c           |    6 +
 drivers/gpu/drm/i915/gt/intel_sa_media.c      |    2 +
 drivers/gpu/drm/i915/gt/intel_sseu_debugfs.c  |    3 +
 drivers/gpu/drm/i915/gt/intel_workarounds.c   |   15 +
 .../gpu/drm/i915/gt/iov/abi/iov_actions_abi.h |  121 +
 .../i915/gt/iov/abi/iov_actions_debug_abi.h   |   24 +
 .../i915/gt/iov/abi/iov_actions_mmio_abi.h    |  130 +
 .../gt/iov/abi/iov_actions_selftest_abi.h     |   55 +
 .../i915/gt/iov/abi/iov_communication_abi.h   |  116 +
 .../gt/iov/abi/iov_communication_mmio_abi.h   |  150 +
 .../gpu/drm/i915/gt/iov/abi/iov_errors_abi.h  |   24 +
 .../drm/i915/gt/iov/abi/iov_messages_abi.h    |   30 +
 .../gpu/drm/i915/gt/iov/abi/iov_version_abi.h |   15 +
 drivers/gpu/drm/i915/gt/iov/intel_iov.c       |  301 ++
 drivers/gpu/drm/i915/gt/iov/intel_iov.h       |   24 +
 .../gpu/drm/i915/gt/iov/intel_iov_debugfs.c   |  116 +
 .../gpu/drm/i915/gt/iov/intel_iov_debugfs.h   |   14 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_event.c |  174 ++
 drivers/gpu/drm/i915/gt/iov/intel_iov_event.h |   18 +
 .../gpu/drm/i915/gt/iov/intel_iov_memirq.c    |  310 ++
 .../gpu/drm/i915/gt/iov/intel_iov_memirq.h    |   20 +
 .../drm/i915/gt/iov/intel_iov_provisioning.c  | 2698 +++++++++++++++++
 .../drm/i915/gt/iov/intel_iov_provisioning.h  |   76 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_query.c |  752 +++++
 drivers/gpu/drm/i915/gt/iov/intel_iov_query.h |   22 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h   |   16 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c |  725 +++++
 drivers/gpu/drm/i915/gt/iov/intel_iov_relay.h |   36 +
 .../gpu/drm/i915/gt/iov/intel_iov_service.c   |  563 ++++
 .../gpu/drm/i915/gt/iov/intel_iov_service.h   |   24 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.c |  458 +++
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.h |   27 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c |  697 +++++
 drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.h |   14 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_types.h |  256 ++
 drivers/gpu/drm/i915/gt/iov/intel_iov_utils.h |  146 +
 .../selftest_live_iov_provisioning.c          |  999 ++++++
 .../iov/selftests/selftest_live_iov_relay.c   |  547 ++++
 .../iov/selftests/selftest_live_iov_service.c |  170 ++
 .../iov/selftests/selftest_mock_iov_relay.c   |  725 +++++
 .../iov/selftests/selftest_mock_iov_service.c |  232 ++
 .../iov/selftests/selftest_perf_iov_relay.c   |  248 ++
 .../iov/selftests/selftest_util_iov_relay.c   |  104 +
 .../drm/i915/gt/uc/abi/guc_actions_pf_abi.h   |  449 +++
 .../drm/i915/gt/uc/abi/guc_actions_vf_abi.h   |  315 ++
 drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h |  207 ++
 .../gpu/drm/i915/gt/uc/abi/guc_messages_abi.h |   30 +
 .../gpu/drm/i915/gt/uc/abi/guc_version_abi.h  |   12 +
 drivers/gpu/drm/i915/gt/uc/intel_gsc_uc.h     |    9 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc.c        |   89 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc.h        |   37 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c    |    3 +
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c     |   40 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h     |    3 +-
 .../gpu/drm/i915/gt/uc/intel_guc_debugfs.c    |    6 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h   |   22 +
 drivers/gpu/drm/i915/gt/uc/intel_guc_rc.h     |    6 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_slpc.h   |    6 +-
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c |  120 +-
 .../gpu/drm/i915/gt/uc/intel_guc_submission.h |   12 +-
 drivers/gpu/drm/i915/gt/uc/intel_huc.h        |    9 +-
 drivers/gpu/drm/i915/gt/uc/intel_uc.c         |  134 +-
 drivers/gpu/drm/i915/gt/uc/intel_uc.h         |    2 +-
 drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c      |   26 +
 drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h      |   40 +-
 drivers/gpu/drm/i915/i915_debugfs.c           |   50 +-
 drivers/gpu/drm/i915/i915_debugfs_params.c    |   11 +
 drivers/gpu/drm/i915/i915_driver.c            |   49 +-
 drivers/gpu/drm/i915/i915_drv.h               |   18 +
 drivers/gpu/drm/i915/i915_gpu_error.c         |   22 +-
 drivers/gpu/drm/i915/i915_irq.c               |   59 +-
 drivers/gpu/drm/i915/i915_params.c            |    4 +
 drivers/gpu/drm/i915/i915_params.h            |    1 +
 drivers/gpu/drm/i915/i915_pci.c               |   58 +-
 drivers/gpu/drm/i915/i915_perf.c              |    4 +
 drivers/gpu/drm/i915/i915_pmu.c               |    2 +-
 drivers/gpu/drm/i915/i915_reg.h               |    8 +
 drivers/gpu/drm/i915/i915_request.c           |    2 +-
 drivers/gpu/drm/i915/i915_sriov.c             |  732 +++++
 drivers/gpu/drm/i915/i915_sriov.h             |   47 +
 drivers/gpu/drm/i915/i915_sriov_sysfs.c       |  616 ++++
 drivers/gpu/drm/i915/i915_sriov_sysfs.h       |   18 +
 drivers/gpu/drm/i915/i915_sriov_sysfs_types.h |   59 +
 drivers/gpu/drm/i915/i915_sriov_types.h       |   41 +
 drivers/gpu/drm/i915/i915_sysfs.c             |    5 +
 drivers/gpu/drm/i915/i915_utils.h             |   13 +
 drivers/gpu/drm/i915/i915_vgpu.c              |    5 +-
 drivers/gpu/drm/i915/i915_virtualization.h    |   32 +
 .../gpu/drm/i915/i915_virtualization_types.h  |   19 +
 drivers/gpu/drm/i915/intel_device_info.c      |   15 +-
 drivers/gpu/drm/i915/intel_device_info.h      |    2 +
 drivers/gpu/drm/i915/intel_pci_config.h       |    5 +
 drivers/gpu/drm/i915/intel_pm.c               |   10 +-
 drivers/gpu/drm/i915/intel_uncore.c           |   98 +-
 .../drm/i915/selftests/i915_live_selftests.h  |    3 +
 .../drm/i915/selftests/i915_mock_selftests.h  |    2 +
 .../drm/i915/selftests/i915_perf_selftests.h  |    1 +
 .../gpu/drm/i915/selftests/mock_gem_device.c  |    1 +
 126 files changed, 15642 insertions(+), 113 deletions(-)
 create mode 100644 Documentation/gpu/rfc/sysfs-i915-iov-obsolete
 create mode 100644 Documentation/gpu/rfc/sysfs-i915-iov-testing
 create mode 100644 drivers/gpu/drm/i915/README.sriov
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_actions_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_actions_debug_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_actions_mmio_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_actions_selftest_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_communication_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_communication_mmio_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_errors_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_messages_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/abi/iov_version_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_event.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_event.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_query.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_query.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_relay.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_service.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_service.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_utils.h
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_provisioning.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_relay.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_service.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_relay.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_service.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_perf_iov_relay.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/selftests/selftest_util_iov_relay.c
 create mode 100644 drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
 create mode 100644 drivers/gpu/drm/i915/gt/uc/abi/guc_version_abi.h
 create mode 100644 drivers/gpu/drm/i915/i915_sriov.c
 create mode 100644 drivers/gpu/drm/i915/i915_sriov.h
 create mode 100644 drivers/gpu/drm/i915/i915_sriov_sysfs.c
 create mode 100644 drivers/gpu/drm/i915/i915_sriov_sysfs.h
 create mode 100644 drivers/gpu/drm/i915/i915_sriov_sysfs_types.h
 create mode 100644 drivers/gpu/drm/i915/i915_sriov_types.h
 create mode 100644 drivers/gpu/drm/i915/i915_virtualization.h
 create mode 100644 drivers/gpu/drm/i915/i915_virtualization_types.h

diff --git a/Documentation/gpu/i915.rst b/Documentation/gpu/i915.rst
index 60ea21734902..516485ea0ed8 100644
--- a/Documentation/gpu/i915.rst
+++ b/Documentation/gpu/i915.rst
@@ -541,6 +541,22 @@ GuC ABI
 .. kernel-doc:: drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
 .. kernel-doc:: drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
 
+GuC Virtualization ABI
+~~~~~~~~~~~~~~~~~~~~~~
+
+.. kernel-doc:: drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
+.. kernel-doc:: drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
+
+GuC VF/PF Virtualization ABI
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. kernel-doc:: drivers/gpu/drm/i915/gt/iov/abi/iov_communication_abi.h
+.. kernel-doc:: drivers/gpu/drm/i915/gt/iov/abi/iov_communication_mmio_abi.h
+.. kernel-doc:: drivers/gpu/drm/i915/gt/iov/abi/iov_messages_abi.h
+.. kernel-doc:: drivers/gpu/drm/i915/gt/iov/abi/iov_actions_abi.h
+.. kernel-doc:: drivers/gpu/drm/i915/gt/iov/abi/iov_actions_mmio_abi.h
+.. kernel-doc:: drivers/gpu/drm/i915/gt/iov/abi/iov_errors_abi.h
+
 HuC
 ---
 .. kernel-doc:: drivers/gpu/drm/i915/gt/uc/intel_huc.c
diff --git a/Documentation/gpu/rfc/sysfs-i915-iov-obsolete b/Documentation/gpu/rfc/sysfs-i915-iov-obsolete
new file mode 100644
index 000000000000..59d8db5a402c
--- /dev/null
+++ b/Documentation/gpu/rfc/sysfs-i915-iov-obsolete
@@ -0,0 +1,106 @@
+obsolete sysfs interface for i915 virtualization
+------------------------------------------------
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/contexts_free
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/contexts_max_quota
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/doorbells_free
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/doorbells_max_quota
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/ggtt_free
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/ggtt_max_quota
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/lmem_free
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/available/lmem_max_quota
+Date:		September 2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		These attributes are *deprecated* !
+
+		To provision VFs please either use explicit values from vGPU
+		profile definitions or rely on the auto-provisioning.
+
+		Detailed related information about available resources could
+		be still viewed from un-official debugfs attributes.
+
+		---
+
+		The sysfs "gt%/available" directory holds attributes specific
+		to the device GT tile that shows maximum values of the resources
+		that can be used for VF provisioning.
+
+		Those attributes may return values that depend on actual system
+		usage so for stable results explicit idling of the driver might
+		be needed.
+
+		contexts_free: (RO) 0..U16_MAX
+
+		contexts_max_quota: (RO) integer 0..U16_MAX
+			Maximum number contexts that are available for VF
+			provisioning.
+			This value shall meet all HW/FW requirements.
+			If this value is zero then no more contexts are available
+			for provisioning.
+
+		doorbells_free: (RO) 0..U16_MAX
+
+		doorbells_max_quota: (RO) integer 0..U16_MAX
+			Maximum number doorbells that are available for VF
+			provisioning.
+			This value shall meet all HW/FW requirements.
+			If this value is zero then no more doorbells are
+			available for provisioning.
+
+		ggtt_free: (RO) 0..U64_MAX
+			Summary size of the all free GGTT regions.
+
+		ggtt_max_quota: (RO) integer 0..U64_MAX
+			Size of the largest free GGTT region (in bytes) that
+			can be used for VF provisioning.
+			This value shall meet all HW/FW requirements.
+			If this value is zero then no more GGTT is available
+			for provisioning.
+
+		lmem_free: (RO) 0..U64_MAX
+			Size of the free LMEM space (in bytes) that could be
+			available for the VF provisioning.
+
+		lmem_max_quota: (RO) 0..U64_MAX
+			Size of the maximum LMEM space (in bytes) that can be
+			used for VF provisioning.
+			This value shall meet all HW/FW requirements.
+			If this value is zero then no more LMEM is available
+			for provisioning.
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/contexts_quota
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/doorbells_quota
+Date:		September 2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		These attributes are *deprecated* !
+
+		Instead of specifying explicit context and doorbells quotas
+		for the PF please use 'spare' attributes that allow to
+		specify minimum sizes of the resources that PF should keep
+		unavailable for the VFs provisioning.
+
+		---
+
+		Writes to these attributes may fail with:
+			-E2BIG if value larger that HW/FW limit.
+			-EDQUOT if value is larger than maximum quota.
+			-ENOSPC if can't allocate required quota.
+			-EIO if GuC refuses to change VF provisioning.
+
+		contexts_quota: (RW) 0..U16_MAX
+			Number of GuC submission contexts assigned to the PF.
+			This value is aligned per HW/FW/SW requirements.
+			Default value is pre-calculated based on number of
+			VFs that could be enabled on the system.
+
+		doorbells_quota: (RW) 0..U16_MAX
+			Number of GuC doorbells assigned to the PF.
+			This value is aligned per HW/FW/SW requirements.
+			Default is 0.
diff --git a/Documentation/gpu/rfc/sysfs-i915-iov-testing b/Documentation/gpu/rfc/sysfs-i915-iov-testing
new file mode 100644
index 000000000000..64ad6c3dd2dc
--- /dev/null
+++ b/Documentation/gpu/rfc/sysfs-i915-iov-testing
@@ -0,0 +1,383 @@
+preliminary sysfs interface for i915 virtualization
+---------------------------------------------------
+
+
+What:		/sys/class/drm/card%/prelim_iov/
+What:		/sys/class/drm/card%/prelim_iov/mode
+Date:		September 2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		The sysfs "iov" directory is used as root of all driver specific
+		attributes used to manage and examine device support for the I/O
+		virtualization (IOV).
+
+		Note that "prelim_" prefix used here indicates that this version
+		of the sysfs ABI is still preliminary and subject to change.
+
+		mode: (RO) string
+			Shows IOV mode (if any) of the device or the driver.
+
+			Intel GVT-g is a full GPU virtualization software
+			solution, supported starting from 5th generation of Intel
+			Core(TM) processors with Intel Graphics processors.
+
+			The Single Root I/O Virtualization (SR-IOV) extension to
+			the PCI Express (PCIe) specification suite, is supported
+			starting from 12th generation of Intel Graphics processors.
+
+			"GVT" - DRM device is running as GVT-g guest.
+
+			"SR-IOV PF" - DRM device is running on PCI Function that
+				      supports the SR-IOV capabilities, known as
+				      Physical Function (PF)
+
+			"SR-IOV VF" - DRM device is running on PCI Function that
+				      is associated with a PF, known as Virtual
+				      Function (VF)
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/
+What:		/sys/class/drm/card%/prelim_iov/vf%/
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+
+		pf/
+			The sysfs "pf" directory holds attributes specific to the
+			PF and will be available only when mode is "SR-IOV PF".
+
+		vf1/
+		vf%/
+			The sysfs "vf1".."vfN" directories hold attributes for
+			the VFs, where VFs are numbered according to the PCI
+			specification starting from 1 to "sriov_totalvfs", and
+			will be available only when mode is "SR-IOV PF".
+
+		pf/gt0/
+		pf/gt%/
+		vf%/gt0/
+		vf%/gt%/
+			The sysfs "gt%" directories hold attributes specific to
+			the device GT tile. Root tile is represented as "gt0" and
+			is always present. If remote tiles are also present and
+			enabled they are represented as "gt1", "gt2, ...
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/auto_provisioning
+What:		/sys/class/drm/card%/prelim_iov/pf/device
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+
+		auto_provisioning: (RW) bool
+			Controls whether the PF driver shall perform automatic
+			provisioning of the VFs when no manual provisioning was
+			performed but VFs are being enabled.
+
+			When this flag is true, VFs will be provisioned with
+			fair amount of hard resources (like GGTT or LMEM), but
+			any scheduling parameters (like execution time) will
+			not be modified.
+
+			This flag will be automatically turned off on any
+			attempt of manual provisioning of the VF using other
+			provisioning attributes.
+
+			This flag can be always turned off.
+			This flag can be manually turned on only if all VFs are
+			fully unprovisioned, otherwise it will return -ESTALE.
+
+		device: symbolic link
+			Backlink to the PCI device entry representing this PF.
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/exec_quantum_ms
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/preempt_timeout_us
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		exec_quantum_ms: (RW) integer 0..U32_MAX
+			GT execution quantum in [ms] assigned to the PF.
+			Requested quantum might be aligned per HW/FW requirements.
+
+			Default is 0 (unlimited).
+
+		preempt_timeout_us: (RW) integer 0..U32_MAX
+			GT preemption timeout in [us] for to the PF.
+			Requested timeout might be aligned per HW/FW requirements.
+
+			Default is 0 (unlimited).
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/ggtt_spare
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/lmem_spare
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/contexts_spare
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/doorbells_spare
+Date:		September 2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+
+		ggtt_spare: (RW) integer 0..U64_MAX
+			Total size of the GGTT address space (in bytes) that PF
+			shall preserve for its own use. PF shall refuse VF GGTT
+			provisioning if remaining GGTT after such provisioning
+			will be smaller than this value.
+
+			Default value is minimal GGTT size as required by the PF.
+
+			PF's minimal GGTT size depends on the platform or driver
+			implementation and can be reinforced by writing 0 to this
+			attribute.
+
+			Requested size may be aligned per HW/FW/SW requirements.
+			If requested size is smaller than minimum then -EINVAL
+			error will be returned.
+
+		lmem_spare: (RW) integer 0..U64_MAX
+			Total size of the LMEM blocks (in bytes) that PF shall
+			preserve for its own use. PF shall refuse to provision
+			VF with LMEM if remaining available LMEM after such
+			provisioning will be smaller than this value.
+
+			This attribute is only available on discrete platforms.
+
+			Default value is minimal LMEM size as required by the PF.
+
+			PF's minimal LMEM size depends on the platform or driver
+			implementation and can be reinforced by writing 0 to this
+			attribute.
+
+			Requested size may be aligned per HW/FW/SW requirements.
+			If requested size is smaller than minimum (but not zero)
+			then -EINVAL error will be returned.
+
+		contexts_spare: (RW) integer 0..U16_MAX
+			Number of the GuC submission contexts that PF shall
+			preserve for its own use. PF shall refuse VF contexts
+			provisioning if remaining available contexts after such
+			provisioning will be smaller than this value.
+
+			Default value is minimal number of contexts as required
+			by the PF.
+
+			PF's minimal contexts number depends on the platform
+			or the driver implementation and can be reinforced by
+			writing 0 to this attribute.
+
+			Requested number may be aligned per HW/FW/SW requirements.
+			If requested count is smaller than minimum (but not zero)
+			then -EINVAL error will be returned.
+
+		doorbells_spare: (RW) integer 0..U16_MAX
+			Number of the GuC doorbells that PF shall preserve for
+			its own use. PF shall refuse VF doorbell provisioning if
+			remaining available doorbells after such provisioning
+			will be smaller than this value.
+
+			Default value is minimal number of contexts as required
+			by the PF.
+
+			PF's minimal contexts number depends on the platform
+			or the driver implementation and can be reinforced by
+			writing 0 to this attribute.
+
+			Requested number may be aligned per HW/FW/SW requirements.
+			If requested count is smaller than minimum (but not zero)
+			then -EINVAL error will be returned.
+
+
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/policy/engine_reset
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/policy/sched_if_idle
+What:		/sys/class/drm/card%/prelim_iov/pf/gt%/policy/sample_period_ms
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		Set of the per-GT policy attributes that corresponds to the
+		configuration KLVs supported by the GuC.
+
+		sched_if_idle: (RW) bool
+			This attribute controls whether strict scheduling is
+			enabled whereby any VF that doesn't have work to submit
+			is still allocated a fixed execution time-slice to
+			ensure active VFs execution is always consitent even
+			during other VF reprovisiong or rebooting events.
+			Changing this param impacts all VFs and takes effect
+			on the next VF switch event.
+
+			Default is false (disabled).
+
+		engine_reset: (RW) bool
+			This attribute controls whether to reset utilized HW
+			engine after VF switch to clean up any stale HW
+			registers left behind by the previous VF.
+
+			Default is false (disabled).
+
+		sample_period_ms: (RW) integer 0..1000
+			This attribute controls a the sample period for tracking
+			adverse event counters. A sample period is the period in
+			millisecs during which events are counted and the total
+			is checked against threshold. The sample period is
+			continuous, i.e. not only during the time-slice of a VF,
+			but across globally. This is applicable for all the VFs.
+
+			Default is 0 (disabled).
+
+			0 = disabled
+			1..1000 = sample period in [ms]
+
+
+What:		/sys/class/drm/card%/prelim_iov/vf%/id
+What:		/sys/class/drm/card%/prelim_iov/vf%/device
+What:		/sys/class/drm/card%/prelim_iov/vf%/control
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+
+		id: (RO) integer 0..U32_MAX
+			VF identifier. VF identifiers start from 1.
+
+		device: symbolic link
+			Backlink to the PCI device entry representing this VF.
+
+			This link is present only if VF is enabled.
+
+		control: (WO) string
+
+			This attribute allows admin to pause, resume or stop
+			handling submission requests from given VF.
+
+			This operation will be performed on all GTs where
+			VF is provisioned.
+
+			Writes to this attribute may fail with:
+				-EIO if GuC refuses such control
+
+			"pause" = will temporarily pause VF scheduling on GuC.
+			To restore VF scheduling admin shall use "resume".
+
+			"resume" = will resume VF scheduling on GuC.
+
+			"stop" = will stop completely VF scheduling on GuC.
+			To restore VF scheduling admin must trigger VF FLR.
+
+
+
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/exec_quantum_ms
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/preempt_timeout_us
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		exec_quantum_ms: (RW) 0..U32_MAX
+			GT execution quantum (in millisecs) assigned to the VF.
+			Requested value might be aligned per HW/FW requirements.
+
+			Default is 0 (unlimited).
+
+		preempt_timeout_us: (RW) 0..U32_MAX
+			GT preemption timeout (in microsecs) allowed for the VF.
+			Requested value might be aligned per HW/FW requirements.
+
+			Default is 0 (unlimited).
+
+
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/ggtt_quota
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/lmem_quota
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/contexts_quota
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/doorbells_quota
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		Writes to these attributes may fail with:
+			-E2BIG if value larger that HW/FW limit.
+			-EDQUOT if value is larger than maximum quota.
+			-ENOSPC if can't allocate required quota.
+			-EIO if GuC refuses to change VF provisioning.
+
+		ggtt_quota: (RW) integer 0..U64_MAX
+			Size of the GGTT region (in bytes) assigned to the VF.
+			Value is aligned per HW/FW requirements.
+
+			Default is 0 (unprovisioned).
+
+		lmem_quota: (RW) integer 0..U64_MAX
+			Size of the local memory (in bytes) assigned to the VF.
+			Value is aligned per HW/FW requirements.
+
+			This attribute is only available on discrete platforms.
+
+			Default is 0 (unprovisioned).
+
+		contexts_quota: (RW) 0..U16_MAX
+			Number of GuC submission contexts assigned to the VF.
+			This value is aligned per HW/FW requirements.
+
+			Default is 0 (unprovisioned).
+
+		doorbells_quota: (RW) 0..U16_MAX
+			Number of GuC doorbells assigned to the VF.
+			This value is aligned per HW/FW requirements.
+
+			Default is 0 (unprovisioned).
+
+
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/state
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+
+		state: (RW) binary
+			This binary attribute allows to save or restore VF's
+			state that is maintained in the GuC.
+
+			Read of this attribute will fail if VF is not paused.
+			Write to this attribute will fail if VF is not idle
+			(provisioned but not running).
+
+			Size of this binary blob is 4096 bytes (4KiB).
+
+
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/threshold/cat_error_count
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/threshold/doorbell_time_us
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/threshold/engine_reset_count
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/threshold/h2g_time_us
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/threshold/irq_time_us
+What:		/sys/class/drm/card%/prelim_iov/vf%/gt%/threshold/page_fault_count
+Date:		2022
+KernelVersion:	DII
+Contact:	gfx-internal-devel@eclists.intel.com
+Description:
+		The sysfs "threshold" directory holds attributes specific
+		to the device GT tile and represents VF security thresholds.
+
+		Default value for all thresholds is 0 (disabled).
+
+		Security events are triggered by the GuC only if global policy
+		attribute "sample_period_ms" is non-zero and related thresholds
+		are also non zero.
+
+		Refer to GuC ABI for details about each threshold category.
+
+		cat_error_count: (RW) integer
+
+		doorbell_time_us: (RW) integer
+
+		engine_reset_count: (RW) integer
+
+		h2g_time_us: (RW) integer
+
+		irq_time_us: (RW) integer
+
+		page_fault_count: (RW) integer
diff --git a/drivers/gpu/drm/i915/Kconfig.debug b/drivers/gpu/drm/i915/Kconfig.debug
index 52e9083a5028..8632a2ed2087 100644
--- a/drivers/gpu/drm/i915/Kconfig.debug
+++ b/drivers/gpu/drm/i915/Kconfig.debug
@@ -169,6 +169,18 @@ config DRM_I915_DEBUG_GUC
 
 	  If in doubt, say "N".
 
+config DRM_I915_DEBUG_IOV
+	bool "Enable additional driver debugging for IOV"
+	depends on DRM_I915
+	default n
+	help
+	  Choose this option to turn on extra driver debugging that may affect
+	  performance but will help resolve IOV related issues.
+
+	  Recommended for driver developers only.
+
+	  If in doubt, say "N".
+
 config DRM_I915_SELFTEST
 	bool "Enable selftests upon driver load"
 	depends on DRM_I915
diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 39e75b8217f6..2b56ad755549 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -220,6 +220,22 @@ i915-y += \
 # graphics system controller (GSC) support
 i915-y += gt/intel_gsc.o
 
+# Virtualization support
+iov-y += \
+	i915_sriov.o \
+	i915_sriov_sysfs.o \
+	gt/iov/intel_iov.o \
+	gt/iov/intel_iov_debugfs.o \
+	gt/iov/intel_iov_event.o \
+	gt/iov/intel_iov_memirq.o \
+	gt/iov/intel_iov_provisioning.o \
+	gt/iov/intel_iov_query.o \
+	gt/iov/intel_iov_relay.o \
+	gt/iov/intel_iov_service.o \
+	gt/iov/intel_iov_state.o \
+	gt/iov/intel_iov_sysfs.o
+i915-y += $(iov-y)
+
 # graphics hardware monitoring (HWMON) support
 i915-$(CONFIG_HWMON) += i915_hwmon.o
 
diff --git a/drivers/gpu/drm/i915/README.sriov b/drivers/gpu/drm/i915/README.sriov
new file mode 100644
index 000000000000..a175aefa0170
--- /dev/null
+++ b/drivers/gpu/drm/i915/README.sriov
@@ -0,0 +1,6 @@
+SR-IOV GENERAL SUPPORT (done)
+D:	Michal Wajdeczko <michal.wajdeczko@intel.com>
+M:	Michal Wajdeczko <michal.wajdeczko@intel.com>
+T:	drm/i915/iov
+T:	drm/i915/pf
+T:	drm/i915/vf
diff --git a/drivers/gpu/drm/i915/display/intel_display_power.c b/drivers/gpu/drm/i915/display/intel_display_power.c
index 96fd45c57d71..4f0f52d204eb 100644
--- a/drivers/gpu/drm/i915/display/intel_display_power.c
+++ b/drivers/gpu/drm/i915/display/intel_display_power.c
@@ -1899,7 +1899,9 @@ void intel_power_domains_init_hw(struct drm_i915_private *i915, bool resume)
 
 	power_domains->initializing = true;
 
-	if (DISPLAY_VER(i915) >= 11) {
+	if (IS_SRIOV_VF(i915)) {
+		/* nop */
+	} else if (DISPLAY_VER(i915) >= 11) {
 		icl_display_core_init(i915, resume);
 	} else if (IS_GEMINILAKE(i915) || IS_BROXTON(i915)) {
 		bxt_display_core_init(i915, resume);
@@ -2217,6 +2219,9 @@ static void intel_power_domains_verify_state(struct drm_i915_private *i915)
 
 void intel_display_power_suspend_late(struct drm_i915_private *i915)
 {
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (DISPLAY_VER(i915) >= 11 || IS_GEMINILAKE(i915) ||
 	    IS_BROXTON(i915)) {
 		bxt_enable_dc9(i915);
@@ -2231,6 +2236,9 @@ void intel_display_power_suspend_late(struct drm_i915_private *i915)
 
 void intel_display_power_resume_early(struct drm_i915_private *i915)
 {
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (DISPLAY_VER(i915) >= 11 || IS_GEMINILAKE(i915) ||
 	    IS_BROXTON(i915)) {
 		gen9_sanitize_dc_state(i915);
@@ -2246,6 +2254,9 @@ void intel_display_power_resume_early(struct drm_i915_private *i915)
 
 void intel_display_power_suspend(struct drm_i915_private *i915)
 {
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (DISPLAY_VER(i915) >= 11) {
 		icl_display_core_uninit(i915);
 		bxt_enable_dc9(i915);
@@ -2259,6 +2270,9 @@ void intel_display_power_suspend(struct drm_i915_private *i915)
 
 void intel_display_power_resume(struct drm_i915_private *i915)
 {
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (DISPLAY_VER(i915) >= 11) {
 		bxt_disable_dc9(i915);
 		icl_display_core_init(i915, true);
diff --git a/drivers/gpu/drm/i915/display/intel_display_power_map.c b/drivers/gpu/drm/i915/display/intel_display_power_map.c
index 452b6cbdfd98..29729e8ef9ec 100644
--- a/drivers/gpu/drm/i915/display/intel_display_power_map.c
+++ b/drivers/gpu/drm/i915/display/intel_display_power_map.c
@@ -1641,7 +1641,9 @@ int intel_display_power_map_init(struct i915_power_domains *power_domains)
 		return 0;
 	}
 
-	if (DISPLAY_VER(i915) >= 14)
+	if (IS_SRIOV_VF(i915))
+		return set_power_wells(power_domains, i9xx_power_wells);
+	else if (DISPLAY_VER(i915) >= 14)
 		return set_power_wells(power_domains, xelpdp_power_wells);
 	else if (IS_DG2(i915))
 		return set_power_wells(power_domains, xehpd_power_wells);
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_cs.c b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
index d4e29da74612..cb76629dca4c 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_cs.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
@@ -52,6 +52,7 @@
 struct engine_info {
 	u8 class;
 	u8 instance;
+	u8 irq_offset;
 	/* mmio bases table *must* be sorted in reverse graphics_ver order */
 	struct engine_mmio_base {
 		u32 graphics_ver : 8;
@@ -63,6 +64,7 @@ static const struct engine_info intel_engines[] = {
 	[RCS0] = {
 		.class = RENDER_CLASS,
 		.instance = 0,
+		.irq_offset = GEN11_RCS0,
 		.mmio_bases = {
 			{ .graphics_ver = 1, .base = RENDER_RING_BASE }
 		},
@@ -70,6 +72,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS0] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 0,
+		.irq_offset = GEN11_BCS,
 		.mmio_bases = {
 			{ .graphics_ver = 6, .base = BLT_RING_BASE }
 		},
@@ -77,6 +80,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS1] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 1,
+		.irq_offset = XEHPC_BCS1,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS1_RING_BASE }
 		},
@@ -84,6 +88,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS2] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 2,
+		.irq_offset = XEHPC_BCS2,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS2_RING_BASE }
 		},
@@ -91,6 +96,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS3] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 3,
+		.irq_offset = XEHPC_BCS3,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS3_RING_BASE }
 		},
@@ -98,6 +104,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS4] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 4,
+		.irq_offset = XEHPC_BCS4,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS4_RING_BASE }
 		},
@@ -105,6 +112,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS5] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 5,
+		.irq_offset = XEHPC_BCS5,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS5_RING_BASE }
 		},
@@ -112,6 +120,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS6] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 6,
+		.irq_offset = XEHPC_BCS6,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS6_RING_BASE }
 		},
@@ -119,6 +128,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS7] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 7,
+		.irq_offset = XEHPC_BCS7,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS7_RING_BASE }
 		},
@@ -126,6 +136,7 @@ static const struct engine_info intel_engines[] = {
 	[BCS8] = {
 		.class = COPY_ENGINE_CLASS,
 		.instance = 8,
+		.irq_offset = XEHPC_BCS8,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHPC_BCS8_RING_BASE }
 		},
@@ -133,6 +144,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS0] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 0,
+		.irq_offset = 32 + GEN11_VCS(0),
 		.mmio_bases = {
 			{ .graphics_ver = 11, .base = GEN11_BSD_RING_BASE },
 			{ .graphics_ver = 6, .base = GEN6_BSD_RING_BASE },
@@ -142,6 +154,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS1] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 1,
+		.irq_offset = 32 + GEN11_VCS(1),
 		.mmio_bases = {
 			{ .graphics_ver = 11, .base = GEN11_BSD2_RING_BASE },
 			{ .graphics_ver = 8, .base = GEN8_BSD2_RING_BASE }
@@ -150,6 +163,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS2] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 2,
+		.irq_offset = 32 + GEN11_VCS(2),
 		.mmio_bases = {
 			{ .graphics_ver = 11, .base = GEN11_BSD3_RING_BASE }
 		},
@@ -157,6 +171,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS3] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 3,
+		.irq_offset = 32 + GEN11_VCS(3),
 		.mmio_bases = {
 			{ .graphics_ver = 11, .base = GEN11_BSD4_RING_BASE }
 		},
@@ -164,6 +179,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS4] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 4,
+		.irq_offset = 32 + GEN11_VCS(4),
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHP_BSD5_RING_BASE }
 		},
@@ -171,6 +187,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS5] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 5,
+		.irq_offset = 32 + GEN11_VCS(5),
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHP_BSD6_RING_BASE }
 		},
@@ -178,6 +195,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS6] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 6,
+		.irq_offset = 32 + GEN11_VCS(6),
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHP_BSD7_RING_BASE }
 		},
@@ -185,6 +203,7 @@ static const struct engine_info intel_engines[] = {
 	[VCS7] = {
 		.class = VIDEO_DECODE_CLASS,
 		.instance = 7,
+		.irq_offset = 32 + GEN11_VCS(7),
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHP_BSD8_RING_BASE }
 		},
@@ -192,6 +211,7 @@ static const struct engine_info intel_engines[] = {
 	[VECS0] = {
 		.class = VIDEO_ENHANCEMENT_CLASS,
 		.instance = 0,
+		.irq_offset = 32 + GEN11_VECS(0),
 		.mmio_bases = {
 			{ .graphics_ver = 11, .base = GEN11_VEBOX_RING_BASE },
 			{ .graphics_ver = 7, .base = VEBOX_RING_BASE }
@@ -200,6 +220,7 @@ static const struct engine_info intel_engines[] = {
 	[VECS1] = {
 		.class = VIDEO_ENHANCEMENT_CLASS,
 		.instance = 1,
+		.irq_offset = 32 + GEN11_VECS(1),
 		.mmio_bases = {
 			{ .graphics_ver = 11, .base = GEN11_VEBOX2_RING_BASE }
 		},
@@ -207,6 +228,7 @@ static const struct engine_info intel_engines[] = {
 	[VECS2] = {
 		.class = VIDEO_ENHANCEMENT_CLASS,
 		.instance = 2,
+		.irq_offset = 32 + GEN11_VECS(2),
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHP_VEBOX3_RING_BASE }
 		},
@@ -214,6 +236,7 @@ static const struct engine_info intel_engines[] = {
 	[VECS3] = {
 		.class = VIDEO_ENHANCEMENT_CLASS,
 		.instance = 3,
+		.irq_offset = 32 + GEN11_VECS(3),
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = XEHP_VEBOX4_RING_BASE }
 		},
@@ -221,6 +244,7 @@ static const struct engine_info intel_engines[] = {
 	[CCS0] = {
 		.class = COMPUTE_CLASS,
 		.instance = 0,
+		.irq_offset = GEN12_CCS0,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = GEN12_COMPUTE0_RING_BASE }
 		}
@@ -228,6 +252,7 @@ static const struct engine_info intel_engines[] = {
 	[CCS1] = {
 		.class = COMPUTE_CLASS,
 		.instance = 1,
+		.irq_offset = GEN12_CCS1,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = GEN12_COMPUTE1_RING_BASE }
 		}
@@ -235,6 +260,7 @@ static const struct engine_info intel_engines[] = {
 	[CCS2] = {
 		.class = COMPUTE_CLASS,
 		.instance = 2,
+		.irq_offset = GEN12_CCS2,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = GEN12_COMPUTE2_RING_BASE }
 		}
@@ -242,6 +268,7 @@ static const struct engine_info intel_engines[] = {
 	[CCS3] = {
 		.class = COMPUTE_CLASS,
 		.instance = 3,
+		.irq_offset = GEN12_CCS3,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = GEN12_COMPUTE3_RING_BASE }
 		}
@@ -249,6 +276,7 @@ static const struct engine_info intel_engines[] = {
 	[GSC0] = {
 		.class = OTHER_CLASS,
 		.instance = OTHER_GSC_INSTANCE,
+		.irq_offset = GEN11_CSME,
 		.mmio_bases = {
 			{ .graphics_ver = 12, .base = MTL_GSC_RING_BASE }
 		}
@@ -369,6 +397,9 @@ static void __sprint_engine_name(struct intel_engine_cs *engine)
 
 void intel_engine_set_hwsp_writemask(struct intel_engine_cs *engine, u32 mask)
 {
+	if (IS_SRIOV_VF(engine->i915))
+		return;
+
 	/*
 	 * Though they added more rings on g4x/ilk, they did not add
 	 * per-engine HWSTAM until gen6.
@@ -495,6 +526,7 @@ static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id,
 	engine->class = info->class;
 	engine->instance = info->instance;
 	engine->logical_mask = BIT(logical_instance);
+	engine->irq_offset = info->irq_offset;
 	__sprint_engine_name(engine);
 
 	if ((engine->class == COMPUTE_CLASS && !RCS_MASK(engine->gt) &&
@@ -894,6 +926,11 @@ static intel_engine_mask_t init_engine_mask(struct intel_gt *gt)
 	engine_mask_apply_compute_fuses(gt);
 	engine_mask_apply_copy_fuses(gt);
 
+	/* GSC0 is not usable by VFs */
+	if (IS_SRIOV_VF(gt->i915)) {
+		info->engine_mask &= ~BIT(GSC0);
+	}
+
 	/*
 	 * The only use of the GSC CS is to load and communicate with the GSC
 	 * FW, so we have no use for it if we don't have the FW.
@@ -1943,6 +1980,10 @@ static void intel_engine_print_registers(struct intel_engine_cs *engine,
 	struct intel_engine_execlists * const execlists = &engine->execlists;
 	u64 addr;
 
+	/* VF can't access these registers */
+	if (IS_SRIOV_VF(dev_priv))
+		return;
+
 	if (engine->id == RENDER_CLASS && IS_GRAPHICS_VER(dev_priv, 4, 7))
 		drm_printf(m, "\tCCID: 0x%08x\n", ENGINE_READ(engine, CCID));
 	if (HAS_EXECLISTS(dev_priv)) {
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_regs.h b/drivers/gpu/drm/i915/gt/intel_engine_regs.h
index 6b9d9f837669..05dc90814a52 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_regs.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_regs.h
@@ -77,7 +77,10 @@
 #define   MODE_IDLE				REG_BIT(9)
 #define   STOP_RING				REG_BIT(8)
 #define   VS_TIMER_DISPATCH			REG_BIT(6)
+#define GEN12_RING_INT_SRC(base)		_MMIO((base) + 0xa4)
 #define RING_IMR(base)				_MMIO((base) + 0xa8)
+#define GEN12_RING_INT_MASK(base)		_MMIO((base) + 0xa8)
+#define GEN12_RING_INT_STATUS(base)		_MMIO((base) + 0xac)
 #define RING_EIR(base)				_MMIO((base) + 0xb0)
 #define RING_EMR(base)				_MMIO((base) + 0xb4)
 #define RING_ESR(base)				_MMIO((base) + 0xb8)
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index e40b0660207e..1135ddd790f9 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -365,6 +365,8 @@ struct intel_engine_cs {
 	u8 class;
 	u8 instance;
 
+	u8 irq_offset;
+
 	u16 uabi_class;
 	u16 uabi_instance;
 
diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index e6cf9d4a0210..051c0dd03754 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -15,8 +15,11 @@
 #include "display/intel_display.h"
 #include "gem/i915_gem_lmem.h"
 
+#include "iov/intel_iov.h"
+
 #include "intel_ggtt_gmch.h"
 #include "intel_gt.h"
+#include "intel_gt_print.h"
 #include "intel_gt_regs.h"
 #include "intel_pci_config.h"
 #include "i915_drv.h"
@@ -220,6 +223,33 @@ static void guc_ggtt_invalidate(struct i915_ggtt *ggtt)
 	}
 }
 
+// FIXME! MISSING!
+enum intel_guc_tlb_inval_mode {
+	INTEL_GUC_TLB_INVAL_MODE_HEAVY = 0x0,
+	INTEL_GUC_TLB_INVAL_MODE_LITE = 0x1,
+};
+int intel_guc_invalidate_tlb_guc(struct intel_guc *guc, enum intel_guc_tlb_inval_mode mode)
+{
+	return -ENOTTY;
+}
+// FIXME! MISSING!
+
+static void gen12vf_ggtt_invalidate(struct i915_ggtt *ggtt)
+{
+	intel_wakeref_t wakeref;
+	struct intel_gt *gt;
+
+	list_for_each_entry(gt, &ggtt->gt_list, ggtt_link) {
+		struct intel_guc *guc = &gt->uc.guc;
+
+		if (!guc->ct.enabled)
+			continue;
+
+		with_intel_runtime_pm(gt->uncore->rpm, wakeref)
+			intel_guc_invalidate_tlb_guc(guc, INTEL_GUC_TLB_INVAL_MODE_HEAVY);
+	}
+}
+
 u64 mtl_ggtt_pte_encode(dma_addr_t addr,
 			enum i915_cache_level level,
 			u32 flags)
@@ -559,6 +589,10 @@ static int init_ggtt(struct i915_ggtt *ggtt)
 	if (ret)
 		return ret;
 
+	ret = intel_iov_init_ggtt(&ggtt->vm.gt->iov);
+	if (ret)
+		return ret;
+
 	mutex_init(&ggtt->error_mutex);
 	if (ggtt->mappable_end) {
 		/*
@@ -767,6 +801,7 @@ static void ggtt_cleanup_hw(struct i915_ggtt *ggtt)
 	mutex_destroy(&ggtt->error_mutex);
 
 	ggtt_release_guc_top(ggtt);
+	intel_iov_fini_ggtt(&ggtt->vm.gt->iov);
 	intel_vgt_deballoon(ggtt);
 
 	ggtt->vm.cleanup(&ggtt->vm);
@@ -1135,6 +1170,44 @@ static int gen6_gmch_probe(struct i915_ggtt *ggtt)
 	return ggtt_probe_common(ggtt, size);
 }
 
+static int gen12vf_ggtt_probe(struct i915_ggtt *ggtt)
+{
+	struct drm_i915_private *i915 = ggtt->vm.i915;
+
+	GEM_BUG_ON(!IS_SRIOV_VF(i915));
+
+	/* there is no apperture on VFs */
+	ggtt->gmadr = (struct resource) DEFINE_RES_MEM(0, 0);
+	ggtt->mappable_end = 0;
+
+	ggtt->vm.alloc_pt_dma = alloc_pt_dma;
+	ggtt->vm.alloc_scratch_dma = alloc_pt_dma;
+	ggtt->vm.lmem_pt_obj_flags = I915_BO_ALLOC_PM_EARLY;
+
+	/* safe guess as native expects the same minimum */
+	/* can't use roundup_pow_of_two(GUC_GGTT_TOP); */
+	ggtt->vm.total = 1ULL << (ilog2(GUC_GGTT_TOP - 1) + 1);
+	ggtt->vm.cleanup = gen6_gmch_remove;
+	ggtt->vm.insert_page = gen8_ggtt_insert_page;
+	ggtt->vm.clear_range = nop_clear_range;
+
+	ggtt->vm.insert_entries = gen8_ggtt_insert_entries;
+
+	/* can't use guc_ggtt_invalidate() */
+	ggtt->invalidate = gen12vf_ggtt_invalidate;
+
+	ggtt->vm.vma_ops.bind_vma    = intel_ggtt_bind_vma;
+	ggtt->vm.vma_ops.unbind_vma  = intel_ggtt_unbind_vma;
+
+	if (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 70))
+		ggtt->vm.pte_encode = mtl_ggtt_pte_encode;
+	else
+		ggtt->vm.pte_encode = gen8_ggtt_pte_encode;
+
+	return ggtt_probe_common(ggtt, sizeof(gen8_pte_t) *
+				 (ggtt->vm.total >> PAGE_SHIFT));
+}
+
 static int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)
 {
 	struct drm_i915_private *i915 = gt->i915;
@@ -1145,7 +1218,9 @@ static int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)
 	ggtt->vm.dma = i915->drm.dev;
 	dma_resv_init(&ggtt->vm._resv);
 
-	if (GRAPHICS_VER(i915) >= 8)
+	if (IS_SRIOV_VF(i915))
+		ret = gen12vf_ggtt_probe(ggtt);
+	else if (GRAPHICS_VER(i915) >= 8)
 		ret = gen8_gmch_probe(ggtt);
 	else if (GRAPHICS_VER(i915) >= 6)
 		ret = gen6_gmch_probe(ggtt);
@@ -1294,3 +1369,83 @@ void i915_ggtt_resume(struct i915_ggtt *ggtt)
 
 	intel_ggtt_restore_fences(ggtt);
 }
+
+/**
+ * i915_ggtt_balloon - reserve fixed space in an GGTT
+ * @ggtt: the &struct i915_ggtt
+ * @start: start offset inside the GGTT,
+ *          must be #I915_GTT_MIN_ALIGNMENT aligned
+ * @end: end offset inside the GGTT,
+ *        must be #I915_GTT_PAGE_SIZE aligned
+ * @node: the &struct drm_mm_node
+ *
+ * i915_ggtt_balloon() tries to reserve the @node from @start to @end inside
+ * GGTT the address space.
+ *
+ * Returns: 0 on success, -ENOSPC if no suitable hole is found.
+ */
+int i915_ggtt_balloon(struct i915_ggtt *ggtt, u64 start, u64 end,
+		      struct drm_mm_node *node)
+{
+	u64 size = end - start;
+	int err;
+
+	GEM_BUG_ON(start >= end);
+	gt_dbg(ggtt->vm.gt, "ballooning GGTT [%#llx-%#llx] %lluK\n",
+	       start, end, size / SZ_1K);
+
+	err = i915_gem_gtt_reserve(&ggtt->vm, NULL, node, size, start,
+				   I915_COLOR_UNEVICTABLE, PIN_NOEVICT);
+	if (unlikely(err)) {
+		gt_err(ggtt->vm.gt, "Failed to balloon GGTT [%#llx-%#llx] %pe\n",
+		       node->start, node->start + node->size, ERR_PTR(err));
+		return err;
+	}
+
+	ggtt->vm.reserved += node->size;
+	return 0;
+}
+
+void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node)
+{
+	if (!drm_mm_node_allocated(node))
+		return;
+
+	gt_dbg(ggtt->vm.gt, "deballooning GGTT [%#llx-%#llx] %lluK\n",
+	       node->start, node->start + node->size, node->size / SZ_1K);
+
+	GEM_BUG_ON(ggtt->vm.reserved < node->size);
+	ggtt->vm.reserved -= node->size;
+	drm_mm_remove_node(node);
+}
+
+static gen8_pte_t tgl_prepare_vf_pte(u16 vfid)
+{
+	GEM_BUG_ON(!FIELD_FIT(TGL_GGTT_PTE_VFID_MASK, vfid));
+
+	return FIELD_PREP(TGL_GGTT_PTE_VFID_MASK, vfid) | GEN8_PAGE_PRESENT;
+}
+
+void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
+			       const struct drm_mm_node *node)
+{
+	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
+	const gen8_pte_t pte = tgl_prepare_vf_pte(vfid);
+	u64 base = node->start;
+	u64 size = node->size;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(ggtt->vm.i915));
+	GEM_BUG_ON(base % PAGE_SIZE);
+	GEM_BUG_ON(size % PAGE_SIZE);
+
+	gt_dbg(ggtt->vm.gt, "GGTT VF%u [%#llx-%#llx] %lluK\n",
+	       vfid, base, base + size, size / SZ_1K);
+
+	gtt_entries += base >> PAGE_SHIFT;
+	while (size) {
+		gen8_set_pte(gtt_entries++, pte);
+		size -= PAGE_SIZE;
+	}
+
+	ggtt->invalidate(ggtt);
+}
diff --git a/drivers/gpu/drm/i915/gt/intel_gsc.c b/drivers/gpu/drm/i915/gt/intel_gsc.c
index bcc3605158db..506c68b4b071 100644
--- a/drivers/gpu/drm/i915/gt/intel_gsc.c
+++ b/drivers/gpu/drm/i915/gt/intel_gsc.c
@@ -315,7 +315,7 @@ void intel_gsc_init(struct intel_gsc *gsc, struct drm_i915_private *i915)
 {
 	unsigned int i;
 
-	if (!HAS_HECI_GSC(i915))
+	if (!HAS_HECI_GSC(i915) || IS_SRIOV_VF(i915))
 		return;
 
 	for (i = 0; i < INTEL_GSC_NUM_INTERFACES; i++)
@@ -327,7 +327,7 @@ void intel_gsc_fini(struct intel_gsc *gsc)
 	struct intel_gt *gt = gsc_to_gt(gsc);
 	unsigned int i;
 
-	if (!HAS_HECI_GSC(gt->i915))
+	if (!HAS_HECI_GSC(gt->i915) || IS_SRIOV_VF(gt->i915))
 		return;
 
 	for (i = 0; i < INTEL_GSC_NUM_INTERFACES; i++)
diff --git a/drivers/gpu/drm/i915/gt/intel_gt.c b/drivers/gpu/drm/i915/gt/intel_gt.c
index 31fa46831dfc..0ee3f0f2a11a 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt.c
@@ -35,6 +35,8 @@
 #include "intel_sa_media.h"
 #include "intel_gt_sysfs.h"
 #include "intel_uncore.h"
+#include "iov/intel_iov.h"
+#include "iov/intel_iov_sysfs.h"
 #include "shmem_utils.h"
 
 void intel_gt_common_init_early(struct intel_gt *gt)
@@ -125,9 +127,15 @@ int intel_gt_assign_ggtt(struct intel_gt *gt)
 
 int intel_gt_init_mmio(struct intel_gt *gt)
 {
-	intel_gt_init_clock_frequency(gt);
+	int ret;
+
+	ret = intel_iov_init_mmio(&gt->iov);
+	if (ret)
+		return ret;
 
+	intel_gt_init_clock_frequency(gt);
 	intel_uc_init_mmio(&gt->uc);
+
 	intel_sseu_info_init(gt);
 	intel_gt_mcr_init(gt);
 
@@ -213,6 +221,13 @@ int intel_gt_init_hw(struct intel_gt *gt)
 
 	intel_mocs_init(gt);
 
+	ret = intel_iov_init_hw(&gt->iov);
+	if (unlikely(ret)) {
+		i915_probe_error(i915, "Enabling IOV failed (%pe)\n",
+				 ERR_PTR(ret));
+		goto out;
+	}
+
 out:
 	intel_uncore_forcewake_put(uncore, FORCEWAKE_ALL);
 	return ret;
@@ -242,6 +257,9 @@ intel_gt_clear_error_registers(struct intel_gt *gt,
 	struct intel_uncore *uncore = gt->uncore;
 	u32 eir;
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (GRAPHICS_VER(i915) != 2)
 		intel_uncore_write(uncore, PGTBL_ER, 0);
 
@@ -386,6 +404,9 @@ void intel_gt_check_and_clear_faults(struct intel_gt *gt)
 {
 	struct drm_i915_private *i915 = gt->i915;
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	/* From GEN8 onwards we only have one 'All Engine Fault Register' */
 	if (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50))
 		xehp_check_faults(gt);
@@ -455,6 +476,7 @@ void intel_gt_driver_register(struct intel_gt *gt)
 
 	intel_gt_debugfs_register(gt);
 	intel_gt_sysfs_register(gt);
+	intel_iov_sysfs_setup(&gt->iov);
 }
 
 static int intel_gt_init_scratch(struct intel_gt *gt, unsigned int size)
@@ -698,10 +720,14 @@ int intel_gt_init(struct intel_gt *gt)
 	 */
 	intel_uncore_forcewake_get(gt->uncore, FORCEWAKE_ALL);
 
+	err = intel_iov_init(&gt->iov);
+	if (unlikely(err))
+		goto out_fw;
+
 	err = intel_gt_init_scratch(gt,
 				    GRAPHICS_VER(gt->i915) == 2 ? SZ_256K : SZ_4K);
 	if (err)
-		goto out_fw;
+		goto err_iov;
 
 	intel_gt_pm_init(gt);
 
@@ -729,6 +755,10 @@ int intel_gt_init(struct intel_gt *gt)
 	if (err)
 		gt_err(gt, "Failed to retrieve hwconfig table: %pe\n", ERR_PTR(err));
 
+	err = intel_iov_init_late(&gt->iov);
+	if (err)
+		goto err_gt;
+
 	err = __engines_record_defaults(gt);
 	if (err)
 		goto err_gt;
@@ -757,6 +787,8 @@ int intel_gt_init(struct intel_gt *gt)
 err_pm:
 	intel_gt_pm_fini(gt);
 	intel_gt_fini_scratch(gt);
+err_iov:
+	intel_iov_fini(&gt->iov);
 out_fw:
 	if (err)
 		intel_gt_set_wedged_on_init(gt);
@@ -766,6 +798,10 @@ int intel_gt_init(struct intel_gt *gt)
 
 void intel_gt_driver_remove(struct intel_gt *gt)
 {
+	intel_gt_fini_clock_frequency(gt);
+
+	intel_iov_fini_hw(&gt->iov);
+
 	__intel_gt_disable(gt);
 
 	intel_migrate_fini(&gt->migrate);
@@ -780,6 +816,9 @@ void intel_gt_driver_unregister(struct intel_gt *gt)
 {
 	intel_wakeref_t wakeref;
 
+	if (!gt->i915->drm.unplugged)
+		intel_iov_sysfs_teardown(&gt->iov);
+
 	intel_gt_sysfs_unregister(gt);
 	intel_rps_driver_unregister(&gt->rps);
 	intel_gsc_fini(&gt->gsc);
@@ -812,6 +851,7 @@ void intel_gt_driver_release(struct intel_gt *gt)
 	intel_gt_fini_scratch(gt);
 	intel_gt_fini_buffer_pool(gt);
 	intel_gt_fini_hwconfig(gt);
+	intel_iov_fini(&gt->iov);
 }
 
 void intel_gt_driver_late_release_all(struct drm_i915_private *i915)
@@ -823,6 +863,7 @@ void intel_gt_driver_late_release_all(struct drm_i915_private *i915)
 	rcu_barrier();
 
 	for_each_gt(gt, i915, id) {
+		intel_iov_release(&gt->iov);
 		intel_uc_driver_late_release(&gt->uc);
 		intel_gt_fini_requests(gt);
 		intel_gt_fini_reset(gt);
@@ -862,6 +903,8 @@ static int intel_gt_tile_setup(struct intel_gt *gt, phys_addr_t phys_addr)
 
 	gt->phys_addr = phys_addr;
 
+	intel_iov_init_early(&gt->iov);
+
 	return 0;
 }
 
@@ -1188,7 +1231,10 @@ void intel_gt_invalidate_tlb(struct intel_gt *gt, u32 seqno)
 		if (tlb_seqno_passed(gt, seqno))
 			goto unlock;
 
-		mmio_invalidate_full(gt);
+		if (IS_SRIOV_VF(gt->i915))
+			gt_WARN_ON_ONCE(gt, "No GUC TLB invalidation yet!");
+		else
+			mmio_invalidate_full(gt);
 
 		write_seqcount_invalidate(&gt->tlb.seqno);
 unlock:
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c
index 7c9be4fd1c8c..8a07d6eb759b 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c
@@ -190,9 +190,18 @@ void intel_gt_init_clock_frequency(struct intel_gt *gt)
 			 USEC_PER_SEC));
 }
 
+void intel_gt_fini_clock_frequency(struct intel_gt *gt)
+{
+	/* Clock registers no longer accessible, stop checking */
+	gt->clock_frequency = 0;
+}
+
 #if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)
 void intel_gt_check_clock_frequency(const struct intel_gt *gt)
 {
+	if (!gt->clock_frequency)
+		return;
+
 	if (gt->clock_frequency != read_clock_frequency(gt->uncore)) {
 		gt_err(gt, "GT clock frequency changed, was %uHz, now %uHz!\n",
 		       gt->clock_frequency,
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h
index 8b03e97a85df..c923b1866b08 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h
@@ -11,6 +11,7 @@
 struct intel_gt;
 
 void intel_gt_init_clock_frequency(struct intel_gt *gt);
+void intel_gt_fini_clock_frequency(struct intel_gt *gt);
 
 #if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)
 void intel_gt_check_clock_frequency(const struct intel_gt *gt);
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c b/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c
index 4dc23b8d3aa2..19340a3d4d32 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c
@@ -12,6 +12,7 @@
 #include "intel_gt_mcr.h"
 #include "intel_gt_pm_debugfs.h"
 #include "intel_sseu_debugfs.h"
+#include "iov/intel_iov_debugfs.h"
 #include "uc/intel_uc_debugfs.h"
 
 int intel_gt_debugfs_reset_show(struct intel_gt *gt, u64 *val)
@@ -100,6 +101,7 @@ void intel_gt_debugfs_register(struct intel_gt *gt)
 	intel_sseu_debugfs_register(gt, root);
 
 	intel_uc_debugfs_register(&gt->uc, root);
+	intel_iov_debugfs_register(&gt->iov, root);
 }
 
 void intel_gt_debugfs_register_files(struct dentry *root,
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_pm_debugfs.c b/drivers/gpu/drm/i915/gt/intel_gt_pm_debugfs.c
index 0f5ba4491e4d..8f8782871857 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_pm_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_pm_debugfs.c
@@ -594,5 +594,8 @@ void intel_gt_pm_debugfs_register(struct intel_gt *gt, struct dentry *root)
 		{ "perf_limit_reasons", &perf_limit_reasons_fops, perf_limit_reasons_eval },
 	};
 
+	if (IS_SRIOV_VF(gt->i915))
+		return;
+
 	intel_gt_debugfs_register_files(root, files, ARRAY_SIZE(files), gt);
 }
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_sysfs_pm.c b/drivers/gpu/drm/i915/gt/intel_gt_sysfs_pm.c
index 28f27091cd3b..5c648749d0df 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_sysfs_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_sysfs_pm.c
@@ -730,6 +730,9 @@ void intel_gt_sysfs_pm_init(struct intel_gt *gt, struct kobject *kobj)
 {
 	int ret;
 
+	if (IS_SRIOV_VF(gt->i915))
+		return;
+
 	intel_sysfs_rc6_init(gt, kobj);
 
 	ret = intel_sysfs_rps_init(gt, kobj);
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_types.h b/drivers/gpu/drm/i915/gt/intel_gt_types.h
index f08c2556aa25..742d43b99996 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt_types.h
@@ -31,6 +31,7 @@
 #include "intel_migrate_types.h"
 #include "intel_wakeref.h"
 #include "intel_wopcm.h"
+#include "iov/intel_iov_types.h"
 
 struct drm_i915_private;
 struct i915_ggtt;
@@ -102,6 +103,7 @@ struct intel_gt {
 	struct intel_uc uc;
 	struct intel_gsc gsc;
 	struct intel_wopcm wopcm;
+	struct intel_iov iov;
 
 	struct {
 		/* Serialize global tlb invalidations */
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.c b/drivers/gpu/drm/i915/gt/intel_gtt.c
index 1e1b34e22cf5..8c79090090f3 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.c
@@ -405,6 +405,9 @@ void gtt_write_workarounds(struct intel_gt *gt)
 	struct drm_i915_private *i915 = gt->i915;
 	struct intel_uncore *uncore = gt->uncore;
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	/*
 	 * This function is for gtt related workarounds. This function is
 	 * called on driver load and after a GPU reset, so you can place
@@ -622,6 +625,9 @@ void setup_private_pat(struct intel_gt *gt)
 
 	GEM_BUG_ON(GRAPHICS_VER(i915) < 8);
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (IS_METEORLAKE(i915))
 		mtl_setup_private_ppat(uncore);
 	else if (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50))
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.h b/drivers/gpu/drm/i915/gt/intel_gtt.h
index ed694a169abd..b2b097591d3f 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.h
@@ -95,9 +95,29 @@ typedef u64 gen8_pte_t;
 #define GEN12_PPGTT_PTE_PAT1    BIT_ULL(4)
 #define GEN12_PPGTT_PTE_PAT0    BIT_ULL(3)
 
+/*
+ *  DOC: GEN12 GGTT Table Entry format
+ *
+ * +----------+---------+---------+-----------------+--------------+---------+
+ * |    63:46 |   45:12 |    11:5 |             4:2 |            1 |       0 |
+ * +==========+=========+=========+=================+==============+=========+
+ * |  Ignored | Address | Ignored | Function Number | Local Memory | Present |
+ * +----------+---------+---------+-----------------+--------------+---------+
+ *
+ * ADL-P/S:
+ * +----------+--------------+-------------------+---------+---------+----------+--------+---------+
+ * |    63:46 |        45:42 |             41:39 |   38:12 |   11:5  |      4:2 |      1 |       0 |
+ * +==========+==============+===================+=========+=========+==========+========+=========+
+ * |  Ignored | MKTME key ID | 2LM Far Memory    | Address | Ignored | Function | Local  | Present |
+ * |          |              | address extension |         |         | Number   | Memory |         |
+ * +----------+--------------+-------------------+---------+---------+----------+--------+---------+
+ *
+ */
+
 #define GEN12_GGTT_PTE_LM		BIT_ULL(1)
 #define MTL_GGTT_PTE_PAT0		BIT_ULL(52)
 #define MTL_GGTT_PTE_PAT1		BIT_ULL(53)
+#define TGL_GGTT_PTE_VFID_MASK		GENMASK_ULL(4, 2)
 #define GEN12_GGTT_PTE_ADDR_MASK	GENMASK_ULL(45, 12)
 #define MTL_GGTT_PTE_PAT_MASK		GENMASK_ULL(53, 52)
 
@@ -597,6 +617,13 @@ static inline bool i915_ggtt_has_aperture(const struct i915_ggtt *ggtt)
 	return ggtt->mappable_end > 0;
 }
 
+int i915_ggtt_balloon(struct i915_ggtt *ggtt, u64 start, u64 end,
+		      struct drm_mm_node *node);
+void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node);
+
+void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
+			       const struct drm_mm_node *node);
+
 int i915_ppgtt_init_hw(struct intel_gt *gt);
 
 struct i915_ppgtt *i915_ppgtt_create(struct intel_gt *gt,
diff --git a/drivers/gpu/drm/i915/gt/intel_llc.c b/drivers/gpu/drm/i915/gt/intel_llc.c
index 1d19c073ba2e..420ca203417d 100644
--- a/drivers/gpu/drm/i915/gt/intel_llc.c
+++ b/drivers/gpu/drm/i915/gt/intel_llc.c
@@ -53,6 +53,9 @@ static bool get_ia_constants(struct intel_llc *llc,
 	struct drm_i915_private *i915 = llc_to_gt(llc)->i915;
 	struct intel_rps *rps = &llc_to_gt(llc)->rps;
 
+	if (IS_SRIOV_VF(i915))
+		return false;
+
 	if (!HAS_LLC(i915) || IS_DGFX(i915))
 		return false;
 
diff --git a/drivers/gpu/drm/i915/gt/intel_lrc.c b/drivers/gpu/drm/i915/gt/intel_lrc.c
index 81a96c52a92b..080b75b19b67 100644
--- a/drivers/gpu/drm/i915/gt/intel_lrc.c
+++ b/drivers/gpu/drm/i915/gt/intel_lrc.c
@@ -18,6 +18,7 @@
 #include "intel_lrc.h"
 #include "intel_lrc_reg.h"
 #include "intel_ring.h"
+#include "iov/intel_iov_reg.h"
 #include "shmem_utils.h"
 
 /*
@@ -914,6 +915,29 @@ static struct i915_ppgtt *vm_alias(struct i915_address_space *vm)
 		return i915_vm_to_ppgtt(vm);
 }
 
+static void init_vf_irq_reg_state(u32 *regs, const struct intel_engine_cs *engine)
+{
+	struct i915_vma *vma = engine->gt->iov.vf.irq.vma;
+
+	GEM_BUG_ON(!IS_SRIOV_VF(engine->i915));
+	GEM_BUG_ON(!vma);
+
+	BUILD_BUG_ON(!IS_ALIGNED(I915_VF_IRQ_STATUS, SZ_4K));
+	BUILD_BUG_ON(!IS_ALIGNED(I915_VF_IRQ_SOURCE, SZ_64));
+
+	regs[GEN12_CTX_LRM_HEADER_0] =
+		MI_LOAD_REGISTER_MEM_GEN8 | MI_SRM_LRM_GLOBAL_GTT | MI_LRI_LRM_CS_MMIO;
+	regs[GEN12_CTX_INT_MASK_REG] = i915_mmio_reg_offset(GEN12_RING_INT_MASK(0));
+	regs[GEN12_CTX_INT_MASK_PTR] = i915_ggtt_offset(vma) + I915_VF_IRQ_ENABLE;
+
+	regs[GEN12_CTX_LRI_HEADER_4] =
+		MI_LOAD_REGISTER_IMM(2) | MI_LRI_FORCE_POSTED | MI_LRI_LRM_CS_MMIO;
+	regs[GEN12_CTX_INT_STATUS_REPORT_PTR] = i915_mmio_reg_offset(GEN12_RING_INT_STATUS(0));
+	regs[GEN12_CTX_INT_STATUS_REPORT_PTR + 1] = i915_ggtt_offset(vma) + I915_VF_IRQ_STATUS;
+	regs[GEN12_CTX_INT_SRC_REPORT_PTR] = i915_mmio_reg_offset(GEN12_RING_INT_SRC(0));
+	regs[GEN12_CTX_INT_SRC_REPORT_PTR + 1] = i915_ggtt_offset(vma) + I915_VF_IRQ_SOURCE;
+}
+
 static void __reset_stop_ring(u32 *regs, const struct intel_engine_cs *engine)
 {
 	int x;
@@ -951,6 +975,9 @@ static void __lrc_init_regs(u32 *regs,
 
 	init_wa_bb_regs(regs, engine);
 
+	if (HAS_MEMORY_IRQ_STATUS(engine->i915))
+		init_vf_irq_reg_state(regs, engine);
+
 	__reset_stop_ring(regs, engine);
 }
 
diff --git a/drivers/gpu/drm/i915/gt/intel_lrc_reg.h b/drivers/gpu/drm/i915/gt/intel_lrc_reg.h
index 304000c7e345..c1c247e31936 100644
--- a/drivers/gpu/drm/i915/gt/intel_lrc_reg.h
+++ b/drivers/gpu/drm/i915/gt/intel_lrc_reg.h
@@ -30,6 +30,14 @@
 
 #define GEN9_CTX_RING_MI_MODE		0x54
 
+/* VF Reg State Context */
+#define GEN12_CTX_LRM_HEADER_0			0x50
+#define GEN12_CTX_INT_MASK_REG			0x51
+#define GEN12_CTX_INT_MASK_PTR			0x52
+#define GEN12_CTX_LRI_HEADER_4			0x55
+#define GEN12_CTX_INT_STATUS_REPORT_PTR		0x56
+#define GEN12_CTX_INT_SRC_REPORT_PTR		0x58
+
 #define ASSIGN_CTX_PDP(ppgtt, reg_state, n) do { \
 	u32 *reg_state__ = (reg_state); \
 	const u64 addr__ = i915_page_dir_dma_addr((ppgtt), (n)); \
diff --git a/drivers/gpu/drm/i915/gt/intel_mocs.c b/drivers/gpu/drm/i915/gt/intel_mocs.c
index 89570f137b2c..b2ecca38fe73 100644
--- a/drivers/gpu/drm/i915/gt/intel_mocs.c
+++ b/drivers/gpu/drm/i915/gt/intel_mocs.c
@@ -577,6 +577,9 @@ static unsigned int get_mocs_settings(const struct drm_i915_private *i915,
 	if (GEM_DEBUG_WARN_ON(table->size > table->n_entries))
 		return 0;
 
+	if (IS_SRIOV_VF(i915))
+		return 0;
+
 	/* WaDisableSkipCaching:skl,bxt,kbl,glk */
 	if (GRAPHICS_VER(i915) == 9) {
 		int i;
diff --git a/drivers/gpu/drm/i915/gt/intel_rc6.c b/drivers/gpu/drm/i915/gt/intel_rc6.c
index 5c91622dfca4..299901e743c6 100644
--- a/drivers/gpu/drm/i915/gt/intel_rc6.c
+++ b/drivers/gpu/drm/i915/gt/intel_rc6.c
@@ -490,6 +490,9 @@ static bool rc6_supported(struct intel_rc6 *rc6)
 	if (!HAS_RC6(i915))
 		return false;
 
+	if (IS_SRIOV_VF(i915))
+		return false;
+
 	if (intel_vgpu_active(i915))
 		return false;
 
diff --git a/drivers/gpu/drm/i915/gt/intel_reset.c b/drivers/gpu/drm/i915/gt/intel_reset.c
index b97d681830e2..30e9d48011c4 100644
--- a/drivers/gpu/drm/i915/gt/intel_reset.c
+++ b/drivers/gpu/drm/i915/gt/intel_reset.c
@@ -659,6 +659,50 @@ static int gen8_reset_engines(struct intel_gt *gt,
 	return ret;
 }
 
+static int gen12_vf_reset(struct intel_gt *gt,
+			  intel_engine_mask_t mask,
+			  unsigned int retry)
+{
+	struct intel_uncore *uncore = gt->uncore;
+	i915_reg_t notify_reg = gt->uc.guc.notify_reg;
+	i915_reg_t send_reg = _MMIO(gt->uc.guc.send_regs.base);
+	u32 request[VF2GUC_VF_RESET_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_VF_RESET),
+	};
+	u32 response;
+	int err;
+
+	/* No engine reset since VFs always run with GuC submission enabled */
+	if (GEM_WARN_ON(mask != ALL_ENGINES))
+		return -ENODEV;
+
+	/*
+	 * Can't use intel_guc_send_mmio() since it uses mutex,
+	 * but we don't expect any other MMIO action in flight,
+	 * as we use them only during init and teardown.
+	 */
+	GEM_WARN_ON(mutex_is_locked(&gt->uc.guc.send_mutex));
+
+	intel_uncore_write_fw(uncore, send_reg, request[0]);
+	intel_uncore_write_fw(uncore, notify_reg, GUC_SEND_TRIGGER);
+
+	err = __intel_wait_for_register_fw(uncore, send_reg,
+					   GUC_HXG_MSG_0_ORIGIN,
+					   FIELD_PREP(GUC_HXG_MSG_0_ORIGIN,
+						      GUC_HXG_ORIGIN_GUC),
+					   1000, 0, &response);
+	if (unlikely(err)) {
+		drm_dbg(&gt->i915->drm, "VF reset not completed (%pe)\n",
+			ERR_PTR(err));
+	} else if (FIELD_GET(GUC_HXG_MSG_0_TYPE, response) != GUC_HXG_TYPE_RESPONSE_SUCCESS) {
+		drm_dbg(&gt->i915->drm, "VF reset not completed (%#x)\n",
+			response);
+	}
+	return 0;
+}
+
 static int mock_reset(struct intel_gt *gt,
 		      intel_engine_mask_t mask,
 		      unsigned int retry)
@@ -676,6 +720,8 @@ static reset_func intel_get_gpu_reset(const struct intel_gt *gt)
 
 	if (is_mock_gt(gt))
 		return mock_reset;
+	else if (IS_SRIOV_VF(i915))
+		return gen12_vf_reset;
 	else if (GRAPHICS_VER(i915) >= 8)
 		return gen8_reset_engines;
 	else if (GRAPHICS_VER(i915) >= 6)
@@ -702,7 +748,7 @@ static int __reset_guc(struct intel_gt *gt)
 
 static bool needs_wa_14015076503(struct intel_gt *gt, intel_engine_mask_t engine_mask)
 {
-	if (!IS_METEORLAKE(gt->i915) || !HAS_ENGINE(gt, GSC0))
+	if (!IS_METEORLAKE(gt->i915) || !HAS_ENGINE(gt, GSC0) || IS_SRIOV_VF(gt->i915))
 		return false;
 
 	if (!__HAS_ENGINE(engine_mask, GSC0))
diff --git a/drivers/gpu/drm/i915/gt/intel_rps.c b/drivers/gpu/drm/i915/gt/intel_rps.c
index f5d7b5126433..da744c1f4a0a 100644
--- a/drivers/gpu/drm/i915/gt/intel_rps.c
+++ b/drivers/gpu/drm/i915/gt/intel_rps.c
@@ -2008,6 +2008,9 @@ void intel_rps_init(struct intel_rps *rps)
 {
 	struct drm_i915_private *i915 = rps_to_i915(rps);
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (rps_uses_slpc(rps))
 		return;
 
@@ -2068,6 +2071,9 @@ void intel_rps_init(struct intel_rps *rps)
 
 void intel_rps_sanitize(struct intel_rps *rps)
 {
+	if (IS_SRIOV_VF(rps_to_i915(rps)))
+		return;
+
 	if (rps_uses_slpc(rps))
 		return;
 
diff --git a/drivers/gpu/drm/i915/gt/intel_sa_media.c b/drivers/gpu/drm/i915/gt/intel_sa_media.c
index e8f3d18c12b8..9522e2aeea27 100644
--- a/drivers/gpu/drm/i915/gt/intel_sa_media.c
+++ b/drivers/gpu/drm/i915/gt/intel_sa_media.c
@@ -8,6 +8,7 @@
 #include "i915_drv.h"
 #include "gt/intel_gt.h"
 #include "gt/intel_sa_media.h"
+#include "gt/iov/intel_iov.h"
 
 int intel_sa_mediagt_setup(struct intel_gt *gt, phys_addr_t phys_addr,
 			   u32 gsi_offset)
@@ -24,6 +25,7 @@ int intel_sa_mediagt_setup(struct intel_gt *gt, phys_addr_t phys_addr,
 	gt->irq_lock = to_gt(i915)->irq_lock;
 	intel_gt_common_init_early(gt);
 	intel_uncore_init_early(uncore, gt);
+	intel_iov_init_early(&gt->iov);
 
 	/*
 	 * Standalone media shares the general MMIO space with the primary
diff --git a/drivers/gpu/drm/i915/gt/intel_sseu_debugfs.c b/drivers/gpu/drm/i915/gt/intel_sseu_debugfs.c
index c2ee5e1826b5..39ff45906872 100644
--- a/drivers/gpu/drm/i915/gt/intel_sseu_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/intel_sseu_debugfs.c
@@ -241,6 +241,9 @@ int intel_sseu_status(struct seq_file *m, struct intel_gt *gt)
 	seq_puts(m, "SSEU Device Info\n");
 	i915_print_sseu_info(m, true, HAS_POOLED_EU(i915), &info->sseu);
 
+	if (IS_SRIOV_VF(i915))
+		return 0;
+
 	seq_puts(m, "SSEU Device Status\n");
 
 	sseu = kzalloc(sizeof(*sseu), GFP_KERNEL);
diff --git a/drivers/gpu/drm/i915/gt/intel_workarounds.c b/drivers/gpu/drm/i915/gt/intel_workarounds.c
index 25c90bb847aa..4417b20a4d4d 100644
--- a/drivers/gpu/drm/i915/gt/intel_workarounds.c
+++ b/drivers/gpu/drm/i915/gt/intel_workarounds.c
@@ -901,6 +901,9 @@ __intel_engine_init_ctx_wa(struct intel_engine_cs *engine,
 {
 	struct drm_i915_private *i915 = engine->i915;
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	wa_init_start(wal, engine->gt, name, engine->name);
 
 	/* Applies to all engines */
@@ -1780,6 +1783,9 @@ void intel_gt_init_workarounds(struct intel_gt *gt)
 {
 	struct i915_wa_list *wal = &gt->wa_list;
 
+	if (IS_SRIOV_VF(gt->i915))
+		return;
+
 	wa_init_start(wal, gt, "GT", "global");
 	gt_init_workarounds(gt, wal);
 	wa_init_finish(wal);
@@ -1881,6 +1887,9 @@ static bool wa_list_verify(struct intel_gt *gt,
 	unsigned int i;
 	bool ok = true;
 
+	if (!wal->count)
+		return 0;
+
 	fw = wal_get_fw_for_rmw(uncore, wal);
 
 	intel_gt_mcr_lock(gt, &flags);
@@ -2223,6 +2232,9 @@ void intel_engine_init_whitelist(struct intel_engine_cs *engine)
 	struct drm_i915_private *i915 = engine->i915;
 	struct i915_wa_list *w = &engine->whitelist;
 
+	if (IS_SRIOV_VF(engine->i915))
+		return;
+
 	wa_init_start(w, engine->gt, "whitelist", engine->name);
 
 	if (IS_METEORLAKE(i915))
@@ -3070,6 +3082,9 @@ void intel_engine_init_workarounds(struct intel_engine_cs *engine)
 {
 	struct i915_wa_list *wal = &engine->wa_list;
 
+	if (IS_SRIOV_VF(engine->i915))
+		return;
+
 	wa_init_start(wal, engine->gt, "engine", engine->name);
 	engine_init_workarounds(engine, wal);
 	wa_init_finish(wal);
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_abi.h
new file mode 100644
index 000000000000..05d34438e507
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_abi.h
@@ -0,0 +1,121 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_ACTIONS_ABI_H_
+#define _ABI_IOV_ACTIONS_ABI_H_
+
+#include "iov_messages_abi.h"
+
+/**
+ * DOC: IOV Actions
+ *
+ * TBD
+ */
+
+/**
+ * DOC: VF2PF_HANDSHAKE
+ *
+ * This `IOV Message`_ is used by the VF to establish ABI version with the PF.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`IOV_ACTION_VF2PF_HANDSHAKE` = 0x0001              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:16 | **MAJOR** - requested major version of the VFPF interface    |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | **MINOR** - requested minor version of the VFPF interface    |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = MBZ                                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:16 | **MAJOR** - agreed major version of the VFPF interface       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | **MINOR** - agreed minor version of the VFPF interface       |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define IOV_ACTION_VF2PF_HANDSHAKE			0x0001
+
+#define VF2PF_HANDSHAKE_REQUEST_MSG_LEN			2u
+#define VF2PF_HANDSHAKE_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+#define VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR		(0xffff << 16)
+#define VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR		(0xffff << 0)
+
+#define VF2PF_HANDSHAKE_RESPONSE_MSG_LEN		2u
+#define VF2PF_HANDSHAKE_RESPONSE_MSG_0_MBZ		GUC_HXG_RESPONSE_MSG_0_DATA0
+#define VF2PF_HANDSHAKE_RESPONSE_MSG_1_MAJOR		(0xffff << 16)
+#define VF2PF_HANDSHAKE_RESPONSE_MSG_1_MINOR		(0xffff << 0)
+
+/**
+ * DOC: VF2PF_QUERY_RUNTIME
+ *
+ * This `IOV Message`_ is used by the VF to query values of runtime registers.
+ *
+ * VF provides @START index to the requested register entry.
+ * VF can use @LIMIT to limit number of returned register entries.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = **LIMIT** - limit number of returned entries         |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`IOV_ACTION_VF2PF_QUERY_RUNTIME` = 0x0101          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | DATA1 = **START** - index of the first requested entry       |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = **COUNT** - number of entries included in response   |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | DATA1 = **REMAINING** - number of remaining entries          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | DATA2 = **REG_OFFSET** - offset of register[START]           |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | DATA3 = **REG_VALUE** - value of register[START]             |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   |       |                                                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |n-1|  31:0 | REG_OFFSET - offset of register[START + x]                   |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | n |  31:0 | REG_VALUE - value of register[START + x]                     |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define IOV_ACTION_VF2PF_QUERY_RUNTIME			0x0101
+
+#define VF2PF_QUERY_RUNTIME_REQUEST_MSG_LEN		2u
+#define VF2PF_QUERY_RUNTIME_REQUEST_MSG_0_LIMIT		GUC_HXG_REQUEST_MSG_0_DATA0
+#define VF2PF_QUERY_RUNTIME_REQUEST_MSG_1_START		GUC_HXG_REQUEST_MSG_n_DATAn
+
+#define VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN	(GUC_HXG_MSG_MIN_LEN + 1u)
+#define VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MAX_LEN	20 // FIXME RELAY_PAYLOAD_MAX_SIZE
+#define VF2PF_QUERY_RUNTIME_RESPONSE_MSG_0_COUNT	GUC_HXG_RESPONSE_MSG_0_DATA0
+#define VF2PF_QUERY_RUNTIME_RESPONSE_MSG_1_REMAINING	GUC_HXG_RESPONSE_MSG_n_DATAn
+#define VF2PF_QUERY_RUNTIME_RESPONSE_DATAn_REG_OFFSETx	GUC_HXG_RESPONSE_MSG_n_DATAn
+#define VF2PF_QUERY_RUNTIME_RESPONSE_DATAn_REG_VALUEx	GUC_HXG_RESPONSE_MSG_n_DATAn
+
+#endif /* _ABI_IOV_ACTIONS_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_debug_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_debug_abi.h
new file mode 100644
index 000000000000..7ce1885cc349
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_debug_abi.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_ACTIONS_DEBUG_ABI_H_
+#define _ABI_IOV_ACTIONS_DEBUG_ABI_H_
+
+#include "iov_actions_abi.h"
+
+/**
+ * DOC: IOV debug actions
+ *
+ * These range of IOV action codes is reserved for debug and may only be
+ * used on selected debug configs.
+ *
+ *  _`IOV_ACTION_DEBUG_ONLY_START` = 0xDEB0
+ *  _`IOV_ACTION_DEBUG_ONLY_END` = 0xDEFF
+ */
+
+#define IOV_ACTION_DEBUG_ONLY_START	0xDEB0
+#define IOV_ACTION_DEBUG_ONLY_END	0xDEFF
+
+#endif /* _ABI_IOV_ACTIONS_DEBUG_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_mmio_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_mmio_abi.h
new file mode 100644
index 000000000000..c9cbef90d364
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_mmio_abi.h
@@ -0,0 +1,130 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_ACTIONS_MMIO_ABI_H_
+#define _ABI_IOV_ACTIONS_MMIO_ABI_H_
+
+#include "iov_messages_abi.h"
+
+/**
+ * DOC: IOV MMIO Opcodes
+ *
+ *  + _`IOV_OPCODE_VF2PF_MMIO_HANDSHAKE` = 0x01
+ *  + _`IOV_OPCODE_VF2PF_MMIO_GET_RUNTIME` = 0x10
+ */
+
+/**
+ * DOC: VF2PF_MMIO_HANDSHAKE
+ *
+ * This VF2PF MMIO message is used by the VF to establish ABI version with the PF.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | MAGIC - see VF2GUC_MMIO_RELAY_SERVICE_                       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 23:16 | OPCODE = IOV_OPCODE_VF2PF_MMIO_HANDSHAKE_                    |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = GUC_ACTION_VF2GUC_MMIO_RELAY_SERVICE_               |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:16 | **MAJOR** - requested major version of the VFPF interface    |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | **MINOR** - requested minor version of the VFPF interface    |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:16 | **MAJOR** - agreed major version of the VFPF interface       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | **MINOR** - agreed minor version of the VFPF interface       |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define IOV_OPCODE_VF2PF_MMIO_HANDSHAKE			0x01
+
+#define VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_LEN		4u
+#define VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_1_MAJOR	(0xffff << 16)
+#define VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_1_MINOR	(0xffff << 0)
+
+#define VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_LEN		4u
+#define VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_0_MBZ		GUC_HXG_RESPONSE_MSG_0_DATA0
+#define VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_1_MAJOR	(0xffff << 16)
+#define VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_1_MINOR	(0xffff << 0)
+
+/**
+ * DOC: VF2PF_MMIO_GET_RUNTIME
+ *
+ * This opcode can be used by VFs to request values of some runtime registers
+ * (fuses) that are not directly available for VFs.
+ *
+ * Only registers that are on the allow-list maintained by the PF are available.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | MAGIC - see VF2GUC_MMIO_RELAY_SERVICE_                       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 23:16 | OPCODE = IOV_OPCODE_VF2PF_MMIO_GET_RUNTIME_                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = GUC_ACTION_VF2GUC_MMIO_RELAY_SERVICE_               |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **OFFSET1** - offset of register1 (can't be zero)            |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **OFFSET2** - offset of register2 (or zero)                  |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **OFFSET3** - offset of register3 (or zero)                  |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | MAGIC - see _VF2GUC_MMIO_RELAY_SERVICE                       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  23:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VALUE1** - value of the register1                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **VALUE2** - value of the register2 (or zero)                |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **VALUE3** - value of the register3 (or zero)                |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define IOV_OPCODE_VF2PF_MMIO_GET_RUNTIME		0x10
+
+#define VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_LEN		4u
+#define VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_n_OFFSETn	GUC_HXG_REQUEST_MSG_n_DATAn
+#define VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_NUM_OFFSET	3u
+
+#define VF2PF_MMIO_GET_RUNTIME_RESPONSE_MSG_LEN		4u
+#define VF2PF_MMIO_GET_RUNTIME_RESPONSE_MSG_n_VALUEn	GUC_HXG_RESPONSE_MSG_n_DATAn
+#define VF2PF_MMIO_GET_RUNTIME_RESPONSE_MSG_NUM_VALUE	3u
+
+#endif /* _ABI_IOV_ACTIONS_MMIO_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_selftest_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_selftest_abi.h
new file mode 100644
index 000000000000..3bd5da099dcb
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_actions_selftest_abi.h
@@ -0,0 +1,55 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_ACTIONS_SELFTEST_ABI_H_
+#define _ABI_IOV_ACTIONS_SELFTEST_ABI_H_
+
+#include "iov_actions_debug_abi.h"
+
+/**
+ * DOC: IOV_ACTION_SELFTEST_RELAY
+ *
+ * This special `IOV Action`_ is used to selftest `IOV communication`_.
+ *
+ * SELFTEST_RELAY_OPCODE_NOP_ will return no data.
+ * SELFTEST_RELAY_OPCODE_ECHO_ will return same data as received.
+ * SELFTEST_RELAY_OPCODE_FAIL_ will always fail with error.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_ or GUC_HXG_TYPE_FAST_REQUEST_   |
+ *  |   |       | or GUC_HXG_TYPE_EVENT_                                       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | **OPCODE**                                                   |
+ *  |   |       |    - _`SELFTEST_RELAY_OPCODE_NOP` = 0x0                      |
+ *  |   |       |    - _`SELFTEST_RELAY_OPCODE_ECHO` = 0xE                     |
+ *  |   |       |    - _`SELFTEST_RELAY_OPCODE_FAIL` = 0xF                     |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`IOV_ACTION_SELFTEST_RELAY`                        |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|  31:0 | **PAYLOAD** optional                                         |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = MBZ                                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|  31:0 | DATAn = only for **OPCODE** SELFTEST_RELAY_OPCODE_ECHO       |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define IOV_ACTION_SELFTEST_RELAY	(IOV_ACTION_DEBUG_ONLY_START + 1)
+#define   SELFTEST_RELAY_OPCODE_NOP		0x0
+#define   SELFTEST_RELAY_OPCODE_ECHO		0xE
+#define   SELFTEST_RELAY_OPCODE_FAIL		0xF
+
+#endif /* _ABI_IOV_ACTIONS_SELFTEST_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_communication_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_communication_abi.h
new file mode 100644
index 000000000000..12863798d66f
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_communication_abi.h
@@ -0,0 +1,116 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_COMMUNICATION_ABI_H_
+#define _ABI_IOV_COMMUNICATION_ABI_H_
+
+#include "gt/uc/abi/guc_communication_ctb_abi.h"
+
+/**
+ * DOC: IOV Communication
+ *
+ * The communication between VFs and PF is based on the relay messages with GuC
+ * acting a proxy agent. All relay messages are defined as `CTB HXG Message`_.
+ * The `IOV Message`_ is embedded in these messages as opaque payload.
+ *
+ * To send `IOV Message`_ to the PF, VFs are using `VF2GUC_RELAY_TO_PF`_
+ * that takes the message identifier as additional parameter.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB Message`_                                                          |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `CTB HXG Message`_                                                  |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   | `HXG Message`_                                                   |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   |  `HXG Request`_                                              |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +---+----------------------------------------------------------+
+ *  |   |   |   |   |  `VF2GUC_RELAY_TO_PF`_                                   |
+ *  |   |   |   |   |                                                          |
+ *  |   |   |   |   +------------+---------------------------------------------+
+ *  |   |   |   |   |            |              +----------------------------+ |
+ *  |   |   |   |   | Message ID |              |     `IOV Message`_         | |
+ *  |   |   |   |   |            |              +----------------------------+ |
+ *  +---+---+---+---+------------+---------------------------------------------+
+ *
+ * The `IOV Message`_ from a VF is delivered to the PF in `GUC2PF_RELAY_FROM_VF`_.
+ * This message contains also identifier of the origin VF and message identifier
+ * that is used in any replies.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB Message`_                                                          |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `CTB HXG Message`_                                                  |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   | `HXG Message`_                                                   |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   |  `HXG Request`_                                              |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +---+----------------------------------------------------------+
+ *  |   |   |   |   |  `GUC2PF_RELAY_FROM_VF`_                                 |
+ *  |   |   |   |   |                                                          |
+ *  |   |   |   |   +------------+------------+--------------------------------+
+ *  |   |   |   |   |            |            | +----------------------------+ |
+ *  |   |   |   |   |   Origin   | Message ID | |     `IOV Message`_         | |
+ *  |   |   |   |   |            |            | +----------------------------+ |
+ *  +---+---+---+---+------------+------------+--------------------------------+
+ *
+ * To send `IOV Message`_ to the particular VF, PF is using `PF2GUC_RELAY_TO_VF`_
+ * that takes target VF identifier and the message identifier.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB Message`_                                                          |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `CTB HXG Message`_                                                  |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   | `HXG Message`_                                                   |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   |  `HXG Request`_                                              |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +---+----------------------------------------------------------+
+ *  |   |   |   |   |  `PF2GUC_RELAY_TO_VF`_                                   |
+ *  |   |   |   |   |                                                          |
+ *  |   |   |   |   +------------+------------+--------------------------------+
+ *  |   |   |   |   |            |            | +----------------------------+ |
+ *  |   |   |   |   |   Target   | Message ID | |     `IOV Message`_         | |
+ *  |   |   |   |   |            |            | +----------------------------+ |
+ *  +---+---+---+---+------------+------------+--------------------------------+
+ *
+ * The `IOV Message`_ from the PF is delivered to VFs in `GUC2VF_RELAY_FROM_PF`_.
+ * The message identifier is used to match IOV requests/response messages.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB Message`_                                                          |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `CTB HXG Message`_                                                  |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   | `HXG Message`_                                                   |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   |  `HXG Request`_                                              |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +---+----------------------------------------------------------+
+ *  |   |   |   |   |  `GUC2VF_RELAY_FROM_PF`_                                 |
+ *  |   |   |   |   |                                                          |
+ *  |   |   |   |   +------------+---------------------------------------------+
+ *  |   |   |   |   |            |              +----------------------------+ |
+ *  |   |   |   |   | Message ID |              |     `IOV Message`_         | |
+ *  |   |   |   |   |            |              +----------------------------+ |
+ *  +---+---+---+---+------------+---------------------------------------------+
+ */
+
+#endif /* _ABI_IOV_COMMUNICATION_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_communication_mmio_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_communication_mmio_abi.h
new file mode 100644
index 000000000000..c62012f86be4
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_communication_mmio_abi.h
@@ -0,0 +1,150 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_COMMUNICATION_MMIO_ABI_H_
+#define _ABI_IOV_COMMUNICATION_MMIO_ABI_H_
+
+#include "gt/uc/abi/guc_communication_ctb_abi.h"
+
+/**
+ * DOC: IOV MMIO Communication
+ *
+ * The communication between VF and PF is based on GuC acting as a proxy agent.
+ *
+ * In case when `CTB based communication`_ is not yet available on VF side,
+ * VF may initiate IOV relay using `GuC MMIO based communication`_.
+ *
+ * The `MMIO HXG Message`_ `VF2GUC_MMIO_RELAY_SERVICE`_ allows the VF to pass
+ * @OPCODE and @DATA1..@DATA3 as a IOV relay data. Additional @MAGIC field will
+ * be returned in any future replies and will help detect communication mismatch.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `MMIO HXG Message`_                                                     |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `HXG Request`_                                                      |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   |  `VF2GUC_MMIO_RELAY_SERVICE`_                                    |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   | MAGIC                                                        |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | OPCODE                                                       |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | DATA1..DATA3                                                 |
+ *  |   |   |   |                                                              |
+ *  +---+---+---+--------------------------------------------------------------+
+ *
+ * The IOV relay data together with requesting VF identifier is then send by the
+ * GuC to the PF using `CTB HXG Message`_ GUC2PF_MMIO_RELAY_SERVICE_ for further
+ * processing.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB HXG Message`_                                                      |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `HXG Event`_                                                        |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   |  `GUC2PF_MMIO_RELAY_SERVICE`_                                    |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   | VFID                                                         |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | MAGIC                                                        |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | OPCODE                                                       |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | DATA1..DATA3                                                 |
+ *  |   |   |   |                                                              |
+ *  +---+---+---+--------------------------------------------------------------+
+ *
+ * After completing processing of the IOV relay data PF shall reply to the VF
+ * using `CTB HXG Message`_ `PF2GUC_MMIO_RELAY_SUCCESS`_
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB HXG Message`_                                                      |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `HXG Request`_                                                      |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   |  `PF2GUC_MMIO_RELAY_SUCCESS`_                                    |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   | VFID                                                         |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | MAGIC                                                        |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | DATA0..DATA3                                                 |
+ *  |   |   |   |                                                              |
+ *  +---+---+---+--------------------------------------------------------------+
+ *
+ * or `CTB HXG Message`_ `PF2GUC_MMIO_RELAY_FAILURE`_
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `CTB HXG Message`_                                                      |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   |  `HXG Request`_                                                      |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   |  `PF2GUC_MMIO_RELAY_FAILURE`_                                    |
+ *  |   |   |                                                                  |
+ *  |   |   +---+--------------------------------------------------------------+
+ *  |   |   |   | VFID                                                         |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | MAGIC                                                        |
+ *  |   |   |   |                                                              |
+ *  |   |   |   +--------------------------------------------------------------+
+ *  |   |   |   | FAULT                                                        |
+ *  |   |   |   |                                                              |
+ *  +---+---+---+--------------------------------------------------------------+
+ *
+ * Above PF messages will be converted by the GuC back to `MMIO HXG Message`_
+ * `HXG Response`_
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `MMIO HXG Message`_                                                     |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   | `HXG Response`_                                                      |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   |  MAGIC                                                           |
+ *  |   |   |                                                                  |
+ *  |   |   +------------------------------------------------------------------+
+ *  |   |   |  DATA0..DATA3                                                    |
+ *  |   |   |                                                                  |
+ *  +---+---+------------------------------------------------------------------+
+ *
+ * or `MMIO HXG Message`_ `HXG Failure`_
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `MMIO HXG Message`_                                                     |
+ *  |                                                                          |
+ *  +===+======================================================================+
+ *  |   | `HXG Failure`_                                                       |
+ *  |   |                                                                      |
+ *  |   +---+------------------------------------------------------------------+
+ *  |   |   |  MAGIC (part of the HINT)                                        |
+ *  |   |   |                                                                  |
+ *  |   |   +------------------------------------------------------------------+
+ *  |   |   |  FAULT (part of the ERROR)                                       |
+ *  |   |   |                                                                  |
+ *  +---+---+------------------------------------------------------------------+
+ *
+ */
+
+#endif /* _ABI_IOV_COMMUNICATION_MMIO_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_errors_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_errors_abi.h
new file mode 100644
index 000000000000..3cc53bdb746f
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_errors_abi.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_ERRORS_ABI_H_
+#define _ABI_IOV_ERRORS_ABI_H_
+
+/**
+ * DOC: IOV Error Codes
+ *
+ * IOV uses error codes that mostly match errno values.
+ */
+
+#define IOV_ERROR_UNDISCLOSED			0
+#define IOV_ERROR_OPERATION_NOT_PERMITTED	1	/* EPERM */
+#define IOV_ERROR_PERMISSION_DENIED		13	/* EACCES */
+#define IOV_ERROR_INVALID_ARGUMENT		22	/* EINVAL */
+#define IOV_ERROR_INVALID_REQUEST_CODE		56	/* EBADRQC */
+#define IOV_ERROR_NO_DATA_AVAILABLE		61	/* ENODATA */
+#define IOV_ERROR_PROTOCOL_ERROR		71	/* EPROTO */
+#define IOV_ERROR_MESSAGE_SIZE			90	/* EMSGSIZE */
+
+#endif /* _ABI_IOV_ERRORS_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_messages_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_messages_abi.h
new file mode 100644
index 000000000000..08c182926a12
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_messages_abi.h
@@ -0,0 +1,30 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_MESSAGES_ABI_H_
+#define _ABI_IOV_MESSAGES_ABI_H_
+
+#include "gt/uc/abi/guc_messages_abi.h"
+
+/**
+ * DOC: IOV Message
+ *
+ * `IOV Message`_ is used in `IOV Communication`_.
+ * Format of the `IOV Message`_ follows format of the generic `HXG Message`_.
+ *
+ *  +--------------------------------------------------------------------------+
+ *  |  `IOV Message`_                                                          |
+ *  +==========================================================================+
+ *  |  `HXG Message`_                                                          |
+ *  +--------------------------------------------------------------------------+
+ *
+ * In particular format of the _`IOV Request` is same as the `HXG Request`_.
+ * Supported actions codes are listed in `IOV Actions`_.
+ *
+ * Format of the _`IOV Failure` is same as `HXG Failure`_.
+ * See `IOV Error Codes`_ for possible error codes.
+ */
+
+#endif /* _ABI_IOV_MESSAGES_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/abi/iov_version_abi.h b/drivers/gpu/drm/i915/gt/iov/abi/iov_version_abi.h
new file mode 100644
index 000000000000..fe3c14bb3167
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/abi/iov_version_abi.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_IOV_VERSION_ABI_H_
+#define _ABI_IOV_VERSION_ABI_H_
+
+#define IOV_VERSION_LATEST_MAJOR		1u
+#define IOV_VERSION_LATEST_MINOR		0u
+/* XXX In future we need to have major.minor base versions per platform */
+#define IOV_VERSION_BASE_MAJOR			1u
+#define IOV_VERSION_BASE_MINOR			0u
+
+#endif /* _ABI_IOV_VERSION_ABI_H_ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov.c b/drivers/gpu/drm/i915/gt/iov/intel_iov.c
new file mode 100644
index 000000000000..b9a1cdf312f3
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov.c
@@ -0,0 +1,301 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "intel_iov.h"
+#include "intel_iov_memirq.h"
+#include "intel_iov_provisioning.h"
+#include "intel_iov_query.h"
+#include "intel_iov_relay.h"
+#include "intel_iov_service.h"
+#include "intel_iov_state.h"
+#include "intel_iov_utils.h"
+
+#include "i915_reg.h"
+
+/**
+ * intel_iov_init_early - Prepare IOV data.
+ * @iov: the IOV struct
+ *
+ * Early initialization of the I/O Virtualization data.
+ */
+void intel_iov_init_early(struct intel_iov *iov)
+{
+	if (intel_iov_is_pf(iov)) {
+		intel_iov_provisioning_init_early(iov);
+		intel_iov_service_init_early(iov);
+		intel_iov_state_init_early(iov);
+	}
+
+	intel_iov_relay_init_early(&iov->relay);
+}
+
+/**
+ * intel_iov_release - Release IOV data.
+ * @iov: the IOV struct
+ *
+ * This function will release any data prepared in @intel_iov_init_early.
+ */
+void intel_iov_release(struct intel_iov *iov)
+{
+	if (intel_iov_is_pf(iov)) {
+		intel_iov_state_release(iov);
+		intel_iov_service_release(iov);
+		intel_iov_provisioning_release(iov);
+	}
+}
+
+/**
+ * intel_iov_init_mmio - Initialize IOV based on MMIO data.
+ * @iov: the IOV struct
+ *
+ * On VF this function will read SR-IOV INIT message from GuC.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_init_mmio(struct intel_iov *iov)
+{
+	int ret;
+
+	if (intel_iov_is_vf(iov)) {
+		ret = intel_iov_query_bootstrap(iov);
+		if (unlikely(ret))
+			return ret;
+		ret = intel_iov_query_config(iov);
+		if (unlikely(ret))
+			return ret;
+		ret = intel_iov_query_runtime(iov, true);
+		if (unlikely(ret))
+			return ret;
+	}
+
+	return 0;
+}
+
+static int vf_tweak_guc_submission(struct intel_iov *iov)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = intel_guc_submission_limit_ids(iov_to_guc(iov),
+					     iov->vf.config.num_ctxs);
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to limit %s to %u (%pe)\n",
+			  "contexts", iov->vf.config.num_ctxs, ERR_PTR(err));
+
+	return err;
+}
+
+/**
+ * intel_iov_init - Initialize IOV.
+ * @iov: the IOV struct
+ *
+ * On PF this function performs initial partitioning of the shared resources
+ * that can't be changed later (GuC submission contexts) to allow early PF
+ * provisioning.
+ *
+ * On VF this function will initialize data used by memory based interrupts.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_init(struct intel_iov *iov)
+{
+	int err;
+
+	if (intel_iov_is_pf(iov))
+		intel_iov_provisioning_init(iov);
+
+	if (intel_iov_is_vf(iov)) {
+		vf_tweak_guc_submission(iov);
+
+		err = intel_iov_memirq_init(iov);
+		if (unlikely(err))
+			return err;
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_fini - Cleanup IOV.
+ * @iov: the IOV struct
+ *
+ * This function will cleanup any data prepared in @intel_iov_init.
+ */
+void intel_iov_fini(struct intel_iov *iov)
+{
+	if (intel_iov_is_pf(iov))
+		intel_iov_provisioning_fini(iov);
+
+	if (intel_iov_is_vf(iov))
+		intel_iov_memirq_fini(iov);
+}
+
+static int vf_balloon_ggtt(struct intel_iov *iov)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	u64 start, end;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	/*
+	 * We can only use part of the GGTT as allocated by PF.
+	 *
+	 *      0                                      GUC_GGTT_TOP
+	 *      |<------------ Total GGTT size ------------------>|
+	 *
+	 *      |<-- VF GGTT base -->|<- size ->|
+	 *
+	 *      +--------------------+----------+-----------------+
+	 *      |////////////////////|   block  |\\\\\\\\\\\\\\\\\|
+	 *      +--------------------+----------+-----------------+
+	 *
+	 *      |<--- balloon[0] --->|<-- VF -->|<-- balloon[1] ->|
+	 */
+
+	start = 0;
+	end = iov->vf.config.ggtt_base;
+	err = i915_ggtt_balloon(ggtt, start, end, &iov->vf.ggtt_balloon[0]);
+	if (unlikely(err))
+		return err;
+
+	start = iov->vf.config.ggtt_base + iov->vf.config.ggtt_size;
+	end = GUC_GGTT_TOP;
+	err = i915_ggtt_balloon(ggtt, start, end, &iov->vf.ggtt_balloon[1]);
+
+	return err;
+}
+
+static void vf_deballoon_ggtt(struct intel_iov *iov)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+
+	i915_ggtt_deballoon(ggtt, &iov->vf.ggtt_balloon[1]);
+	i915_ggtt_deballoon(ggtt, &iov->vf.ggtt_balloon[0]);
+}
+
+/**
+ * intel_iov_init_ggtt - Initialize GGTT for SR-IOV.
+ * @iov: the IOV struct
+ *
+ * On the VF this function will balloon GGTT to make sure only assigned region
+ * will be used for allocations.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_init_ggtt(struct intel_iov *iov)
+{
+	int err;
+
+	if (intel_iov_is_vf(iov)) {
+		err = vf_balloon_ggtt(iov);
+		if (unlikely(err))
+			return err;
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_fini_ggtt - Cleanup SR-IOV hardware support.
+ * @iov: the IOV struct
+ */
+void intel_iov_fini_ggtt(struct intel_iov *iov)
+{
+	if (intel_iov_is_vf(iov))
+		vf_deballoon_ggtt(iov);
+}
+
+static void pf_enable_ggtt_guest_update(struct intel_iov *iov)
+{
+	struct intel_gt *gt = iov_to_gt(iov);
+
+	/* Guest Direct GGTT Update Enable */
+	intel_uncore_write(gt->uncore, GEN12_VIRTUAL_CTRL_REG,
+			   GEN12_GUEST_GTT_UPDATE_EN);
+}
+
+/**
+ * intel_iov_init_hw - Initialize SR-IOV hardware support.
+ * @iov: the IOV struct
+ *
+ * PF must configure hardware to enable VF's access to GGTT.
+ * PF also updates here runtime info (snapshot of registers values)
+ * that will be shared with VFs.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_init_hw(struct intel_iov *iov)
+{
+	int err;
+
+	if (intel_iov_is_pf(iov)) {
+		pf_enable_ggtt_guest_update(iov);
+		intel_iov_service_update(iov);
+		intel_iov_provisioning_restart(iov);
+		intel_iov_state_reset(iov);
+	}
+
+	if (intel_iov_is_vf(iov)) {
+		err = intel_iov_query_runtime(iov, false);
+		if (unlikely(err))
+			return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_fini_hw - Cleanup data initialized in iov_init_hw.
+ * @iov: the IOV struct
+ */
+void intel_iov_fini_hw(struct intel_iov *iov)
+{
+	if (intel_iov_is_pf(iov))
+		intel_iov_service_reset(iov);
+
+	if (intel_iov_is_vf(iov))
+		intel_iov_query_fini(iov);
+}
+
+/**
+ * intel_iov_init_late - Late initialization of SR-IOV support.
+ * @iov: the IOV struct
+ *
+ * This function continues necessary initialization of the SR-IOV
+ * support in the driver and the hardware.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_init_late(struct intel_iov *iov)
+{
+	struct intel_gt *gt = iov_to_gt(iov);
+
+	if (intel_iov_is_pf(iov)) {
+		/*
+		 * GuC submission must be working on PF to allow VFs to work.
+		 * If unavailable, mark as PF error, but it's safe to continue.
+		 */
+		if (unlikely(!intel_uc_uses_guc_submission(&gt->uc))) {
+			pf_update_status(iov, -EIO, "GuC");
+			return 0;
+		}
+	}
+
+	if (intel_iov_is_vf(iov)) {
+		/*
+		 * If we try to start VF driver without GuC submission enabled,
+		 * then use -EIO error to keep driver alive but without GEM.
+		 */
+		if (!intel_uc_uses_guc_submission(&gt->uc)) {
+			dev_warn(gt->i915->drm.dev, "GuC submission is %s\n",
+				 str_enabled_disabled(false));
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov.h b/drivers/gpu/drm/i915/gt/iov/intel_iov.h
new file mode 100644
index 000000000000..3cc5d18f470c
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_H__
+#define __INTEL_IOV_H__
+
+struct intel_iov;
+
+void intel_iov_init_early(struct intel_iov *iov);
+void intel_iov_release(struct intel_iov *iov);
+
+int intel_iov_init_mmio(struct intel_iov *iov);
+int intel_iov_init_ggtt(struct intel_iov *iov);
+void intel_iov_fini_ggtt(struct intel_iov *iov);
+int intel_iov_init(struct intel_iov *iov);
+void intel_iov_fini(struct intel_iov *iov);
+
+int intel_iov_init_hw(struct intel_iov *iov);
+void intel_iov_fini_hw(struct intel_iov *iov);
+int intel_iov_init_late(struct intel_iov *iov);
+
+#endif /* __INTEL_IOV_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c
new file mode 100644
index 000000000000..884a39815d03
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c
@@ -0,0 +1,116 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include <drm/drm_print.h>
+
+#include "gt/intel_gt_debugfs.h"
+#include "gt/intel_gt.h"
+#include "intel_iov.h"
+#include "intel_iov_utils.h"
+#include "intel_iov_debugfs.h"
+#include "intel_iov_event.h"
+#include "intel_iov_provisioning.h"
+#include "intel_iov_query.h"
+
+static bool eval_is_pf(void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)data)->iov;
+
+	return intel_iov_is_pf(iov);
+}
+
+static bool eval_is_vf(void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)data)->iov;
+
+	return intel_iov_is_vf(iov);
+}
+
+static int ggtt_provisioning_show(struct seq_file *m, void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)m->private)->iov;
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	return intel_iov_provisioning_print_ggtt(iov, &p);
+}
+DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(ggtt_provisioning);
+
+static int ggtt_available_show(struct seq_file *m, void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)m->private)->iov;
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	return intel_iov_provisioning_print_available_ggtt(iov, &p);
+}
+DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(ggtt_available);
+
+static int ctxs_provisioning_show(struct seq_file *m, void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)m->private)->iov;
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	return intel_iov_provisioning_print_ctxs(iov, &p);
+}
+DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(ctxs_provisioning);
+
+static int dbs_provisioning_show(struct seq_file *m, void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)m->private)->iov;
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	return intel_iov_provisioning_print_dbs(iov, &p);
+}
+DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(dbs_provisioning);
+
+static int adverse_events_show(struct seq_file *m, void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)m->private)->iov;
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	return intel_iov_event_print_events(iov, &p);
+}
+DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(adverse_events);
+
+static int vf_self_config_show(struct seq_file *m, void *data)
+{
+	struct intel_iov *iov = &((struct intel_gt *)m->private)->iov;
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	intel_iov_query_print_config(iov, &p);
+	return 0;
+}
+DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(vf_self_config);
+
+/**
+ * intel_iov_debugfs_register - Register IOV specific entries in GT debugfs.
+ * @iov: the IOV struct
+ * @root: the GT debugfs root directory entry
+ *
+ * Some IOV entries are GT related so better to show them under GT debugfs.
+ */
+void intel_iov_debugfs_register(struct intel_iov *iov, struct dentry *root)
+{
+	static const struct intel_gt_debugfs_file files[] = {
+		{ "ggtt_provisioning", &ggtt_provisioning_fops, eval_is_pf },
+		{ "ggtt_available", &ggtt_available_fops, eval_is_pf },
+		{ "contexts_provisioning", &ctxs_provisioning_fops, eval_is_pf },
+		{ "doorbells_provisioning", &dbs_provisioning_fops, eval_is_pf },
+		{ "adverse_events", &adverse_events_fops, eval_is_pf },
+		{ "self_config", &vf_self_config_fops, eval_is_vf },
+	};
+	struct dentry *dir;
+
+	if (unlikely(!root))
+		return;
+
+	if (!intel_iov_is_enabled(iov))
+		return;
+
+	dir = debugfs_create_dir("iov", root);
+	if (IS_ERR(root))
+		return;
+
+	intel_gt_debugfs_register_files(dir, files, ARRAY_SIZE(files), iov_to_gt(iov));
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.h
new file mode 100644
index 000000000000..d90567ff74e7
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.h
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_DEBUGFS_H__
+#define __INTEL_IOV_DEBUGFS_H__
+
+struct intel_iov;
+struct dentry;
+
+void intel_iov_debugfs_register(struct intel_iov *iov, struct dentry *root);
+
+#endif /* __INTEL_IOV_DEBUGFS_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_event.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_event.c
new file mode 100644
index 000000000000..4866b4b2da41
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_event.c
@@ -0,0 +1,174 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "intel_iov.h"
+#include "intel_iov_event.h"
+#include "intel_iov_utils.h"
+
+/**
+ * intel_iov_event_reset - Reset event data for VF.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ *
+ * This function is for PF only.
+ */
+void intel_iov_event_reset(struct intel_iov *iov, u32 vfid)
+{
+	int e;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+
+	if (unlikely(!iov->pf.state.data))
+		return;
+
+	for (e = 0; e < IOV_THRESHOLD_MAX; e++)
+		iov->pf.state.data[vfid].adverse_events[e] = 0;
+}
+
+static int threshold_key_to_enum(u32 threshold)
+{
+	switch (threshold) {
+#define __iov_threshold_key_to_enum(K, ...) \
+	case GUC_KLV_VF_CFG_THRESHOLD_##K##_KEY: return IOV_THRESHOLD_##K;
+	IOV_THRESHOLDS(__iov_threshold_key_to_enum)
+#undef __iov_threshold_key_to_enum
+	}
+	return -1; /* not found */
+}
+
+static void pf_update_event_counter(struct intel_iov *iov, u32 vfid,
+				    enum intel_iov_threshold e)
+{
+	++iov->pf.state.data[vfid].adverse_events[e];
+}
+
+#define I915_UEVENT_THRESHOLD_EXCEEDED	"THRESHOLD_EXCEEDED"
+#define I915_UEVENT_THRESHOLD_ID	"THRESHOLD_ID"
+#define I915_UEVENT_VFID		"VF_ID"
+
+static void pf_emit_threshold_uevent(struct intel_iov *iov, u32 vfid, u32 threshold)
+{
+	struct kobject *kobj = &iov_to_i915(iov)->drm.primary->kdev->kobj;
+	char *envp[] = {
+		I915_UEVENT_THRESHOLD_EXCEEDED"=1",
+		kasprintf(GFP_KERNEL, I915_UEVENT_THRESHOLD_ID"=%#x", threshold),
+		kasprintf(GFP_KERNEL, I915_UEVENT_VFID"=%u", vfid),
+		NULL,
+	};
+
+	kobject_uevent_env(kobj, KOBJ_CHANGE, envp);
+
+	kfree(envp[1]);
+	kfree(envp[2]);
+}
+
+static void pf_emit_log_message(struct intel_iov *iov, u32 vfid, int e)
+{
+	dev_info_ratelimited(iov_to_dev(iov), "VF%u has exceeded '%s' threshold\n",
+			     vfid, intel_iov_threshold_to_string(e));
+}
+
+static int pf_handle_vf_threshold_event(struct intel_iov *iov, u32 vfid, u32 threshold)
+{
+	int e = threshold_key_to_enum(threshold);
+
+	if (unlikely(!vfid || vfid > pf_get_totalvfs(iov)))
+		return -EINVAL;
+
+	if (unlikely(GEM_WARN_ON(e < 0)))
+		return -EINVAL;
+
+	IOV_DEBUG(iov, "VF%u threshold %04x\n", vfid, threshold);
+
+	pf_update_event_counter(iov, vfid, e);
+
+	if (IS_ENABLED(CONFIG_DRM_I915_SELFTEST))
+		pf_emit_threshold_uevent(iov, vfid, threshold);
+
+	pf_emit_log_message(iov, vfid, e);
+
+	return 0;
+}
+
+/**
+ * intel_iov_event_process_guc2pf - Handle adverse event notification from GuC.
+ * @iov: the IOV struct
+ * @msg: message from the GuC
+ * @len: length of the message
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_event_process_guc2pf(struct intel_iov *iov,
+				   const u32 *msg, u32 len)
+{
+	u32 vfid;
+	u32 threshold;
+
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) != GUC_ACTION_GUC2PF_ADVERSE_EVENT);
+
+	if (unlikely(!intel_iov_is_pf(iov)))
+		return -EPROTO;
+
+	if (unlikely(FIELD_GET(GUC2PF_ADVERSE_EVENT_EVENT_MSG_0_MBZ, msg[0])))
+		return -EPFNOSUPPORT;
+
+	if (unlikely(len != GUC2PF_ADVERSE_EVENT_EVENT_MSG_LEN))
+		return -EPROTO;
+
+	vfid = FIELD_GET(GUC2PF_ADVERSE_EVENT_EVENT_MSG_1_VFID, msg[1]);
+	threshold = FIELD_GET(GUC2PF_ADVERSE_EVENT_EVENT_MSG_2_THRESHOLD, msg[2]);
+
+	return pf_handle_vf_threshold_event(iov, vfid, threshold);
+}
+
+/**
+ * intel_iov_event_print_events - Print adverse events.
+ * @iov: the IOV struct
+ * @p: the DRM printer
+ *
+ * Print adverse events counters for all VFs.
+ * VFs with no events are ignored.
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_event_print_events(struct intel_iov *iov, struct drm_printer *p)
+{
+	unsigned int n, total_vfs = pf_get_totalvfs(iov);
+	const struct intel_iov_data *data;
+	int e;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (unlikely(!iov->pf.state.data))
+		return -ENODATA;
+
+	for (n = 1; n <= total_vfs; n++) {
+		data = &iov->pf.state.data[n];
+
+		for (e = 0; e < IOV_THRESHOLD_MAX; e++)
+			if (data->adverse_events[e])
+				break;
+		if (e >= IOV_THRESHOLD_MAX)
+			continue;
+
+#define __format_iov_threshold(...) "%s:%u "
+#define __value_iov_threshold(K, N, ...), #N, data->adverse_events[IOV_THRESHOLD_##K]
+
+		drm_printf(p, "VF%u:\t" IOV_THRESHOLDS(__format_iov_threshold) "\n",
+			   n IOV_THRESHOLDS(__value_iov_threshold));
+
+#undef __format_iov_threshold
+#undef __value_iov_threshold
+
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_event.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_event.h
new file mode 100644
index 000000000000..c9593b203d4c
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_event.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_EVENT_H__
+#define __INTEL_IOV_EVENT_H__
+
+#include <linux/types.h>
+
+struct drm_printer;
+struct intel_iov;
+
+void intel_iov_event_reset(struct intel_iov *iov, u32 vfid);
+int intel_iov_event_process_guc2pf(struct intel_iov *iov, const u32 *msg, u32 len);
+int intel_iov_event_print_events(struct intel_iov *iov, struct drm_printer *p);
+
+#endif /* __INTEL_IOV_EVENT_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c
new file mode 100644
index 000000000000..0e03fb107b16
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c
@@ -0,0 +1,310 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2020 Intel Corporation
+ */
+
+#include "intel_iov.h"
+#include "intel_iov_memirq.h"
+#include "intel_iov_reg.h"
+#include "intel_iov_utils.h"
+#include "gem/i915_gem_lmem.h"
+#include "gem/i915_gem_region.h"
+#include "gt/intel_breadcrumbs.h"
+#include "gt/intel_gt.h"
+#include "gt/intel_gt_print.h"
+#include "gt/intel_gt_regs.h"
+#include "gt/uc/abi/guc_actions_vf_abi.h"
+
+#ifdef CONFIG_DRM_I915_DEBUG_IOV
+#define MEMIRQ_DEBUG(_gt, _f, ...) gt_dbg((_gt), "IRQ " _f, ##__VA_ARGS__)
+#else
+#define MEMIRQ_DEBUG(...)
+#endif
+
+/**
+ * Memory based irq page layout
+ * We use a single page to contain the different objects used for memory
+ * based irq (which are also called "page" in the specs, even if they
+ * aren't page-sized). The address of those objects are them programmed
+ * in the HW via LRI and LRM in the context image.
+ *
+ * - Interrupt Status Report page: this page contains the interrupt
+ *   status vectors for each unit. Each bit in the interrupt vectors is
+ *   converted to a byte, with the byte being set to 0xFF when an
+ *   interrupt is triggered; interrupt vectors are 16b big so each unit
+ *   gets 16B. One space is reseved for each bit in one of the
+ *   GEN11_GT_INTR_DWx registers, so this object needs a total of 1024B.
+ *   This object needs to be 4k aligned.
+ *
+ * - Interrupt Source Report page: this is the equivalent of the
+ *   GEN11_GT_INTR_DWx registers, with each bit in those registers being
+ *   mapped to a byte here. The offsets are the same, just bytes instead
+ *   of bits. This object needs to be cacheline aligned.
+ *
+ * - Interrupt Mask: the HW needs a location to fetch the interrupt
+ *   mask vector to be used by the LRM in the context, so we just use
+ *   the next available space in the interrupt page
+ */
+
+static int vf_create_memirq_data(struct intel_iov *iov)
+{
+	struct drm_i915_private *i915 = iov_to_i915(iov);
+	struct drm_i915_gem_object *obj;
+	void *vaddr;
+	int err;
+	u32 *enable_vector;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(!HAS_MEMORY_IRQ_STATUS(i915));
+	GEM_BUG_ON(iov->vf.irq.obj);
+
+	obj = i915_gem_object_create_shmem(i915, SZ_4K);
+	if (IS_ERR(obj)) {
+		err = PTR_ERR(obj);
+		goto out;
+	}
+
+	vaddr = i915_gem_object_pin_map_unlocked(obj, i915_coherent_map_type(i915, obj, true));
+	if (IS_ERR(vaddr)) {
+		err = PTR_ERR(vaddr);
+		goto out_obj;
+	}
+
+	iov->vf.irq.obj = obj;
+	iov->vf.irq.vaddr = vaddr;
+
+	enable_vector = (u32 *)(vaddr + I915_VF_IRQ_ENABLE);
+	/*XXX: we should start with all irqs disabled: 0xffff0000 */
+	*enable_vector = 0xffff;
+
+	return 0;
+
+out_obj:
+	i915_gem_object_put(obj);
+out:
+	IOV_DEBUG(iov, "failed %d\n", err);
+	return err;
+}
+
+static int vf_map_memirq_data(struct intel_iov *iov)
+{
+	struct intel_gt *gt = iov_to_gt(iov);
+	struct i915_vma *vma;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(!iov->vf.irq.obj);
+
+	vma = i915_vma_instance(iov->vf.irq.obj, &gt->ggtt->vm, NULL);
+	if (IS_ERR(vma)) {
+		err = PTR_ERR(vma);
+		goto out;
+	}
+
+	err = i915_vma_pin(vma, 0, 0, PIN_GLOBAL);
+	if (err)
+		goto out_vma;
+
+	iov->vf.irq.vma = vma;
+
+	return 0;
+
+out_vma:
+	i915_gem_object_put(iov->vf.irq.obj);
+out:
+	IOV_DEBUG(iov, "failed %pe\n", ERR_PTR(err));
+	return err;
+}
+
+static void vf_release_memirq_data(struct intel_iov *iov)
+{
+	i915_vma_unpin_and_release(&iov->vf.irq.vma, I915_VMA_RELEASE_MAP);
+	iov->vf.irq.obj = NULL;
+	iov->vf.irq.vaddr = NULL;
+}
+
+/**
+ * intel_iov_memirq_init - Initialize data used by memory based interrupts.
+ * @iov: the IOV struct
+ *
+ * Allocate Interrupt Source Report page and Interrupt Status Report page
+ * used by memory based interrupts.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_memirq_init(struct intel_iov *iov)
+{
+	int err;
+
+	if (!HAS_MEMORY_IRQ_STATUS(iov_to_i915(iov)))
+		return 0;
+
+	err = vf_create_memirq_data(iov);
+	if (unlikely(err))
+		return err;
+
+	err = vf_map_memirq_data(iov);
+	if (unlikely(err))
+		return err;
+
+	return 0;
+}
+
+/**
+ * intel_iov_irq_fini - Release data used by memory based interrupts.
+ * @iov: the IOV struct
+ *
+ * Release data used by memory based interrupts.
+ */
+void intel_iov_memirq_fini(struct intel_iov *iov)
+{
+	if (!HAS_MEMORY_IRQ_STATUS(iov_to_i915(iov)))
+		return;
+
+	vf_release_memirq_data(iov);
+}
+
+/**
+ * intel_iov_memirq_prepare_guc - Prepare GuC to use memory based interrrupts.
+ * @iov: the IOV struct
+ *
+ * Register Interrupt Source Report page and Interrupt Status Report page
+ * within GuC to correctly handle memory based interrrupts from GuC.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_memirq_prepare_guc(struct intel_iov *iov)
+{
+	struct intel_gt *gt = iov_to_gt(iov);
+	struct intel_guc *guc = &gt->uc.guc;
+	u32 base, source, status;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(!HAS_MEMORY_IRQ_STATUS(iov_to_i915(iov)));
+
+	base = intel_guc_ggtt_offset(guc, iov->vf.irq.vma);
+	source = base + I915_VF_IRQ_SOURCE + GEN11_GUC;
+	status = base + I915_VF_IRQ_STATUS + GEN11_GUC * SZ_16;
+
+	err = intel_guc_self_cfg64(guc, GUC_KLV_SELF_CFG_MEMIRQ_SOURCE_ADDR_KEY,
+				   source);
+	if (unlikely(err))
+		goto failed;
+
+	err = intel_guc_self_cfg64(guc, GUC_KLV_SELF_CFG_MEMIRQ_STATUS_ADDR_KEY,
+				   status);
+	if (unlikely(err))
+		goto failed;
+
+	return 0;
+
+failed:
+	IOV_ERROR(iov, "Failed to register MEMIRQ %#x:%#x (%pe)\n",
+		  source, status, ERR_PTR(err));
+	return err;
+}
+
+/**
+ * intel_iov_memirq_reset - TBD
+ * @iov: the IOV struct
+ *
+ * TBD.
+ */
+void intel_iov_memirq_reset(struct intel_iov *iov)
+{
+	u8 *irq = iov->vf.irq.vaddr;
+	u32 *val = (u32 *)(irq + I915_VF_IRQ_ENABLE);
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	if (irq)
+		*val = 0;
+}
+
+/**
+ * intel_iov_memirq_postinstall - TBD
+ * @iov: the IOV struct
+ *
+ * TBD.
+ */
+void intel_iov_memirq_postinstall(struct intel_iov *iov)
+{
+	u8 *irq = iov->vf.irq.vaddr;
+	u32 *val = (u32 *)(irq + I915_VF_IRQ_ENABLE);
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	if (irq)
+		*val = 0xffff;
+}
+
+static void __engine_mem_irq_handler(struct intel_engine_cs *engine, u8 *status)
+{
+	struct intel_gt __maybe_unused *gt = engine->gt;
+
+	MEMIRQ_DEBUG(gt, "STATUS %s %*ph\n", engine->name, 16, status);
+
+	if (READ_ONCE(status[ilog2(GT_RENDER_USER_INTERRUPT)]) == 0xFF) {
+		WRITE_ONCE(status[ilog2(GT_RENDER_USER_INTERRUPT)], 0x00);
+		intel_engine_signal_breadcrumbs(engine);
+		tasklet_hi_schedule(&engine->sched_engine->tasklet);
+	}
+}
+
+static void __guc_mem_irq_handler(struct intel_guc *guc, u8 *status)
+{
+	struct intel_gt __maybe_unused *gt = guc_to_gt(guc);
+
+	MEMIRQ_DEBUG(gt, "STATUS %s %*ph\n", "GUC", 16, status);
+
+	if (READ_ONCE(status[ilog2(GUC_INTR_GUC2HOST)]) == 0xFF) {
+		WRITE_ONCE(status[ilog2(GUC_INTR_GUC2HOST)], 0x00);
+		intel_guc_to_host_event_handler(guc);
+	}
+}
+
+/**
+ * intel_iov_memirq_handler - TBD
+ * @iov: the IOV struct
+ *
+ * TBD.
+ */
+void intel_iov_memirq_handler(struct intel_iov *iov)
+{
+	struct intel_gt *gt = iov_to_gt(iov);
+	u8 *irq = iov->vf.irq.vaddr;
+	u8 * const source_base = irq + I915_VF_IRQ_SOURCE;
+	u8 * const status_base = irq + I915_VF_IRQ_STATUS;
+	u8 *source, value;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id id;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	if (!irq)
+		return;
+
+	MEMIRQ_DEBUG(gt, "SOURCE %*ph\n", 32, source_base);
+	MEMIRQ_DEBUG(gt, "SOURCE %*ph\n", 32, source_base + 32);
+
+	/* TODO: Only check active engines */
+	for_each_engine(engine, gt, id) {
+		source = source_base + engine->irq_offset;
+		value = READ_ONCE(*source);
+		if (value == 0xff) {
+			WRITE_ONCE(*source, 0x00);
+			__engine_mem_irq_handler(engine, status_base +
+						 engine->irq_offset * SZ_16);
+		}
+	}
+
+	/* GuC must be check separately */
+	source = source_base + GEN11_GUC;
+	value = READ_ONCE(*source);
+	if (value == 0xff) {
+		WRITE_ONCE(*source, 0x00);
+		__guc_mem_irq_handler(&gt->uc.guc, status_base +
+				      GEN11_GUC * SZ_16);
+	}
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.h
new file mode 100644
index 000000000000..ef64dd4fff0e
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.h
@@ -0,0 +1,20 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2020 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_MEMIRQ_H__
+#define __INTEL_IOV_MEMIRQ_H__
+
+struct intel_iov;
+
+int intel_iov_memirq_init(struct intel_iov *iov);
+void intel_iov_memirq_fini(struct intel_iov *iov);
+
+int intel_iov_memirq_prepare_guc(struct intel_iov *iov);
+
+void intel_iov_memirq_reset(struct intel_iov *iov);
+void intel_iov_memirq_postinstall(struct intel_iov *iov);
+void intel_iov_memirq_handler(struct intel_iov *iov);
+
+#endif /* __INTEL_IOV_MEMIRQ_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
new file mode 100644
index 000000000000..5442716dcfe1
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
@@ -0,0 +1,2698 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "intel_iov.h"
+#include "intel_iov_provisioning.h"
+#include "intel_iov_utils.h"
+#include "gt/intel_gt.h"
+#include "gt/uc/abi/guc_actions_pf_abi.h"
+#include "gt/uc/abi/guc_klvs_abi.h"
+
+#define MAKE_GUC_KLV(__K) \
+	(FIELD_PREP(GUC_KLV_0_KEY, GUC_KLV_##__K##_KEY) | \
+	 FIELD_PREP(GUC_KLV_0_LEN, GUC_KLV_##__K##_LEN))
+
+static int pf_verify_config_klvs(struct intel_iov *iov, const u32 *cfg, u32 cfg_size);
+
+static void pf_init_reprovisioning_worker(struct intel_iov *iov);
+static void pf_start_reprovisioning_worker(struct intel_iov *iov);
+static void pf_fini_reprovisioning_worker(struct intel_iov *iov);
+
+/*
+ * Resource configuration for VFs provisioning is maintained in the
+ * flexible array where:
+ *   - entry [0] contains resource config for the PF,
+ *   - entries [1..n] contain provisioning configs for VF1..VFn::
+ *
+ *       <--------------------------- 1 + total_vfs ----------->
+ *      +-------+-------+-------+-----------------------+-------+
+ *      |   0   |   1   |   2   |                       |   n   |
+ *      +-------+-------+-------+-----------------------+-------+
+ *      |  PF   |  VF1  |  VF2  |      ...     ...      |  VFn  |
+ *      +-------+-------+-------+-----------------------+-------+
+ */
+
+/**
+ * intel_iov_provisioning_init_early - Allocate structures for provisioning.
+ * @iov: the IOV struct
+ *
+ * VFs provisioning requires some data to be stored on the PF. Allocate
+ * flexible structures to hold all required information for every possible
+ * VF. In case of allocation failure PF will be in error state and will not
+ * be able to create VFs.
+ *
+ * This function can only be called on PF.
+ */
+void intel_iov_provisioning_init_early(struct intel_iov *iov)
+{
+	struct intel_iov_config *configs;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(iov->pf.provisioning.configs);
+
+	configs = kcalloc(1 + pf_get_totalvfs(iov), sizeof(*configs), GFP_KERNEL);
+	if (unlikely(!configs)) {
+		pf_update_status(iov, -ENOMEM, "provisioning");
+		return;
+	}
+
+	iov->pf.provisioning.configs = configs;
+	mutex_init(&iov->pf.provisioning.lock);
+	pf_init_reprovisioning_worker(iov);
+}
+
+/**
+ * intel_iov_provisioning_release - Release structures used for provisioning.
+ * @iov: the IOV struct
+ *
+ * Release structures used for provisioning.
+ * This function can only be called on PF.
+ */
+void intel_iov_provisioning_release(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_destroy(&iov->pf.provisioning.lock);
+	kfree(fetch_and_zero(&iov->pf.provisioning.configs));
+}
+
+/*
+ * Return: number of klvs that were successfully parsed and saved,
+ *         negative error code on failure.
+ */
+static int guc_action_update_policy_cfg(struct intel_guc *guc, u64 addr, u32 size)
+{
+	u32 request[] = {
+		GUC_ACTION_PF2GUC_UPDATE_VGT_POLICY,
+		lower_32_bits(addr),
+		upper_32_bits(addr),
+		size,
+	};
+
+	return intel_guc_send(guc, request, ARRAY_SIZE(request));
+}
+
+/*
+ * Return: 0 on success, -ENOKEY if klv was not parsed, -EPROTO if reply was malformed,
+ *         negative error code on failure.
+ */
+static int guc_update_policy_klv32(struct intel_guc *guc, u16 key, u32 value)
+{
+	const u32 len = 1; /* 32bit value fits into 1 klv dword */
+	const u32 cfg_size = (GUC_KLV_LEN_MIN + len);
+	struct i915_vma *vma;
+	u32 *cfg;
+	int ret;
+
+	ret = intel_guc_allocate_and_map_vma(guc, cfg_size * sizeof(u32), &vma, (void **)&cfg);
+	if (unlikely(ret))
+		return ret;
+
+	*cfg++ = FIELD_PREP(GUC_KLV_0_KEY, key) | FIELD_PREP(GUC_KLV_0_LEN, len);
+	*cfg++ = value;
+
+	ret = guc_action_update_policy_cfg(guc, intel_guc_ggtt_offset(guc, vma), cfg_size);
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+
+	if (unlikely(ret < 0))
+		return ret;
+	if (unlikely(!ret))
+		return -ENOKEY;
+	if (unlikely(ret > 1))
+		return -EPROTO;
+
+	return 0;
+}
+
+static const char *policy_key_to_string(u16 key)
+{
+	switch (key) {
+	case GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_KEY:
+		return "sched_if_idle";
+	case GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD_KEY:
+		return "sample_period";
+	case GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH_KEY:
+		return "reset_engine";
+	default:
+		return "<invalid>";
+	}
+}
+
+static int pf_update_bool_policy(struct intel_iov *iov, u16 key, bool *policy, bool value)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	const char *name = policy_key_to_string(key);
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	IOV_DEBUG(iov, "updating policy %#04x (%s) %s -> %s\n",
+		  key, name, str_enable_disable(*policy), str_enable_disable(value));
+
+	err = guc_update_policy_klv32(guc, key, value);
+	if (unlikely(err))
+		goto failed;
+
+	*policy = value;
+	return 0;
+
+failed:
+	IOV_ERROR(iov, "Failed to %s '%s' policy (%pe)\n",
+		  str_enable_disable(value), name, ERR_PTR(err));
+	return err;
+}
+
+static int pf_update_policy_u32(struct intel_iov *iov, u16 key, u32 *policy, u32 value)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	const char *name = policy_key_to_string(key);
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	IOV_DEBUG(iov, "updating policy %#04x (%s) %u -> %u\n",
+		  key, name, *policy, value);
+
+	err = guc_update_policy_klv32(guc, key, value);
+	if (unlikely(err))
+		goto failed;
+
+	*policy = value;
+	return 0;
+
+failed:
+	IOV_ERROR(iov, "Failed to update policy '%s=%u' (%pe)\n",
+		  name, value, ERR_PTR(err));
+	return err;
+}
+
+static int pf_provision_sched_if_idle(struct intel_iov *iov, bool enable)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_update_bool_policy(iov, GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_KEY,
+				     &iov->pf.provisioning.policies.sched_if_idle, enable);
+}
+
+static int pf_reprovision_sched_if_idle(struct intel_iov *iov)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_provision_sched_if_idle(iov, iov->pf.provisioning.policies.sched_if_idle);
+}
+
+/**
+ * intel_iov_provisioning_set_sched_if_idle - Set 'sched_if_idle' policy.
+ * @iov: the IOV struct
+ * @enable: controls sched_if_idle policy
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_sched_if_idle(struct intel_iov *iov, bool enable)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_sched_if_idle(iov, enable);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_sched_if_idle - Get 'sched_if_idle' policy.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+bool intel_iov_provisioning_get_sched_if_idle(struct intel_iov *iov)
+{
+	bool enable;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	enable = iov->pf.provisioning.policies.sched_if_idle;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return enable;
+}
+
+static int pf_provision_reset_engine(struct intel_iov *iov, bool enable)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_update_bool_policy(iov, GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH_KEY,
+				     &iov->pf.provisioning.policies.reset_engine, enable);
+}
+
+static int pf_reprovision_reset_engine(struct intel_iov *iov)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_provision_reset_engine(iov, iov->pf.provisioning.policies.reset_engine);
+}
+
+/**
+ * intel_iov_provisioning_set_reset_engine - Set 'reset_engine' policy.
+ * @iov: the IOV struct
+ * @enable: controls reset_engine policy
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_reset_engine(struct intel_iov *iov, bool enable)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_reset_engine(iov, enable);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_reset_engine - Get 'reset_engine' policy.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+bool intel_iov_provisioning_get_reset_engine(struct intel_iov *iov)
+{
+	bool enable;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	enable = iov->pf.provisioning.policies.reset_engine;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return enable;
+}
+
+static int pf_provision_sample_period(struct intel_iov *iov, u32 value)
+{
+	return pf_update_policy_u32(iov, GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD_KEY,
+				    &iov->pf.provisioning.policies.sample_period, value);
+}
+
+static int pf_reprovision_sample_period(struct intel_iov *iov)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_provision_sample_period(iov, iov->pf.provisioning.policies.sample_period);
+}
+
+/**
+ * intel_iov_provisioning_set_sample_period - Set 'sample_period' policy.
+ * @iov: the IOV struct
+ * @value: sample period in milliseconds
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_sample_period(struct intel_iov *iov, u32 value)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_sample_period(iov, value);
+
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_sample_period - Get 'sample_period' policy.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u32 intel_iov_provisioning_get_sample_period(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	return iov->pf.provisioning.policies.sample_period;
+}
+
+static inline bool pf_is_auto_provisioned(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	return iov->pf.provisioning.auto_mode;
+}
+
+static void pf_set_auto_provisioning(struct intel_iov *iov, bool value)
+{
+	if (pf_is_auto_provisioned(iov) == value)
+		return;
+
+	IOV_DEBUG(iov, "%ps auto provisioning: %s\n",
+		  __builtin_return_address(0), str_yes_no(value));
+	iov->pf.provisioning.auto_mode = value;
+}
+
+static bool pf_is_vf_enabled(struct intel_iov *iov, unsigned int id)
+{
+	return id <= pf_get_numvfs(iov);
+}
+
+static bool pf_is_config_pushed(struct intel_iov *iov, unsigned int id)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	return id <= iov->pf.provisioning.num_pushed;
+}
+
+static bool pf_needs_push_config(struct intel_iov *iov, unsigned int id)
+{
+	return id != PFID && pf_is_vf_enabled(iov, id) && pf_is_config_pushed(iov, id);
+}
+
+/*
+ * Return: number of klvs that were successfully parsed and saved,
+ *         negative error code on failure.
+ */
+static int guc_action_update_vf_cfg(struct intel_guc *guc, u32 vfid,
+				    u64 addr, u32 size)
+{
+	u32 request[] = {
+		GUC_ACTION_PF2GUC_UPDATE_VF_CFG,
+		vfid,
+		lower_32_bits(addr),
+		upper_32_bits(addr),
+		size,
+	};
+
+	return intel_guc_send(guc, request, ARRAY_SIZE(request));
+}
+
+/*
+ * Return: 0 on success, -ENOKEY if klv was not parsed, -EPROTO if reply was malformed,
+ *         negative error code on failure.
+ */
+static int guc_update_vf_klv32(struct intel_guc *guc, u32 vfid, u16 key, u32 value)
+{
+	const u32 len = 1; /* 32bit value fits into 1 klv dword */
+	const u32 cfg_size = (GUC_KLV_LEN_MIN + len);
+	struct i915_vma *vma;
+	u32 *blob, *cfg;
+	int ret;
+
+	ret = intel_guc_allocate_and_map_vma(guc, cfg_size * sizeof(u32), &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+
+	cfg = blob;
+	*cfg++ = FIELD_PREP(GUC_KLV_0_KEY, key) | FIELD_PREP(GUC_KLV_0_LEN, len);
+	*cfg++ = value;
+
+	GEM_WARN_ON(pf_verify_config_klvs(&guc_to_gt(guc)->iov, blob, cfg_size));
+	ret = guc_action_update_vf_cfg(guc, vfid, intel_guc_ggtt_offset(guc, vma), cfg_size);
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+
+	if (unlikely(ret < 0))
+		return ret;
+	if (unlikely(!ret))
+		return -ENOKEY;
+	if (unlikely(ret > 1))
+		return -EPROTO;
+
+	return 0;
+}
+
+static int guc_update_vf_klv64(struct intel_guc *guc, u32 vfid, u16 key, u64 value)
+{
+	const u32 len = 2; /* 64bit value fits into 2 klv dwords */
+	const u32 cfg_size = (GUC_KLV_LEN_MIN + len);
+	struct i915_vma *vma;
+	u32 *blob, *cfg;
+	int ret;
+
+	ret = intel_guc_allocate_and_map_vma(guc, cfg_size * sizeof(u32), &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+
+	cfg = blob;
+	*cfg++ = FIELD_PREP(GUC_KLV_0_KEY, key) | FIELD_PREP(GUC_KLV_0_LEN, len);
+	*cfg++ = lower_32_bits(value);
+	*cfg++ = upper_32_bits(value);
+
+	GEM_WARN_ON(pf_verify_config_klvs(&guc_to_gt(guc)->iov, blob, cfg_size));
+	ret = guc_action_update_vf_cfg(guc, vfid, intel_guc_ggtt_offset(guc, vma), cfg_size);
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+
+	if (unlikely(ret < 0))
+		return ret;
+	if (unlikely(!ret))
+		return -ENOKEY;
+	if (unlikely(ret > 1))
+		return -EPROTO;
+
+	return 0;
+}
+
+static u64 pf_get_ggtt_alignment(struct intel_iov *iov)
+{
+	/* this might be platform dependent */
+	return SZ_4K;
+}
+
+static u64 pf_get_min_spare_ggtt(struct intel_iov *iov)
+{
+	/* this might be platform dependent */
+	return SZ_64M; /* XXX: preliminary */
+}
+
+static u64 pf_get_spare_ggtt(struct intel_iov *iov)
+{
+	u64 spare;
+
+	spare = iov->pf.provisioning.spare.ggtt_size;
+	spare = max_t(u64, spare, pf_get_min_spare_ggtt(iov));
+
+	return spare;
+}
+
+/**
+ * intel_iov_provisioning_set_spare_ggtt - Set size of the PF spare GGTT.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_spare_ggtt(struct intel_iov *iov, u64 size)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (size && size < pf_get_min_spare_ggtt(iov))
+		return -EINVAL;
+
+	if (check_round_up_overflow(size, pf_get_ggtt_alignment(iov), &size))
+		size = U64_MAX;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	iov->pf.provisioning.spare.ggtt_size = size;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_get_spare_ggtt - Get size of the PF spare GGTT.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u64 intel_iov_provisioning_get_spare_ggtt(struct intel_iov *iov)
+{
+	u64 spare;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	spare = pf_get_spare_ggtt(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return spare;
+}
+
+static u64 pf_get_free_ggtt(struct intel_iov *iov)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	const struct drm_mm *mm = &ggtt->vm.mm;
+	const struct drm_mm_node *entry;
+	u64 alignment = pf_get_ggtt_alignment(iov);
+	u64 spare = pf_get_spare_ggtt(iov);
+	u64 hole_min_start = ggtt->pin_bias;
+	u64 hole_start, hole_end;
+	u64 free_ggtt = 0;
+
+	mutex_lock(&ggtt->vm.mutex);
+
+	drm_mm_for_each_hole(entry, mm, hole_start, hole_end) {
+		hole_start = max(hole_start, hole_min_start);
+		hole_start = ALIGN(hole_start, alignment);
+		hole_end = ALIGN_DOWN(hole_end, alignment);
+		if (hole_start >= hole_end)
+			continue;
+		free_ggtt += hole_end - hole_start;
+	}
+
+	mutex_unlock(&ggtt->vm.mutex);
+
+	return free_ggtt > spare ? free_ggtt - spare : 0;
+}
+
+static u64 pf_get_max_ggtt(struct intel_iov *iov)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	const struct drm_mm *mm = &ggtt->vm.mm;
+	const struct drm_mm_node *entry;
+	u64 alignment = pf_get_ggtt_alignment(iov);
+	u64 spare = pf_get_spare_ggtt(iov);
+	u64 hole_min_start = ggtt->pin_bias;
+	u64 hole_start, hole_end, hole_size;
+	u64 max_hole = 0;
+
+	mutex_lock(&ggtt->vm.mutex);
+
+	drm_mm_for_each_hole(entry, mm, hole_start, hole_end) {
+		hole_start = max(hole_start, hole_min_start);
+		hole_start = ALIGN(hole_start, alignment);
+		hole_end = ALIGN_DOWN(hole_end, alignment);
+		if (hole_start >= hole_end)
+			continue;
+		hole_size = hole_end - hole_start;
+		IOV_DEBUG(iov, "start %llx size %lluK\n", hole_start, hole_size / SZ_1K);
+		spare -= min3(spare, hole_size, max_hole);
+		max_hole = max(max_hole, hole_size);
+	}
+
+	mutex_unlock(&ggtt->vm.mutex);
+
+	IOV_DEBUG(iov, "spare %lluK\n", spare / SZ_1K);
+	return max_hole > spare ? max_hole - spare : 0;
+}
+
+static bool pf_is_valid_config_ggtt(struct intel_iov *iov, unsigned int id)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return drm_mm_node_allocated(&iov->pf.provisioning.configs[id].ggtt_region);
+}
+
+static int __pf_push_config_ggtt(struct intel_iov *iov, unsigned int id, u64 start, u64 size)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	int err;
+
+	if (!pf_needs_push_config(iov, id))
+		return 0;
+
+	err = guc_update_vf_klv64(guc, id, GUC_KLV_VF_CFG_GGTT_SIZE_KEY, size);
+	if (unlikely(err))
+		return err;
+
+	err = guc_update_vf_klv64(guc, id, GUC_KLV_VF_CFG_GGTT_START_KEY, start);
+	if (unlikely(err))
+		return err;
+
+	return 0;
+}
+
+static int pf_push_config_ggtt(struct intel_iov *iov, unsigned int id, u64 start, u64 size)
+{
+	struct intel_gt *media_gt = iov_to_i915(iov)->media_gt;
+	int err, err2 = 0;
+
+	err = __pf_push_config_ggtt(iov, id, start, size);
+
+	if (media_gt) {
+		GEM_BUG_ON(!iov_is_root(iov));
+		err2 = __pf_push_config_ggtt(&media_gt->iov, id, start, size);
+	}
+
+	return err ?: err2;
+}
+
+static int pf_provision_ggtt(struct intel_iov *iov, unsigned int id, u64 size)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_iov_config *config = &provisioning->configs[id];
+	struct drm_mm_node *node = &config->ggtt_region;
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	u64 alignment = pf_get_ggtt_alignment(iov);
+	int err;
+
+	if (iov_to_gt(iov)->type == GT_MEDIA)
+		return -ENODEV;
+
+	if (check_round_up_overflow(size, alignment, &size))
+		return -EOVERFLOW;
+
+	if (drm_mm_node_allocated(node)) {
+		if (size == node->size)
+			return 0;
+
+		err = pf_push_config_ggtt(iov, id, 0, 0);
+release:
+		i915_ggtt_set_space_owner(ggtt, 0, node);
+
+		mutex_lock(&ggtt->vm.mutex);
+		drm_mm_remove_node(node);
+		mutex_unlock(&ggtt->vm.mutex);
+
+		if (unlikely(err))
+			return err;
+	}
+	GEM_BUG_ON(drm_mm_node_allocated(node));
+
+	if (!size)
+		return 0;
+
+	if (size > ggtt->vm.total)
+		return -E2BIG;
+
+	if (size > pf_get_max_ggtt(iov))
+		return -EDQUOT;
+
+	mutex_lock(&ggtt->vm.mutex);
+	err = i915_gem_gtt_insert(&ggtt->vm, NULL, node, size, alignment,
+		I915_COLOR_UNEVICTABLE,
+		ggtt->pin_bias, GUC_GGTT_TOP,
+		PIN_HIGH);
+	mutex_unlock(&ggtt->vm.mutex);
+	if (unlikely(err))
+		return err;
+
+	i915_ggtt_set_space_owner(ggtt, id, node);
+
+	err = pf_push_config_ggtt(iov, id, node->start, node->size);
+	if (unlikely(err))
+		goto release;
+
+	IOV_DEBUG(iov, "VF%u provisioned GGTT %llx-%llx (%lluK)\n",
+		  id, node->start, node->start + node->size - 1, node->size / SZ_1K);
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_set_ggtt - Provision VF with GGTT.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @size: requested GGTT size
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_ggtt(struct intel_iov *iov, unsigned int id, u64 size)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	bool reprovisioning;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+	GEM_BUG_ON(id == PFID);
+	GEM_BUG_ON(iov_to_gt(iov)->type == GT_MEDIA);
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	reprovisioning = pf_is_valid_config_ggtt(iov, id) || size;
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_ggtt(iov, id, size);
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to provision VF%u with %llu of GGTT (%pe)\n",
+			  id, size, ERR_PTR(err));
+	else if (reprovisioning)
+		pf_mark_manual_provisioning(iov);
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_ggtt - Query size of GGTT provisioned for VF.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function can only be called on PF.
+ */
+u64 intel_iov_provisioning_get_ggtt(struct intel_iov *iov, unsigned int id)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct drm_mm_node *node = &provisioning->configs[id].ggtt_region;
+	u64 size;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+	GEM_BUG_ON(id == PFID);
+
+	if (iov_to_gt(iov)->type == GT_MEDIA)
+		node = &iov_get_root(iov)->pf.provisioning.configs[id].ggtt_region;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	size = drm_mm_node_allocated(node) ? node->size : 0;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return size;
+}
+
+/**
+ * intel_iov_provisioning_query_free_ggtt - Query free GGTT available for provisioning.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u64 intel_iov_provisioning_query_free_ggtt(struct intel_iov *iov)
+{
+	u64 size;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	size = pf_get_free_ggtt(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return size;
+}
+
+/**
+ * intel_iov_provisioning_query_max_ggtt - Query max GGTT available for provisioning.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u64 intel_iov_provisioning_query_max_ggtt(struct intel_iov *iov)
+{
+	u64 size;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	size = pf_get_max_ggtt(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return size;
+}
+
+static u16 pf_get_min_spare_ctxs(struct intel_iov *iov)
+{
+	return SZ_256;
+}
+
+static u16 pf_get_spare_ctxs(struct intel_iov *iov)
+{
+	u16 spare;
+
+	spare = iov->pf.provisioning.spare.num_ctxs;
+	spare = max_t(u16, spare, pf_get_min_spare_ctxs(iov));
+
+	return spare;
+}
+
+/**
+ * intel_iov_provisioning_get_spare_ctxs - Get number of the PF's spare contexts.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_get_spare_ctxs(struct intel_iov *iov)
+{
+	u16 spare;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	spare = pf_get_spare_ctxs(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return spare;
+}
+
+/**
+ * intel_iov_provisioning_set_spare_ctxs - Set number of the PF's spare contexts.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_spare_ctxs(struct intel_iov *iov, u16 spare)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (spare > GUC_MAX_CONTEXT_ID)
+		return -EINVAL;
+
+	if (spare && spare < pf_get_min_spare_ctxs(iov))
+		return -EINVAL;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	iov->pf.provisioning.spare.num_ctxs = spare;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return 0;
+}
+
+static bool pf_is_valid_config_ctxs(struct intel_iov *iov, unsigned int id)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return iov->pf.provisioning.configs[id].num_ctxs;
+}
+
+static int pf_push_config_ctxs(struct intel_iov *iov, unsigned int id, u16 begin, u16 num)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	int err;
+
+	if (!pf_needs_push_config(iov, id))
+		return 0;
+
+	err = guc_update_vf_klv32(guc, id, GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID_KEY, begin);
+	if (unlikely(err))
+		return err;
+
+	err = guc_update_vf_klv32(guc, id, GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY, num);
+	if (unlikely(err))
+		return err;
+
+	return 0;
+}
+
+/*
+ * To facilitate the implementation of dynamic context provisioning, we introduced
+ * the concept of granularity of contexts. For this purpose, we divided all contexts
+ * into packages with size CTXS_GRANULARITY. The exception is the first package, whose
+ * size is CTXS_MODULO, because GUC_MAX_CONTEXT_ID is an odd number.
+ */
+#define CTXS_GRANULARITY 128
+#define CTXS_MODULO (GUC_MAX_CONTEXT_ID % CTXS_GRANULARITY)
+#define CTXS_DELTA (CTXS_GRANULARITY - CTXS_MODULO)
+
+static u16 ctxs_bitmap_total_bits(void)
+{
+	return ALIGN(GUC_MAX_CONTEXT_ID, CTXS_GRANULARITY) / CTXS_GRANULARITY;
+}
+
+static u16 __encode_ctxs_count(u16 num_ctxs, bool first)
+{
+	GEM_BUG_ON(!first && !IS_ALIGNED(num_ctxs, CTXS_GRANULARITY));
+	GEM_BUG_ON(first && !IS_ALIGNED(num_ctxs + CTXS_DELTA, CTXS_GRANULARITY));
+
+	return (!first) ? num_ctxs / CTXS_GRANULARITY :
+			  (num_ctxs + CTXS_DELTA) / CTXS_GRANULARITY;
+}
+
+static u16 encode_vf_ctxs_count(u16 num_ctxs)
+{
+	return __encode_ctxs_count(num_ctxs, false);
+}
+
+static u16 __encode_ctxs_start(u16 start_ctx, bool first)
+{
+	if (!start_ctx)
+		return 0;
+
+	GEM_BUG_ON(!first && !IS_ALIGNED(start_ctx + CTXS_DELTA, CTXS_GRANULARITY));
+	GEM_BUG_ON(first && start_ctx);
+
+	return (!first) ? (start_ctx + CTXS_DELTA) / CTXS_GRANULARITY : 0;
+}
+
+static u16 __decode_ctxs_count(u16 num_bits, bool first)
+{
+	return (!first) ? num_bits * CTXS_GRANULARITY :
+			  num_bits * CTXS_GRANULARITY - CTXS_DELTA;
+}
+
+static u16 decode_vf_ctxs_count(u16 num_bits)
+{
+	return __decode_ctxs_count(num_bits, false);
+}
+
+static u16 decode_pf_ctxs_count(u16 num_bits)
+{
+	return __decode_ctxs_count(num_bits, true);
+}
+
+static u16 __decode_ctxs_start(u16 start_bit, bool first)
+{
+	GEM_BUG_ON(first && start_bit);
+
+	return (!first) ? start_bit * CTXS_GRANULARITY - CTXS_DELTA : 0;
+}
+
+static u16 decode_vf_ctxs_start(u16 start_bit)
+{
+	return __decode_ctxs_start(start_bit, false);
+}
+
+static u16 pf_get_ctxs_quota(struct intel_iov *iov, unsigned int id)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return iov->pf.provisioning.configs[id].num_ctxs;
+}
+
+static u16 align_ctxs(unsigned int id, u16 num_ctxs)
+{
+	if (num_ctxs == 0)
+		return 0;
+
+	num_ctxs = ALIGN(num_ctxs, CTXS_GRANULARITY);
+	return id ? num_ctxs : num_ctxs - CTXS_DELTA;
+}
+
+static unsigned long *pf_get_ctxs_bitmap(struct intel_iov *iov)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	unsigned int id, total_vfs = pf_get_totalvfs(iov);
+	const u16 total_bits = ctxs_bitmap_total_bits();
+	unsigned long *ctxs_bitmap = bitmap_zalloc(total_bits, GFP_KERNEL);
+
+	if (unlikely(!ctxs_bitmap))
+		return NULL;
+
+	for (id = 0; id <= total_vfs; id++) {
+		struct intel_iov_config *config = &provisioning->configs[id];
+
+		if (!config->num_ctxs)
+			continue;
+
+		bitmap_set(ctxs_bitmap, __encode_ctxs_start(config->begin_ctx, !id),
+			   __encode_ctxs_count(config->num_ctxs, !id));
+	}
+
+	/* caller must use bitmap_free */
+	return ctxs_bitmap;
+}
+
+static int pf_alloc_vf_ctxs_range(struct intel_iov *iov, unsigned int id, u16 num_ctxs)
+{
+	unsigned long *ctxs_bitmap = pf_get_ctxs_bitmap(iov);
+	u16 num_bits = encode_vf_ctxs_count(num_ctxs);
+	u16 max_size = U16_MAX;
+	u16 index = U16_MAX;
+	u16 last_equal = 0;
+	unsigned int rs, re;
+
+	if (unlikely(!ctxs_bitmap))
+		return -ENOMEM;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	for_each_clear_bitrange(rs, re, ctxs_bitmap, ctxs_bitmap_total_bits()) {
+		u16 size_bits = re - rs;
+
+		/*
+		 * The best-fit hole would be one that was as close to the end as possible and
+		 * equal to the number of contexts searched.
+		 * Second, we look for a hole that is as small as possible but larger than
+		 * the required size
+		 *
+		 */
+		if (size_bits == num_bits) {
+			last_equal = rs;
+		} else if (size_bits > num_bits && num_bits < max_size) {
+			index = re - num_bits;
+			max_size = size_bits;
+		}
+	}
+
+	bitmap_free(ctxs_bitmap);
+
+	if (last_equal != 0)
+		index = last_equal;
+
+	if (index >= U16_MAX)
+		return -ENOSPC;
+
+	return decode_vf_ctxs_start(index);
+}
+
+static int pf_alloc_ctxs_range(struct intel_iov *iov, unsigned int id, u16 num_ctxs)
+{
+	int ret;
+
+	ret = pf_alloc_vf_ctxs_range(iov, id, num_ctxs);
+
+	if (ret >= 0)
+		IOV_DEBUG(iov, "ctxs found %u-%u (%u)\n", ret, ret + num_ctxs - 1, num_ctxs);
+
+	return ret;
+}
+
+static int __pf_provision_vf_ctxs(struct intel_iov *iov, unsigned int id, u16 start_ctx, u16 num_ctxs)
+{
+	struct intel_iov_config *config = &iov->pf.provisioning.configs[id];
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id == PFID);
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	config->begin_ctx = start_ctx;
+	config->num_ctxs = num_ctxs;
+
+	return 0;
+}
+
+static int __pf_provision_ctxs(struct intel_iov *iov, unsigned int id, u16 start_ctx, u16 num_ctxs)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	err = pf_push_config_ctxs(iov, id, start_ctx, num_ctxs);
+	if (unlikely(err)) {
+		__pf_provision_vf_ctxs(iov, id, 0, 0);
+		return err;
+	}
+
+	return __pf_provision_vf_ctxs(iov, id, start_ctx, num_ctxs);
+}
+
+static u16 pf_get_ctxs_max_quota(struct intel_iov *iov);
+
+static int pf_provision_ctxs(struct intel_iov *iov, unsigned int id, u16 num_ctxs)
+{
+	u16 ctxs_quota = align_ctxs(id, num_ctxs);
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (id == PFID)
+		return -EOPNOTSUPP;
+
+	if (ctxs_quota == pf_get_ctxs_quota(iov, id))
+		return 0;
+
+	IOV_DEBUG(iov, "provisioning VF%u with %hu contexts (aligned to %hu)\n",
+		  id, num_ctxs, ctxs_quota);
+
+	ret = __pf_provision_ctxs(iov, id, 0, 0);
+	if (!num_ctxs || ret)
+		return ret;
+
+	if (ctxs_quota > pf_get_ctxs_max_quota(iov))
+		return -EDQUOT;
+
+	ret = pf_alloc_ctxs_range(iov, id, ctxs_quota);
+	if (ret >= 0)
+		return __pf_provision_ctxs(iov, id, ret, ctxs_quota);
+
+	return ret;
+}
+
+/**
+ * intel_iov_provisioning_set_ctxs - Provision VF with contexts.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @num_ctxs: requested contexts
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_ctxs(struct intel_iov *iov, unsigned int id, u16 num_ctxs)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	bool reprovisioning;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	reprovisioning = pf_is_valid_config_ctxs(iov, id) || num_ctxs;
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_ctxs(iov, id, num_ctxs);
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to provision VF%u with %hu contexts (%pe)\n",
+			  id, num_ctxs, ERR_PTR(err));
+	else if (reprovisioning && id != PFID)
+		pf_mark_manual_provisioning(iov);
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_ctxs - Get VF contexts quota.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_get_ctxs(struct intel_iov *iov, unsigned int id)
+{
+	u16 num_ctxs;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	num_ctxs = pf_get_ctxs_quota(iov, id);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return num_ctxs;
+}
+
+static u16 pf_reserved_ctxs(struct intel_iov *iov)
+{
+	u16 used = intel_guc_submission_ids_in_use(iov_to_guc(iov));
+	u16 quota = pf_get_ctxs_quota(iov, PFID);
+	u16 spare = pf_get_spare_ctxs(iov);
+	u16 avail = quota - used;
+
+	GEM_BUG_ON(quota < used);
+	if (spare < avail)
+		return 0;
+
+	return align_ctxs(!PFID, spare - avail);
+}
+
+static u16 pf_get_ctxs_free(struct intel_iov *iov)
+{
+	u16 reserved = encode_vf_ctxs_count(pf_reserved_ctxs(iov));
+	unsigned long *ctxs_bitmap = pf_get_ctxs_bitmap(iov);
+	unsigned int rs, re;
+	u16 sum = 0;
+
+	if (unlikely(!ctxs_bitmap))
+		return 0;
+
+	for_each_clear_bitrange(rs, re, ctxs_bitmap, ctxs_bitmap_total_bits()) {
+		IOV_DEBUG(iov, "ctxs hole %u-%u (%u)\n", decode_vf_ctxs_start(rs),
+			  decode_vf_ctxs_start(re) - 1, decode_vf_ctxs_count(re - rs));
+		sum += re - rs;
+	}
+	bitmap_free(ctxs_bitmap);
+
+	sum = sum > reserved ? sum - reserved : 0;
+	return decode_vf_ctxs_count(sum);
+}
+
+/**
+ * intel_iov_provisioning_query_free_ctxs - Get number of total unused contexts.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_query_free_ctxs(struct intel_iov *iov)
+{
+	u16 num_ctxs;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	num_ctxs = pf_get_ctxs_free(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return num_ctxs;
+}
+
+static u16 pf_get_ctxs_max_quota(struct intel_iov *iov)
+{
+	u16 reserved = encode_vf_ctxs_count(pf_reserved_ctxs(iov));
+	unsigned long *ctxs_bitmap = pf_get_ctxs_bitmap(iov);
+	unsigned int rs, re;
+	u16 max = 0;
+
+	if (unlikely(!ctxs_bitmap))
+		return 0;
+
+	for_each_clear_bitrange(rs, re, ctxs_bitmap, ctxs_bitmap_total_bits()) {
+		IOV_DEBUG(iov, "ctxs hole %u-%u (%u)\n", decode_vf_ctxs_start(rs),
+			  decode_vf_ctxs_start(re) - 1, decode_vf_ctxs_count(re - rs));
+		reserved -= min3(reserved, (u16)(re - rs), max);
+		max = max_t(u16, max, re - rs);
+	}
+	bitmap_free(ctxs_bitmap);
+
+	max = max > reserved ? max - reserved : 0;
+	return decode_vf_ctxs_count(max);
+}
+
+/**
+ * intel_iov_provisioning_query_max_ctxs - Get maximum available contexts quota.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_query_max_ctxs(struct intel_iov *iov)
+{
+	u16 num_ctxs;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	num_ctxs = pf_get_ctxs_max_quota(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return num_ctxs;
+}
+
+static u16 pf_get_min_spare_dbs(struct intel_iov *iov)
+{
+	/* we don't use doorbells yet */
+	return 0;
+}
+
+static u16 pf_get_spare_dbs(struct intel_iov *iov)
+{
+	u16 spare;
+
+	spare = iov->pf.provisioning.spare.num_dbs;
+	spare = max_t(u16, spare, pf_get_min_spare_dbs(iov));
+
+	return spare;
+}
+
+/**
+ * intel_iov_provisioning_get_spare_dbs - Get number of the PF's spare doorbells.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_get_spare_dbs(struct intel_iov *iov)
+{
+	u16 spare;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	spare = pf_get_spare_dbs(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return spare;
+}
+
+/**
+ * intel_iov_provisioning_set_spare_dbs - Set number of the PF's spare doorbells.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_spare_dbs(struct intel_iov *iov, u16 spare)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (spare > GUC_NUM_DOORBELLS)
+		return -EINVAL;
+
+	if (spare && spare < pf_get_min_spare_dbs(iov))
+		return -EINVAL;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	iov->pf.provisioning.spare.num_dbs = spare;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return 0;
+}
+
+static bool pf_is_valid_config_dbs(struct intel_iov *iov, unsigned int id)
+{
+	struct intel_iov_config *config = &iov->pf.provisioning.configs[id];
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return config->num_dbs || config->begin_db;
+}
+
+static unsigned long *pf_get_dbs_bitmap(struct intel_iov *iov)
+{
+	unsigned long *dbs_bitmap = bitmap_zalloc(GUC_NUM_DOORBELLS, GFP_KERNEL);
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	unsigned int n, total_vfs = pf_get_totalvfs(iov);
+	struct intel_iov_config *config;
+
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	if (unlikely(!dbs_bitmap))
+		return NULL;
+
+	/* don't count PF here, we will treat it differently */
+	for (n = 1; n <= total_vfs; n++) {
+		config = &provisioning->configs[n];
+		if (!config->num_dbs)
+			continue;
+		bitmap_set(dbs_bitmap, config->begin_db, config->num_dbs);
+	}
+
+	/* caller must use bitmap_free */
+	return dbs_bitmap;
+}
+
+static int pf_alloc_dbs_range(struct intel_iov *iov, u16 num_dbs)
+{
+	unsigned long *dbs_bitmap = pf_get_dbs_bitmap(iov);
+	unsigned long index;
+	int used;
+
+	if (unlikely(!dbs_bitmap))
+		return -ENOMEM;
+
+	used = bitmap_weight(dbs_bitmap, GUC_NUM_DOORBELLS);
+	if (used + pf_get_spare_dbs(iov) + num_dbs > GUC_NUM_DOORBELLS)
+		return -EDQUOT;
+
+	index = bitmap_find_next_zero_area(dbs_bitmap, GUC_NUM_DOORBELLS, 0, num_dbs, 0);
+	bitmap_free(dbs_bitmap);
+
+	if (index >= GUC_NUM_DOORBELLS)
+		return -ENOSPC;
+
+	IOV_DEBUG(iov, "dbs found %lu-%lu (%u)\n",
+		  index, index + num_dbs - 1, num_dbs);
+	return index;
+}
+
+static int pf_push_config_dbs(struct intel_iov *iov, unsigned int id, u16 begin, u16 num)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	int err;
+
+	if (!pf_needs_push_config(iov, id))
+		return 0;
+
+	err = guc_update_vf_klv32(guc, id, GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID_KEY, begin);
+	if (unlikely(err))
+		return err;
+
+	err = guc_update_vf_klv32(guc, id, GUC_KLV_VF_CFG_NUM_DOORBELLS_KEY, num);
+	if (unlikely(err))
+		return err;
+
+	return 0;
+}
+
+static int pf_provision_dbs(struct intel_iov *iov, unsigned int id, u16 num_dbs)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_iov_config *config = &provisioning->configs[id];
+	int err, ret;
+
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	if (num_dbs == config->num_dbs)
+		return 0;
+
+	IOV_DEBUG(iov, "provisioning VF%u with %hu doorbells\n", id, num_dbs);
+
+	if (config->num_dbs) {
+		config->begin_db = 0;
+		config->num_dbs = 0;
+
+		err = pf_push_config_dbs(iov, id, 0, 0);
+		if (unlikely(err))
+			return err;
+	}
+
+	if (!num_dbs)
+		return 0;
+
+	ret = pf_alloc_dbs_range(iov, num_dbs);
+	if (unlikely(ret < 0))
+		return ret;
+
+	err = pf_push_config_dbs(iov, id, ret, num_dbs);
+	if (unlikely(err))
+		return err;
+
+	config->begin_db = ret;
+	config->num_dbs = num_dbs;
+
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_set_dbs - Set VF doorbells quota.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @num_dbs: requested doorbells
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_dbs(struct intel_iov *iov, unsigned int id, u16 num_dbs)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	bool reprovisioning;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	reprovisioning = pf_is_valid_config_dbs(iov, id) || num_dbs;
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_dbs(iov, id, num_dbs);
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to provision VF%u with %hu doorbells (%pe)\n",
+			  id, num_dbs, ERR_PTR(err));
+	else if (reprovisioning && id != PFID)
+		pf_mark_manual_provisioning(iov);
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_dbs - Get VF doorbells quota.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_get_dbs(struct intel_iov *iov, unsigned int id)
+{
+	u16 num_dbs;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	num_dbs = iov->pf.provisioning.configs[id].num_dbs;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return num_dbs;
+}
+
+static u16 pf_get_free_dbs(struct intel_iov *iov)
+{
+	unsigned long *dbs_bitmap = pf_get_dbs_bitmap(iov);
+	int used;
+
+	if (unlikely(!dbs_bitmap))
+		return 0;
+
+	used = bitmap_weight(dbs_bitmap, GUC_NUM_DOORBELLS);
+	GEM_WARN_ON(used > GUC_NUM_DOORBELLS);
+
+	bitmap_free(dbs_bitmap);
+
+	return GUC_NUM_DOORBELLS - used;
+}
+
+/**
+ * intel_iov_provisioning_query_free_dbs - Get available doorbells.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_query_free_dbs(struct intel_iov *iov)
+{
+	u16 num_dbs;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	num_dbs = pf_get_free_dbs(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return num_dbs;
+}
+
+static u16 pf_get_max_dbs(struct intel_iov *iov)
+{
+	unsigned long *dbs_bitmap = pf_get_dbs_bitmap(iov);
+	unsigned int rs, re;
+	u16 limit = 0;
+
+	if (unlikely(!dbs_bitmap))
+		return 0;
+
+	for_each_clear_bitrange(rs, re, dbs_bitmap, GUC_NUM_DOORBELLS) {
+		IOV_DEBUG(iov, "dbs hole %u-%u (%u)\n", rs, re, re - rs);
+		limit = max_t(u16, limit, re - rs);
+	}
+	bitmap_free(dbs_bitmap);
+
+	return limit;
+}
+
+/**
+ * intel_iov_provisioning_query_max_dbs - Get maximum available doorbells quota.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+u16 intel_iov_provisioning_query_max_dbs(struct intel_iov *iov)
+{
+	u16 num_dbs;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	num_dbs = pf_get_max_dbs(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return num_dbs;
+}
+
+static const char *exec_quantum_unit(u32 exec_quantum)
+{
+	return exec_quantum ? "ms" : "(inifinity)";
+}
+
+static int pf_push_config_exec_quantum(struct intel_iov *iov, unsigned int id, u32 exec_quantum)
+{
+	return guc_update_vf_klv32(iov_to_guc(iov), id,
+				   GUC_KLV_VF_CFG_EXEC_QUANTUM_KEY, exec_quantum);
+}
+
+static int pf_provision_exec_quantum(struct intel_iov *iov, unsigned int id,
+				     u32 exec_quantum)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_iov_config *config = &provisioning->configs[id];
+	int err;
+
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	if (exec_quantum == config->exec_quantum)
+		return 0;
+
+	err = pf_push_config_exec_quantum(iov, id, exec_quantum);
+	if (unlikely(err))
+		return err;
+
+	config->exec_quantum = exec_quantum;
+
+	IOV_DEBUG(iov, "VF%u provisioned with %u%s execution quantum\n",
+		  id, exec_quantum, exec_quantum_unit(exec_quantum));
+	return 0;
+}
+
+static int pf_reprovision_exec_quantum(struct intel_iov *iov, unsigned int id)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_push_config_exec_quantum(iov, id,
+					   iov->pf.provisioning.configs[id].exec_quantum);
+}
+
+/**
+ * intel_iov_provisioning_set_exec_quantum - Provision VF with execution quantum.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @exec_quantum: requested execution quantum
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_exec_quantum(struct intel_iov *iov, unsigned int id,
+					    u32 exec_quantum)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_exec_quantum(iov, id, exec_quantum);
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to provision VF%u with %u%s execution quantum (%pe)\n",
+			  id, exec_quantum, exec_quantum_unit(exec_quantum), ERR_PTR(err));
+	else if (exec_quantum && id != PFID)
+		pf_mark_manual_provisioning(iov);
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_exec_quantum - Get VF execution quantum.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function can only be called on PF.
+ */
+u32 intel_iov_provisioning_get_exec_quantum(struct intel_iov *iov, unsigned int id)
+{
+	u32 exec_quantum;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	exec_quantum = iov->pf.provisioning.configs[id].exec_quantum;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return exec_quantum;
+}
+
+static const char *preempt_timeout_unit(u32 preempt_timeout)
+{
+	return preempt_timeout ? "us" : "(inifinity)";
+}
+
+static int pf_push_config_preempt_timeout(struct intel_iov *iov, unsigned int id,
+					  u32 preempt_timeout)
+{
+	return guc_update_vf_klv32(iov_to_guc(iov), id,
+				   GUC_KLV_VF_CFG_PREEMPT_TIMEOUT_KEY, preempt_timeout);
+}
+
+static int pf_provision_preempt_timeout(struct intel_iov *iov, unsigned int id,
+					u32 preempt_timeout)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_iov_config *config = &provisioning->configs[id];
+	int err;
+
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	if (preempt_timeout == config->preempt_timeout)
+		return 0;
+
+	err = pf_push_config_preempt_timeout(iov, id, preempt_timeout);
+	if (unlikely(err))
+		return err;
+
+	config->preempt_timeout = preempt_timeout;
+
+	IOV_DEBUG(iov, "VF%u provisioned with %u%s preemption timeout\n",
+		  id, preempt_timeout, preempt_timeout_unit(preempt_timeout));
+	return 0;
+}
+
+static int pf_reprovision_preempt_timeout(struct intel_iov *iov, unsigned int id)
+{
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	return pf_push_config_preempt_timeout(iov, id,
+					      iov->pf.provisioning.configs[id].preempt_timeout);
+}
+
+/**
+ * intel_iov_provisioning_set_preempt_timeout - Provision VF with preemption timeout.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @preempt_timeout: requested preemption timeout
+ */
+int intel_iov_provisioning_set_preempt_timeout(struct intel_iov *iov, unsigned int id, u32 preempt_timeout)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_preempt_timeout(iov, id, preempt_timeout);
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to provision VF%u with %u%s preemption timeout (%pe)\n",
+			  id, preempt_timeout, preempt_timeout_unit(preempt_timeout), ERR_PTR(err));
+	else if (preempt_timeout && id != PFID)
+		pf_mark_manual_provisioning(iov);
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_preempt_timeout - Get VF preemption timeout.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function can only be called on PF.
+ */
+u32 intel_iov_provisioning_get_preempt_timeout(struct intel_iov *iov, unsigned int id)
+{
+	u32 preempt_timeout;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	preempt_timeout = iov->pf.provisioning.configs[id].preempt_timeout;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return preempt_timeout;
+}
+
+static u32 intel_iov_threshold_to_klv_key(enum intel_iov_threshold threshold)
+{
+	switch (threshold) {
+#define __iov_threshold_enum_to_klv(K, ...) \
+	case IOV_THRESHOLD_##K: return GUC_KLV_VF_CFG_THRESHOLD_##K##_KEY;
+	IOV_THRESHOLDS(__iov_threshold_enum_to_klv)
+#undef __iov_threshold_enum_to_klv
+	}
+	GEM_BUG_ON(true);
+	return 0; /* unreachable */
+}
+
+static int pf_provision_threshold(struct intel_iov *iov, unsigned int id,
+				  enum intel_iov_threshold threshold, u32 value)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_iov_config *config = &provisioning->configs[id];
+	int err;
+
+	GEM_BUG_ON(threshold >= IOV_THRESHOLD_MAX);
+
+	if (value == config->thresholds[threshold])
+		return 0;
+
+	err = guc_update_vf_klv32(iov_to_guc(iov), id,
+				  intel_iov_threshold_to_klv_key(threshold), value);
+	if (unlikely(err))
+		return err;
+
+	config->thresholds[threshold] = value;
+
+	IOV_DEBUG(iov, "VF%u threshold %s=%u\n",
+		  id, intel_iov_threshold_to_string(threshold), value);
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_set_threshold - Set threshold for the VF.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @threshold: threshold identifier
+ * @value: requested threshold value
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_set_threshold(struct intel_iov *iov, unsigned int id,
+					 enum intel_iov_threshold threshold, u32 value)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_provision_threshold(iov, id, threshold, value);
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to set threshold %s=%u for VF%u (%pe)\n",
+			  intel_iov_threshold_to_string(threshold), value, id, ERR_PTR(err));
+	else if (value)
+		pf_mark_manual_provisioning(iov);
+
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_get_threshold - Get threshold of the VF.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ * @threshold: threshold identifier
+ *
+ * This function can only be called on PF.
+ */
+u32 intel_iov_provisioning_get_threshold(struct intel_iov *iov, unsigned int id,
+					 enum intel_iov_threshold threshold)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+	GEM_BUG_ON(threshold >= IOV_THRESHOLD_MAX);
+
+	return iov->pf.provisioning.configs[id].thresholds[threshold];
+}
+
+static void pf_unprovision_thresholds(struct intel_iov *iov, unsigned int id)
+{
+#define __iov_threshold_unprovision(K, ...) pf_provision_threshold(iov, id, IOV_THRESHOLD_##K, 0);
+	IOV_THRESHOLDS(__iov_threshold_unprovision)
+#undef __iov_threshold_unprovision
+}
+
+static void pf_assign_ctxs_for_pf(struct intel_iov *iov)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	u16 total_vfs = pf_get_totalvfs(iov);
+	const u16 total_ctxs_bits = ctxs_bitmap_total_bits();
+	u16 pf_ctxs_bits;
+	u16 pf_ctxs;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(!total_vfs);
+	GEM_BUG_ON(provisioning->configs[0].num_ctxs);
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	pf_ctxs_bits = total_ctxs_bits - ((total_ctxs_bits / (1 + total_vfs)) * total_vfs);
+	pf_ctxs = decode_pf_ctxs_count(pf_ctxs_bits);
+
+	IOV_DEBUG(iov, "config: %s %u = %u pf + %u available\n",
+		  "contexts", GUC_MAX_CONTEXT_ID, pf_ctxs, GUC_MAX_CONTEXT_ID - pf_ctxs);
+
+	provisioning->configs[0].begin_ctx = 0;
+	provisioning->configs[0].num_ctxs = pf_ctxs;
+
+	/* make sure to do not use context ids beyond our limit */
+	err = intel_guc_submission_limit_ids(iov_to_guc(iov), pf_ctxs);
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to limit PF %s to %u (%pe)\n",
+			  "contexts", pf_ctxs, ERR_PTR(err));
+}
+
+/**
+ * intel_iov_provisioning_init - Perform initial provisioning of the resources.
+ * @iov: the IOV struct
+ *
+ * Some resources shared between PF and VFs need to partitioned early, as PF
+ * allocation can't be changed later, only VFs allocations can be modified until
+ * all VFs are enabled. Perform initial partitioning to get fixed PF resources.
+ *
+ * This function can only be called on PF.
+ */
+void intel_iov_provisioning_init(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (unlikely(pf_in_error(iov)))
+		return;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	pf_assign_ctxs_for_pf(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+}
+
+static bool pf_is_auto_provisioning_enabled(struct intel_iov *iov)
+{
+	return i915_sriov_pf_is_auto_provisioning_enabled(iov_to_i915(iov));
+}
+
+static void pf_unprovision_config(struct intel_iov *iov, unsigned int id)
+{
+	pf_provision_ggtt(iov, id, 0);
+	pf_provision_ctxs(iov, id, 0);
+	pf_provision_dbs(iov, id, 0);
+	pf_provision_exec_quantum(iov, id, 0);
+	pf_provision_preempt_timeout(iov, id, 0);
+
+	pf_unprovision_thresholds(iov, id);
+}
+
+static void pf_unprovision_all(struct intel_iov *iov)
+{
+	unsigned int num_vfs = pf_get_totalvfs(iov);
+	unsigned int n;
+
+	for (n = num_vfs; n > 0; n--)
+		pf_unprovision_config(iov, n);
+}
+
+static void pf_auto_unprovision(struct intel_iov *iov)
+{
+	if (pf_is_auto_provisioned(iov))
+		pf_unprovision_all(iov);
+
+	pf_set_auto_provisioning(iov, false);
+}
+
+static int pf_auto_provision_ggtt(struct intel_iov *iov, unsigned int num_vfs)
+{
+	u64 __maybe_unused free = pf_get_free_ggtt(iov);
+	u64 available = pf_get_max_ggtt(iov);
+	u64 alignment = pf_get_ggtt_alignment(iov);
+	u64 fair;
+	unsigned int n;
+	int err;
+
+	/*
+	 * can't rely only on 'free_ggtt' as all VFs allocations must be continous
+	 * use 'max_ggtt' instead, on fresh/idle system those should be similar
+	 * and both already accounts for the spare GGTT
+	 */
+
+	fair = div_u64(available, num_vfs);
+	fair = ALIGN_DOWN(fair, alignment);
+
+	IOV_DEBUG(iov, "GGTT available(%llu/%llu) fair(%u x %llu)\n",
+		  available, free, num_vfs, fair);
+	if (!fair)
+		return -ENOSPC;
+
+	for (n = 1; n <= num_vfs; n++) {
+		if (pf_is_valid_config_ggtt(iov, n))
+			return -EUCLEAN;
+
+		err = pf_provision_ggtt(iov, n, fair);
+		if (unlikely(err))
+			return err;
+	}
+
+	return 0;
+}
+
+static int pf_auto_provision_ctxs(struct intel_iov *iov, unsigned int num_vfs)
+{
+	u16 n, fair;
+	u16 available;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	available = pf_get_ctxs_free(iov);
+	fair = ALIGN_DOWN(available / num_vfs, CTXS_GRANULARITY);
+
+	if (!fair)
+		return -ENOSPC;
+
+	IOV_DEBUG(iov, "contexts available(%hu) fair(%u x %hu)\n", available, num_vfs, fair);
+
+	for (n = 1; n <= num_vfs; n++) {
+		if (pf_is_valid_config_ctxs(iov, n))
+			return -EUCLEAN;
+
+		err = pf_provision_ctxs(iov, n, fair);
+		if (unlikely(err))
+			return err;
+	}
+
+	return 0;
+}
+
+static int pf_auto_provision_dbs(struct intel_iov *iov, unsigned int num_vfs)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	u16 available, fair;
+	unsigned int n;
+	int err;
+
+	available = GUC_NUM_DOORBELLS - provisioning->configs[0].num_dbs;
+	fair = available / num_vfs;
+
+	IOV_DEBUG(iov, "doorbells available(%hu) fair(%u x %hu)\n",
+		  available, num_vfs, fair);
+	if (!fair)
+		return -ENOSPC;
+
+	for (n = 1; n <= num_vfs; n++) {
+		if (pf_is_valid_config_dbs(iov, n))
+			return -EUCLEAN;
+
+		err = pf_provision_dbs(iov, n, fair);
+		if (unlikely(err))
+			return err;
+	}
+
+	return 0;
+}
+
+static int pf_auto_provision(struct intel_iov *iov, unsigned int num_vfs)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(num_vfs > pf_get_totalvfs(iov));
+	GEM_BUG_ON(num_vfs < 1);
+
+	if (!pf_is_auto_provisioning_enabled(iov)) {
+		err = -EPERM;
+		goto fail;
+	}
+
+	pf_set_auto_provisioning(iov, true);
+
+	if (iov_to_gt(iov)->type != GT_MEDIA) {
+		err = pf_auto_provision_ggtt(iov, num_vfs);
+		if (unlikely(err))
+			goto fail;
+	}
+
+	err = pf_auto_provision_ctxs(iov, num_vfs);
+	if (unlikely(err))
+		goto fail;
+
+	err = pf_auto_provision_dbs(iov, num_vfs);
+	if (unlikely(err))
+		goto fail;
+
+	return 0;
+fail:
+	IOV_ERROR(iov, "Failed to auto provision %u VFs (%pe)",
+		  num_vfs, ERR_PTR(err));
+	pf_auto_unprovision(iov);
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_auto() - Perform auto provisioning of VFs
+ * @iov: the IOV struct
+ * @num_vfs: number of VFs to auto configure or 0 to unprovision
+ *
+ * Perform auto provisioning by allocating fair amount of available
+ * resources for each VF that are to be enabled.
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_provisioning_auto(struct intel_iov *iov, unsigned int num_vfs)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	if (num_vfs)
+		err = pf_auto_provision(iov, num_vfs);
+	else
+		err = 0, pf_auto_unprovision(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return err;
+}
+
+static int pf_validate_config(struct intel_iov *iov, unsigned int id)
+{
+	bool valid_ggtt = pf_is_valid_config_ggtt(iov, id);
+	bool valid_ctxs = pf_is_valid_config_ctxs(iov, id);
+	bool valid_dbs = pf_is_valid_config_dbs(iov, id);
+	bool valid_any = valid_ggtt || valid_ctxs || valid_dbs;
+	bool valid_all = valid_ggtt && valid_ctxs;
+
+	/* we don't require doorbells, but will check if were assigned */
+
+	if (iov_to_gt(iov)->type == GT_MEDIA) {
+		struct intel_iov *root = iov_get_root(iov);
+
+		GEM_BUG_ON(iov_is_root(iov));
+		valid_ggtt = pf_is_valid_config_ggtt(root, id);
+		valid_any = valid_ctxs || valid_dbs;
+		valid_all = valid_any && valid_ggtt && valid_ctxs && pf_validate_config(root, id);
+	}
+
+	if (!valid_all) {
+		IOV_DEBUG(iov, "%u: invalid config: %s%s%s\n", id,
+			  valid_ggtt ? "" : "GGTT ",
+			  valid_ctxs ? "" : "contexts ",
+			  valid_dbs ? "" : "doorbells ");
+		return valid_any ? -ENOKEY : -ENODATA;
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_verify() - TBD
+ * @iov: the IOV struct
+ * @num_vfs: number of VFs configurations to verify
+ *
+ * Verify that VFs configurations are valid.
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_provisioning_verify(struct intel_iov *iov, unsigned int num_vfs)
+{
+	unsigned int num_empty = 0;
+	unsigned int num_valid = 0;
+	unsigned int n;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(num_vfs > pf_get_totalvfs(iov));
+	GEM_BUG_ON(num_vfs < 1);
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	for (n = 1; n <= num_vfs; n++) {
+		err = pf_validate_config(iov, n);
+		if (!err)
+			num_valid++;
+		else if (err == -ENODATA)
+			num_empty++;
+	}
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	IOV_DEBUG(iov, "found valid(%u) invalid(%u) empty(%u) configs\n",
+		  num_valid, num_vfs - num_valid, num_empty);
+
+	if (num_empty == num_vfs)
+		return -ENODATA;
+
+	if (num_valid + num_empty != num_vfs)
+		return -ENOKEY;
+
+	return 0;
+}
+
+/* Return: number of configuration dwords written */
+static u32 encode_config_ggtt(u32 *cfg, const struct intel_iov_config *config)
+{
+	u32 n = 0;
+
+	if (drm_mm_node_allocated(&config->ggtt_region)) {
+		cfg[n++] = MAKE_GUC_KLV(VF_CFG_GGTT_START);
+		cfg[n++] = lower_32_bits(config->ggtt_region.start);
+		cfg[n++] = upper_32_bits(config->ggtt_region.start);
+
+		cfg[n++] = MAKE_GUC_KLV(VF_CFG_GGTT_SIZE);
+		cfg[n++] = lower_32_bits(config->ggtt_region.size);
+		cfg[n++] = upper_32_bits(config->ggtt_region.size);
+	}
+
+	return n;
+}
+
+/* Return: number of configuration dwords written */
+static u32 encode_config(u32 *cfg, const struct intel_iov_config *config)
+{
+	u32 n = 0;
+
+	n += encode_config_ggtt(cfg, config);
+
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_BEGIN_CONTEXT_ID);
+	cfg[n++] = config->begin_ctx;
+
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_NUM_CONTEXTS);
+	cfg[n++] = config->num_ctxs;
+
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID);
+	cfg[n++] = config->begin_db;
+
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS);
+	cfg[n++] = config->num_dbs;
+
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_EXEC_QUANTUM);
+	cfg[n++] = config->exec_quantum;
+
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_PREEMPT_TIMEOUT);
+	cfg[n++] = config->preempt_timeout;
+
+#define __encode_threshold(K, ...) \
+	cfg[n++] = MAKE_GUC_KLV(VF_CFG_THRESHOLD_##K); \
+	cfg[n++] = config->thresholds[IOV_THRESHOLD_##K];
+
+	IOV_THRESHOLDS(__encode_threshold)
+#undef __encode_threshold
+
+	return n;
+}
+
+static int pf_verify_config_klvs(struct intel_iov *iov, const u32 *cfg, u32 cfg_size)
+{
+	while (cfg_size) {
+		u32 key __maybe_unused = FIELD_GET(GUC_KLV_0_KEY, *cfg);
+		u32 len = FIELD_GET(GUC_KLV_0_LEN, *cfg);
+
+		GEM_BUG_ON(cfg_size < GUC_KLV_LEN_MIN);
+		cfg += GUC_KLV_LEN_MIN;
+		cfg_size -= GUC_KLV_LEN_MIN;
+		GEM_BUG_ON(cfg_size < len);
+
+		switch (len) {
+		case 1:
+			IOV_DEBUG(iov, "{ key %04x : 32b value %u }\n",
+				  key, cfg[0]);
+			break;
+		case 2:
+			IOV_DEBUG(iov, "{ key %04x : 64b value %#llx }\n",
+				  key, make_u64(cfg[1], cfg[0]));
+			break;
+		default:
+			IOV_DEBUG(iov, "{ key %04x : %u dwords value %*ph }\n",
+				  key, len, (int)(len * sizeof(u32)), cfg);
+			break;
+		}
+
+		cfg += len;
+		cfg_size -= len;
+	}
+
+	return 0;
+}
+
+static int pf_push_configs(struct intel_iov *iov, unsigned int num)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int n;
+	u32 cfg_size;
+	u32 cfg_addr;
+	u32 *cfg;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	err = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&cfg);
+	if (unlikely(err))
+		return err;
+
+	cfg_addr = intel_guc_ggtt_offset(guc, vma);
+
+	for (n = 1; n <= num; n++) {
+		cfg_size = 0;
+
+		err = pf_validate_config(iov, n);
+		if (err != -ENODATA)
+			cfg_size = encode_config(cfg, &provisioning->configs[n]);
+
+		if (iov_to_gt(iov)->type == GT_MEDIA) {
+			struct intel_iov *root = iov_get_root(iov);
+			struct intel_iov_config *config = &root->pf.provisioning.configs[n];
+
+			cfg_size += encode_config_ggtt(cfg + cfg_size, config);
+		}
+
+		GEM_BUG_ON(cfg_size * sizeof(u32) > SZ_4K);
+		if (IS_ENABLED(CONFIG_DRM_I915_SELFTEST)) {
+			err = pf_verify_config_klvs(iov, cfg, cfg_size);
+			if (unlikely(err < 0))
+				goto fail;
+		}
+
+		if (cfg_size) {
+			err = guc_action_update_vf_cfg(guc, n, cfg_addr, cfg_size);
+			if (unlikely(err < 0))
+				goto fail;
+		}
+	}
+	err = 0;
+	provisioning->num_pushed = num;
+
+fail:
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return err;
+}
+
+static int pf_push_no_configs(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	unsigned int n;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	for (n = iov->pf.provisioning.num_pushed; n > 0; n--) {
+		err = guc_action_update_vf_cfg(guc, n, 0, 0);
+		if (unlikely(err < 0))
+			break;
+	}
+	iov->pf.provisioning.num_pushed = n;
+
+	return n ? -ESTALE : 0;
+}
+
+/**
+ * intel_iov_provisioning_push() - Push provisioning configs to GuC.
+ * @iov: the IOV struct
+ * @num: number of configurations to push
+ *
+ * Push provisioning configs for @num VFs or reset configs for previously
+ * configured VFs.
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_provisioning_push(struct intel_iov *iov, unsigned int num)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(num > pf_get_totalvfs(iov));
+
+	err = pf_get_status(iov);
+	if (unlikely(err < 0))
+		goto fail;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	if (num)
+		err = pf_push_configs(iov, num);
+	else
+		err = pf_push_no_configs(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	if (unlikely(err))
+		goto fail;
+
+	return 0;
+fail:
+	IOV_ERROR(iov, "Failed to push configurations (%pe)", ERR_PTR(err));
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_fini - Unprovision all resources.
+ * @iov: the IOV struct
+ *
+ * This function can only be called on PF.
+ */
+void intel_iov_provisioning_fini(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	pf_fini_reprovisioning_worker(iov);
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	pf_unprovision_all(iov);
+	mutex_unlock(pf_provisioning_mutex(iov));
+}
+
+/**
+ * intel_iov_provisioning_restart() - Restart provisioning state.
+ * @iov: the IOV struct
+ *
+ * Mark provisioning state as not pushed to GuC.
+ *
+ * This function shall be called only on PF.
+ */
+void intel_iov_provisioning_restart(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	iov->pf.provisioning.num_pushed = 0;
+
+	if (pf_get_status(iov) > 0)
+		pf_start_reprovisioning_worker(iov);
+}
+
+static void pf_reprovision_pf(struct intel_iov *iov)
+{
+	IOV_DEBUG(iov, "reprovisioning PF\n");
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	pf_reprovision_sched_if_idle(iov);
+	pf_reprovision_reset_engine(iov);
+	pf_reprovision_sample_period(iov);
+	pf_reprovision_exec_quantum(iov, PFID);
+	pf_reprovision_preempt_timeout(iov, PFID);
+	mutex_unlock(pf_provisioning_mutex(iov));
+}
+
+/*
+ * pf_do_reprovisioning - Push again provisioning of the resources.
+ * @iov: the IOV struct from within the GT to be affected
+ */
+static void pf_do_reprovisioning(struct intel_iov *iov)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	unsigned int numvfs = pf_get_numvfs(iov);
+	intel_wakeref_t wakeref;
+
+	with_intel_runtime_pm(rpm, wakeref)
+		pf_reprovision_pf(iov);
+
+	if (!numvfs)
+		return;
+
+	IOV_DEBUG(iov, "reprovisioning %u VFs\n", numvfs);
+	with_intel_runtime_pm(rpm, wakeref)
+		intel_iov_provisioning_push(iov, numvfs);
+}
+
+/*
+ * pf_reprovisioning_worker_func - Worker to re-push provisioning of the resources.
+ * @w: the worker struct from inside IOV struct
+ *
+ * After GuC reset, provisioning information within is lost. This worker function
+ * allows to schedule re-sending the provisioning outside of reset handler.
+ */
+static void pf_reprovisioning_worker_func(struct work_struct *w)
+{
+	struct intel_iov *iov = container_of(w, typeof(*iov), pf.provisioning.worker);
+
+	pf_do_reprovisioning(iov);
+}
+
+static void pf_init_reprovisioning_worker(struct intel_iov *iov)
+{
+	INIT_WORK(&iov->pf.provisioning.worker, pf_reprovisioning_worker_func);
+}
+
+static void pf_start_reprovisioning_worker(struct intel_iov *iov)
+{
+	queue_work(system_unbound_wq, &iov->pf.provisioning.worker);
+}
+
+static void pf_fini_reprovisioning_worker(struct intel_iov *iov)
+{
+	cancel_work_sync(&iov->pf.provisioning.worker);
+}
+
+/**
+ * intel_iov_provisioning_clear - Clear VF provisioning data.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_clear(struct intel_iov *iov, unsigned int id)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	struct intel_guc *guc = iov_to_guc(iov);
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+	GEM_BUG_ON(id == PFID);
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	with_intel_runtime_pm(rpm, wakeref) {
+		err = guc_action_update_vf_cfg(guc, id, 0, 0);
+		if (!err)
+			pf_unprovision_config(iov, id);
+	}
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to unprovision VF%u (%pe)\n",
+			  id, ERR_PTR(err));
+
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_print_ggtt - Print GGTT provisioning data.
+ * @iov: the IOV struct
+ * @p: the DRM printer
+ *
+ * Print GGTT provisioning data for all VFs.
+ * VFs without GGTT provisioning are ignored.
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_print_ggtt(struct intel_iov *iov, struct drm_printer *p)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	unsigned int n, total_vfs = pf_get_totalvfs(iov);
+	const struct intel_iov_config *config;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (unlikely(!provisioning))
+		return -ENODATA;
+
+	for (n = 1; n <= total_vfs; n++) {
+		config = &provisioning->configs[n];
+		if (!drm_mm_node_allocated(&config->ggtt_region))
+			continue;
+
+		drm_printf(p, "VF%u:\t%#08llx-%#08llx\t(%lluK)\n",
+				n,
+				config->ggtt_region.start,
+				config->ggtt_region.start + config->ggtt_region.size - 1,
+				config->ggtt_region.size / SZ_1K);
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_print_available_ggtt - Print available GGTT ranges.
+ * @iov: the IOV struct
+ * @p: the DRM printer
+ *
+ * Print GGTT ranges that are available for the provisioning.
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_print_available_ggtt(struct intel_iov *iov, struct drm_printer *p)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	const struct drm_mm *mm = &ggtt->vm.mm;
+	const struct drm_mm_node *entry;
+	u64 alignment = pf_get_ggtt_alignment(iov);
+	u64 spare = pf_get_spare_ggtt(iov);
+	u64 hole_min_start = ggtt->pin_bias;
+	u64 hole_start, hole_end, hole_size;
+	u64 avail, total = 0;
+
+	mutex_lock(&ggtt->vm.mutex);
+
+	drm_mm_for_each_hole(entry, mm, hole_start, hole_end) {
+		hole_start = max(hole_start, hole_min_start);
+		hole_start = ALIGN(hole_start, alignment);
+		hole_end = ALIGN_DOWN(hole_end, alignment);
+		if (hole_start >= hole_end)
+			continue;
+		hole_size = hole_end - hole_start;
+		total += hole_size;
+
+		drm_printf(p, "range:\t%#08llx-%#08llx\t(%lluK)\n",
+			   hole_start, hole_end - 1,
+			   hole_size / SZ_1K);
+	}
+
+	mutex_unlock(&ggtt->vm.mutex);
+
+	avail = total > spare ? total - spare : 0;
+	drm_printf(p, "total:\t%llu\t(%lluK)\n", total, total / SZ_1K);
+	drm_printf(p, "avail:\t%llu\t(%lluK)\n", avail, avail / SZ_1K);
+
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_print_ctxs - Print contexts provisioning data.
+ * @iov: the IOV struct
+ * @p: the DRM printer
+ *
+ * Print contexts provisioning data for all VFs.
+ * VFs without contexts provisioning are ignored.
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_print_ctxs(struct intel_iov *iov, struct drm_printer *p)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	unsigned int n, total_vfs = pf_get_totalvfs(iov);
+	const struct intel_iov_config *config;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (unlikely(!provisioning))
+		return -ENODATA;
+
+	for (n = 1; n <= total_vfs; n++) {
+		config = &provisioning->configs[n];
+		if (!config->num_ctxs)
+			continue;
+
+		drm_printf(p, "VF%u:\t%hu-%u\t(%hu)\n",
+				n,
+				config->begin_ctx,
+				config->begin_ctx + config->num_ctxs - 1,
+				config->num_ctxs);
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_provisioning_print_dbs - Print doorbells provisioning data.
+ * @iov: the IOV struct
+ * @p: the DRM printer
+ *
+ * Print doorbells provisioning data for all VFs.
+ * VFs without doorbells provisioning are ignored.
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_print_dbs(struct intel_iov *iov, struct drm_printer *p)
+{
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	unsigned int n, total_vfs = pf_get_totalvfs(iov);
+	const struct intel_iov_config *config;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (unlikely(!provisioning))
+		return -ENODATA;
+
+	for (n = 1; n <= total_vfs; n++) {
+		config = &provisioning->configs[n];
+		if (!config->num_dbs)
+			continue;
+
+		drm_printf(p, "VF%u:\t%hu-%u\t(%hu)\n",
+				n,
+				config->begin_db,
+				config->begin_db + config->num_dbs - 1,
+				config->num_dbs);
+	}
+
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+
+static int pf_push_self_config(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	u64 ggtt_start = intel_wopcm_guc_size(&iov_to_gt(iov)->wopcm);
+	u64 ggtt_size = GUC_GGTT_TOP - ggtt_start;
+	int err = 0;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(intel_wopcm_guc_size(&iov_to_gt(iov)->wopcm) > GUC_GGTT_TOP);
+
+	err |= guc_update_vf_klv64(guc, PFID, GUC_KLV_VF_CFG_GGTT_START_KEY, ggtt_start);
+	err |= guc_update_vf_klv64(guc, PFID, GUC_KLV_VF_CFG_GGTT_SIZE_KEY, ggtt_size);
+
+	err |= guc_update_vf_klv32(guc, PFID, GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID_KEY, 0);
+	err |= guc_update_vf_klv32(guc, PFID, GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY, GUC_MAX_CONTEXT_ID);
+
+	err |= guc_update_vf_klv32(guc, PFID, GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID_KEY, 0);
+	err |= guc_update_vf_klv32(guc, PFID, GUC_KLV_VF_CFG_NUM_DOORBELLS_KEY, GUC_NUM_DOORBELLS);
+
+	return err ? -EREMOTEIO : 0;
+}
+
+/**
+ * intel_iov_provisioning_force_vgt_mode - Turn on GuC virtualization mode.
+ * @iov: the IOV struct
+ *
+ * By default GuC starts in 'native' mode and enables 'virtualization' mode
+ * only after it receives from the PF some VF's configuration data. While this
+ * happens naturally while PF begins VFs provisioning, we might need this sooner
+ * during selftests. This function will perform minimal provisioning steps to
+ * let GuC believe it has to switch 'virtualization' mode.
+ *
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_force_vgt_mode(struct intel_iov *iov)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	assert_rpm_wakelock_held(iov_to_gt(iov)->uncore->rpm);
+
+	if (pf_get_status(iov) < 0)
+		return -EIO;
+
+	if (iov->pf.provisioning.self_done)
+		return 0;
+
+	err = pf_push_self_config(iov);
+	if (err) {
+		IOV_ERROR(iov, "Failed to force VGT mode (%pe)\n", ERR_PTR(err));
+		return err;
+	}
+
+	iov->pf.provisioning.self_done = true;
+	return 0;
+}
+
+#include "selftests/selftest_live_iov_provisioning.c"
+#endif /* CONFIG_DRM_I915_SELFTEST */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h
new file mode 100644
index 000000000000..0641a5f6e2da
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h
@@ -0,0 +1,76 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_PROVISIONING_H__
+#define __INTEL_IOV_PROVISIONING_H__
+
+#include <linux/types.h>
+#include "intel_iov_types.h"
+
+struct drm_printer;
+struct intel_iov;
+
+void intel_iov_provisioning_init_early(struct intel_iov *iov);
+void intel_iov_provisioning_release(struct intel_iov *iov);
+void intel_iov_provisioning_init(struct intel_iov *iov);
+void intel_iov_provisioning_fini(struct intel_iov *iov);
+
+int intel_iov_provisioning_set_sched_if_idle(struct intel_iov *iov, bool enable);
+bool intel_iov_provisioning_get_sched_if_idle(struct intel_iov *iov);
+int intel_iov_provisioning_set_reset_engine(struct intel_iov *iov, bool enable);
+bool intel_iov_provisioning_get_reset_engine(struct intel_iov *iov);
+int intel_iov_provisioning_set_sample_period(struct intel_iov *iov, u32 value);
+u32 intel_iov_provisioning_get_sample_period(struct intel_iov *iov);
+
+void intel_iov_provisioning_restart(struct intel_iov *iov);
+int intel_iov_provisioning_auto(struct intel_iov *iov, unsigned int num_vfs);
+int intel_iov_provisioning_verify(struct intel_iov *iov, unsigned int num_vfs);
+int intel_iov_provisioning_push(struct intel_iov *iov, unsigned int num);
+
+int intel_iov_provisioning_set_ggtt(struct intel_iov *iov, unsigned int id, u64 size);
+u64 intel_iov_provisioning_get_ggtt(struct intel_iov *iov, unsigned int id);
+int intel_iov_provisioning_set_spare_ggtt(struct intel_iov *iov, u64 size);
+u64 intel_iov_provisioning_get_spare_ggtt(struct intel_iov *iov);
+u64 intel_iov_provisioning_query_free_ggtt(struct intel_iov *iov);
+u64 intel_iov_provisioning_query_max_ggtt(struct intel_iov *iov);
+
+int intel_iov_provisioning_set_ctxs(struct intel_iov *iov, unsigned int id, u16 num_ctxs);
+u16 intel_iov_provisioning_get_ctxs(struct intel_iov *iov, unsigned int id);
+int intel_iov_provisioning_set_spare_ctxs(struct intel_iov *iov, u16 spare);
+u16 intel_iov_provisioning_get_spare_ctxs(struct intel_iov *iov);
+u16 intel_iov_provisioning_query_max_ctxs(struct intel_iov *iov);
+u16 intel_iov_provisioning_query_free_ctxs(struct intel_iov *iov);
+
+int intel_iov_provisioning_set_dbs(struct intel_iov *iov, unsigned int id, u16 num_dbs);
+u16 intel_iov_provisioning_get_dbs(struct intel_iov *iov, unsigned int id);
+int intel_iov_provisioning_set_spare_dbs(struct intel_iov *iov, u16 spare);
+u16 intel_iov_provisioning_get_spare_dbs(struct intel_iov *iov);
+u16 intel_iov_provisioning_query_free_dbs(struct intel_iov *iov);
+u16 intel_iov_provisioning_query_max_dbs(struct intel_iov *iov);
+
+int intel_iov_provisioning_set_exec_quantum(struct intel_iov *iov, unsigned int id, u32 exec_quantum);
+u32 intel_iov_provisioning_get_exec_quantum(struct intel_iov *iov, unsigned int id);
+
+int intel_iov_provisioning_set_preempt_timeout(struct intel_iov *iov, unsigned int id, u32 preempt_timeout);
+u32 intel_iov_provisioning_get_preempt_timeout(struct intel_iov *iov, unsigned int id);
+
+int intel_iov_provisioning_set_threshold(struct intel_iov *iov, unsigned int id,
+					 enum intel_iov_threshold threshold, u32 value);
+u32 intel_iov_provisioning_get_threshold(struct intel_iov *iov, unsigned int id,
+					 enum intel_iov_threshold threshold);
+
+int intel_iov_provisioning_clear(struct intel_iov *iov, unsigned int id);
+
+int intel_iov_provisioning_print_ggtt(struct intel_iov *iov, struct drm_printer *p);
+int intel_iov_provisioning_print_ctxs(struct intel_iov *iov, struct drm_printer *p);
+int intel_iov_provisioning_print_dbs(struct intel_iov *iov, struct drm_printer *p);
+
+int intel_iov_provisioning_print_available_ggtt(struct intel_iov *iov, struct drm_printer *p);
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+int intel_iov_provisioning_force_vgt_mode(struct intel_iov *iov);
+#endif /* CONFIG_DRM_I915_SELFTEST */
+
+#endif /* __INTEL_IOV_PROVISIONING_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c
new file mode 100644
index 000000000000..ed7ea1f9d3c5
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c
@@ -0,0 +1,752 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include <drm/drm_print.h>
+#include <linux/bitfield.h>
+#include <linux/crc32.h>
+
+#include "abi/iov_actions_abi.h"
+#include "abi/iov_actions_mmio_abi.h"
+#include "abi/iov_version_abi.h"
+#include "gt/intel_gt_regs.h"
+#include "gt/uc/abi/guc_actions_vf_abi.h"
+#include "gt/uc/abi/guc_klvs_abi.h"
+#include "gt/uc/abi/guc_version_abi.h"
+#include "gt/uc/intel_guc_print.h"
+#include "i915_drv.h"
+#include "intel_iov_relay.h"
+#include "intel_iov_utils.h"
+#include "intel_iov_types.h"
+#include "intel_iov_query.h"
+
+static int guc_action_vf_reset(struct intel_guc *guc)
+{
+	u32 request[GUC_HXG_REQUEST_MSG_MIN_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_VF_RESET),
+	};
+	int ret;
+
+	ret = intel_guc_send_mmio(guc, request, ARRAY_SIZE(request), NULL, 0);
+
+	return ret > 0 ? -EPROTO : ret;
+}
+
+static int vf_reset_guc_state(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = guc_action_vf_reset(guc);
+	if (unlikely(err))
+		IOV_PROBE_ERROR(iov, "Failed to reset GuC state (%pe)\n",
+				ERR_PTR(err));
+
+	return err;
+}
+
+static int guc_action_match_version(struct intel_guc *guc, u32 *branch,
+				    u32 *major, u32 *minor, u32 *patch)
+{
+	u32 request[VF2GUC_MATCH_VERSION_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION,
+			   GUC_ACTION_VF2GUC_MATCH_VERSION),
+		FIELD_PREP(VF2GUC_MATCH_VERSION_REQUEST_MSG_1_BRANCH,
+			   *branch) |
+		FIELD_PREP(VF2GUC_MATCH_VERSION_REQUEST_MSG_1_MAJOR,
+			   *major) |
+		FIELD_PREP(VF2GUC_MATCH_VERSION_REQUEST_MSG_1_MINOR,
+			   *minor),
+	};
+	u32 response[VF2GUC_MATCH_VERSION_RESPONSE_MSG_LEN];
+	int ret;
+
+	ret = intel_guc_send_mmio(guc, request, ARRAY_SIZE(request),
+				  response, ARRAY_SIZE(response));
+	if (unlikely(ret < 0))
+		return ret;
+
+	GEM_BUG_ON(ret != VF2GUC_MATCH_VERSION_RESPONSE_MSG_LEN);
+	if (unlikely(FIELD_GET(VF2GUC_MATCH_VERSION_RESPONSE_MSG_0_MBZ, response[0])))
+		return -EPROTO;
+
+	*branch = FIELD_GET(VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_BRANCH, response[1]);
+	*major = FIELD_GET(VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_MAJOR, response[1]);
+	*minor = FIELD_GET(VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_MINOR, response[1]);
+	*patch = FIELD_GET(VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_PATCH, response[1]);
+
+	return 0;
+}
+
+static int vf_handshake_with_guc(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	u32 branch, major, minor, patch;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	/* XXX for now, all platforms use same latest version */
+	branch = GUC_VERSION_BRANCH_ANY;
+	major = GUC_VF_VERSION_LATEST_MAJOR;
+	minor = GUC_VF_VERSION_LATEST_MINOR;
+
+	err = guc_action_match_version(guc, &branch, &major, &minor, &patch);
+	if (unlikely(err))
+		goto fail;
+
+	/* XXX we only support one version, there must be a match */
+	if (major != GUC_VF_VERSION_LATEST_MAJOR || minor != GUC_VF_VERSION_LATEST_MINOR)
+		goto fail;
+
+	guc_info(iov_to_guc(iov), "interface version %u.%u.%u.%u\n",
+		 branch, major, minor, patch);
+
+	iov->vf.config.guc_abi.branch = branch;
+	iov->vf.config.guc_abi.major = major;
+	iov->vf.config.guc_abi.minor = minor;
+	iov->vf.config.guc_abi.patch = patch;
+	return 0;
+
+fail:
+	IOV_PROBE_ERROR(iov, "Unable to confirm version %u.%u (%pe)\n",
+			major, minor, ERR_PTR(err));
+
+	/* try again with *any* just to query which version is supported */
+	branch = GUC_VERSION_BRANCH_ANY;
+	major = GUC_VERSION_MAJOR_ANY;
+	minor = GUC_VERSION_MINOR_ANY;
+	if (!guc_action_match_version(guc, &branch, &major, &minor, &patch))
+		IOV_PROBE_ERROR(iov, "Found interface version %u.%u.%u.%u\n",
+				branch, major, minor, patch);
+
+	return err;
+}
+
+/**
+ * intel_iov_query_bootstrap - Query interface version data over MMIO.
+ * @iov: the IOV struct
+ *
+ * This function is for VF use only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_query_bootstrap(struct intel_iov *iov)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = vf_reset_guc_state(iov);
+	if (unlikely(err))
+		return err;
+
+	err = vf_handshake_with_guc(iov);
+	if (unlikely(err))
+		return err;
+
+	return 0;
+}
+
+static int guc_action_query_single_klv(struct intel_guc *guc, u32 key,
+				       u32 *value, u32 value_len)
+{
+	u32 request[VF2GUC_QUERY_SINGLE_KLV_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION,
+			   GUC_ACTION_VF2GUC_QUERY_SINGLE_KLV),
+		FIELD_PREP(VF2GUC_QUERY_SINGLE_KLV_REQUEST_MSG_1_KEY, key),
+	};
+	u32 response[VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_MAX_LEN];
+	u32 length;
+	int ret;
+
+	ret = intel_guc_send_mmio(guc, request, ARRAY_SIZE(request),
+				  response, ARRAY_SIZE(response));
+	if (unlikely(ret < 0))
+		return ret;
+
+	GEM_BUG_ON(ret != VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_MAX_LEN);
+	if (unlikely(FIELD_GET(VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_0_MBZ, response[0])))
+		return -EPROTO;
+
+	length = FIELD_GET(VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_0_LENGTH, response[0]);
+	if (unlikely(length > value_len))
+		return -EOVERFLOW;
+	if (unlikely(length < value_len))
+		return -ENODATA;
+
+	GEM_BUG_ON(length != value_len);
+	switch (value_len) {
+	default:
+		GEM_BUG_ON(value_len);
+		return -EINVAL;
+	case 3:
+		value[2] = FIELD_GET(VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_3_VALUE96, response[3]);
+		fallthrough;
+	case 2:
+		value[1] = FIELD_GET(VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_2_VALUE64, response[2]);
+		fallthrough;
+	case 1:
+		value[0] = FIELD_GET(VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_1_VALUE32, response[1]);
+		fallthrough;
+	case 0:
+		break;
+	}
+
+	return 0;
+}
+
+static int guc_action_query_single_klv32(struct intel_guc *guc, u32 key, u32 *value32)
+{
+	return guc_action_query_single_klv(guc, key, value32, 1);
+}
+
+static int guc_action_query_single_klv64(struct intel_guc *guc, u32 key, u64 *value64)
+{
+	u32 value[2];
+	int err;
+
+	err = guc_action_query_single_klv(guc, key, value, ARRAY_SIZE(value));
+	if (unlikely(err))
+		return err;
+
+	*value64 = (u64)value[1] << 32 | value[0];
+	return 0;
+}
+
+static int vf_get_ggtt_info(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	u64 start, size;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(iov->vf.config.ggtt_size);
+
+	err = guc_action_query_single_klv64(guc, GUC_KLV_VF_CFG_GGTT_START_KEY, &start);
+	if (unlikely(err))
+		return err;
+
+	err = guc_action_query_single_klv64(guc, GUC_KLV_VF_CFG_GGTT_SIZE_KEY, &size);
+	if (unlikely(err))
+		return err;
+
+	IOV_DEBUG(iov, "GGTT %#llx-%#llx = %lluK\n",
+		  start, start + size - 1, size / SZ_1K);
+
+	iov->vf.config.ggtt_base = start;
+	iov->vf.config.ggtt_size = size;
+
+	return iov->vf.config.ggtt_size ? 0 : -ENODATA;
+}
+
+static int vf_get_submission_cfg(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	u32 num_ctxs, num_dbs;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(iov->vf.config.num_ctxs);
+
+	err = guc_action_query_single_klv32(guc, GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY, &num_ctxs);
+	if (unlikely(err))
+		return err;
+
+	err = guc_action_query_single_klv32(guc, GUC_KLV_VF_CFG_NUM_DOORBELLS_KEY, &num_dbs);
+	if (unlikely(err))
+		return err;
+
+	IOV_DEBUG(iov, "CTXs %u DBs %u\n", num_ctxs, num_dbs);
+
+	iov->vf.config.num_ctxs = num_ctxs;
+	iov->vf.config.num_dbs = num_dbs;
+
+	return iov->vf.config.num_ctxs ? 0 : -ENODATA;
+}
+
+/**
+ * intel_iov_query_config - Query IOV config data over MMIO.
+ * @iov: the IOV struct
+ *
+ * This function is for VF use only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_query_config(struct intel_iov *iov)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = vf_get_ggtt_info(iov);
+	if (unlikely(err))
+		return err;
+
+	err = vf_get_submission_cfg(iov);
+	if (unlikely(err))
+		return err;
+
+	return 0;
+}
+
+static int iov_action_handshake(struct intel_iov *iov, u32 *major, u32 *minor)
+{
+	u32 request[VF2PF_HANDSHAKE_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, *major) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, *minor),
+	};
+	u32 response[VF2PF_HANDSHAKE_RESPONSE_MSG_LEN];
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	ret = intel_iov_relay_send_to_pf(&iov->relay,
+					 request, ARRAY_SIZE(request),
+					 response, ARRAY_SIZE(response));
+	if (unlikely(ret < 0))
+		return ret;
+
+	if (unlikely(ret != VF2PF_HANDSHAKE_RESPONSE_MSG_LEN))
+		return -EPROTO;
+
+	if (unlikely(FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_0_MBZ, response[0])))
+		return -EPROTO;
+
+	*major = FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MAJOR, response[1]);
+	*minor = FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MINOR, response[1]);
+
+	return 0;
+}
+
+static int vf_handshake_with_pf(struct intel_iov *iov)
+{
+	u32 major_wanted = IOV_VERSION_LATEST_MAJOR;
+	u32 minor_wanted = IOV_VERSION_LATEST_MINOR;
+	u32 major = major_wanted, minor = minor_wanted;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = iov_action_handshake(iov, &major, &minor);
+	if (unlikely(err))
+		goto failed;
+
+	IOV_DEBUG(iov, "Using ABI %u.%02u\n", major, minor);
+	return 0;
+
+failed:
+	IOV_PROBE_ERROR(iov, "Unable to confirm ABI version %u.%02u (%pe)\n",
+			major, minor, ERR_PTR(err));
+	return err;
+}
+
+/**
+ * intel_iov_query_version - Query IOV version info.
+ * @iov: the IOV struct
+ *
+ * This function is for VF use only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_query_version(struct intel_iov *iov)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = vf_handshake_with_pf(iov);
+	if (unlikely(err))
+		goto failed;
+
+	return 0;
+
+failed:
+	IOV_PROBE_ERROR(iov, "Failed to get version info (%pe)\n", ERR_PTR(err));
+	return err;
+}
+
+static const i915_reg_t tgl_early_regs[] = {
+	RPM_CONFIG0,			/* _MMIO(0x0D00) */
+	GEN10_MIRROR_FUSE3,		/* _MMIO(0x9118) */
+	GEN11_EU_DISABLE,		/* _MMIO(0x9134) */
+	GEN11_GT_SLICE_ENABLE,		/* _MMIO(0x9138) */
+	GEN12_GT_GEOMETRY_DSS_ENABLE,	/* _MMIO(0x913C) */
+	GEN11_GT_VEBOX_VDBOX_DISABLE,	/* _MMIO(0x9140) */
+	CTC_MODE,			/* _MMIO(0xA26C) */
+	GEN11_HUC_KERNEL_LOAD_INFO,	/* _MMIO(0xC1DC) */
+};
+
+static const i915_reg_t mtl_early_regs[] = {
+	RPM_CONFIG0,			/* _MMIO(0x0D00) */
+	XEHP_FUSE4,			/* _MMIO(0x9114) */
+	GEN10_MIRROR_FUSE3,		/* _MMIO(0x9118) */
+	HSW_PAVP_FUSE1,			/* _MMIO(0x911C) */
+	XEHP_EU_ENABLE,			/* _MMIO(0x9134) */
+	GEN12_GT_GEOMETRY_DSS_ENABLE,	/* _MMIO(0x913C) */
+	GEN11_GT_VEBOX_VDBOX_DISABLE,	/* _MMIO(0x9140) */
+	GEN12_GT_COMPUTE_DSS_ENABLE,	/* _MMIO(0x9144) */
+	XEHPC_GT_COMPUTE_DSS_ENABLE_EXT,/* _MMIO(0x9148) */
+	CTC_MODE,			/* _MMIO(0xA26C) */
+	GEN11_HUC_KERNEL_LOAD_INFO,	/* _MMIO(0xC1DC) */
+	MTL_GT_ACTIVITY_FACTOR,		/* _MMIO(0x138010) */
+};
+
+static const i915_reg_t *get_early_regs(struct drm_i915_private *i915,
+					unsigned int *size)
+{
+	const i915_reg_t *regs;
+
+	if (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 70)) {
+		regs = mtl_early_regs;
+		*size = ARRAY_SIZE(mtl_early_regs);
+	} else if (IS_TIGERLAKE(i915) || IS_ALDERLAKE_S(i915) || IS_ALDERLAKE_P(i915)) {
+		regs = tgl_early_regs;
+		*size = ARRAY_SIZE(tgl_early_regs);
+	} else {
+		MISSING_CASE(GRAPHICS_VER(i915));
+		regs = ERR_PTR(-ENODEV);
+		*size = 0;
+	}
+
+	return regs;
+}
+
+static void vf_cleanup_runtime_info(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	kfree(iov->vf.runtime.regs);
+	iov->vf.runtime.regs = NULL;
+	iov->vf.runtime.regs_size = 0;
+}
+
+static int vf_prepare_runtime_info(struct intel_iov *iov, unsigned int regs_size,
+				   unsigned int alignment)
+{
+	unsigned int regs_size_up = roundup(regs_size, alignment);
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(iov->vf.runtime.regs_size && !iov->vf.runtime.regs);
+
+	iov->vf.runtime.regs = krealloc(iov->vf.runtime.regs,
+					regs_size_up * sizeof(struct vf_runtime_reg),
+					__GFP_ZERO | GFP_NOWAIT | __GFP_NOWARN);
+	if (unlikely(!iov->vf.runtime.regs))
+		return -ENOMEM;
+
+	iov->vf.runtime.regs_size = regs_size;
+
+	return regs_size_up;
+}
+
+static void vf_show_runtime_info(struct intel_iov *iov)
+{
+	struct vf_runtime_reg *vf_regs = iov->vf.runtime.regs;
+	unsigned int size = iov->vf.runtime.regs_size;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	for (; size--; vf_regs++) {
+		IOV_DEBUG(iov, "RUNTIME reg[%#x] = %#x\n",
+			  vf_regs->offset, vf_regs->value);
+	}
+}
+
+static int guc_send_mmio_relay(struct intel_guc *guc, const u32 *request, u32 len,
+			       u32 *response, u32 response_size)
+{
+	u32 magic1, magic2;
+	int ret;
+
+	GEM_BUG_ON(len < VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_MIN_LEN);
+	GEM_BUG_ON(response_size < VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_MIN_LEN);
+
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, request[0]) != GUC_HXG_ORIGIN_HOST);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, request[0]) != GUC_HXG_TYPE_REQUEST);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_REQUEST_MSG_0_ACTION, request[0]) !=
+			     GUC_ACTION_VF2GUC_MMIO_RELAY_SERVICE);
+
+	magic1 = FIELD_GET(VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_0_MAGIC, request[0]);
+
+	ret = intel_guc_send_mmio(guc, request, len, response, response_size);
+	if (unlikely(ret < 0))
+		return ret;
+
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, response[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, response[0]) != GUC_HXG_TYPE_RESPONSE_SUCCESS);
+
+	magic2 = FIELD_GET(VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_0_MAGIC, response[0]);
+
+	if (unlikely(magic1 != magic2))
+		return -EPROTO;
+
+	return ret;
+}
+
+static u32 mmio_relay_header(u32 opcode, u32 magic)
+{
+	return FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+	       FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+	       FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_MMIO_RELAY_SERVICE) |
+	       FIELD_PREP(VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_0_MAGIC, magic) |
+	       FIELD_PREP(VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_0_OPCODE, opcode);
+}
+
+static int vf_handshake_with_pf_mmio(struct intel_iov *iov)
+{
+	u32 major_wanted = IOV_VERSION_LATEST_MAJOR;
+	u32 minor_wanted = IOV_VERSION_LATEST_MINOR;
+	u32 request[VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_MAX_LEN] = {
+		mmio_relay_header(IOV_OPCODE_VF2PF_MMIO_HANDSHAKE, 0xF),
+		FIELD_PREP(VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_1_MAJOR, major_wanted) |
+		FIELD_PREP(VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_1_MINOR, minor_wanted),
+	};
+	u32 response[VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_MAX_LEN];
+	u32 major, minor;
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	ret = guc_send_mmio_relay(iov_to_guc(iov), request, ARRAY_SIZE(request),
+				  response, ARRAY_SIZE(response));
+	if (unlikely(ret < 0))
+		goto failed;
+
+	major = FIELD_GET(VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_1_MAJOR, response[1]);
+	minor = FIELD_GET(VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_1_MINOR, response[1]);
+	if (unlikely(major != major_wanted || minor != minor_wanted)) {
+		ret = -ENOPKG;
+		goto failed;
+	}
+
+	IOV_DEBUG(iov, "Using ABI %u.%02u\n", major, minor);
+	return 0;
+
+failed:
+	IOV_PROBE_ERROR(iov, "Unable to confirm ABI version %u.%02u (%pe)\n",
+			major_wanted, minor_wanted, ERR_PTR(ret));
+	return -ECONNREFUSED;
+}
+
+static int vf_get_runtime_info_mmio(struct intel_iov *iov)
+{
+	u32 request[VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_MAX_LEN];
+	u32 response[VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_MAX_LEN];
+	u32 chunk = VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_NUM_OFFSET;
+	unsigned int size, size_up, i, n;
+	struct vf_runtime_reg *vf_regs;
+	const i915_reg_t *regs;
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	BUILD_BUG_ON(VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_NUM_OFFSET >
+		     VF2PF_MMIO_GET_RUNTIME_RESPONSE_MSG_NUM_VALUE);
+
+	regs = get_early_regs(iov_to_i915(iov), &size);
+	if (IS_ERR(regs)) {
+		ret = PTR_ERR(regs);
+		goto failed;
+	}
+	if (!size)
+		return 0;
+
+	/*
+	 * We want to allocate slightly larger buffer in order to align
+	 * ourselves with GuC interface and avoid out-of-bounds write.
+	 */
+	ret = vf_prepare_runtime_info(iov, size, chunk);
+	if (unlikely(ret < 0))
+		goto failed;
+	vf_regs = iov->vf.runtime.regs;
+	size_up = ret;
+	GEM_BUG_ON(!size_up);
+
+	for (i = 0; i < size; i++)
+		vf_regs[i].offset = i915_mmio_reg_offset(regs[i]);
+
+	for (i = 0; i < size_up; i += chunk) {
+
+		request[0] = mmio_relay_header(IOV_OPCODE_VF2PF_MMIO_GET_RUNTIME, 0);
+
+		for (n = 0; n < chunk; n++)
+			request[1 + n] = vf_regs[i + n].offset;
+
+		/* we will use few bits from crc32 as magic */
+		u32p_replace_bits(request, crc32_le(0, (void *)request, sizeof(request)),
+				  VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_0_MAGIC);
+
+		ret = guc_send_mmio_relay(iov_to_guc(iov), request, ARRAY_SIZE(request),
+					  response, ARRAY_SIZE(response));
+		if (unlikely(ret < 0))
+			goto failed;
+		GEM_BUG_ON(ret != ARRAY_SIZE(response));
+
+		for (n = 0; n < chunk; n++)
+			vf_regs[i + n].value = response[1 + n];
+	}
+
+	return 0;
+
+failed:
+	vf_cleanup_runtime_info(iov);
+	return ret;
+}
+
+static int vf_get_runtime_info_relay(struct intel_iov *iov)
+{
+	struct drm_i915_private *i915 = iov_to_i915(iov);
+	u32 request[VF2PF_QUERY_RUNTIME_REQUEST_MSG_LEN];
+	u32 response[VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MAX_LEN];
+	u32 limit = (ARRAY_SIZE(response) - VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN) / 2;
+	u32 start = 0;
+	u32 count, remaining, num, i;
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(!limit);
+	assert_rpm_wakelock_held(&i915->runtime_pm);
+
+	request[0] = FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		     FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		     FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_QUERY_RUNTIME) |
+		     FIELD_PREP(VF2PF_QUERY_RUNTIME_REQUEST_MSG_0_LIMIT, limit);
+
+repeat:
+	request[1] = FIELD_PREP(VF2PF_QUERY_RUNTIME_REQUEST_MSG_1_START, start);
+	ret = intel_iov_relay_send_to_pf(&iov->relay,
+					 request, ARRAY_SIZE(request),
+					 response, ARRAY_SIZE(response));
+	if (unlikely(ret < 0))
+		goto failed;
+
+	if (unlikely(ret < VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN)) {
+		ret = -EPROTO;
+		goto failed;
+	}
+	if (unlikely((ret - VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN) % 2)) {
+		ret = -EPROTO;
+		goto failed;
+	}
+
+	num = (ret - VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN) / 2;
+	count = FIELD_GET(VF2PF_QUERY_RUNTIME_RESPONSE_MSG_0_COUNT, response[0]);
+	remaining = FIELD_GET(VF2PF_QUERY_RUNTIME_RESPONSE_MSG_1_REMAINING, response[1]);
+
+	IOV_DEBUG(iov, "count=%u num=%u ret=%d start=%u remaining=%u\n",
+		  count, num, ret, start, remaining);
+
+	if (unlikely(count != num)) {
+		ret = -EPROTO;
+		goto failed;
+	}
+
+	if (start == 0) {
+		ret = vf_prepare_runtime_info(iov, num + remaining, 1);
+		if (unlikely(ret < 0))
+			goto failed;
+	} else if (unlikely(start + num > iov->vf.runtime.regs_size)) {
+		ret = -EPROTO;
+		goto failed;
+	}
+
+	for (i = 0; i < num; ++i) {
+		struct vf_runtime_reg *reg = &iov->vf.runtime.regs[start + i];
+
+		reg->offset = response[VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN + 2 * i];
+		reg->value = response[VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN + 2 * i + 1];
+	}
+
+	if (remaining) {
+		start += num;
+		goto repeat;
+	}
+
+	return 0;
+
+failed:
+	vf_cleanup_runtime_info(iov);
+	return ret;
+}
+
+/**
+ * intel_iov_query_runtime - Query IOV runtime data.
+ * @iov: the IOV struct
+ * @early: use early MMIO access
+ *
+ * This function is for VF use only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_query_runtime(struct intel_iov *iov, bool early)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	if (early) {
+		err = vf_handshake_with_pf_mmio(iov);
+		if (unlikely(err))
+			goto failed;
+	}
+
+	if (early)
+		err = vf_get_runtime_info_mmio(iov);
+	else
+		err = vf_get_runtime_info_relay(iov);
+	if (unlikely(err))
+		goto failed;
+
+	vf_show_runtime_info(iov);
+	return 0;
+
+failed:
+	IOV_PROBE_ERROR(iov, "Failed to get runtime info (%pe)\n",
+			ERR_PTR(err));
+	return err;
+}
+
+/**
+ * intel_iov_query_fini - Cleanup all queried IOV data.
+ * @iov: the IOV struct
+ *
+ * This function is for VF use only.
+ */
+void intel_iov_query_fini(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	vf_cleanup_runtime_info(iov);
+}
+
+/**
+ * intel_iov_query_print_config - Print queried VF config.
+ * @iov: the IOV struct
+ * @p: the DRM printer
+ *
+ * This function is for VF use only.
+ */
+void intel_iov_query_print_config(struct intel_iov *iov, struct drm_printer *p)
+{
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	drm_printf(p, "GGTT range:\t%#08llx-%#08llx\n",
+			iov->vf.config.ggtt_base,
+			iov->vf.config.ggtt_base + iov->vf.config.ggtt_size - 1);
+	drm_printf(p, "GGTT size:\t%lluK\n", iov->vf.config.ggtt_size / SZ_1K);
+
+	drm_printf(p, "contexts:\t%hu\n", iov->vf.config.num_ctxs);
+	drm_printf(p, "doorbells:\t%hu\n", iov->vf.config.num_dbs);
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h
new file mode 100644
index 000000000000..f4737adf0155
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h
@@ -0,0 +1,22 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_QUERY_H__
+#define __INTEL_IOV_QUERY_H__
+
+#include <linux/types.h>
+
+struct drm_printer;
+struct intel_iov;
+
+int intel_iov_query_bootstrap(struct intel_iov *iov);
+int intel_iov_query_config(struct intel_iov *iov);
+int intel_iov_query_version(struct intel_iov *iov);
+int intel_iov_query_runtime(struct intel_iov *iov, bool early);
+void intel_iov_query_fini(struct intel_iov *iov);
+
+void intel_iov_query_print_config(struct intel_iov *iov, struct drm_printer *p);
+
+#endif /* __INTEL_IOV_QUERY_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h
new file mode 100644
index 000000000000..7ec0b60f0a46
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h
@@ -0,0 +1,16 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2020 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_REG_H__
+#define __INTEL_IOV_REG_H__
+
+/* ISR */
+#define I915_VF_IRQ_STATUS 0x0
+/* IIR */
+#define I915_VF_IRQ_SOURCE 0x400
+/* IMR */
+#define I915_VF_IRQ_ENABLE 0x440
+
+#endif /* __INTEL_IOV_REG_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c
new file mode 100644
index 000000000000..53cc1a676769
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c
@@ -0,0 +1,725 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include <linux/bitfield.h>
+
+#include "abi/iov_actions_abi.h"
+#include "abi/iov_actions_selftest_abi.h"
+#include "abi/iov_errors_abi.h"
+#include "abi/iov_messages_abi.h"
+#include "gt/intel_gt.h"
+#include "intel_iov.h"
+#include "intel_iov_relay.h"
+#include "intel_iov_service.h"
+#include "intel_iov_utils.h"
+#include "intel_runtime_pm.h"
+#include "i915_drv.h"
+#include "i915_gem.h"
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+static int relay_selftest_process_msg(struct intel_iov_relay *, u32, u32, const u32 *, u32);
+static int relay_selftest_guc_send_nb(struct intel_guc *, const u32 *, u32, u32);
+#define intel_guc_send    relay_selftest_override_not_available
+#define intel_guc_ct_send relay_selftest_override_not_available
+#define intel_guc_send_nb relay_selftest_guc_send_nb
+#endif
+
+static struct intel_iov *relay_to_iov(struct intel_iov_relay *relay)
+{
+	return container_of(relay, struct intel_iov, relay);
+}
+
+static struct intel_gt *relay_to_gt(struct intel_iov_relay *relay)
+{
+	return iov_to_gt(relay_to_iov(relay));
+}
+
+static struct intel_guc *relay_to_guc(struct intel_iov_relay *relay)
+{
+	return &relay_to_gt(relay)->uc.guc;
+}
+
+static struct drm_i915_private *relay_to_i915(struct intel_iov_relay *relay)
+{
+	return relay_to_gt(relay)->i915;
+}
+
+__maybe_unused
+static struct device *relay_to_dev(struct intel_iov_relay *relay)
+{
+	return relay_to_i915(relay)->drm.dev;
+}
+
+#define RELAY_DEBUG(_r, _f, ...) \
+	IOV_DEBUG(relay_to_iov(_r), "relay: " _f, ##__VA_ARGS__)
+#define RELAY_ERROR(_r, _f, ...) \
+	IOV_ERROR(relay_to_iov(_r), "relay: " _f, ##__VA_ARGS__)
+#define RELAY_PROBE_ERROR(_r, _f, ...) \
+	IOV_PROBE_ERROR(relay_to_iov(_r), "relay: " _f, ##__VA_ARGS__)
+
+/*
+ * How long should we wait for the response?
+ * For default timeout use CONFIG_DRM_I915_HEARTBEAT_INTERVAL like CTB does.
+ * If hearbeat interval is not enabled then wait forever.
+ */
+#define RELAY_TIMEOUT	(CONFIG_DRM_I915_HEARTBEAT_INTERVAL ?: MAX_SCHEDULE_TIMEOUT)
+
+static u32 relay_get_next_fence(struct intel_iov_relay *relay)
+{
+	u32 fence;
+
+	spin_lock(&relay->lock);
+	fence = ++relay->last_fence;
+	if (unlikely(!fence))
+		fence = relay->last_fence = 1;
+	spin_unlock(&relay->lock);
+	return fence;
+}
+
+struct pending_relay {
+	struct list_head link;
+	struct completion done;
+	u32 target;
+	u32 fence;
+	int reply;
+	u32 *response; /* can't be null */
+	u32 response_size;
+};
+
+static int pf_relay_send(struct intel_iov_relay *relay, u32 target,
+			 u32 relay_id, const u32 *msg, u32 len)
+{
+	u32 request[PF2GUC_RELAY_TO_VF_REQUEST_MSG_MAX_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_PF2GUC_RELAY_TO_VF),
+		FIELD_PREP(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, target),
+		FIELD_PREP(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, relay_id),
+	};
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(!target && !I915_SELFTEST_ONLY(relay->selftest.enable_loopback));
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(len + PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN >
+		   PF2GUC_RELAY_TO_VF_REQUEST_MSG_MAX_LEN);
+
+	memcpy(&request[PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN], msg, 4 * len);
+
+retry:
+	err = intel_guc_send_nb(relay_to_guc(relay), request,
+				PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN + len, 0);
+	if (unlikely(err == -EBUSY))
+		goto retry;
+
+	return err;
+}
+
+static int vf_relay_send(struct intel_iov_relay *relay,
+			 u32 relay_id, const u32 *msg, u32 len)
+{
+	u32 request[VF2GUC_RELAY_TO_PF_REQUEST_MSG_MAX_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_RELAY_TO_PF),
+		FIELD_PREP(VF2GUC_RELAY_TO_PF_REQUEST_MSG_1_RELAY_ID, relay_id),
+	};
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_VF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(len + VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN >
+		   VF2GUC_RELAY_TO_PF_REQUEST_MSG_MAX_LEN);
+
+	memcpy(&request[VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN], msg, 4 * len);
+
+retry:
+	err = intel_guc_send_nb(relay_to_guc(relay), request,
+				VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN + len, 0);
+	if (unlikely(err == -EBUSY))
+		goto retry;
+
+	return err;
+}
+
+static int relay_send(struct intel_iov_relay *relay, u32 target,
+		      u32 relay_id, const u32 *msg, u32 len)
+{
+	int err;
+
+	GEM_BUG_ON(!len);
+	RELAY_DEBUG(relay, "sending %s.%u to %u = %*ph\n",
+		    hxg_type_to_string(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0])),
+		    relay_id, target, 4 * len, msg);
+
+	if (target || I915_SELFTEST_ONLY(relay->selftest.enable_loopback))
+		err = pf_relay_send(relay, target, relay_id, msg, len);
+	else
+		err = vf_relay_send(relay, relay_id, msg, len);
+
+	if (unlikely(err < 0))
+		RELAY_PROBE_ERROR(relay, "Failed to send %s.%u to %u (%pe) %*ph\n",
+				  hxg_type_to_string(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0])),
+				  relay_id, target, ERR_PTR(err), 4 * len, msg);
+	return err;
+}
+
+/**
+ * intel_iov_relay_reply_to_vf - Send reply message to VF.
+ * @relay: the Relay struct
+ * @target: target VF number
+ * @relay_id: relay message ID (must match message ID from the request)
+ * @msg: response message (can't be NULL)
+ * @len: length of the response message (in dwords, can't be 0)
+ *
+ * This function will embed and send provided `IOV Message`_ to the GuC.
+ *
+ * This function can only be used by driver running in SR-IOV PF mode.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_relay_reply_to_vf(struct intel_iov_relay *relay, u32 target,
+				u32 relay_id, const u32 *msg, u32 len)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(!target && !I915_SELFTEST_ONLY(relay->selftest.enable_loopback));
+	GEM_BUG_ON(len < GUC_HXG_MSG_MIN_LEN);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) == GUC_HXG_TYPE_REQUEST);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) == GUC_HXG_TYPE_EVENT);
+
+	return relay_send(relay, target, relay_id, msg, len);
+}
+
+static int relay_send_success(struct intel_iov_relay *relay, u32 target,
+			       u32 relay_id, u32 data)
+{
+	u32 msg[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS) |
+		FIELD_PREP(GUC_HXG_RESPONSE_MSG_0_DATA0, data),
+	};
+
+	GEM_WARN_ON(!FIELD_FIT(GUC_HXG_RESPONSE_MSG_0_DATA0, data));
+
+	return relay_send(relay, target, relay_id, msg, ARRAY_SIZE(msg));
+}
+
+/**
+ * intel_iov_relay_reply_ack_to_vf - Send simple success response to VF.
+ * @relay: the Relay struct
+ * @target: target VF number (can't be 0)
+ * @relay_id: relay message ID (must match message ID from the request)
+ * @data: optional data
+ *
+ * This utility function will prepare success response message based on
+ * given return data and and embed it in relay message for the GuC.
+ *
+ * This function can only be used by driver running in SR-IOV PF mode.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_relay_reply_ack_to_vf(struct intel_iov_relay *relay, u32 target,
+				    u32 relay_id, u32 data)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(!target && !I915_SELFTEST_ONLY(relay->selftest.enable_loopback));
+
+	return relay_send_success(relay, target, relay_id, data);
+}
+
+static u32 from_err_to_iov_error(int err)
+{
+	GEM_BUG_ON(err >= 0);
+	return -err;
+}
+
+static u32 sanitize_iov_error(u32 error)
+{
+	/* XXX TBD if generic error codes will be allowed */
+	if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST))
+		error = IOV_ERROR_UNDISCLOSED;
+	return error;
+}
+
+static u32 sanitize_iov_error_hint(u32 hint)
+{
+	/* XXX TBD if generic error codes will be allowed */
+	if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST))
+		hint = 0;
+	return hint;
+}
+
+static int relay_send_failure(struct intel_iov_relay *relay, u32 target,
+			      u32 relay_id, u32 error, u32 hint)
+{
+	u32 msg[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_FAILURE) |
+		FIELD_PREP(GUC_HXG_FAILURE_MSG_0_HINT, hint) |
+		FIELD_PREP(GUC_HXG_FAILURE_MSG_0_ERROR, error),
+	};
+
+	GEM_WARN_ON(!FIELD_FIT(GUC_HXG_FAILURE_MSG_0_ERROR, error));
+	GEM_WARN_ON(!FIELD_FIT(GUC_HXG_FAILURE_MSG_0_HINT, hint));
+
+	return relay_send(relay, target, relay_id, msg, ARRAY_SIZE(msg));
+}
+
+/**
+ * intel_iov_relay_reply_err_to_vf - Send failure response to VF.
+ * @relay: the Relay struct
+ * @target: target VF number (can't be 0)
+ * @relay_id: relay message ID (must match message ID from the request)
+ * @err: errno code (must be < 0)
+ *
+ * This utility function will prepare failure response message based on
+ * given error and hint and and embed it in relay message for the GuC.
+ *
+ * This function can only be used by driver running in SR-IOV PF mode.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_relay_reply_err_to_vf(struct intel_iov_relay *relay, u32 target,
+				    u32 relay_id, int err)
+{
+	u32 error = from_err_to_iov_error(err);
+
+	GEM_BUG_ON(!IS_SRIOV_PF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(!target && !I915_SELFTEST_ONLY(relay->selftest.enable_loopback));
+
+	return relay_send_failure(relay, target, relay_id,
+				 sanitize_iov_error(error), 0);
+}
+
+/**
+ * intel_iov_relay_reply_error_to_vf - Reply with error and hint to VF.
+ * @relay: the Relay struct
+ * @target: target VF number (can't be 0)
+ * @relay_id: relay message ID (must match message ID from the request)
+ * @error: error code
+ * @hint: additional optional hint
+ *
+ * This utility function will prepare failure response message based on
+ * given error and hint and and embed it in relay message for the GuC.
+ *
+ * This function can only be used by driver running in SR-IOV PF mode.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_relay_reply_error_to_vf(struct intel_iov_relay *relay, u32 target,
+				      u32 relay_id, u16 error, u16 hint)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(relay_to_i915(relay)));
+	GEM_BUG_ON(!target);
+
+	return relay_send_failure(relay, target, relay_id,
+				 sanitize_iov_error(error),
+				 sanitize_iov_error_hint(hint));
+}
+
+static int relay_send_and_wait(struct intel_iov_relay *relay, u32 target,
+			       u32 relay_id, const u32 *msg, u32 len,
+			       u32 *buf, u32 buf_size)
+{
+	unsigned long timeout = msecs_to_jiffies(RELAY_TIMEOUT);
+	u32 action;
+	u32 data0;
+	struct pending_relay pending;
+	int ret;
+	long n;
+
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_HOST);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_REQUEST);
+
+	action = FIELD_GET(GUC_HXG_REQUEST_MSG_0_ACTION, msg[0]);
+	data0 = FIELD_GET(GUC_HXG_REQUEST_MSG_0_DATA0, msg[0]);
+	RELAY_DEBUG(relay, "%s.%u to %u action %#x:%u\n",
+		    hxg_type_to_string(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0])),
+		    relay_id, target, action, data0);
+
+	init_completion(&pending.done);
+	pending.target = target;
+	pending.fence = relay_id;
+	pending.reply = -ENOMSG;
+	pending.response = buf;
+	pending.response_size = buf_size;
+
+	/* list ordering does not need to match fence ordering */
+	spin_lock(&relay->lock);
+	list_add_tail(&pending.link, &relay->pending_relays);
+	spin_unlock(&relay->lock);
+
+retry:
+	ret = relay_send(relay, target, relay_id, msg, len);
+	if (unlikely(ret < 0))
+		goto unlink;
+
+wait:
+	n = wait_for_completion_timeout(&pending.done, timeout);
+	RELAY_DEBUG(relay, "%u.%u wait n=%ld\n", target, relay_id, n);
+	if (unlikely(n == 0)) {
+		ret = -ETIME;
+		goto unlink;
+	}
+
+	RELAY_DEBUG(relay, "%u.%u reply=%d\n", target, relay_id, pending.reply);
+	if (unlikely(pending.reply != 0)) {
+		reinit_completion(&pending.done);
+		ret = pending.reply;
+		if (ret == -EAGAIN)
+			goto retry;
+		if (ret == -EBUSY)
+			goto wait;
+		if (ret > 0)
+			ret = -ret;
+		goto unlink;
+	}
+
+	GEM_BUG_ON(pending.response_size > buf_size);
+	ret = pending.response_size;
+	RELAY_DEBUG(relay, "%u.%u response %*ph\n", target, relay_id, 4 * ret, buf);
+
+unlink:
+	spin_lock(&relay->lock);
+	list_del(&pending.link);
+	spin_unlock(&relay->lock);
+
+	if (unlikely(ret < 0)) {
+		RELAY_PROBE_ERROR(relay, "Unsuccessful %s.%u %#x:%u to %u (%pe) %*ph\n",
+				  hxg_type_to_string(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0])),
+				  relay_id, action, data0, target, ERR_PTR(ret), 4 * len, msg);
+	}
+
+	return ret;
+}
+
+/**
+ * intel_iov_relay_send_to_vf - Send message to VF.
+ * @relay: the Relay struct
+ * @target: target VF number
+ * @data: request payload data
+ * @dat_len: length of the payload data (in dwords, can be 0)
+ * @buf: placeholder for the response message
+ * @buf_size: size of the response message placeholder (in dwords)
+ *
+ * This function embed provided `IOV Message`_ into GuC relay.
+ *
+ * This function can only be used by driver running in SR-IOV PF mode.
+ *
+ * Return: Non-negative response length (in dwords) or
+ *         a negative error code on failure.
+ */
+int intel_iov_relay_send_to_vf(struct intel_iov_relay *relay, u32 target,
+			       const u32 *msg, u32 len, u32 *buf, u32 buf_size)
+{
+	u32 relay_type;
+	u32 relay_id;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(!target && !I915_SELFTEST_ONLY(relay->selftest.enable_loopback));
+	GEM_BUG_ON(len < GUC_HXG_MSG_MIN_LEN);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_HOST);
+
+	relay_type = FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]);
+	relay_id = relay_get_next_fence(relay);
+
+	if (relay_type == GUC_HXG_TYPE_EVENT)
+		return relay_send(relay, target, relay_id, msg, len);
+
+	GEM_BUG_ON(relay_type != GUC_HXG_TYPE_REQUEST);
+	return relay_send_and_wait(relay, target, relay_id, msg, len, buf, buf_size);
+}
+
+/**
+ * intel_iov_relay_send_to_pf - Send message to PF.
+ * @relay: the Relay struct
+ * @msg: message to be sent
+ * @len: length of the message (in dwords, can't be 0)
+ * @buf: placeholder for the response message
+ * @buf_size: size of the response message placeholder (in dwords)
+ *
+ * This function embed provided `IOV Message`_ into GuC relay.
+ *
+ * This function can only be used by driver running in SR-IOV VF mode.
+ *
+ * Return: Non-negative response length (in dwords) or
+ *         a negative error code on failure.
+ */
+int intel_iov_relay_send_to_pf(struct intel_iov_relay *relay,
+			       const u32 *msg, u32 len, u32 *buf, u32 buf_size)
+{
+	u32 relay_type;
+	u32 relay_id;
+
+	GEM_BUG_ON(!IS_SRIOV_VF(relay_to_i915(relay)) &&
+		   !I915_SELFTEST_ONLY(relay->selftest.disable_strict));
+	GEM_BUG_ON(len < GUC_HXG_MSG_MIN_LEN);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_HOST);
+
+	relay_type = FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]);
+	relay_id = relay_get_next_fence(relay);
+
+	if (relay_type == GUC_HXG_TYPE_EVENT)
+		return relay_send(relay, 0, relay_id, msg, len);
+
+	GEM_BUG_ON(relay_type != GUC_HXG_TYPE_REQUEST);
+	return relay_send_and_wait(relay, 0, relay_id, msg, len, buf, buf_size);
+}
+
+static int relay_handle_reply(struct intel_iov_relay *relay, u32 origin,
+			      u32 relay_id, int reply, const u32 *msg, u32 len)
+{
+	struct pending_relay *pending;
+	int err = -ESRCH;
+
+	spin_lock(&relay->lock);
+	list_for_each_entry(pending, &relay->pending_relays, link) {
+		if (pending->target != origin || pending->fence != relay_id) {
+			RELAY_DEBUG(relay, "%u.%u still awaits response\n",
+				    pending->target, pending->fence);
+			continue;
+		}
+		err = 0;
+		if (reply == 0) {
+			if (unlikely(len > pending->response_size)) {
+				reply = -ENOBUFS;
+				err = -ENOBUFS;
+			} else {
+				pending->response[0] = FIELD_GET(GUC_HXG_RESPONSE_MSG_0_DATA0, msg[0]);
+				memcpy(pending->response + 1, msg + 1, 4 * (len - 1));
+				pending->response_size = len;
+			}
+		}
+		pending->reply = reply;
+		complete_all(&pending->done);
+		break;
+	}
+	spin_unlock(&relay->lock);
+
+	return err;
+}
+
+static int relay_handle_failure(struct intel_iov_relay *relay, u32 origin,
+				u32 relay_id, const u32 *msg, u32 len)
+{
+	int error = FIELD_GET(GUC_HXG_FAILURE_MSG_0_ERROR, msg[0]);
+	u32 hint __maybe_unused = FIELD_GET(GUC_HXG_FAILURE_MSG_0_HINT, msg[0]);
+
+	GEM_BUG_ON(!len);
+	RELAY_DEBUG(relay, "%u.%u error %#x (%pe) hint %u debug %*ph\n",
+		    origin, relay_id, error, ERR_PTR(-error), hint, 4 * (len - 1), msg + 1);
+
+	return relay_handle_reply(relay, origin, relay_id, error ?: -ERFKILL, NULL, 0);
+}
+
+static int relay_handle_request(struct intel_iov_relay *relay, u32 origin,
+				u32 relay_id, const u32 *msg, u32 len)
+{
+	struct intel_iov *iov = relay_to_iov(relay);
+	struct drm_i915_private *i915 = relay_to_i915(relay);
+	struct intel_runtime_pm *rpm = &i915->runtime_pm;
+	intel_wakeref_t wakeref = intel_runtime_pm_get(rpm);
+	int err = -EOPNOTSUPP;
+
+	if (intel_iov_is_pf(iov))
+		err = intel_iov_service_process_msg(iov, origin,
+						    relay_id, msg, len);
+
+	if (unlikely(err < 0)) {
+		u32 error = from_err_to_iov_error(err);
+
+		RELAY_ERROR(relay, "Failed to handle %s.%u from %u (%pe) %*ph\n",
+			    hxg_type_to_string(GUC_HXG_TYPE_REQUEST), relay_id,
+			    origin, ERR_PTR(err), 4 * len, msg);
+		err = relay_send_failure(relay, origin, relay_id,
+					 origin ? sanitize_iov_error(error) : error, 0);
+	}
+
+	intel_runtime_pm_put(rpm, wakeref);
+	return err;
+}
+
+static int relay_handle_event(struct intel_iov_relay *relay, u32 origin,
+			      u32 relay_id, const u32 *msg, u32 len)
+{
+	return -EOPNOTSUPP;
+}
+
+static int relay_process_msg(struct intel_iov_relay *relay, u32 origin,
+			     u32 relay_id, const u32 *relay_msg, u32 relay_len)
+{
+	u32 relay_type;
+	int err;
+
+	if (I915_SELFTEST_ONLY(!relay_selftest_process_msg(relay, origin, relay_id,
+							   relay_msg, relay_len)))
+		return 0;
+
+	if (unlikely(relay_len < GUC_HXG_MSG_MIN_LEN))
+		return -EPROTO;
+
+	if (FIELD_GET(GUC_HXG_MSG_0_ORIGIN, relay_msg[0]) != GUC_HXG_ORIGIN_HOST)
+		return -EPROTO;
+
+	relay_type = FIELD_GET(GUC_HXG_MSG_0_TYPE, relay_msg[0]);
+	RELAY_DEBUG(relay, "received %s.%u from %u = %*ph\n",
+		    hxg_type_to_string(relay_type), relay_id, origin,
+		    4 * relay_len, relay_msg);
+
+	switch (relay_type) {
+	case GUC_HXG_TYPE_REQUEST:
+		err = relay_handle_request(relay, origin, relay_id, relay_msg, relay_len);
+		break;
+	case GUC_HXG_TYPE_EVENT:
+		err = relay_handle_event(relay, origin, relay_id, relay_msg, relay_len);
+		break;
+	case GUC_HXG_TYPE_RESPONSE_SUCCESS:
+		err = relay_handle_reply(relay, origin, relay_id, 0, relay_msg, relay_len);
+		break;
+	case GUC_HXG_TYPE_NO_RESPONSE_BUSY:
+		err = relay_handle_reply(relay, origin, relay_id, -EBUSY, NULL, 0);
+		break;
+	case GUC_HXG_TYPE_NO_RESPONSE_RETRY:
+		err = relay_handle_reply(relay, origin, relay_id, -EAGAIN, NULL, 0);
+		break;
+	case GUC_HXG_TYPE_RESPONSE_FAILURE:
+		err = relay_handle_failure(relay, origin, relay_id, relay_msg, relay_len);
+		break;
+	default:
+		err = -EBADRQC;
+	}
+
+	if (unlikely(err))
+		RELAY_ERROR(relay, "Failed to process %s.%u from %u (%pe) %*ph\n",
+			    hxg_type_to_string(relay_type), relay_id, origin,
+			    ERR_PTR(err),  4 * relay_len, relay_msg);
+
+	return err;
+}
+
+/**
+ * intel_iov_relay_process_guc2pf - Handle relay notification message from the GuC.
+ * @relay: the Relay struct
+ * @msg: message to be handled
+ * @len: length of the message (in dwords)
+ *
+ * This function will handle RELAY messages received from the GuC.
+ *
+ * This function can only be used if driver is running in SR-IOV PF mode.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_relay_process_guc2pf(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	u32 origin, relay_id;
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+	if (unlikely(!IS_ERR_OR_NULL(relay->selftest.guc2pf))) {
+		int ret = relay->selftest.guc2pf(relay, msg, len);
+
+		if (ret != -ENOTTY) {
+			relay->selftest.guc2pf = ERR_PTR(ret < 0 ? ret : 0);
+			return ret;
+		}
+	}
+#endif
+
+	if (unlikely(!IS_SRIOV_PF(relay_to_i915(relay)) &&
+		     !I915_SELFTEST_ONLY(relay->selftest.disable_strict)))
+		return -EPERM;
+
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) != GUC_ACTION_GUC2PF_RELAY_FROM_VF);
+
+	if (unlikely(len < GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN))
+		return -EPROTO;
+
+	if (unlikely(len > GUC2PF_RELAY_FROM_VF_EVENT_MSG_MAX_LEN))
+		return -EMSGSIZE;
+
+	if (unlikely(FIELD_GET(GUC_HXG_EVENT_MSG_0_DATA0, msg[0])))
+		return -EPFNOSUPPORT;
+
+	origin = FIELD_GET(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID, msg[1]);
+	relay_id = FIELD_GET(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID, msg[2]);
+
+	if (unlikely(!origin))
+		return -EPROTO;
+
+	return relay_process_msg(relay, origin, relay_id,
+				 msg + GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN,
+				 len - GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN);
+}
+
+/**
+ * intel_iov_relay_process_guc2vf - Handle relay notification message from the GuC.
+ * @relay: the Relay struct
+ * @msg: message to be handled
+ * @len: length of the message (in dwords)
+ *
+ * This function will handle RELAY messages received from the GuC.
+ *
+ * This function can only be used if driver is running in SR-IOV VF mode.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_relay_process_guc2vf(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	struct drm_i915_private *i915 = relay_to_i915(relay);
+	u32 relay_id;
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+	if (unlikely(!IS_ERR_OR_NULL(relay->selftest.guc2vf))) {
+		int ret = relay->selftest.guc2vf(relay, msg, len);
+
+		if (ret != -ENOTTY) {
+			relay->selftest.guc2vf = ERR_PTR(ret < 0 ? ret : 0);
+			return ret;
+		}
+	}
+#endif
+
+	if (unlikely(!IS_SRIOV_VF(i915)) &&
+		     !(I915_SELFTEST_ONLY(relay->selftest.disable_strict) ||
+		       I915_SELFTEST_ONLY(relay->selftest.enable_loopback)))
+		return -EPERM;
+
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) != GUC_ACTION_GUC2VF_RELAY_FROM_PF);
+
+	if (unlikely(len < GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN))
+		return -EPROTO;
+
+	if (unlikely(len > GUC2VF_RELAY_FROM_PF_EVENT_MSG_MAX_LEN))
+		return -EMSGSIZE;
+
+	if (unlikely(FIELD_GET(GUC_HXG_EVENT_MSG_0_DATA0, msg[0])))
+		return -EPFNOSUPPORT;
+
+	relay_id = FIELD_GET(GUC2VF_RELAY_FROM_PF_EVENT_MSG_1_RELAY_ID, msg[1]);
+
+	return relay_process_msg(relay, 0, relay_id,
+				 msg + GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN,
+				 len - GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN);
+}
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+#undef intel_guc_send
+#undef intel_guc_send_nb
+#undef intel_guc_ct_send
+#include "selftests/selftest_util_iov_relay.c"
+#include "selftests/selftest_mock_iov_relay.c"
+#include "selftests/selftest_live_iov_relay.c"
+#include "selftests/selftest_perf_iov_relay.c"
+#endif
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.h
new file mode 100644
index 000000000000..ccd08fb35484
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.h
@@ -0,0 +1,36 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_RELAY_H__
+#define __INTEL_IOV_RELAY_H__
+
+#include "intel_iov_types.h"
+
+static inline void intel_iov_relay_init_early(struct intel_iov_relay *relay)
+{
+	spin_lock_init(&relay->lock);
+	INIT_LIST_HEAD(&relay->pending_relays);
+}
+
+int intel_iov_relay_send_to_vf(struct intel_iov_relay *relay, u32 target,
+			       const u32 *msg, u32 len, u32 *buf, u32 buf_size);
+int intel_iov_relay_reply_to_vf(struct intel_iov_relay *relay, u32 target,
+				u32 relay_id, const u32 *msg, u32 len);
+int intel_iov_relay_reply_ack_to_vf(struct intel_iov_relay *relay, u32 target,
+				    u32 relay_id, u32 data);
+int intel_iov_relay_reply_err_to_vf(struct intel_iov_relay *relay, u32 target,
+				    u32 relay_id, int err);
+int intel_iov_relay_reply_error_to_vf(struct intel_iov_relay *relay, u32 target,
+				      u32 relay_id, u16 error, u16 hint);
+
+int intel_iov_relay_send_to_pf(struct intel_iov_relay *relay,
+			       const u32 *msg, u32 len, u32 *buf, u32 buf_size);
+
+int intel_iov_relay_process_guc2pf(struct intel_iov_relay *relay,
+				   const u32 *msg, u32 len);
+int intel_iov_relay_process_guc2vf(struct intel_iov_relay *relay,
+				   const u32 *msg, u32 len);
+
+#endif /* __INTEL_IOV_RELAY_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_service.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_service.c
new file mode 100644
index 000000000000..6125ac0aa9cc
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_service.c
@@ -0,0 +1,563 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include <linux/bitfield.h>
+#include <linux/bsearch.h>
+
+#include "abi/iov_actions_abi.h"
+#include "abi/iov_actions_mmio_abi.h"
+#include "abi/iov_errors_abi.h"
+#include "abi/iov_messages_abi.h"
+#include "abi/iov_version_abi.h"
+
+#include "gt/intel_gt_regs.h"
+
+#include "intel_iov_relay.h"
+#include "intel_iov_service.h"
+#include "intel_iov_types.h"
+#include "intel_iov_utils.h"
+
+static void __uncore_read_many(struct intel_uncore *uncore, unsigned int count,
+			       const i915_reg_t *regs, u32 *values)
+{
+	while (count--) {
+		*values++ = intel_uncore_read(uncore, *regs++);
+	}
+}
+
+static const i915_reg_t tgl_runtime_regs[] = {
+	RPM_CONFIG0,			/* _MMIO(0x0D00) */
+	GEN10_MIRROR_FUSE3,		/* _MMIO(0x9118) */
+	GEN11_EU_DISABLE,		/* _MMIO(0x9134) */
+	GEN11_GT_SLICE_ENABLE,		/* _MMIO(0x9138) */
+	GEN12_GT_GEOMETRY_DSS_ENABLE,	/* _MMIO(0x913C) */
+	GEN11_GT_VEBOX_VDBOX_DISABLE,	/* _MMIO(0x9140) */
+	CTC_MODE,			/* _MMIO(0xA26C) */
+	GEN11_HUC_KERNEL_LOAD_INFO,	/* _MMIO(0xC1DC) */
+	GEN9_TIMESTAMP_OVERRIDE,	/* _MMIO(0x44074) */
+};
+
+static const i915_reg_t mtl_runtime_regs[] = {
+	RPM_CONFIG0,			/* _MMIO(0x0D00) */
+	XEHP_FUSE4,			/* _MMIO(0x9114) */
+	GEN10_MIRROR_FUSE3,		/* _MMIO(0x9118) */
+	HSW_PAVP_FUSE1,			/* _MMIO(0x911C) */
+	XEHP_EU_ENABLE,			/* _MMIO(0x9134) */
+	GEN12_GT_GEOMETRY_DSS_ENABLE,	/* _MMIO(0x913C) */
+	GEN11_GT_VEBOX_VDBOX_DISABLE,	/* _MMIO(0x9140) */
+	GEN12_GT_COMPUTE_DSS_ENABLE,	/* _MMIO(0x9144) */
+	XEHPC_GT_COMPUTE_DSS_ENABLE_EXT,/* _MMIO(0x9148) */
+	CTC_MODE,			/* _MMIO(0xA26C) */
+	GEN11_HUC_KERNEL_LOAD_INFO,	/* _MMIO(0xC1DC) */
+	GEN9_TIMESTAMP_OVERRIDE,	/* _MMIO(0x44074) */
+	MTL_GT_ACTIVITY_FACTOR,		/* _MMIO(0x138010) */
+};
+
+static const i915_reg_t *get_runtime_regs(struct drm_i915_private *i915,
+					  unsigned int *size)
+{
+	const i915_reg_t *regs;
+
+	if (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 70)) {
+		regs = mtl_runtime_regs;
+		*size = ARRAY_SIZE(mtl_runtime_regs);
+	} else if (IS_TIGERLAKE(i915) || IS_ALDERLAKE_S(i915) || IS_ALDERLAKE_P(i915))  {
+		regs = tgl_runtime_regs;
+		*size = ARRAY_SIZE(tgl_runtime_regs);
+	} else {
+		MISSING_CASE(GRAPHICS_VER(i915));
+		regs = ERR_PTR(-ENODEV);
+		*size = 0;
+	}
+
+	return regs;
+}
+
+static bool regs_selftest(const i915_reg_t *regs, unsigned int count)
+{
+	u32 offset = 0;
+
+	while (IS_ENABLED(CONFIG_DRM_I915_SELFTEST) && count--) {
+		if (i915_mmio_reg_offset(*regs) < offset) {
+			pr_err("invalid register order: %#x < %#x\n",
+				i915_mmio_reg_offset(*regs), offset);
+			return false;
+		}
+		offset = i915_mmio_reg_offset(*regs++);
+	}
+
+	return true;
+}
+
+static int pf_alloc_runtime_info(struct intel_iov *iov)
+{
+	const i915_reg_t *regs;
+	unsigned int size;
+	u32 *values;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(iov->pf.service.runtime.size);
+	GEM_BUG_ON(iov->pf.service.runtime.regs);
+	GEM_BUG_ON(iov->pf.service.runtime.values);
+
+	regs = get_runtime_regs(iov_to_i915(iov), &size);
+	if (IS_ERR(regs))
+		return PTR_ERR(regs);
+
+	if (unlikely(!size))
+		return 0;
+
+	if (unlikely(!regs_selftest(regs, size)))
+		return -EBADSLT;
+
+	values = kcalloc(size, sizeof(u32), GFP_KERNEL);
+	if (!values)
+		return -ENOMEM;
+
+	iov->pf.service.runtime.size = size;
+	iov->pf.service.runtime.regs = regs;
+	iov->pf.service.runtime.values = values;
+
+	return 0;
+}
+
+static void pf_release_runtime_info(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	kfree(iov->pf.service.runtime.values);
+	iov->pf.service.runtime.values = NULL;
+	iov->pf.service.runtime.regs = NULL;
+	iov->pf.service.runtime.size = 0;
+}
+
+static void pf_prepare_runtime_info(struct intel_iov *iov)
+{
+	const i915_reg_t *regs;
+	unsigned int size;
+	u32 *values;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.service.runtime.size)
+		return;
+
+	size = iov->pf.service.runtime.size;
+	regs = iov->pf.service.runtime.regs;
+	values = iov->pf.service.runtime.values;
+
+	__uncore_read_many(iov_to_gt(iov)->uncore, size, regs, values);
+
+	while (size--) {
+		IOV_DEBUG(iov, "reg[%#x] = %#x\n",
+			  i915_mmio_reg_offset(*regs++), *values++);
+	}
+}
+
+static void pf_reset_runtime_info(struct intel_iov *iov)
+{
+	unsigned int size;
+	u32 *values;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.service.runtime.size)
+		return;
+
+	size = iov->pf.service.runtime.size;
+	values = iov->pf.service.runtime.values;
+
+	while (size--)
+		*values++ = 0;
+}
+
+/**
+ * intel_iov_service_init_early - Early initialization of the PF IOV services.
+ * @iov: the IOV struct
+ *
+ * Performs early initialization of the IOV PF services, including preparation
+ * of the runtime info that will be shared with VFs.
+ *
+ * This function can only be called on PF.
+ */
+void intel_iov_service_init_early(struct intel_iov *iov)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	err = pf_alloc_runtime_info(iov);
+	if (unlikely(err))
+		pf_update_status(iov, err, "runtime");
+}
+
+/**
+ * intel_iov_service_release - Cleanup PF IOV services.
+ * @iov: the IOV struct
+ *
+ * Releases any data allocated during initialization.
+ *
+ * This function can only be called on PF.
+ */
+void intel_iov_service_release(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	pf_release_runtime_info(iov);
+}
+
+/**
+ * intel_iov_service_update - Update PF IOV services.
+ * @iov: the IOV struct
+ *
+ * Updates runtime data shared with VFs.
+ *
+ * This function can be called more than once.
+ * This function can only be called on PF.
+ */
+void intel_iov_service_update(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	pf_prepare_runtime_info(iov);
+}
+
+/**
+ * intel_iov_service_reset - Update PF IOV services.
+ * @iov: the IOV struct
+ *
+ * Resets runtime data to avoid sharing stale info with VFs.
+ *
+ * This function can be called more than once.
+ * This function can only be called on PF.
+ */
+void intel_iov_service_reset(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	pf_reset_runtime_info(iov);
+}
+
+static int reply_handshake(struct intel_iov *iov, u32 origin,
+			   u32 relay_id, const u32 *msg, u32 len)
+{
+	struct intel_iov_relay *relay = &iov->relay;
+	u32 response[VF2PF_HANDSHAKE_RESPONSE_MSG_LEN];
+	u32 wanted_major, wanted_minor;
+	u32 major, minor, mbz;
+
+	GEM_BUG_ON(!origin &&
+		   !I915_SELFTEST_ONLY(relay->selftest.enable_loopback));
+
+	if (unlikely(len != VF2PF_HANDSHAKE_REQUEST_MSG_LEN))
+		return -EMSGSIZE;
+
+	wanted_major = FIELD_GET(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, msg[1]);
+	wanted_minor = FIELD_GET(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, msg[1]);
+	IOV_DEBUG(iov, "VF%u wants ABI version %u.%02u\n", origin,
+		  wanted_major, wanted_minor);
+
+	mbz = FIELD_GET(VF2PF_HANDSHAKE_REQUEST_MSG_0_MBZ, msg[0]);
+	if (unlikely(mbz))
+		return -EINVAL;
+
+	if (!wanted_major && !wanted_minor) {
+		major = IOV_VERSION_LATEST_MAJOR;
+		minor = IOV_VERSION_LATEST_MINOR;
+	} else if (wanted_major > IOV_VERSION_LATEST_MAJOR) {
+		major = IOV_VERSION_LATEST_MAJOR;
+		minor = IOV_VERSION_LATEST_MINOR;
+	} else if (wanted_major < IOV_VERSION_BASE_MAJOR) {
+		return -EINVAL;
+	} else if (wanted_major < IOV_VERSION_LATEST_MAJOR) {
+		major = wanted_major;
+		minor = wanted_minor;
+	} else {
+		major = wanted_major;
+		minor = min_t(u32, IOV_VERSION_LATEST_MINOR, wanted_minor);
+	}
+
+	response[0] = FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		      FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS) |
+		      FIELD_PREP(GUC_HXG_RESPONSE_MSG_0_DATA0, 0);
+
+	response[1] = FIELD_PREP(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MAJOR, major) |
+		      FIELD_PREP(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MINOR, minor);
+	return intel_iov_relay_reply_to_vf(relay, origin, relay_id,
+					   response, ARRAY_SIZE(response));
+}
+
+static int pf_reply_runtime_query(struct intel_iov *iov, u32 origin,
+				  u32 relay_id, const u32 *msg, u32 len)
+{
+	struct intel_iov_runtime_regs *runtime = &iov->pf.service.runtime;
+	u32 response[VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MAX_LEN];
+	u32 max_chunk = (ARRAY_SIZE(response) - VF2PF_QUERY_RUNTIME_RESPONSE_MSG_MIN_LEN) / 2;
+	u32 limit, start, chunk, i;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (unlikely(len > VF2PF_QUERY_RUNTIME_REQUEST_MSG_LEN))
+		return -EMSGSIZE;
+	if (unlikely(len < VF2PF_QUERY_RUNTIME_REQUEST_MSG_LEN))
+		return -EPROTO;
+
+	limit = FIELD_GET(VF2PF_QUERY_RUNTIME_REQUEST_MSG_0_LIMIT, msg[0]);
+	start = FIELD_GET(VF2PF_QUERY_RUNTIME_REQUEST_MSG_1_START, msg[1]);
+	if (unlikely(start > runtime->size))
+		return -EINVAL;
+
+	chunk = min_t(u32, runtime->size - start, max_chunk);
+	if (limit)
+		chunk = min_t(u32, chunk, limit);
+
+	response[0] = FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		      FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS) |
+		      FIELD_PREP(VF2PF_QUERY_RUNTIME_RESPONSE_MSG_0_COUNT, chunk);
+
+	response[1] = FIELD_PREP(VF2PF_QUERY_RUNTIME_RESPONSE_MSG_1_REMAINING,
+				 runtime->size - start - chunk);
+
+	for (i = 0; i < chunk; ++i) {
+		i915_reg_t reg = runtime->regs[start + i];
+		u32 offset = i915_mmio_reg_offset(reg);
+		u32 value = runtime->values[start + i];
+
+		response[2 + 2 * i] = offset;
+		response[2 + 2 * i + 1] = value;
+	}
+
+	return intel_iov_relay_reply_to_vf(&iov->relay, origin, relay_id,
+					   response, 2 + 2 * chunk);
+}
+
+/**
+ * intel_iov_service_process_msg - Service request message from VF.
+ * @iov: the IOV struct
+ * @origin: origin VF number
+ * @relay_id: message ID
+ * @msg: request message
+ * @len: length of the message (in dwords)
+ *
+ * This function processes `IOV Message`_ from the VF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_service_process_msg(struct intel_iov *iov, u32 origin,
+				  u32 relay_id, const u32 *msg, u32 len)
+{
+	int err = -EOPNOTSUPP;
+	u32 action;
+	u32 data;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(len < GUC_HXG_MSG_MIN_LEN);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_REQUEST);
+
+	action = FIELD_GET(GUC_HXG_REQUEST_MSG_0_ACTION, msg[0]);
+	data = FIELD_GET(GUC_HXG_REQUEST_MSG_0_DATA0, msg[0]);
+	IOV_DEBUG(iov, "servicing action %#x:%u from %u\n", action, data, origin);
+
+	if (!origin && !I915_SELFTEST_ONLY(iov->relay.selftest.enable_loopback))
+		return -EPROTO;
+
+	switch (action) {
+	case IOV_ACTION_VF2PF_HANDSHAKE:
+		err = reply_handshake(iov, origin, relay_id, msg, len);
+		break;
+	case IOV_ACTION_VF2PF_QUERY_RUNTIME:
+		err = pf_reply_runtime_query(iov, origin, relay_id, msg, len);
+		break;
+	default:
+		break;
+	}
+
+	return err;
+}
+
+static int send_mmio_relay_error(struct intel_iov *iov,
+				 u32 vfid, u32 magic, int fault)
+{
+	u32 request[PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_PF2GUC_MMIO_RELAY_FAILURE),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_1_VFID, vfid),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_2_MAGIC, magic) |
+		FIELD_PREP(PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_2_FAULT, fault),
+	};
+
+	return intel_guc_send(iov_to_guc(iov), request, ARRAY_SIZE(request));
+}
+
+static int send_mmio_relay_reply(struct intel_iov *iov,
+				 u32 vfid, u32 magic, u32 data[4])
+{
+	u32 request[PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_PF2GUC_MMIO_RELAY_SUCCESS),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_1_VFID, vfid),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_2_MAGIC, magic) |
+		FIELD_PREP(PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_2_DATA0, data[0]),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_n_DATAx, data[1]),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_n_DATAx, data[2]),
+		FIELD_PREP(PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_n_DATAx, data[3]),
+	};
+
+	return intel_guc_send(iov_to_guc(iov), request, ARRAY_SIZE(request));
+}
+
+static int reply_mmio_relay_handshake(struct intel_iov *iov,
+				      u32 vfid, u32 magic, const u32 *msg)
+{
+	u32 data[PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_NUM_DATA + 1] = { };
+	u32 wanted_major = FIELD_GET(VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_1_MAJOR, msg[1]);
+	u32 wanted_minor = FIELD_GET(VF2PF_MMIO_HANDSHAKE_REQUEST_MSG_1_MINOR, msg[1]);
+	u32 major = 0, minor = 0;
+	int fault = 0;
+
+	IOV_DEBUG(iov, "VF%u wants ABI version %u.%02u\n", vfid, wanted_major, wanted_minor);
+
+	/* XXX for now we only support single major version (latest) */
+
+	if (!wanted_major && !wanted_minor) {
+		major = IOV_VERSION_LATEST_MAJOR;
+		minor = IOV_VERSION_LATEST_MINOR;
+	} else if (wanted_major > IOV_VERSION_LATEST_MAJOR) {
+		major = IOV_VERSION_LATEST_MAJOR;
+		minor = IOV_VERSION_LATEST_MINOR;
+	} else if (wanted_major < IOV_VERSION_LATEST_MAJOR) {
+		fault = ENOPKG;
+	} else {
+		GEM_BUG_ON(wanted_major != IOV_VERSION_LATEST_MAJOR);
+		GEM_BUG_ON(IOV_VERSION_LATEST_MAJOR != 1);
+
+		if (unlikely(!msg[0] || msg[2] || msg[3])) {
+			fault = EPROTO;
+		} else {
+			major = wanted_major;
+			minor = min_t(u32, IOV_VERSION_LATEST_MINOR, wanted_minor);
+		}
+	}
+
+	if (fault)
+		return send_mmio_relay_error(iov, vfid, magic, fault);
+
+	IOV_DEBUG(iov, "VF%u will use ABI version %u.%02u\n", vfid, major, minor);
+
+	data[1] = FIELD_PREP(VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_1_MAJOR, major) |
+		  FIELD_PREP(VF2PF_MMIO_HANDSHAKE_RESPONSE_MSG_1_MINOR, minor);
+
+	return send_mmio_relay_reply(iov, vfid, magic, data);
+}
+
+static int __i915_reg_cmp(const void *a, const void *b)
+{
+	return i915_mmio_reg_offset(*(const i915_reg_t *)a) -
+	       i915_mmio_reg_offset(*(const i915_reg_t *)b);
+}
+
+static int lookup_reg_index(struct intel_iov *iov, u32 offset)
+{
+	i915_reg_t key = _MMIO(offset);
+	i915_reg_t *found = bsearch(&key, iov->pf.service.runtime.regs,
+				    iov->pf.service.runtime.size, sizeof(key),
+				    __i915_reg_cmp);
+
+	return found ? found - iov->pf.service.runtime.regs : -ENODATA;
+}
+
+static int reply_mmio_relay_get_reg(struct intel_iov *iov,
+				    u32 vfid, u32 magic, const u32 *msg)
+{
+	u32 data[PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_NUM_DATA + 1] = { };
+	unsigned int i;
+	int found;
+
+	BUILD_BUG_ON(VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_NUM_OFFSET >
+		     GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_NUM_DATA);
+	BUILD_BUG_ON(VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_NUM_OFFSET !=
+		     PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_NUM_DATA);
+
+	if (unlikely(!msg[0]))
+		return -EPROTO;
+	if (unlikely(!msg[1]))
+		return -EINVAL;
+
+	for (i = 0; i < VF2PF_MMIO_GET_RUNTIME_REQUEST_MSG_NUM_OFFSET; i++) {
+		u32 offset = msg[i + 1];
+
+		if (unlikely(!offset))
+			continue;
+		found = lookup_reg_index(iov, offset);
+		if (found < 0)
+			return -EACCES;
+		data[i + 1] = iov->pf.service.runtime.values[found];
+	}
+
+	return send_mmio_relay_reply(iov, vfid, magic, data);
+}
+
+/**
+ * intel_iov_service_process_mmio_relay - Process MMIO Relay notification.
+ * @iov: the IOV struct
+ * @msg: mmio relay notification data
+ * @len: length of the message data (in dwords)
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_service_process_mmio_relay(struct intel_iov *iov, const u32 *msg,
+					 u32 len)
+{
+	struct drm_i915_private *i915 = iov_to_i915(iov);
+	struct intel_runtime_pm *rpm = &i915->runtime_pm;
+	intel_wakeref_t wakeref;
+	u32 vfid, magic, opcode;
+	int err = -EPROTO;
+
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) !=
+		   GUC_ACTION_GUC2PF_MMIO_RELAY_SERVICE);
+
+	if (unlikely(!IS_SRIOV_PF(i915)))
+		return -EPERM;
+	if (unlikely(len != GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_LEN))
+		return -EPROTO;
+
+	vfid = FIELD_GET(GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_1_VFID, msg[1]);
+	magic = FIELD_GET(GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_2_MAGIC, msg[2]);
+	opcode = FIELD_GET(GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_2_OPCODE, msg[2]);
+
+	if (unlikely(!vfid))
+		return -EPROTO;
+
+	wakeref = intel_runtime_pm_get(rpm);
+
+	switch (opcode) {
+	case IOV_OPCODE_VF2PF_MMIO_HANDSHAKE:
+		err = reply_mmio_relay_handshake(iov, vfid, magic, msg + 2);
+		break;
+	case IOV_OPCODE_VF2PF_MMIO_GET_RUNTIME:
+		err = reply_mmio_relay_get_reg(iov, vfid, magic, msg + 2);
+		break;
+	default:
+		IOV_DEBUG(iov, "unsupported request %#x from VF%u\n",
+				opcode, vfid);
+		err = -EOPNOTSUPP;
+	}
+
+	if (unlikely(err < 0))
+		send_mmio_relay_error(iov, vfid, magic, -err);
+
+	intel_runtime_pm_put(rpm, wakeref);
+	return err;
+}
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+#include "selftests/selftest_mock_iov_service.c"
+#include "selftests/selftest_live_iov_service.c"
+#endif
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_service.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_service.h
new file mode 100644
index 000000000000..949ebeb6ed64
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_service.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_SERVICE_H__
+#define __INTEL_IOV_SERVICE_H__
+
+#include <linux/types.h>
+
+struct intel_iov;
+
+void intel_iov_service_init_early(struct intel_iov *iov);
+void intel_iov_service_update(struct intel_iov *iov);
+void intel_iov_service_reset(struct intel_iov *iov);
+void intel_iov_service_release(struct intel_iov *iov);
+
+int intel_iov_service_process_msg(struct intel_iov *iov, u32 origin,
+				  u32 relay_id, const u32 *msg, u32 len);
+
+int intel_iov_service_process_mmio_relay(struct intel_iov *iov, const u32 *msg,
+					 u32 len);
+
+#endif /* __INTEL_IOV_SERVICE_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
new file mode 100644
index 000000000000..f0e085a1f8ac
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
@@ -0,0 +1,458 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "intel_iov.h"
+#include "intel_iov_event.h"
+#include "intel_iov_state.h"
+#include "intel_iov_utils.h"
+#include "gt/intel_gt.h"
+#include "gt/uc/abi/guc_actions_pf_abi.h"
+
+static void pf_state_worker_func(struct work_struct *w);
+
+/**
+ * intel_iov_state_init_early - Allocate structures for VFs state data.
+ * @iov: the IOV struct
+ *
+ * VFs state data is maintained in the flexible array where:
+ *   - entry [0] contains state data of the PF (if applicable),
+ *   - entries [1..n] contain state data of VF1..VFn::
+ *
+ *       <--------------------------- 1 + total_vfs ----------->
+ *      +-------+-------+-------+-----------------------+-------+
+ *      |   0   |   1   |   2   |                       |   n   |
+ *      +-------+-------+-------+-----------------------+-------+
+ *      |  PF   |  VF1  |  VF2  |      ...     ...      |  VFn  |
+ *      +-------+-------+-------+-----------------------+-------+
+ *
+ * This function can only be called on PF.
+ */
+void intel_iov_state_init_early(struct intel_iov *iov)
+{
+	struct intel_iov_data *data;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(iov->pf.state.data);
+
+	INIT_WORK(&iov->pf.state.worker, pf_state_worker_func);
+
+	data = kcalloc(1 + pf_get_totalvfs(iov), sizeof(*data), GFP_KERNEL);
+	if (unlikely(!data)) {
+		pf_update_status(iov, -ENOMEM, "state");
+		return;
+	}
+
+	iov->pf.state.data = data;
+}
+
+/**
+ * intel_iov_state_release - Release structures used VFs data.
+ * @iov: the IOV struct
+ *
+ * Release structures used for VFs data.
+ * This function can only be called on PF.
+ */
+void intel_iov_state_release(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	cancel_work_sync(&iov->pf.state.worker);
+	kfree(fetch_and_zero(&iov->pf.state.data));
+}
+
+static void pf_reset_vf_state(struct intel_iov *iov, u32 vfid)
+{
+	iov->pf.state.data[vfid].state = 0;
+}
+
+/**
+ * intel_iov_state_reset - Reset VFs data.
+ * @iov: the IOV struct
+ *
+ * Reset VFs data.
+ * This function can only be called on PF.
+ */
+void intel_iov_state_reset(struct intel_iov *iov)
+{
+	u16 n;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.state.data)
+		return;
+
+	for (n = 0; n < 1 + pf_get_totalvfs(iov); n++) {
+		pf_reset_vf_state(iov, n);
+	}
+}
+
+static int guc_action_vf_control_cmd(struct intel_guc *guc, u32 vfid, u32 cmd)
+{
+	u32 request[PF2GUC_VF_CONTROL_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_PF2GUC_VF_CONTROL),
+		FIELD_PREP(PF2GUC_VF_CONTROL_REQUEST_MSG_1_VFID, vfid),
+		FIELD_PREP(PF2GUC_VF_CONTROL_REQUEST_MSG_2_COMMAND, cmd),
+	};
+	int ret;
+
+	ret = intel_guc_send(guc, request, ARRAY_SIZE(request));
+	return ret > 0 ? -EPROTO : ret;
+}
+
+static int pf_control_vf(struct intel_iov *iov, u32 vfid, u32 cmd)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+	GEM_BUG_ON(!vfid);
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = guc_action_vf_control_cmd(iov_to_guc(iov), vfid, cmd);
+
+	return err;
+}
+
+static int pf_trigger_vf_flr_start(struct intel_iov *iov, u32 vfid)
+{
+	int err;
+
+	err = pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_FLR_START);
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to start FLR for VF%u (%pe)\n",
+			  vfid, ERR_PTR(err));
+	return err;
+}
+
+static int pf_trigger_vf_flr_finish(struct intel_iov *iov, u32 vfid)
+{
+	int err;
+
+	err = pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_FLR_FINISH);
+	if (unlikely(err))
+		IOV_ERROR(iov, "Failed to confirm FLR for VF%u (%pe)\n",
+			  vfid, ERR_PTR(err));
+	return err;
+}
+
+static void pf_clear_vf_ggtt_entries(struct intel_iov *iov, u32 vfid)
+{
+	struct intel_iov_config *config = &iov->pf.provisioning.configs[vfid];
+	struct intel_gt *gt = iov_to_gt(iov);
+
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	if (!drm_mm_node_allocated(&config->ggtt_region))
+		return;
+
+	i915_ggtt_set_space_owner(gt->ggtt, vfid, &config->ggtt_region);
+}
+
+static int pf_process_vf_flr_finish(struct intel_iov *iov, u32 vfid)
+{
+	intel_iov_event_reset(iov, vfid);
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	pf_clear_vf_ggtt_entries(iov, vfid);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return pf_trigger_vf_flr_finish(iov, vfid);
+}
+
+/* Return: true if more processing is needed */
+static bool pf_process_vf(struct intel_iov *iov, u32 vfid)
+{
+	unsigned long *state = &iov->pf.state.data[vfid].state;
+	int err;
+
+	if (test_and_clear_bit(IOV_VF_NEEDS_FLR_START, state)) {
+		err = pf_trigger_vf_flr_start(iov, vfid);
+		if (err == -EBUSY) {
+			set_bit(IOV_VF_NEEDS_FLR_START, state);
+			return true;
+		}
+		if (err) {
+			set_bit(IOV_VF_FLR_FAILED, state);
+			clear_bit(IOV_VF_FLR_IN_PROGRESS, state);
+			return false;
+		}
+		return true;
+	}
+
+	if (test_and_clear_bit(IOV_VF_FLR_DONE_RECEIVED, state)) {
+		set_bit(IOV_VF_NEEDS_FLR_FINISH, state);
+		return true;
+	}
+
+	if (test_and_clear_bit(IOV_VF_NEEDS_FLR_FINISH, state)) {
+		err = pf_process_vf_flr_finish(iov, vfid);
+		if (err == -EBUSY) {
+			set_bit(IOV_VF_NEEDS_FLR_FINISH, state);
+			return true;
+		}
+		if (err) {
+			set_bit(IOV_VF_FLR_FAILED, state);
+			clear_bit(IOV_VF_FLR_IN_PROGRESS, state);
+			return false;
+		}
+		clear_bit(IOV_VF_FLR_IN_PROGRESS, state);
+		return false;
+	}
+
+	return false;
+}
+
+static void pf_queue_worker(struct intel_iov *iov)
+{
+	queue_work(system_unbound_wq, &iov->pf.state.worker);
+}
+
+static void pf_process_all_vfs(struct intel_iov *iov)
+{
+	unsigned int num_vfs = pf_get_totalvfs(iov);
+	unsigned int n;
+	bool more = false;
+
+	/* only VFs need processing */
+	for (n = 1; n <= num_vfs; n++)
+		more |= pf_process_vf(iov, n);
+
+	if (more)
+		pf_queue_worker(iov);
+}
+
+static void pf_state_worker_func(struct work_struct *w)
+{
+	struct intel_iov *iov = container_of(w, struct intel_iov, pf.state.worker);
+
+	pf_process_all_vfs(iov);
+}
+
+/**
+ * DOC: VF FLR Flow
+ *
+ *          PF                        GUC             PCI
+ * ========================================================
+ *          |                          |               |
+ * (1)      |                          |<------- FLR --|
+ *          |                          |               :
+ * (2)      |<----------- NOTIFY FLR --|
+ *         [ ]                         |
+ * (3)     [ ]                         |
+ *         [ ]                         |
+ *          |-- START FLR ------------>|
+ *          |                         [ ]
+ * (4)      |                         [ ]
+ *          |                         [ ]
+ *          |<------------- FLR DONE --|
+ *         [ ]                         |
+ * (5)     [ ]                         |
+ *         [ ]                         |
+ *          |-- FINISH FLR ----------->|
+ *          |                          |
+ *
+ * Step 1: PCI HW generates interrupt to GuC about VF FLR
+ * Step 2: GuC FW sends G2H notification to PF about VF FLR
+ * Step 3: PF sends H2G request to GuC to start VF FLR sequence
+ * Step 4: GuC FW performs VF FLR cleanups and notifies PF when done
+ * Step 5: PF performs VF FLR cleanups and notifies GuC FW when finished
+ */
+
+static void pf_init_vf_flr(struct intel_iov *iov, u32 vfid)
+{
+	unsigned long *state = &iov->pf.state.data[vfid].state;
+
+	if (test_and_set_bit(IOV_VF_FLR_IN_PROGRESS, state)) {
+		IOV_DEBUG(iov, "VF%u FLR is already in progress\n", vfid);
+		return;
+	}
+
+	set_bit(IOV_VF_NEEDS_FLR_START, state);
+	pf_queue_worker(iov);
+}
+
+static void pf_handle_vf_flr(struct intel_iov *iov, u32 vfid)
+{
+	struct device *dev = iov_to_dev(iov);
+	struct intel_gt *gt;
+	unsigned int gtid;
+
+	if (!iov_is_root(iov)) {
+		IOV_ERROR(iov, "Unexpected VF%u FLR notification\n", vfid);
+		return;
+	}
+
+	dev_info(dev, "VF%u FLR\n", vfid);
+
+	for_each_gt(gt, iov_to_i915(iov), gtid)
+		pf_init_vf_flr(&gt->iov, vfid);
+}
+
+static void pf_handle_vf_flr_done(struct intel_iov *iov, u32 vfid)
+{
+	unsigned long *state = &iov->pf.state.data[vfid].state;
+
+	set_bit(IOV_VF_FLR_DONE_RECEIVED, state);
+	pf_queue_worker(iov);
+}
+
+static void pf_handle_vf_pause_done(struct intel_iov *iov, u32 vfid)
+{
+	struct device *dev = iov_to_dev(iov);
+
+	dev_info(dev, "VF%u %s\n", vfid, "paused");
+}
+
+static int pf_handle_vf_event(struct intel_iov *iov, u32 vfid, u32 eventid)
+{
+	switch (eventid) {
+	case GUC_PF_NOTIFY_VF_FLR:
+		pf_handle_vf_flr(iov, vfid);
+		break;
+	case GUC_PF_NOTIFY_VF_FLR_DONE:
+		pf_handle_vf_flr_done(iov, vfid);
+		break;
+	case GUC_PF_NOTIFY_VF_PAUSE_DONE:
+		pf_handle_vf_pause_done(iov, vfid);
+		break;
+	default:
+		return -ENOPKG;
+	}
+
+	return 0;
+}
+
+static int pf_handle_pf_event(struct intel_iov *iov, u32 eventid)
+{
+	switch (eventid) {
+	case GUC_PF_NOTIFY_VF_ENABLE:
+		IOV_DEBUG(iov, "VFs %s/%s\n", str_enabled_disabled(true), str_enabled_disabled(false));
+		break;
+	default:
+		return -ENOPKG;
+	}
+
+	return 0;
+}
+
+/**
+ * intel_iov_state_process_guc2pf - Handle VF state notification from GuC.
+ * @iov: the IOV struct
+ * @msg: message from the GuC
+ * @len: length of the message
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_process_guc2pf(struct intel_iov *iov,
+				   const u32 *msg, u32 len)
+{
+	u32 vfid;
+	u32 eventid;
+
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) != GUC_ACTION_GUC2PF_VF_STATE_NOTIFY);
+
+	if (unlikely(!intel_iov_is_pf(iov)))
+		return -EPROTO;
+
+	if (unlikely(FIELD_GET(GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_0_MBZ, msg[0])))
+		return -EPFNOSUPPORT;
+
+	if (unlikely(len != GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_LEN))
+		return -EPROTO;
+
+	vfid = FIELD_GET(GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_1_VFID, msg[1]);
+	eventid = FIELD_GET(GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_2_EVENT, msg[2]);
+
+	if (unlikely(vfid > pf_get_totalvfs(iov)))
+		return -EINVAL;
+
+	return vfid ? pf_handle_vf_event(iov, vfid, eventid) : pf_handle_pf_event(iov, eventid);
+}
+
+/**
+ * intel_iov_state_start_flr - Start VF FLR sequence.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ *
+ * This function is for PF only.
+ */
+void intel_iov_state_start_flr(struct intel_iov *iov, u32 vfid)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+	GEM_BUG_ON(!vfid);
+
+	pf_init_vf_flr(iov, vfid);
+}
+
+/**
+ * intel_iov_state_no_flr - Test if VF FLR is not in progress.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ *
+ * This function is for PF only.
+ *
+ * Return: true if FLR is not pending or in progress.
+ */
+bool intel_iov_state_no_flr(struct intel_iov *iov, u32 vfid)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+	GEM_BUG_ON(!vfid);
+
+	return !test_bit(IOV_VF_FLR_IN_PROGRESS, &iov->pf.state.data[vfid].state);
+}
+
+/**
+ * intel_iov_state_pause_vf - Pause VF.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_pause_vf(struct intel_iov *iov, u32 vfid)
+{
+	return pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_PAUSE);
+}
+
+/**
+ * intel_iov_state_resume_vf - Resume VF.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_resume_vf(struct intel_iov *iov, u32 vfid)
+{
+	return pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_RESUME);
+}
+
+/**
+ * intel_iov_state_stop_vf - Stop VF.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_stop_vf(struct intel_iov *iov, u32 vfid)
+{
+	return pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_STOP);
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
new file mode 100644
index 000000000000..1b0db54e4baf
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
@@ -0,0 +1,27 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_STATE_H__
+#define __INTEL_IOV_STATE_H__
+
+#include <linux/types.h>
+
+struct intel_iov;
+
+void intel_iov_state_init_early(struct intel_iov *iov);
+void intel_iov_state_release(struct intel_iov *iov);
+void intel_iov_state_reset(struct intel_iov *iov);
+
+void intel_iov_state_start_flr(struct intel_iov *iov, u32 vfid);
+bool intel_iov_state_no_flr(struct intel_iov *iov, u32 vfid);
+
+int intel_iov_state_pause_vf(struct intel_iov *iov, u32 vfid);
+int intel_iov_state_resume_vf(struct intel_iov *iov, u32 vfid);
+int intel_iov_state_stop_vf(struct intel_iov *iov, u32 vfid);
+
+int intel_iov_state_process_guc2pf(struct intel_iov *iov,
+				   const u32 *msg, u32 len);
+
+#endif /* __INTEL_IOV_STATE_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c
new file mode 100644
index 000000000000..6f060e4d52e4
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c
@@ -0,0 +1,697 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "intel_iov_provisioning.h"
+#include "intel_iov_sysfs.h"
+#include "intel_iov_types.h"
+#include "intel_iov_utils.h"
+
+/*
+ * /sys/class/drm/card*
+ *  iov
+ *    pf/
+ *     gt0/
+ *       ...
+ *    vf1/
+ *     gt0/
+ *       ...
+ */
+
+#define IOV_KOBJ_GTn_NAME "gt%u"
+
+struct iov_kobj {
+	struct kobject base;
+	struct intel_iov *iov;
+};
+#define to_iov_kobj(x) container_of(x, struct iov_kobj, base)
+
+static struct intel_iov *kobj_to_iov(struct kobject *kobj)
+{
+	return to_iov_kobj(kobj)->iov;
+}
+
+static unsigned int kobj_to_id(struct kobject *kobj)
+{
+	return to_sriov_ext_kobj(kobj->parent)->id;
+}
+
+struct iov_attr {
+	struct attribute attr;
+	ssize_t (*show)(struct intel_iov *iov, unsigned int id, char *buf);
+	ssize_t (*store)(struct intel_iov *iov, unsigned int id,
+			 const char *buf, size_t count);
+};
+#define to_iov_attr(x) container_of(x, struct iov_attr, attr)
+
+#define IOV_ATTR(name) \
+static struct iov_attr name##_iov_attr = \
+	__ATTR(name, 0644, name##_iov_attr_show, name##_iov_attr_store)
+
+#define IOV_ATTR_RO(name) \
+static struct iov_attr name##_iov_attr = \
+	__ATTR(name, 0444, name##_iov_attr_show, NULL)
+
+/* common attributes */
+
+static ssize_t exec_quantum_ms_iov_attr_show(struct intel_iov *iov,
+					     unsigned int id, char *buf)
+{
+	u32 exec_quantum = intel_iov_provisioning_get_exec_quantum(iov, id);
+
+	return sysfs_emit(buf, "%u\n", exec_quantum);
+}
+
+static ssize_t exec_quantum_ms_iov_attr_store(struct intel_iov *iov,
+					      unsigned int id,
+					      const char *buf, size_t count)
+{
+	u32 exec_quantum;
+	int err;
+
+	err = kstrtou32(buf, 0, &exec_quantum);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_exec_quantum(iov, id, exec_quantum);
+	return err ?: count;
+}
+
+static ssize_t preempt_timeout_us_iov_attr_show(struct intel_iov *iov,
+						unsigned int id, char *buf)
+{
+	u32 preempt_timeout = intel_iov_provisioning_get_preempt_timeout(iov, id);
+
+	return sysfs_emit(buf, "%u\n", preempt_timeout);
+}
+
+static ssize_t preempt_timeout_us_iov_attr_store(struct intel_iov *iov,
+						 unsigned int id,
+						 const char *buf, size_t count)
+{
+	u32 preempt_timeout;
+	int err;
+
+	err = kstrtou32(buf, 0, &preempt_timeout);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_preempt_timeout(iov, id, preempt_timeout);
+	return err ?: count;
+}
+
+IOV_ATTR(exec_quantum_ms);
+IOV_ATTR(preempt_timeout_us);
+
+static struct attribute *iov_attrs[] = {
+	&exec_quantum_ms_iov_attr.attr,
+	&preempt_timeout_us_iov_attr.attr,
+	NULL
+};
+
+static const struct attribute_group iov_attr_group = {
+	.attrs = iov_attrs,
+};
+
+static const struct attribute_group *default_iov_attr_groups[] = {
+	&iov_attr_group,
+	NULL
+};
+
+/* PF only attributes */
+
+static ssize_t ggtt_spare_iov_attr_show(struct intel_iov *iov,
+					unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%llu\n", intel_iov_provisioning_get_spare_ggtt(iov));
+}
+
+static ssize_t ggtt_spare_iov_attr_store(struct intel_iov *iov,
+					 unsigned int id,
+					 const char *buf, size_t count)
+{
+	u64 size;
+	int err;
+
+	err = kstrtou64(buf, 0, &size);
+	if (err)
+		return err;
+
+	GEM_WARN_ON(id);
+	err = intel_iov_provisioning_set_spare_ggtt(iov, size);
+	return err ?: count;
+}
+
+static ssize_t contexts_spare_iov_attr_show(struct intel_iov *iov,
+					    unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_get_spare_ctxs(iov));
+}
+
+static ssize_t contexts_spare_iov_attr_store(struct intel_iov *iov,
+					     unsigned int id,
+					     const char *buf, size_t count)
+{
+	u16 spare;
+	int err;
+
+	err = kstrtou16(buf, 0, &spare);
+	if (err)
+		return err;
+
+	GEM_WARN_ON(id);
+	err = intel_iov_provisioning_set_spare_ctxs(iov, spare);
+	return err ?: count;
+}
+
+static ssize_t doorbells_spare_iov_attr_show(struct intel_iov *iov,
+					     unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_get_spare_dbs(iov));
+}
+
+static ssize_t doorbells_spare_iov_attr_store(struct intel_iov *iov,
+					      unsigned int id,
+					      const char *buf, size_t count)
+{
+	u16 spare;
+	int err;
+
+	err = kstrtou16(buf, 0, &spare);
+	if (err)
+		return err;
+
+	GEM_WARN_ON(id);
+	err = intel_iov_provisioning_set_spare_dbs(iov, spare);
+	return err ?: count;
+}
+
+static ssize_t ggtt_free_iov_attr_show(struct intel_iov *iov,
+				       unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%llu\n", intel_iov_provisioning_query_free_ggtt(iov));
+}
+
+static ssize_t ggtt_max_quota_iov_attr_show(struct intel_iov *iov,
+					    unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%llu\n", intel_iov_provisioning_query_max_ggtt(iov));
+}
+
+static ssize_t contexts_free_iov_attr_show(struct intel_iov *iov, unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_query_free_ctxs(iov));
+}
+
+static ssize_t contexts_max_quota_iov_attr_show(struct intel_iov *iov, unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_query_max_ctxs(iov));
+}
+
+static ssize_t doorbells_free_iov_attr_show(struct intel_iov *iov,
+					    unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_query_free_dbs(iov));
+}
+
+static ssize_t doorbells_max_quota_iov_attr_show(struct intel_iov *iov,
+						 unsigned int id, char *buf)
+{
+	GEM_WARN_ON(id);
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_query_max_dbs(iov));
+}
+
+static ssize_t sched_if_idle_iov_attr_show(struct intel_iov *iov,
+					   unsigned int id, char *buf)
+{
+	u32 value = intel_iov_provisioning_get_sched_if_idle(iov);
+
+	return sysfs_emit(buf, "%u\n", value);
+}
+
+static ssize_t sched_if_idle_iov_attr_store(struct intel_iov *iov,
+					    unsigned int id,
+					    const char *buf, size_t count)
+{
+	bool value;
+	int err;
+
+	err = kstrtobool(buf, &value);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_sched_if_idle(iov, value);
+	return err ?: count;
+}
+
+static ssize_t engine_reset_iov_attr_show(struct intel_iov *iov,
+					  unsigned int id, char *buf)
+{
+	u32 value = intel_iov_provisioning_get_reset_engine(iov);
+
+	return sysfs_emit(buf, "%u\n", value);
+}
+
+static ssize_t engine_reset_iov_attr_store(struct intel_iov *iov,
+					   unsigned int id,
+					   const char *buf, size_t count)
+{
+	bool value;
+	int err;
+
+	err = kstrtobool(buf, &value);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_reset_engine(iov, value);
+	return err ?: count;
+}
+
+static ssize_t sample_period_ms_iov_attr_show(struct intel_iov *iov,
+					      unsigned int id, char *buf)
+{
+	u32 value = intel_iov_provisioning_get_sample_period(iov);
+
+	return sysfs_emit(buf, "%u\n", value);
+}
+
+static ssize_t sample_period_ms_iov_attr_store(struct intel_iov *iov,
+					       unsigned int id,
+					       const char *buf, size_t count)
+{
+	u32 value;
+	int err;
+
+	err = kstrtou32(buf, 0, &value);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_sample_period(iov, value);
+	return err ?: count;
+}
+
+IOV_ATTR(ggtt_spare);
+IOV_ATTR(contexts_spare);
+IOV_ATTR(doorbells_spare);
+
+IOV_ATTR_RO(ggtt_free);
+IOV_ATTR_RO(ggtt_max_quota);
+IOV_ATTR_RO(contexts_free);
+IOV_ATTR_RO(contexts_max_quota);
+IOV_ATTR_RO(doorbells_free);
+IOV_ATTR_RO(doorbells_max_quota);
+
+IOV_ATTR(sched_if_idle);
+IOV_ATTR(engine_reset);
+IOV_ATTR(sample_period_ms);
+
+static struct attribute *pf_attrs[] = {
+	&ggtt_spare_iov_attr.attr,
+	&contexts_spare_iov_attr.attr,
+	&doorbells_spare_iov_attr.attr,
+	NULL
+};
+
+static const struct attribute_group pf_attr_group = {
+	.attrs = pf_attrs,
+};
+
+static struct attribute *pf_available_attrs[] = {
+	&ggtt_free_iov_attr.attr,
+	&ggtt_max_quota_iov_attr.attr,
+	&contexts_free_iov_attr.attr,
+	&contexts_max_quota_iov_attr.attr,
+	&doorbells_free_iov_attr.attr,
+	&doorbells_max_quota_iov_attr.attr,
+	NULL
+};
+
+static const struct attribute_group pf_available_attr_group = {
+	.name = "available",
+	.attrs = pf_available_attrs,
+};
+
+static struct attribute *pf_policies_attrs[] = {
+	&sched_if_idle_iov_attr.attr,
+	&engine_reset_iov_attr.attr,
+	&sample_period_ms_iov_attr.attr,
+	NULL
+};
+
+static const struct attribute_group pf_policies_attr_group = {
+	.name = "policies",
+	.attrs = pf_policies_attrs,
+};
+
+static const struct attribute_group *pf_attr_groups[] = {
+	&pf_attr_group,
+	&pf_available_attr_group,
+	&pf_policies_attr_group,
+	NULL
+};
+
+/* VFs only attributes */
+
+static ssize_t ggtt_quota_iov_attr_show(struct intel_iov *iov,
+					unsigned int id, char *buf)
+{
+	u64 size = intel_iov_provisioning_get_ggtt(iov, id);
+
+	return sysfs_emit(buf, "%llu\n", size);
+}
+
+static ssize_t ggtt_quota_iov_attr_store(struct intel_iov *iov,
+					 unsigned int id,
+					 const char *buf, size_t count)
+{
+	u64 size;
+	int err;
+
+	err = kstrtou64(buf, 0, &size);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_ggtt(iov, id, size);
+	return err ?: count;
+}
+
+static ssize_t contexts_quota_iov_attr_show(struct intel_iov *iov,
+					    unsigned int id, char *buf)
+{
+	u16 num_ctxs = intel_iov_provisioning_get_ctxs(iov, id);
+
+	return sysfs_emit(buf, "%hu\n", num_ctxs);
+}
+
+static ssize_t contexts_quota_iov_attr_store(struct intel_iov *iov,
+					     unsigned int id,
+					     const char *buf, size_t count)
+{
+	u16 num_ctxs;
+	int err;
+
+	err = kstrtou16(buf, 0, &num_ctxs);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_ctxs(iov, id, num_ctxs);
+	return err ?: count;
+}
+
+static ssize_t doorbells_quota_iov_attr_show(struct intel_iov *iov,
+					     unsigned int id, char *buf)
+{
+	return sysfs_emit(buf, "%hu\n", intel_iov_provisioning_get_dbs(iov, id));
+}
+
+static ssize_t doorbells_quota_iov_attr_store(struct intel_iov *iov,
+					      unsigned int id,
+					      const char *buf, size_t count)
+{
+	u16 num_dbs;
+	int err;
+
+	err = kstrtou16(buf, 0, &num_dbs);
+	if (err)
+		return err;
+
+	err = intel_iov_provisioning_set_dbs(iov, id, num_dbs);
+	return err ?: count;
+}
+
+IOV_ATTR(ggtt_quota);
+IOV_ATTR(contexts_quota);
+IOV_ATTR(doorbells_quota);
+
+static struct attribute *vf_attrs[] = {
+	&ggtt_quota_iov_attr.attr,
+	&contexts_quota_iov_attr.attr,
+	&doorbells_quota_iov_attr.attr,
+	NULL
+};
+
+#define __iov_threshold_to_attr_impl(K, N, A) \
+static ssize_t A##_iov_attr_show(struct intel_iov *iov, unsigned int id, char *buf)	\
+{											\
+	u32 value = intel_iov_provisioning_get_threshold(iov, id, IOV_THRESHOLD_##K);	\
+											\
+	return sysfs_emit(buf, "%u\n", value);						\
+}											\
+											\
+static ssize_t A##_iov_attr_store(struct intel_iov *iov, unsigned int id,		\
+				  const char *buf, size_t count)			\
+{											\
+	u32 value;									\
+	int err;									\
+											\
+	err = kstrtou32(buf, 0, &value);						\
+	if (err)									\
+		return err;								\
+											\
+	err = intel_iov_provisioning_set_threshold(iov, id, IOV_THRESHOLD_##K, value);	\
+	return err ?: count;								\
+}											\
+											\
+IOV_ATTR(A);
+
+IOV_THRESHOLDS(__iov_threshold_to_attr_impl)
+#undef __iov_threshold_to_attr_impl
+
+static struct attribute *vf_threshold_attrs[] = {
+#define __iov_threshold_to_attr_list(K, N, A) \
+	&A##_iov_attr.attr,
+	IOV_THRESHOLDS(__iov_threshold_to_attr_list)
+#undef __iov_threshold_to_attr_list
+	NULL
+};
+
+static umode_t vf_attr_is_visible(struct kobject *kobj,
+				  struct attribute *attr, int index)
+{
+	struct intel_iov *iov = kobj_to_iov(kobj);
+
+	if (attr == &ggtt_quota_iov_attr.attr && iov_to_gt(iov)->type == GT_MEDIA)
+		return attr->mode & 0444;
+
+	return attr->mode;
+}
+
+static const struct attribute_group vf_attr_group = {
+	.attrs = vf_attrs,
+	.is_visible = vf_attr_is_visible,
+};
+
+static const struct attribute_group vf_threshold_attr_group = {
+	.name = "threshold",
+	.attrs = vf_threshold_attrs,
+};
+
+static const struct attribute_group *vf_attr_groups[] = {
+	&vf_attr_group,
+	&vf_threshold_attr_group,
+	NULL
+};
+
+static const struct attribute_group **iov_attr_groups(unsigned int id)
+{
+	return id ? vf_attr_groups : pf_attr_groups;
+}
+
+/* no user serviceable parts below */
+
+static ssize_t iov_attr_show(struct kobject *kobj,
+			     struct attribute *attr, char *buf)
+{
+	struct iov_attr *iov_attr = to_iov_attr(attr);
+	struct intel_iov *iov = kobj_to_iov(kobj);
+	unsigned int id = kobj_to_id(kobj);
+
+	return iov_attr->show ? iov_attr->show(iov, id, buf) : -EIO;
+}
+
+static ssize_t iov_attr_store(struct kobject *kobj,
+			      struct attribute *attr,
+			      const char *buf, size_t count)
+{
+	struct iov_attr *iov_attr = to_iov_attr(attr);
+	struct intel_iov *iov = kobj_to_iov(kobj);
+	unsigned int id = kobj_to_id(kobj);
+
+	return iov_attr->store ? iov_attr->store(iov, id, buf, count) : -EIO;
+}
+
+static const struct sysfs_ops iov_sysfs_ops = {
+	.show = iov_attr_show,
+	.store = iov_attr_store,
+};
+
+static struct kobject *iov_kobj_alloc(struct intel_iov *iov)
+{
+	struct iov_kobj *iov_kobj;
+
+	iov_kobj = kzalloc(sizeof(*iov_kobj), GFP_KERNEL);
+	if (!iov_kobj)
+		return NULL;
+
+	iov_kobj->iov = iov;
+
+	return &iov_kobj->base;
+}
+
+static void iov_kobj_release(struct kobject *kobj)
+{
+	struct iov_kobj *iov_kobj = to_iov_kobj(kobj);
+
+	kfree(iov_kobj);
+}
+
+static struct kobj_type iov_ktype = {
+	.release = iov_kobj_release,
+	.sysfs_ops = &iov_sysfs_ops,
+	.default_groups = default_iov_attr_groups,
+};
+
+static int pf_setup_provisioning(struct intel_iov *iov)
+{
+	struct i915_sriov_ext_kobj **parents = iov_to_i915(iov)->sriov.pf.sysfs.kobjs;
+	struct kobject **kobjs;
+	struct kobject *kobj;
+	unsigned int count = 1 + pf_get_totalvfs(iov);
+	unsigned int n;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!parents) {
+		err = -ENODEV;
+		goto failed;
+	}
+
+	err = i915_inject_probe_error(iov_to_i915(iov), -ENOMEM);
+	if (unlikely(err))
+		goto failed;
+
+	kobjs = kcalloc(count, sizeof(*kobjs), GFP_KERNEL);
+	if (unlikely(!kobjs)) {
+		err = -ENOMEM;
+		goto failed;
+	}
+
+	for (n = 0; n < count; n++) {
+		struct kobject *parent;
+
+		err = i915_inject_probe_error(iov_to_i915(iov), -ENOMEM);
+		if (unlikely(err)) {
+			kobj = NULL;
+			goto failed_kobj_n;
+		}
+
+		kobj = iov_kobj_alloc(iov);
+		if (unlikely(!kobj)) {
+			err = -ENOMEM;
+			goto failed_kobj_n;
+		}
+
+		parent = &parents[n]->base;
+
+		err = kobject_init_and_add(kobj, &iov_ktype, parent, IOV_KOBJ_GTn_NAME,
+					   iov_to_gt(iov)->info.id);
+		if (unlikely(err))
+			goto failed_kobj_n;
+
+		err = i915_inject_probe_error(iov_to_i915(iov), -EEXIST);
+		if (unlikely(err))
+			goto failed_kobj_n;
+
+		err = sysfs_create_groups(kobj, iov_attr_groups(n));
+		if (unlikely(err))
+			goto failed_kobj_n;
+
+		kobjs[n] = kobj;
+	}
+
+	GEM_BUG_ON(iov->pf.sysfs.entries);
+	iov->pf.sysfs.entries = kobjs;
+	return 0;
+
+failed_kobj_n:
+	if (kobj)
+		kobject_put(kobj);
+	while (n--) {
+		sysfs_remove_groups(kobjs[n], iov_attr_groups(n));
+		kobject_put(kobjs[n]);
+	}
+	kfree(kobjs);
+failed:
+	return err;
+}
+
+static void pf_teardown_provisioning(struct intel_iov *iov)
+{
+	struct kobject **kobjs;
+	unsigned int count = 1 + pf_get_totalvfs(iov);
+	unsigned int n;
+
+	kobjs = fetch_and_zero(&iov->pf.sysfs.entries);
+	if (!kobjs)
+		return;
+
+	for (n = 0; n < count; n++) {
+		sysfs_remove_groups(kobjs[n], iov_attr_groups(n));
+		kobject_put(kobjs[n]);
+	}
+
+	kfree(kobjs);
+}
+
+/**
+ * intel_iov_sysfs_setup - Setup GT IOV sysfs.
+ * @iov: the IOV struct
+ *
+ * Setup GT IOV provisioning sysfs.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_sysfs_setup(struct intel_iov *iov)
+{
+	int err;
+
+	if (!intel_iov_is_pf(iov))
+		return 0;
+
+	if (pf_in_error(iov))
+		return 0;
+
+	err = pf_setup_provisioning(iov);
+	if (unlikely(err))
+		goto failed;
+
+	return 0;
+
+failed:
+	IOV_PROBE_ERROR(iov, "Failed to setup sysfs (%pe)\n", ERR_PTR(err));
+	return err;
+}
+
+/**
+ * intel_iov_sysfs_teardown - Cleanup GT IOV sysfs.
+ * @iov: the IOV struct
+ *
+ * Remove GT IOV provisioning sysfs.
+ */
+void intel_iov_sysfs_teardown(struct intel_iov *iov)
+{
+	if (!intel_iov_is_pf(iov))
+		return;
+
+	pf_teardown_provisioning(iov);
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.h
new file mode 100644
index 000000000000..8aa066131353
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.h
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_SYSFS_H__
+#define __INTEL_IOV_SYSFS_H__
+
+struct intel_iov;
+
+int intel_iov_sysfs_setup(struct intel_iov *iov);
+void intel_iov_sysfs_teardown(struct intel_iov *iov);
+
+#endif /* __INTEL_IOV_SYSFS_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
new file mode 100644
index 000000000000..71d9c4316858
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
@@ -0,0 +1,256 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_TYPES_H__
+#define __INTEL_IOV_TYPES_H__
+
+#include <linux/mutex.h>
+#include <linux/spinlock.h>
+#include <drm/drm_mm.h>
+#include "i915_reg.h"
+#include "i915_selftest.h"
+#include "intel_wakeref.h"
+
+/*
+ * Super macro that combines different names used with adverse events:
+ * threshold key name (used in code), friendly name (used in dmesg)
+ * and attribute name (used in sysfs):
+ *
+ *	threshold(Key, Name, Attribute)
+ */
+#define IOV_THRESHOLDS(threshold) \
+	threshold(CAT_ERR, cat_error, cat_error_count) \
+	threshold(ENGINE_RESET, engine_reset, engine_reset_count) \
+	threshold(PAGE_FAULT, page_fault, page_fault_count) \
+	threshold(H2G_STORM, guc_storm, h2g_time_us) \
+	threshold(IRQ_STORM, irq_storm, irq_time_us) \
+	threshold(DOORBELL_STORM, dbs_storm, doorbell_time_us) \
+	/*end*/
+
+enum intel_iov_threshold {
+#define __to_intel_iov_threshold_enum(K, ...) IOV_THRESHOLD_##K,
+IOV_THRESHOLDS(__to_intel_iov_threshold_enum)
+#undef __to_intel_iov_threshold_enum
+};
+
+#define __count_iov_thresholds(...) + 1
+#define IOV_THRESHOLD_MAX (0 IOV_THRESHOLDS(__count_iov_thresholds))
+
+/**
+ * struct intel_iov_config - IOV configuration data.
+ * @ggtt_region: GGTT region.
+ * @num_ctxs: number of GuC submission contexts.
+ * @begin_ctx: start index of GuC contexts.
+ * @num_dbs: number of GuC doorbells.
+ * @begin_db: start index of GuC doorbells.
+ * @exec_quantum: execution-quantum in milliseconds.
+ * @preempt_timeout: preemption timeout in microseconds.
+ */
+struct intel_iov_config {
+	struct drm_mm_node ggtt_region;
+	u16 num_ctxs;
+	u16 begin_ctx;
+	u16 num_dbs;
+	u16 begin_db;
+	u32 exec_quantum;
+	u32 preempt_timeout;
+	u32 thresholds[IOV_THRESHOLD_MAX];
+};
+
+/**
+ * struct intel_iov_spare_config - PF spare configuration data.
+ * @ggtt_size: GGTT size.
+ * @num_ctxs: number of GuC submission contexts.
+ * @num_dbs: number of GuC doorbells.
+ */
+struct intel_iov_spare_config {
+	u64 ggtt_size;
+	u16 num_ctxs;
+	u16 num_dbs;
+};
+
+/**
+ * struct intel_iov_sysfs - IOV sysfs data.
+ * @entries: array with kobjects that represent PF and VFs.
+ */
+struct intel_iov_sysfs {
+	struct kobject **entries;
+};
+
+/**
+ * struct intel_iov_policies - IOV policies.
+ * @sched_if_idle: controls strict scheduling.
+ * @reset_engine: controls engines reset on VF switch.
+ * @sample_period: sample period of adverse events in milliseconds.
+ */
+struct intel_iov_policies {
+	bool sched_if_idle;
+	bool reset_engine;
+	u32 sample_period;
+};
+
+/**
+ * struct intel_iov_provisioning - IOV provisioning data.
+ * @auto_mode: indicates manual or automatic provisioning mode.
+ * @policies: provisioning policies.
+ * @configs: flexible array with configuration data for PF and VFs.
+ * @lock: protects provisionining data
+ */
+struct intel_iov_provisioning {
+	bool auto_mode;
+	unsigned int num_pushed;
+	struct work_struct worker;
+	struct intel_iov_policies policies;
+	struct intel_iov_spare_config spare;
+	struct intel_iov_config *configs;
+	struct mutex lock;
+
+	I915_SELFTEST_DECLARE(bool self_done);
+};
+
+#define VFID(n)		(n)
+#define PFID		VFID(0)
+
+/**
+ * struct intel_iov_data - Data related to one VF.
+ * @state: VF state bits
+ */
+struct intel_iov_data {
+	unsigned long state;
+#define IOV_VF_FLR_IN_PROGRESS		0
+#define IOV_VF_NEEDS_FLR_START		1
+#define IOV_VF_FLR_DONE_RECEIVED	2
+#define IOV_VF_NEEDS_FLR_FINISH		3
+#define IOV_VF_FLR_FAILED		(BITS_PER_LONG - 1)
+	unsigned int adverse_events[IOV_THRESHOLD_MAX];
+};
+
+/**
+ * struct intel_iov_state - Placeholder for all VFs data.
+ * @worker: event processing worker
+ */
+struct intel_iov_state {
+	struct work_struct worker;
+	struct intel_iov_data *data;
+};
+
+/**
+ * struct intel_iov_runtime_regs - Register runtime info shared with VFs.
+ * @size: size of the regs and value arrays.
+ * @regs: pointer to static array with register offsets.
+ * @values: pointer to array with captured register values.
+ */
+struct intel_iov_runtime_regs {
+	u32 size;
+	const i915_reg_t *regs;
+	u32 *values;
+};
+
+/**
+ * struct intel_iov_service - Placeholder for service data shared with VFs.
+ * @runtime: register runtime info shared with VFs.
+ */
+struct intel_iov_service {
+	struct intel_iov_runtime_regs runtime;
+};
+
+/**
+ * struct intel_iov_vf_runtime - Placeholder for the VF runtime data.
+ * @regs_size: size of runtime register array.
+ * @regs: pointer to array of register offset/value pairs.
+ */
+struct intel_iov_vf_runtime {
+	u32 regs_size;
+	struct vf_runtime_reg {
+		u32 offset;
+		u32 value;
+	} *regs;
+};
+
+/**
+ * struct intel_iov_memirq - IOV interrupts data.
+ * @obj: GEM object with memory interrupt data.
+ * @vma: VMA of the object.
+ * @vaddr: pointer to memory interrupt data.
+ */
+struct intel_iov_memirq {
+	struct drm_i915_gem_object *obj;
+	struct i915_vma *vma;
+	void *vaddr;
+};
+
+/**
+ * struct intel_iov_relay - IOV Relay Communication data.
+ * @lock: protects #pending_relays and #last_fence.
+ * @pending_relays: list of relay requests that await a response.
+ * @last_fence: fence used with last message.
+ */
+struct intel_iov_relay {
+	spinlock_t lock;
+	struct list_head pending_relays;
+	u32 last_fence;
+
+	I915_SELFTEST_DECLARE(struct {
+		int (*host2guc)(struct intel_iov_relay *, const u32 *, u32);
+		int (*guc2pf)(struct intel_iov_relay *, const u32 *, u32);
+		int (*guc2vf)(struct intel_iov_relay *, const u32 *, u32);
+		void *data;
+		bool disable_strict : 1;
+		bool enable_loopback : 1;
+	} selftest);
+};
+
+/**
+ * struct intel_iov_vf_config - VF configuration data.
+ * @ggtt_base: base of GGTT region.
+ * @ggtt_size: size of GGTT region.
+ * @num_ctxs: number of GuC submission contexts.
+ * @num_dbs: number of GuC doorbells.
+ */
+struct intel_iov_vf_config {
+	struct {
+		u8 branch;
+		u8 major;
+		u8 minor;
+		u8 patch;
+	} guc_abi;
+	u64 ggtt_base;
+	u64 ggtt_size;
+	u16 num_ctxs;
+	u16 num_dbs;
+};
+
+/**
+ * struct intel_iov - I/O Virtualization related data.
+ * @pf.sysfs: sysfs data.
+ * @pf.provisioning: provisioning data.
+ * @pf.service: placeholder for service data.
+ * @pf.state: placeholder for VFs data.
+ * @vf.config: configuration of the resources assigned to VF.
+ * @vf.runtime: retrieved runtime info.
+ * @vf.irq: Memory based interrupts data.
+ * @relay: data related to VF/PF communication based on GuC Relay messages.
+ */
+struct intel_iov {
+	union {
+		struct {
+			struct intel_iov_sysfs sysfs;
+			struct intel_iov_provisioning provisioning;
+			struct intel_iov_service service;
+			struct intel_iov_state state;
+		} pf;
+
+		struct {
+			struct intel_iov_vf_config config;
+			struct intel_iov_vf_runtime runtime;
+			struct drm_mm_node ggtt_balloon[2];
+			struct intel_iov_memirq irq;
+		} vf;
+	};
+
+	struct intel_iov_relay relay;
+};
+
+#endif /* __INTEL_IOV_TYPES_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_utils.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_utils.h
new file mode 100644
index 000000000000..61358211c96b
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_utils.h
@@ -0,0 +1,146 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_UTILS_H__
+#define __INTEL_IOV_UTILS_H__
+
+#include "i915_drv.h"
+#include "gt/intel_gt_print.h"
+
+static inline struct intel_gt *iov_to_gt(struct intel_iov *iov)
+{
+	return container_of(iov, struct intel_gt, iov);
+}
+
+static inline struct intel_guc *iov_to_guc(struct intel_iov *iov)
+{
+	return &iov_to_gt(iov)->uc.guc;
+}
+
+static inline struct drm_i915_private *iov_to_i915(struct intel_iov *iov)
+{
+	return iov_to_gt(iov)->i915;
+}
+
+static inline struct device *iov_to_dev(struct intel_iov *iov)
+{
+	return iov_to_i915(iov)->drm.dev;
+}
+
+static inline struct intel_iov *iov_get_root(struct intel_iov *iov)
+{
+	return &to_gt(iov_to_i915(iov))->iov;
+}
+
+static inline bool iov_is_root(struct intel_iov *iov)
+{
+	return iov == iov_get_root(iov);
+}
+
+static inline bool iov_is_remote(struct intel_iov *iov)
+{
+	return !iov_is_root(iov);
+}
+
+static inline bool intel_iov_is_pf(struct intel_iov *iov)
+{
+	return IS_SRIOV_PF(iov_to_i915(iov));
+}
+
+static inline bool intel_iov_is_vf(struct intel_iov *iov)
+{
+	return IS_SRIOV_VF(iov_to_i915(iov));
+}
+
+static inline bool intel_iov_is_enabled(struct intel_iov *iov)
+{
+	return intel_iov_is_pf(iov) || intel_iov_is_vf(iov);
+}
+
+static inline u16 pf_get_totalvfs(struct intel_iov *iov)
+{
+	return i915_sriov_pf_get_totalvfs(iov_to_i915(iov));
+}
+
+static inline u16 pf_get_numvfs(struct intel_iov *iov)
+{
+	return pci_num_vf(to_pci_dev(iov_to_dev(iov)));
+}
+
+static inline bool pf_in_error(struct intel_iov *iov)
+{
+	return i915_sriov_pf_aborted(iov_to_i915(iov));
+}
+
+static inline int pf_get_status(struct intel_iov *iov)
+{
+	return i915_sriov_pf_status(iov_to_i915(iov));
+}
+
+static inline struct mutex *pf_provisioning_mutex(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	/* always use mutex from the root tile */
+	return &iov_get_root(iov)->pf.provisioning.lock;
+}
+
+#define IOV_ERROR(_iov, _fmt, ...) \
+	gt_notice(iov_to_gt(_iov), "IOV: " _fmt, ##__VA_ARGS__)
+#define IOV_PROBE_ERROR(_iov, _fmt, ...) \
+	gt_probe_error(iov_to_gt(_iov), "IOV: " _fmt, ##__VA_ARGS__)
+
+#ifdef CONFIG_DRM_I915_DEBUG_IOV
+#define IOV_DEBUG(_iov, _fmt, ...) \
+	gt_dbg(iov_to_gt(_iov), "IOV: " _fmt, ##__VA_ARGS__)
+#else
+#define IOV_DEBUG(_iov, _fmt, ...) typecheck(struct intel_iov *, _iov)
+#endif
+
+static inline void pf_update_status(struct intel_iov *iov, int status, const char *reason)
+{
+	GEM_BUG_ON(status >= 0);
+	IOV_PROBE_ERROR(iov, "Initialization failed (%pe) %s\n", ERR_PTR(status), reason);
+	i915_sriov_pf_abort(iov_to_i915(iov), status);
+}
+
+static inline void pf_mark_manual_provisioning(struct intel_iov *iov)
+{
+	i915_sriov_pf_set_auto_provisioning(iov_to_i915(iov), false);
+}
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+#define IOV_SELFTEST_ERROR(_iov, _fmt, ...) \
+	IOV_ERROR((_iov), "selftest/%s: " _fmt, __func__, ##__VA_ARGS__)
+
+#define intel_iov_live_subtests(T, data) ({ \
+	typecheck(struct intel_iov *, data); \
+	__i915_subtests(__func__, \
+			__intel_iov_live_setup, __intel_iov_live_teardown, \
+			T, ARRAY_SIZE(T), data); \
+})
+
+static inline int __intel_iov_live_setup(void *data)
+{
+	return __intel_gt_live_setup(iov_to_gt(data));
+}
+
+static inline int __intel_iov_live_teardown(int err, void *data)
+{
+	return __intel_gt_live_teardown(err, iov_to_gt(data));
+}
+#endif /* IS_ENABLED(CONFIG_DRM_I915_SELFTEST) */
+
+static inline const char *intel_iov_threshold_to_string(enum intel_iov_threshold threshold)
+{
+	switch (threshold) {
+#define __iov_threshold_to_string(K, N, ...) \
+	case IOV_THRESHOLD_##K: return #N;
+	IOV_THRESHOLDS(__iov_threshold_to_string)
+	}
+#undef __iov_threshold_to_string
+	return "<invalid>";
+}
+
+#endif /* __INTEL_IOV_UTILS_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_provisioning.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_provisioning.c
new file mode 100644
index 000000000000..6907dc9b4eef
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_provisioning.c
@@ -0,0 +1,999 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#include "selftests/i915_random.h"
+
+/* XXX pick policy key that is safe to use */
+#define GUC_KLV_VGT_POLICY_EXAMPLE_KEY			GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_KEY
+#define GUC_KLV_VGT_POLICY_EXAMPLE_LEN			GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_LEN
+
+/* XXX make sure this policy key does not exist ! */
+#define GUC_KLV_VGT_POLICY_DOES_NOT_EXIST_KEY		0x8DDD
+#define GUC_KLV_VGT_POLICY_DOES_NOT_EXIST_LEN		1u
+
+static int pf_guc_accepts_example_policy_key(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	int ret;
+
+	ret = guc_update_policy_klv32(guc, GUC_KLV_VGT_POLICY_EXAMPLE_KEY, 0);
+	if (ret) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't accept example key, %d\n", ret);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int pf_guc_ignores_unknown_policy_key(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	int ret;
+
+	ret = guc_update_policy_klv32(guc, GUC_KLV_VGT_POLICY_DOES_NOT_EXIST_KEY, 0);
+	if (ret != -ENOKEY) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't ignore unknown key, %d\n", ret);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int __guc_try_update_policy(struct intel_guc *guc, u64 addr, u32 size, u32 len)
+{
+	u32 request[GUC_CTB_MSG_MAX_LEN - GUC_CTB_MSG_MIN_LEN] = {
+		GUC_ACTION_PF2GUC_UPDATE_VGT_POLICY,
+		lower_32_bits(addr),
+		upper_32_bits(addr),
+		size,
+		POISON_END,
+		/* ... */
+	};
+	unsigned int n;
+
+	BUILD_BUG_ON(ARRAY_SIZE(request) == PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN);
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(len > ARRAY_SIZE(request));
+
+	for (n = PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN; n < len; n++)
+		request[n] = POISON_END;
+
+	return intel_guc_ct_send(&guc->ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST);
+}
+
+static int guc_try_update_policy(struct intel_guc *guc, u64 addr, u32 size)
+{
+	return __guc_try_update_policy(guc, addr, size, PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN);
+}
+
+static int pf_guc_parses_flexible_policy_keys(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	static const unsigned int max_klv_len = SZ_64K - 1;
+	unsigned int blob_size = sizeof(u32) * (GUC_KLV_LEN_MIN + max_klv_len);
+	unsigned int len;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, blob_size, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	for (len = 0; len <= max_klv_len; len++) {
+		IOV_DEBUG(iov, "len=%u\n", len);
+
+		klvs = blob;
+		*klvs++ = FIELD_PREP(GUC_KLV_0_KEY, GUC_KLV_VGT_POLICY_DOES_NOT_EXIST_KEY) |
+			  FIELD_PREP(GUC_KLV_0_LEN, len);
+		*klvs++ = len;
+
+		ret = guc_try_update_policy(guc, addr, GUC_KLV_LEN_MIN + len);
+		if (ret < 0) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't parse flexible key len=%u, %d\n",
+					   len, ret);
+			result = -EPROTO;
+			break;
+		}
+
+		if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+			len += i915_prandom_u32_max_state(len, &prng);
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+static int pf_guc_accepts_duplicated_policy_keys(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int n, num_klvs = 1 + i915_prandom_u32_max_state(16, &prng);
+	unsigned int klv_size = GUC_KLV_LEN_MIN + GUC_KLV_VGT_POLICY_EXAMPLE_LEN;
+	unsigned int blob_size = sizeof(u32) * klv_size * num_klvs;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret;
+
+	ret = intel_guc_allocate_and_map_vma(guc, blob_size, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+	klvs = blob;
+
+	IOV_DEBUG(iov, "num_klvs=%u\n", num_klvs);
+	for (n = 0; n < num_klvs; n++) {
+		*klvs++ = MAKE_GUC_KLV(VGT_POLICY_EXAMPLE);
+		klvs += GUC_KLV_VGT_POLICY_EXAMPLE_LEN;
+	}
+	pf_verify_config_klvs(iov, blob, klvs - blob);
+
+	ret = guc_try_update_policy(guc, addr, klvs - blob);
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+
+	if (ret != num_klvs) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't accept duplicated KLV (n=%u), %d\n",
+				   num_klvs, ret);
+		return -EPROTO;
+	}
+
+	return 0;
+}
+
+static int pf_guc_parses_mixed_policy_keys(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int n, num_klvs = 2 + i915_prandom_u32_max_state(16, &prng);
+	unsigned int klv_size = GUC_KLV_LEN_MIN + GUC_KLV_VGT_POLICY_EXAMPLE_LEN;
+	unsigned int other_klv_size = GUC_KLV_LEN_MIN + GUC_KLV_VGT_POLICY_DOES_NOT_EXIST_LEN;
+	unsigned int blob_size = sizeof(u32) * max(klv_size, other_klv_size) * num_klvs;
+	unsigned int p, patterns[] = {
+		/* make sure to test with first known KLV and the opposite */
+		[0] = GENMASK(num_klvs - 1, 0) & 0x5555,
+		[1] = GENMASK(num_klvs - 1, 0) & ~patterns[0],
+		[2] = i915_prandom_u32_max_state(GENMASK(num_klvs - 1, 0), &prng),
+	};
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, blob_size, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	for (p = 0; p < ARRAY_SIZE(patterns); p++) {
+		unsigned int pattern = patterns[p];
+
+		klvs = blob;
+		for (n = 0; n < num_klvs; n++) {
+			if (pattern & BIT(n)) {
+				*klvs++ = MAKE_GUC_KLV(VGT_POLICY_EXAMPLE);
+				klvs += GUC_KLV_VGT_POLICY_EXAMPLE_LEN;
+			} else {
+				*klvs++ = MAKE_GUC_KLV(VGT_POLICY_DOES_NOT_EXIST);
+				klvs += GUC_KLV_VGT_POLICY_DOES_NOT_EXIST_LEN;
+			}
+		}
+		pf_verify_config_klvs(iov, blob, klvs - blob);
+
+		ret = guc_try_update_policy(guc, addr, klvs - blob);
+
+		if (ret != hweight32(pattern)) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't parse mixed KLVs (%u/%u p=%#x), %d\n",
+					   hweight32(pattern), num_klvs, pattern, ret);
+			result = -EPROTO;
+			break;
+		}
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+static int pf_guc_rejects_invalid_update_policy_params(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	klvs = blob;
+	*klvs++ = MAKE_GUC_KLV(VGT_POLICY_EXAMPLE);
+	*klvs++ = 0;
+	klvs_size = klvs - blob;
+
+	ret = guc_try_update_policy(guc, 0, klvs_size);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject zero address, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+	ret = guc_try_update_policy(guc, addr, 0);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject zero size, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+	ret = guc_try_update_policy(guc, addr, klvs_size - 1);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject truncated blob, %d\n", ret);
+		goto release; /* XXX firmware bug GUC-4622 */
+		result = -EPROTO;
+		goto release;
+	}
+
+release:
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+static int pf_guc_rejects_incomplete_update_policy_hxg(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int len;
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	klvs = blob;
+	*klvs++ = MAKE_GUC_KLV(VGT_POLICY_EXAMPLE);
+	*klvs++ = 0;
+	klvs_size = klvs - blob;
+
+	for (len = GUC_HXG_REQUEST_MSG_MIN_LEN;
+	     len < PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN; len++) {
+		ret = __guc_try_update_policy(guc, addr, klvs_size, len);
+		if (ret != -EIO) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't reject incomplete HXG len=%u, %d\n",
+					   len, ret);
+			result = -EPROTO;
+			break;
+		}
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+static int pf_guc_accepts_extended_update_policy_hxg(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int len;
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	klvs = blob;
+	*klvs++ = MAKE_GUC_KLV(VGT_POLICY_EXAMPLE);
+	*klvs++ = 0;
+	klvs_size = klvs - blob;
+
+	/*
+	 * GuC team claims that they will always accept messages longer than
+	 * defined in current ABI as this will allow future extensions.
+	 */
+	for (len = PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN + 1;
+	     len < GUC_CTB_MSG_MAX_LEN - GUC_CTB_MSG_MIN_LEN; len++) {
+		IOV_DEBUG(iov, "len=%u\n", len);
+
+		ret = __guc_try_update_policy(guc, addr, klvs_size, len);
+		if (ret != 1) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't accepts extended HXG len=%u, %d\n",
+					   len, ret);
+			result = -EPROTO;
+			break;
+		}
+
+		if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+			len += i915_prandom_u32_max_state(len, &prng);
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+#define IOV_POLICY_KLVS(config) \
+	config(SCHED_IF_IDLE) \
+	config(ADVERSE_SAMPLE_PERIOD) \
+	config(RESET_AFTER_VF_SWITCH) \
+	/*end*/
+
+static int pf_guc_rejects_broken_policy_klv(void *arg)
+{
+	I915_RND_STATE(prng);
+	static const unsigned int max_klv_len = SZ_64K - 1;
+	static const struct {
+		u16 key;
+		u16 len;
+	} policies[] = {
+#define config(K) { .key = GUC_KLV_VGT_POLICY_##K##_KEY, .len = GUC_KLV_VGT_POLICY_##K##_LEN },
+	IOV_POLICY_KLVS(config)
+#undef config
+	};
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int blob_size = sizeof(u32) * (GUC_KLV_LEN_MIN + max_klv_len);
+	unsigned int n, len;
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, blob_size, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	for (n = 0; n < ARRAY_SIZE(policies); n++) {
+		for (len = 0; len <= max_klv_len; len++) {
+			if (len == policies[n].len)
+				continue;
+			IOV_DEBUG(iov, "len=%u\n", len);
+
+			klvs = blob;
+			*klvs++ = FIELD_PREP(GUC_KLV_0_KEY, policies[n].key) |
+				  FIELD_PREP(GUC_KLV_0_LEN, len);
+			*klvs++ = len;
+			klvs_size = GUC_KLV_LEN_MIN + len;
+
+			ret = guc_try_update_policy(guc, addr, klvs_size);
+			if (ret != -EIO) {
+				IOV_SELFTEST_ERROR(iov,
+						   "GuC didn't reject KLV %s/%04x len=%u, %d\n",
+						   policy_key_to_string(policies[n].key),
+						   policies[n].key, len, ret);
+				result = -EPROTO;
+				break;
+			}
+
+			if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+				len += i915_prandom_u32_max_state(len, &prng);
+		}
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return 0; /* XXX firmware bug GUC-4363 */
+	return result;
+}
+
+/* XXX pick config key that is safe to use */
+#define GUC_KLV_VF_CFG_EXAMPLE_KEY			GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR_KEY
+#define GUC_KLV_VF_CFG_EXAMPLE_LEN			GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR_LEN
+
+/* XXX make sure this config key does not exist ! */
+#define GUC_KLV_VF_CFG_DOES_NOT_EXIST_KEY		0x8ADD
+#define GUC_KLV_VF_CFG_DOES_NOT_EXIST_LEN		1u
+
+static int pf_guc_accepts_example_config_key(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	int ret;
+
+	ret = guc_update_vf_klv32(guc, vfid, GUC_KLV_VF_CFG_EXAMPLE_KEY, 0);
+	if (ret) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't accept example key, %d\n", ret);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int pf_guc_ignores_unknown_config_key(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	int ret;
+
+	ret = guc_update_vf_klv32(guc, vfid, GUC_KLV_VF_CFG_DOES_NOT_EXIST_KEY, 0);
+	if (ret != -ENOKEY) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't ignore example key, %d\n", ret);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int __guc_try_update_config(struct intel_guc *guc, u32 vfid, u64 addr, u32 size, u32 len)
+{
+	u32 request[GUC_CTB_MSG_MAX_LEN - GUC_CTB_MSG_MIN_LEN] = {
+		GUC_ACTION_PF2GUC_UPDATE_VF_CFG,
+		vfid,
+		lower_32_bits(addr),
+		upper_32_bits(addr),
+		size,
+		POISON_END,
+		/* ... */
+	};
+	unsigned int n;
+
+	BUILD_BUG_ON(ARRAY_SIZE(request) == PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN);
+	GEM_BUG_ON(!len);
+	GEM_BUG_ON(len > ARRAY_SIZE(request));
+
+	for (n = PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_LEN; n < len; n++)
+		request[n] = POISON_END;
+
+	return intel_guc_ct_send(&guc->ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST);
+}
+
+static int guc_try_update_config(struct intel_guc *guc, u32 vfid, u64 addr, u32 size)
+{
+	return __guc_try_update_config(guc, vfid, addr, size, PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_LEN);
+}
+
+static int pf_guc_parses_flexible_config_keys(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	static const unsigned int max_klv_len = SZ_64K - 1;
+	unsigned int blob_size = sizeof(u32) * (GUC_KLV_LEN_MIN + max_klv_len);
+	unsigned int len;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, blob_size, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	for (len = 0; len <= max_klv_len; len++) {
+		IOV_DEBUG(iov, "len=%u\n", len);
+
+		klvs = blob;
+		*klvs++ = FIELD_PREP(GUC_KLV_0_KEY, GUC_KLV_VF_CFG_DOES_NOT_EXIST_KEY) |
+			  FIELD_PREP(GUC_KLV_0_LEN, len);
+		*klvs++ = len;
+
+		ret = guc_try_update_config(guc, vfid, addr, GUC_KLV_LEN_MIN + len);
+		if (ret < 0) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't parse flexible key len=%u, %d\n",
+					   len, ret);
+			result = -EPROTO;
+			break;
+		}
+
+		if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+			len += i915_prandom_u32_max_state(len, &prng);
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+static int pf_guc_rejects_invalid_update_config_params(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	klvs = blob;
+	*klvs++ = MAKE_GUC_KLV(VF_CFG_EXAMPLE);
+	*klvs++ = 0;
+	klvs_size = klvs - blob;
+
+	ret = guc_try_update_config(guc, pf_get_totalvfs(iov) + 1, addr, klvs_size);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject invalid VF, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+	ret = guc_try_update_config(guc, vfid, 0, klvs_size);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject zero address, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+	ret = guc_try_update_config(guc, vfid, 0, klvs_size);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject zero address, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+	ret = guc_try_update_config(guc, vfid, addr, 0);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject zero size, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+	ret = guc_try_update_config(guc, vfid, addr, klvs_size - 1);
+	if (ret != -EIO) {
+		IOV_SELFTEST_ERROR(iov, "GuC didn't reject truncated blob, %d\n", ret);
+		result = -EPROTO;
+		goto release;
+	}
+
+release:
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+static int pf_guc_rejects_incomplete_update_config_hxg(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int len;
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	klvs = blob;
+	*klvs++ = MAKE_GUC_KLV(VGT_POLICY_EXAMPLE);
+	*klvs++ = 0;
+	klvs_size = klvs - blob;
+
+	for (len = GUC_HXG_REQUEST_MSG_MIN_LEN;
+	     len < PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_LEN; len++) {
+		ret = __guc_try_update_config(guc, vfid, addr, klvs_size, len);
+		if (ret != -EIO) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't reject incomplete HXG len=%u, %d\n",
+					   len, ret);
+			result = -EPROTO;
+			break;
+		}
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return 0; /* XXX firmware bug GUC-4364 */
+	return result;
+}
+
+static int pf_guc_accepts_extended_update_config_hxg(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int len;
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, SZ_4K, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	klvs = blob;
+	*klvs++ = MAKE_GUC_KLV(VF_CFG_EXAMPLE);
+	*klvs++ = 0;
+	klvs_size = klvs - blob;
+
+	/*
+	 * GuC team claims that they will always accept messages longer than
+	 * defined in current ABI as this will allow future extensions.
+	 */
+	for (len = PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_LEN + 1;
+	     len < GUC_CTB_MSG_MAX_LEN - GUC_CTB_MSG_MIN_LEN; len++) {
+		IOV_DEBUG(iov, "len=%u\n", len);
+
+		ret = __guc_try_update_config(guc, vfid, addr, klvs_size, len);
+		if (ret != 1) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't accept extended HXG len=%u, %d\n",
+					   len, ret);
+			result = -EPROTO;
+			break;
+		}
+
+		if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+			len += i915_prandom_u32_max_state(len, &prng);
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return result;
+}
+
+#define config_threshold(K, ...) config(THRESHOLD_##K)
+#define IOV_VF_CFG_KLVS(config) \
+	config(GGTT_START) \
+	config(GGTT_SIZE) \
+	config(NUM_CONTEXTS) \
+	config(BEGIN_CONTEXT_ID) \
+	config(NUM_DOORBELLS) \
+	config(BEGIN_DOORBELL_ID) \
+	config(EXEC_QUANTUM) \
+	config(PREEMPT_TIMEOUT) \
+	IOV_THRESHOLDS(config_threshold) \
+	/*end*/
+
+static int pf_guc_rejects_broken_config_klv(void *arg)
+{
+	I915_RND_STATE(prng);
+	static const unsigned int max_klv_len = SZ_64K - 1;
+	static const struct {
+		u16 key;
+		u16 len;
+	} configs[] = {
+#define config(K) { .key = GUC_KLV_VF_CFG_##K##_KEY, .len = GUC_KLV_VF_CFG_##K##_LEN },
+	IOV_VF_CFG_KLVS(config)
+#undef config
+	};
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	unsigned int blob_size = sizeof(u32) * (GUC_KLV_LEN_MIN + max_klv_len);
+	unsigned int n, len;
+	u32 vfid = i915_prandom_u32_max_state(1 + pf_get_totalvfs(iov), &prng);
+	u32 klvs_size;
+	u32 *blob;
+	u32 *klvs;
+	u64 addr;
+	int ret, result = 0;
+
+	ret = intel_guc_allocate_and_map_vma(guc, blob_size, &vma, (void **)&blob);
+	if (unlikely(ret))
+		return ret;
+	addr = intel_guc_ggtt_offset(guc, vma);
+
+	for (n = 0; n < ARRAY_SIZE(configs); n++) {
+		for (len = 0; len <= max_klv_len; len++) {
+			if (len == configs[n].len)
+				continue;
+			IOV_DEBUG(iov, "len=%u\n", len);
+
+			klvs = blob;
+			*klvs++ = FIELD_PREP(GUC_KLV_0_KEY, configs[n].key) |
+				  FIELD_PREP(GUC_KLV_0_LEN, len);
+			*klvs++ = len;
+			klvs_size = GUC_KLV_LEN_MIN + len;
+
+			ret = guc_try_update_config(guc, vfid, addr, klvs_size);
+			if (ret != -EIO) {
+				IOV_SELFTEST_ERROR(iov,
+						   "GuC didn't reject KLV %04x len=%u, %d\n",
+						   configs[n].key, len, ret);
+				result = -EPROTO;
+				break;
+			}
+
+			if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+				len += i915_prandom_u32_max_state(len, &prng);
+		}
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+	return 0; /* XXX firmware bug GUC-4363 */
+	return result;
+}
+
+struct klv {
+	u32 keylen;
+	union {
+		u32 value32;
+		u64 value64;
+	};
+};
+
+static int pf_update_vf_klvs(struct intel_iov *iov, u32 vfid,
+			     const struct klv *klvs, unsigned int num_klvs)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	unsigned int n;
+	int ret;
+
+	for (n = 0; n < num_klvs; n++) {
+		const struct klv *klv = &klvs[n];
+		u32 key = FIELD_GET(GUC_KLV_0_KEY, klv->keylen);
+		u32 len = FIELD_GET(GUC_KLV_0_LEN, klv->keylen);
+
+		switch (len) {
+		case 1:
+			ret = guc_update_vf_klv32(guc, vfid, key, klv->value32);
+			break;
+		case 2:
+			ret = guc_update_vf_klv64(guc, vfid, key, klv->value64);
+			break;
+		default:
+			MISSING_CASE(len);
+			ret = -ENODEV;
+		}
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "Can't update VF%u KLV%04x, %d\n",
+					   vfid, key, ret);
+			return ret;
+		}
+	}
+	return 0;
+}
+
+static int pf_guc_accepts_config_zero(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	u32 vfid = max_t(u32, 1, i915_prandom_u32_max_state(pf_get_totalvfs(iov), &prng));
+	struct klv zero[] = {
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_START), .value64 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_SIZE), .value64 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_CONTEXT_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_CONTEXTS), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS), .value32 = 0 },
+#define make_threshold_klv(K, ...) \
+		{ MAKE_GUC_KLV(VF_CFG_THRESHOLD_##K), .value32 = 0 },
+		IOV_THRESHOLDS(make_threshold_klv)
+#undef make_threshold_klv
+	};
+	unsigned int *order;
+	unsigned int n;
+	int ret;
+
+	if (!IS_ENABLED(CONFIG_DRM_I915_SELFTEST_BROKEN))
+		return 0; /* XXX GUC-4416 */
+
+	order = i915_random_order(ARRAY_SIZE(zero), &prng);
+	for (n = 0; n < ARRAY_SIZE(zero); n++) {
+		unsigned int pos = order ? order[n] : n;
+
+		ret = pf_update_vf_klvs(iov, vfid, &zero[pos], 1);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "Failed to update #%u KLV%04x, %d\n", n + 1,
+					   FIELD_GET(GUC_KLV_0_KEY, zero[pos].keylen), ret);
+			while (n--) {
+				pos = order ? order[n] : n;
+				IOV_SELFTEST_ERROR(iov, "Previous #%u KLV%04x was OK\n", n + 1,
+						   FIELD_GET(GUC_KLV_0_KEY, zero[pos].keylen));
+			}
+			break;
+		}
+	}
+
+	kfree(order);
+	return ret;
+}
+
+static int pf_guc_accepts_config_resets(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	struct intel_guc *guc = iov_to_guc(iov);
+	u32 vfid = max_t(u32, 1, i915_prandom_u32_max_state(pf_get_totalvfs(iov), &prng));
+	static const struct klv empty[] = { };
+	static const struct klv second_empty[] = { };
+	struct klv incomplete[] = {
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS), .value32 = 1 },
+	};
+	struct klv complete[] = {
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_START), .value64 = GUC_GGTT_TOP - SZ_4K },
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_SIZE), .value64 = SZ_4K },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_CONTEXT_ID), .value32 = GUC_MAX_CONTEXT_ID - 1 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_CONTEXTS), .value32 = 1 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS), .value32 = 1 },
+	};
+	struct {
+		const char *name;
+		const struct klv *klvs;
+		unsigned int num_klvs;
+		bool valid;
+	} testcases[] = {
+#define TC(X, C) { .name = #X, .klvs = X, .num_klvs = ARRAY_SIZE(X), .valid = C }
+		TC(empty, true),
+		TC(incomplete, true),
+		TC(complete, true),
+		TC(second_empty, true),
+#undef TC
+		{ }
+	}, *tc = testcases;
+	int ret, result = 0;
+
+	for (; tc->name; tc++) {
+		IOV_DEBUG(iov, "running %s (valid=%s)\n", tc->name, str_yes_no(tc->valid));
+		if (!tc->valid)
+			continue;
+
+		ret = pf_update_vf_klvs(iov, vfid, tc->klvs, tc->num_klvs);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "Can't provision VF%u with %s config, %d\n",
+					   vfid, tc->name, ret);
+			result = -EPROTO;
+			break;
+		}
+		ret = guc_try_update_config(guc, vfid, 0, 0);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't reset VF%u %s config, %d\n",
+					   vfid, tc->name, ret);
+			result = -EPROTO;
+			break;
+		}
+	}
+
+	return 0; /* XXX GUC-4414 */
+	return result;
+}
+
+static int pf_guc_accepts_config_updates(void *arg)
+{
+	I915_RND_STATE(prng);
+	struct intel_iov *iov = arg;
+	u32 vfid = max_t(u32, 1, i915_prandom_u32_max_state(pf_get_totalvfs(iov), &prng));
+	struct klv config[] = {
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_START), .value64 = GUC_GGTT_TOP - SZ_4K },
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_SIZE), .value64 = SZ_4K },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_CONTEXT_ID), .value32 = GUC_MAX_CONTEXT_ID - 1 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_CONTEXTS), .value32 = 1 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS), .value32 = 1 },
+	};
+	struct klv update[] = {
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_START), .value64 = GUC_GGTT_TOP - SZ_1M },
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_SIZE), .value64 = SZ_1M },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_CONTEXT_ID), .value32 = GUC_MAX_CONTEXT_ID - 2 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_CONTEXTS), .value32 = 2 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID), .value32 = 1 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS), .value32 = 2 },
+	};
+	struct klv zero[] = {
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_START), .value64 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_GGTT_SIZE), .value64 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_CONTEXT_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_CONTEXTS), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_BEGIN_DOORBELL_ID), .value32 = 0 },
+		{ MAKE_GUC_KLV(VF_CFG_NUM_DOORBELLS), .value32 = 0 },
+	};
+	struct {
+		const char *name;
+		const struct klv *klvs;
+		unsigned int num_klvs;
+	} testcases[] = {
+#define TC(X) { .name = #X, .klvs = X, .num_klvs = ARRAY_SIZE(X) }
+		TC(config),
+		TC(update),
+		TC(config),
+		TC(zero),
+		TC(zero),
+#undef TC
+		{ }
+	}, *tc = testcases, *prev = NULL;
+	int ret;
+
+	for (; tc->name; prev = tc++) {
+		ret = pf_update_vf_klvs(iov, vfid, tc->klvs, tc->num_klvs);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "Failed to update config %s to %s, %d\n",
+					   prev ? prev->name : "default", tc->name, ret);
+			break;
+		}
+	}
+
+	return ret;
+}
+
+int selftest_live_iov_provisioning(struct drm_i915_private *i915)
+{
+	static const struct i915_subtest pf_policy_tests[] = {
+		SUBTEST(pf_guc_accepts_example_policy_key),
+		SUBTEST(pf_guc_ignores_unknown_policy_key),
+		SUBTEST(pf_guc_parses_flexible_policy_keys),
+		SUBTEST(pf_guc_accepts_duplicated_policy_keys),
+		SUBTEST(pf_guc_parses_mixed_policy_keys),
+		SUBTEST(pf_guc_rejects_invalid_update_policy_params),
+		SUBTEST(pf_guc_rejects_incomplete_update_policy_hxg),
+		SUBTEST(pf_guc_accepts_extended_update_policy_hxg),
+		SUBTEST(pf_guc_rejects_broken_policy_klv),
+	};
+	static const struct i915_subtest pf_config_tests[] = {
+		SUBTEST(pf_guc_accepts_example_config_key),
+		SUBTEST(pf_guc_ignores_unknown_config_key),
+		SUBTEST(pf_guc_parses_flexible_config_keys),
+		SUBTEST(pf_guc_rejects_invalid_update_config_params),
+		SUBTEST(pf_guc_rejects_incomplete_update_config_hxg),
+		SUBTEST(pf_guc_accepts_extended_update_config_hxg),
+		SUBTEST(pf_guc_rejects_broken_config_klv),
+		SUBTEST(pf_guc_accepts_config_zero),
+		SUBTEST(pf_guc_accepts_config_resets),
+		SUBTEST(pf_guc_accepts_config_updates),
+	};
+	intel_wakeref_t wakeref;
+	int err = 0;
+
+	if (!IS_SRIOV_PF(i915))
+		return 0;
+
+	if (i915_sriov_pf_status(i915) < 0)
+		return -EHOSTDOWN;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id) {
+			struct intel_iov *iov = &gt->iov;
+
+			err = intel_iov_provisioning_force_vgt_mode(iov);
+			if (err)
+				break;
+			err = intel_iov_live_subtests(pf_policy_tests, iov);
+			if (err)
+				break;
+			err = intel_iov_live_subtests(pf_config_tests, iov);
+			if (err)
+				break;
+		}
+	}
+
+	return err;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_relay.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_relay.c
new file mode 100644
index 000000000000..b3ef7dcedf53
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_relay.c
@@ -0,0 +1,547 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#include "gt/iov/intel_iov_provisioning.h"
+
+static int guc2vf_payload_checker(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	struct payload_params *expected = relay->selftest.data;
+
+	/* this must be GUC2VF handler */
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) != GUC_ACTION_GUC2VF_RELAY_FROM_PF);
+
+	if (FIELD_GET(GUC2VF_RELAY_FROM_PF_EVENT_MSG_1_RELAY_ID, msg[1]) != expected->relayid)
+		return -ENOTTY;
+
+	if (len < GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN)
+		return -EPROTO;
+
+	if (len > GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN + expected->len)
+		return -EMSGSIZE;
+
+	if (len != GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN + expected->len)
+		return -ENOMSG;
+
+	if (memcmp(msg + GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN, expected->data,
+		   sizeof(u32) * expected->len))
+		return -EBADMSG;
+
+	return 0;
+}
+
+static int guc2pf_payload_checker(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	struct payload_params *expected = relay->selftest.data;
+
+	/* this must be GUC2PF handler */
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_GUC);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, msg[0]) != GUC_ACTION_GUC2PF_RELAY_FROM_VF);
+
+	if (FIELD_GET(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID, msg[1]) != expected->vfid)
+		return -ENOTTY;
+
+	if (FIELD_GET(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID, msg[2]) != expected->relayid)
+		return -ENOTTY;
+
+	if (len < GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN)
+		return -EPROTO;
+
+	if (len > GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN + expected->len)
+		return -EMSGSIZE;
+
+	if (len != GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN + expected->len)
+		return -ENOMSG;
+
+	if (memcmp(msg + GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN, expected->data,
+		   sizeof(u32) * expected->len))
+		return -EBADMSG;
+
+	return 0;
+}
+
+static int pf_guc_loopback_to_vf(struct intel_iov *iov, bool fast, u32 len_min, u32 len_max)
+{
+	struct intel_guc_ct *ct = &iov_to_guc(iov)->ct;
+	u32 request[PF2GUC_RELAY_TO_VF_REQUEST_MSG_MAX_LEN] = {
+		MSG_PF2GUC_RELAY_TO_VF(0), /* loopback */
+		/* ... */
+	};
+	struct payload_params params;
+	u32 n, len;
+	int ret = 0;
+
+	GEM_BUG_ON(len_min > len_max);
+	GEM_BUG_ON(len_max > PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA);
+
+	/* fill relay data with some pattern */
+	for (n = 0; n < len_max; n++)
+		request[PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN + n] =
+			FIELD_PREP(PF2GUC_RELAY_TO_VF_REQUEST_MSG_n_RELAY_DATAx,
+				   SELFTEST_RELAY_DATA + n);
+
+	for (n = len_min; n <= len_max; n++) {
+
+		params.relayid = SELFTEST_RELAY_ID + n;
+		params.data = request + PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN;
+		params.len = n;
+
+		WRITE_ONCE(iov->relay.selftest.data, &params);
+		WRITE_ONCE(iov->relay.selftest.guc2vf, guc2vf_payload_checker);
+
+		request[2] = FIELD_PREP(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, params.relayid);
+		len = PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN + n;
+
+		ret = intel_guc_ct_send(ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST |
+					(fast ? INTEL_GUC_CT_SEND_NB : 0));
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "failed to send (nb=%s) payload len=%u, %d\n",
+					   str_yes_no(fast), n, ret);
+			break;
+		}
+
+		ret = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.guc2vf)), 200);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "didn't receive message len=%u, %d\n", n, ret);
+			break;
+		}
+
+		ret = PTR_ERR_OR_ZERO(iov->relay.selftest.guc2vf);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "received invalid message len=%u, %d\n", n, ret);
+			break;
+		}
+	}
+
+	WRITE_ONCE(iov->relay.selftest.guc2vf, NULL);
+	WRITE_ONCE(iov->relay.selftest.data, NULL);
+
+	return ret;
+}
+
+static int pf_guc_loopback_min_msg_to_vf(void *arg)
+{
+	return pf_guc_loopback_to_vf(arg, false, 0, 0);
+}
+
+static int pf_guc_loopback_hxg_msg_to_vf(void *arg)
+{
+	return pf_guc_loopback_to_vf(arg, false, GUC_HXG_MSG_MIN_LEN, GUC_HXG_MSG_MIN_LEN);
+}
+
+static int pf_guc_loopback_any_msg_to_vf(void *arg)
+{
+	return pf_guc_loopback_to_vf(arg, false, 0, PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA);
+}
+
+static int pf_guc_loopback_any_msg_to_vf_nb(void *arg)
+{
+	return pf_guc_loopback_to_vf(arg, true, 0, PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA);
+}
+
+static int pf_guc_rejects_incomplete_to_vf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc_ct *ct = &iov_to_guc(iov)->ct;
+	u32 request[PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN] = {
+		MSG_PF2GUC_RELAY_TO_VF(1),
+	};
+	unsigned int len;
+	int ret = 0;
+
+	for (len = GUC_HXG_REQUEST_MSG_MIN_LEN; len < ARRAY_SIZE(request); len++) {
+		ret = intel_guc_ct_send(ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST);
+		if (ret != -EIO) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't reject incomplete HXG len=%u, %d\n",
+					   len, ret);
+			ret = -EPROTO;
+			break;
+		}
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static int pf_guc_rejects_invalid_to_vf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc_ct *ct = &iov_to_guc(iov)->ct;
+	u32 invalid_vfid = i915_sriov_pf_get_device_totalvfs(iov_to_i915(iov)) + 1;
+	u32 request[] = {
+		MSG_PF2GUC_RELAY_TO_VF(invalid_vfid),
+	};
+	unsigned int len;
+	int ret = 0;
+
+	for (len = GUC_HXG_REQUEST_MSG_MIN_LEN; len <= ARRAY_SIZE(request); len++) {
+		ret = intel_guc_ct_send(ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST);
+		if (ret != -EIO) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't reject invalid VF%u len=%u, %d\n",
+					   invalid_vfid, len, ret);
+			ret = -EPROTO;
+			break;
+		}
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static int pf_guc_loopback_to_pf(struct intel_iov *iov, bool fast, u32 len_min, u32 len_max)
+{
+	struct intel_guc_ct *ct = &iov_to_guc(iov)->ct;
+	u32 request[VF2GUC_RELAY_TO_PF_REQUEST_MSG_MAX_LEN] = {
+		MSG_VF2GUC_RELAY_TO_PF,
+		/* ... */
+	};
+	struct payload_params params;
+	u32 n, len;
+	int ret = 0;
+
+	GEM_BUG_ON(len_min > len_max);
+	GEM_BUG_ON(len_max > VF2GUC_RELAY_TO_PF_REQUEST_MSG_NUM_RELAY_DATA);
+
+	/* fill relay data with some pattern */
+	for (n = 0; n < len_max; n++)
+		request[VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN + n] =
+			FIELD_PREP(VF2GUC_RELAY_TO_PF_REQUEST_MSG_n_RELAY_DATAx,
+				   SELFTEST_RELAY_DATA - n);
+
+	for (n = len_min; n <= len_max; n++) {
+
+		params.vfid = PFID;
+		params.relayid = SELFTEST_RELAY_ID + n;
+		params.len = n;
+		params.data = request + VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN;
+
+		WRITE_ONCE(iov->relay.selftest.data, &params);
+		WRITE_ONCE(iov->relay.selftest.guc2pf, guc2pf_payload_checker);
+
+		request[1] = FIELD_PREP(VF2GUC_RELAY_TO_PF_REQUEST_MSG_1_RELAY_ID, params.relayid);
+		len = VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN + n;
+
+		ret = intel_guc_ct_send(ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST |
+					(fast ? INTEL_GUC_CT_SEND_NB : 0));
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "failed to send (nb=%s) payload len=%u, %d\n",
+					   str_yes_no(fast), n, ret);
+			break;
+		}
+
+		ret = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.guc2pf)), 200);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "didn't receive message len=%u, %d\n", n, ret);
+			break;
+		}
+
+		ret = PTR_ERR_OR_ZERO(iov->relay.selftest.guc2pf);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "received invalid message len=%u, %d\n", n, ret);
+			break;
+		}
+	}
+
+	WRITE_ONCE(iov->relay.selftest.guc2pf, NULL);
+	WRITE_ONCE(iov->relay.selftest.data, NULL);
+
+	return ret;
+}
+
+static int pf_guc_loopback_min_msg_to_pf(void *arg)
+{
+	return pf_guc_loopback_to_pf(arg, false, 0, 0);
+}
+
+static int pf_guc_loopback_hxg_msg_to_pf(void *arg)
+{
+	return pf_guc_loopback_to_pf(arg, false, GUC_HXG_MSG_MIN_LEN, GUC_HXG_MSG_MIN_LEN);
+}
+
+static int pf_guc_loopback_any_msg_to_pf(void *arg)
+{
+	return pf_guc_loopback_to_pf(arg, false, 0, VF2GUC_RELAY_TO_PF_REQUEST_MSG_NUM_RELAY_DATA);
+}
+
+static int pf_guc_loopback_any_msg_to_pf_nb(void *arg)
+{
+	return pf_guc_loopback_to_pf(arg, true, 0, VF2GUC_RELAY_TO_PF_REQUEST_MSG_NUM_RELAY_DATA);
+}
+
+static int pf_guc_rejects_incomplete_to_pf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	struct intel_guc_ct *ct = &iov_to_guc(iov)->ct;
+	u32 request[VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN] = {
+		MSG_VF2GUC_RELAY_TO_PF,
+	};
+	unsigned int len;
+	int ret = 0;
+
+	for (len = GUC_HXG_REQUEST_MSG_MIN_LEN; len < ARRAY_SIZE(request); len++) {
+		ret = intel_guc_ct_send(ct, request, len, NULL, 0, INTEL_GUC_CT_SEND_SELFTEST);
+		if (ret != -EIO) {
+			IOV_SELFTEST_ERROR(iov, "GuC didn't reject incomplete HXG len=%u, %d\n",
+					   len, ret);
+			ret = -EPROTO;
+			break;
+		}
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static int pf_loopback_one_way_to_vf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		MSG_IOV_SELFTEST_RELAY_EVENT(SELFTEST_RELAY_OPCODE_NOP),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		/* ... */
+	};
+	struct payload_params params;
+	unsigned int n;
+	int ret = 0;
+
+	iov->relay.selftest.enable_loopback = 1;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		params.relayid = SELFTEST_RELAY_ID + n;
+		params.vfid = PFID; /* loopback */
+		params.data = msg;
+		params.len = n;
+
+		WRITE_ONCE(iov->relay.selftest.data, &params);
+		WRITE_ONCE(iov->relay.selftest.guc2vf, guc2vf_payload_checker);
+
+		/* can't use intel_iov_relay_send_to_vf() as we need our relayid */
+		ret = relay_send(&iov->relay, params.vfid, params.relayid, msg, n);
+
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "len=%u, %d\n", n, ret);
+			break;
+		}
+
+		ret = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.guc2vf)), 200);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "message not received len=%u, %d\n", n, ret);
+			break;
+		}
+
+		ret = PTR_ERR_OR_ZERO(iov->relay.selftest.guc2vf);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "corrupted message len=%u, %d\n", n, ret);
+			break;
+		}
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+	WRITE_ONCE(iov->relay.selftest.guc2vf, NULL);
+	WRITE_ONCE(iov->relay.selftest.data, NULL);
+
+	return ret;
+}
+
+static int pf_full_loopback_to_vf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		MSG_IOV_SELFTEST_RELAY(SELFTEST_RELAY_OPCODE_NOP),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		/* ... */
+	};
+	u32 buf[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA];
+	unsigned int n;
+	int ret = 0;
+
+	iov->relay.selftest.enable_loopback = 1;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		ret = intel_iov_relay_send_to_vf(&iov->relay, PFID, msg, n,
+						 buf, ARRAY_SIZE(buf));
+
+		if (ret < 0) {
+			IOV_SELFTEST_ERROR(iov, "failed to send msg len=%u, %d\n", n, ret);
+			break;
+		}
+
+		if (ret != GUC_HXG_MSG_MIN_LEN) {
+			IOV_SELFTEST_ERROR(iov, "unexpected nop reply len=%u, %d\n", n, ret);
+			ret = -ENODATA;
+			break;
+		}
+
+		ret = 0;
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+
+	return ret;
+}
+
+static int pf_loopback_one_way_to_pf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg[VF2GUC_RELAY_TO_PF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		MSG_IOV_SELFTEST_RELAY_EVENT(SELFTEST_RELAY_OPCODE_NOP),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		/* ... */
+	};
+	struct payload_params params;
+	unsigned int n;
+	int ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		params.vfid = PFID;
+		params.relayid = SELFTEST_RELAY_ID + n;
+		params.data = msg;
+		params.len = n;
+
+		WRITE_ONCE(iov->relay.selftest.data, &params);
+		WRITE_ONCE(iov->relay.selftest.guc2pf, guc2pf_payload_checker);
+
+		/* can't use intel_iov_relay_send_to_pf() as we need our relayid */
+		ret = relay_send(&iov->relay, 0, params.relayid, msg, n);
+
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "len=%u, %d\n", n, ret);
+			break;
+		}
+
+		ret = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.guc2pf)), 200);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "message not received len=%u, %d\n", n, ret);
+			break;
+		}
+
+		ret = PTR_ERR_OR_ZERO(iov->relay.selftest.guc2pf);
+		if (ret) {
+			IOV_SELFTEST_ERROR(iov, "corrupted message len=%u, %d\n", n, ret);
+			break;
+		}
+	}
+
+	iov->relay.selftest.disable_strict = 0;
+	WRITE_ONCE(iov->relay.selftest.guc2pf, NULL);
+	WRITE_ONCE(iov->relay.selftest.data, NULL);
+
+	return ret;
+}
+
+static int relay_request_to_pf(struct intel_iov *iov)
+{
+	u32 msg[VF2GUC_RELAY_TO_PF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		MSG_IOV_SELFTEST_RELAY(SELFTEST_RELAY_OPCODE_NOP),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		/* ... */
+	};
+	u32 buf[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA];
+	unsigned int n;
+	int ret = 0;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		ret = intel_iov_relay_send_to_pf(&iov->relay, msg, n,
+						 buf, ARRAY_SIZE(buf));
+
+		if (ret < 0) {
+			IOV_SELFTEST_ERROR(iov, "failed to send len=%u, %d\n", n, ret);
+			break;
+		}
+
+		if (ret != GUC_HXG_MSG_MIN_LEN) {
+			IOV_SELFTEST_ERROR(iov, "unexpected nop reply len=%u, %d\n", n, ret);
+			ret = -ENODATA;
+			break;
+		}
+
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static int pf_full_loopback_to_pf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.enable_loopback = 1;
+
+	err = relay_request_to_pf(iov);
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+
+	return err;
+}
+
+static int vf_send_request_to_pf(void *arg)
+{
+	return relay_request_to_pf(arg);
+}
+
+int selftest_live_iov_relay(struct drm_i915_private *i915)
+{
+	static const struct i915_subtest pf_tests[] = {
+		SUBTEST(pf_guc_loopback_min_msg_to_vf),
+		SUBTEST(pf_guc_loopback_hxg_msg_to_vf),
+		SUBTEST(pf_guc_loopback_any_msg_to_vf),
+		SUBTEST(pf_guc_loopback_any_msg_to_vf_nb),
+		SUBTEST(pf_guc_rejects_incomplete_to_vf),
+		SUBTEST(pf_guc_rejects_invalid_to_vf),
+		SUBTEST(pf_guc_loopback_min_msg_to_pf),
+		SUBTEST(pf_guc_loopback_hxg_msg_to_pf),
+		SUBTEST(pf_guc_loopback_any_msg_to_pf),
+		SUBTEST(pf_guc_loopback_any_msg_to_pf_nb),
+		SUBTEST(pf_guc_rejects_incomplete_to_pf),
+		SUBTEST(pf_loopback_one_way_to_vf),
+		SUBTEST(pf_full_loopback_to_vf),
+		SUBTEST(pf_loopback_one_way_to_pf),
+		SUBTEST(pf_full_loopback_to_pf),
+	};
+	static const struct i915_subtest vf_tests[] = {
+		SUBTEST(vf_send_request_to_pf),
+	};
+	intel_wakeref_t wakeref;
+	int err = 0;
+
+	if (!IS_SRIOV(i915))
+		return 0;
+
+	if (IS_SRIOV_PF(i915) && i915_sriov_pf_status(i915) < 0)
+		return -EHOSTDOWN;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id) {
+			struct intel_iov *iov = &gt->iov;
+
+			if (IS_SRIOV_PF(i915)) {
+				err = intel_iov_provisioning_force_vgt_mode(iov);
+				if (err)
+					break;
+				err = intel_iov_live_subtests(pf_tests, iov);
+			} else if (IS_SRIOV_VF(i915)) {
+				err = intel_iov_live_subtests(vf_tests, iov);
+			}
+			if (err)
+				break;
+		}
+	}
+
+	return err;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_service.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_service.c
new file mode 100644
index 000000000000..55c290830882
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_live_iov_service.c
@@ -0,0 +1,170 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#include "gt/iov/abi/iov_actions_abi.h"
+#include "gt/iov/abi/iov_version_abi.h"
+#include "gt/iov/intel_iov_provisioning.h"
+
+static int handshake(struct intel_iov *iov, u32 major, u32 minor, bool ignore_vers_match)
+{
+	u32 request[VF2PF_HANDSHAKE_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, major) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, minor),
+	};
+	u32 response[VF2PF_HANDSHAKE_RESPONSE_MSG_LEN];
+	u32 major_resp, minor_resp;
+	int ret = 0;
+
+	IOV_DEBUG(iov, "try handshaking %u.%u\n", major, minor);
+
+	ret = intel_iov_relay_send_to_pf(&iov->relay, request, ARRAY_SIZE(request), response,
+					 ARRAY_SIZE(response));
+	if (ret < 0) {
+		IOV_SELFTEST_ERROR(iov, "Handshake %u.%u failed (%pe)", major, minor,
+				   ERR_PTR(ret));
+		goto out;
+	}
+
+	if (ret != VF2PF_HANDSHAKE_RESPONSE_MSG_LEN) {
+		IOV_SELFTEST_ERROR(iov, "Handshake %u.%u unexpected reply msg len (%d != %u)",
+				   major, minor, ret, VF2PF_HANDSHAKE_RESPONSE_MSG_LEN);
+		ret = -EPROTO;
+		goto out;
+	}
+
+	if (FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_0_MBZ, response[0])) {
+		IOV_SELFTEST_ERROR(iov, "Handshake %u.%u unexpected reply data (%u != 0)", major,
+				   minor, FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_0_MBZ,
+				   response[0]));
+		ret = -EPROTO;
+		goto out;
+	}
+
+	if (ignore_vers_match)
+		return 0;
+
+	major_resp = FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MAJOR, response[1]);
+	minor_resp = FIELD_GET(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MINOR, response[1]);
+
+	if (major_resp > major) {
+		IOV_SELFTEST_ERROR(iov, "Handshake %u.%u unexpected version: %u.%u",
+				   major, minor, major_resp, minor_resp);
+		ret = -ERANGE;
+		goto out;
+	}
+
+	if (major_resp == major && minor_resp > minor) {
+		IOV_SELFTEST_ERROR(iov, "Handshake %u.%u unexpected version: %u.%u",
+				   major, minor, major_resp, minor_resp);
+		ret = -ERANGE;
+		goto out;
+	}
+out:
+	return ret < 0 ? ret : 0;
+}
+
+static int pf_loopback_handshake_baseline(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.enable_loopback = 1;
+
+	ret = handshake(iov, 1, 0, false);
+
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.enable_loopback = 0;
+
+	return ret;
+}
+
+static int pf_loopback_handshake_latest(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.enable_loopback = 1;
+
+	ret = handshake(iov, IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR, false);
+
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.enable_loopback = 0;
+
+	return ret;
+}
+
+static int vf_handshake_query(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int ret;
+
+	ret = handshake(iov, 0, 0, true);
+
+	return ret;
+}
+
+static int vf_handshake_fallback_minor(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int ret;
+
+	ret = handshake(iov, IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR + 1, false);
+
+	return ret;
+}
+
+static int vf_handshake_fallback_major_minor(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int ret;
+
+	ret = handshake(iov, IOV_VERSION_LATEST_MAJOR + 1, IOV_VERSION_LATEST_MINOR + 1, false);
+
+	return ret;
+}
+
+int selftest_live_iov_service(struct drm_i915_private *i915)
+{
+	static const struct i915_subtest pf_tests[] = {
+		SUBTEST(pf_loopback_handshake_baseline),
+		SUBTEST(pf_loopback_handshake_latest),
+	};
+	static const struct i915_subtest vf_tests[] = {
+		SUBTEST(vf_handshake_query),
+		SUBTEST(vf_handshake_fallback_minor),
+		SUBTEST(vf_handshake_fallback_major_minor),
+	};
+	intel_wakeref_t wakeref;
+	int err = 0;
+
+	if (!IS_SRIOV(i915))
+		return 0;
+
+	if (IS_SRIOV_PF(i915) && i915_sriov_pf_status(i915) < 0)
+		return -EHOSTDOWN;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id) {
+			struct intel_iov *iov = &gt->iov;
+
+			if (IS_SRIOV_PF(i915))
+				err = intel_iov_live_subtests(pf_tests, iov);
+			else if (IS_SRIOV_VF(i915))
+				err = intel_iov_live_subtests(vf_tests, iov);
+			if (err)
+				break;
+		}
+	}
+
+	return err;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_relay.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_relay.c
new file mode 100644
index 000000000000..91b8083d9f18
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_relay.c
@@ -0,0 +1,725 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#include "selftests/mock_gem_device.h"
+
+static int host2guc_success(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	IOV_DEBUG(relay_to_iov(relay), "attempt to send [%u] %*ph\n", len, 4 * len, msg);
+	return 0;
+}
+
+static int mock_accepts_min_msg(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_guc2pf[GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+	};
+	u32 msg_guc2vf[GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+	};
+	int err, ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.host2guc = host2guc_success;
+
+	do {
+		err = intel_iov_relay_process_guc2pf(&iov->relay, msg_guc2pf, ARRAY_SIZE(msg_guc2pf));
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "GUC2PF was rejected %d (%pe)\n", err, ERR_PTR(err));
+			ret = -ENOTSOCK;
+			break;
+		}
+
+		err = intel_iov_relay_process_guc2vf(&iov->relay, msg_guc2vf, ARRAY_SIZE(msg_guc2vf));
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "GUC2VF was rejected %d (%pe)\n", err, ERR_PTR(err));
+			ret = -ENOTSOCK;
+			break;
+		}
+	} while (0);
+
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+
+	return ret;
+}
+
+static int mock_drops_msg_if_native(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_guc2pf[GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+	};
+	u32 msg_guc2vf[GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+	};
+	int err, ret = 0;
+
+	iov->relay.selftest.host2guc = host2guc_success;
+
+	do {
+		err = intel_iov_relay_process_guc2pf(&iov->relay, msg_guc2pf, ARRAY_SIZE(msg_guc2pf));
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", "guc2pf", err, ERR_PTR(err));
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "GUC2PF was not rejected\n");
+			ret = -ENOTSOCK;
+			break;
+		}
+
+		err = intel_iov_relay_process_guc2vf(&iov->relay, msg_guc2vf, ARRAY_SIZE(msg_guc2vf));
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", "guc2vf", err, ERR_PTR(err));
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "GUC2VF was not rejected\n");
+			ret = -ENOTSOCK;
+			break;
+		}
+	} while (0);
+
+	iov->relay.selftest.host2guc = NULL;
+
+	return ret;
+}
+
+static int mock_drops_malformed_guc2pf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_no_vfid[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2PF_RELAY_FROM_VF),
+	};
+	u32 msg_no_relayid[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2PF_RELAY_FROM_VF),
+		FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID, VFID(1)),
+	};
+	u32 msg_unexpected_subaction[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_DATA0, /* unexpected */ 1) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2PF_RELAY_FROM_VF),
+		FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID, VFID(1)),
+		FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID, SELFTEST_RELAY_ID),
+	};
+	u32 msg_unexpected_vfid[GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN] = {
+		MSG_GUC2PF_RELAY_FROM_VF(0),
+	};
+	u32 msg_too_long[GUC2PF_RELAY_FROM_VF_EVENT_MSG_MAX_LEN + 1] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+	};
+	struct {
+		const char *name;
+		const u32 *msg;
+		u32 len;
+	} testcases[] = {
+#define TC(X) { #X, X, ARRAY_SIZE(X) }
+		TC(msg_no_vfid),
+		TC(msg_no_relayid),
+		TC(msg_unexpected_subaction),
+		TC(msg_unexpected_vfid),
+		TC(msg_too_long),
+#undef TC
+		{ }
+	}, *tc = testcases;
+	int err, ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.host2guc = host2guc_success;
+
+	while (tc->name) {
+		err = intel_iov_relay_process_guc2pf(&iov->relay, tc->msg, tc->len);
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", tc->name, err, ERR_PTR(err));
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "%s was not rejected\n", tc->name);
+			ret = -ENOTSOCK;
+			break;
+		}
+		tc++;
+	}
+
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+
+	return ret;
+}
+
+static int mock_drops_malformed_guc2vf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_no_relayid[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2VF_RELAY_FROM_PF),
+	};
+	u32 msg_unexpected_subaction[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_DATA0, /* unexpected */ 1) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2VF_RELAY_FROM_PF),
+		FIELD_PREP(GUC2VF_RELAY_FROM_PF_EVENT_MSG_1_RELAY_ID, SELFTEST_RELAY_ID),
+	};
+	u32 msg_too_long[GUC2VF_RELAY_FROM_PF_EVENT_MSG_MAX_LEN + 1] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+	};
+	struct {
+		const char *name;
+		const u32 *msg;
+		u32 len;
+	} testcases[] = {
+#define TC(X) { #X, X, ARRAY_SIZE(X) }
+		TC(msg_no_relayid),
+		TC(msg_unexpected_subaction),
+		TC(msg_too_long),
+#undef TC
+		{ }
+	}, *tc = testcases;
+	int err, ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.host2guc = host2guc_success;
+
+	while (tc->name) {
+		err = intel_iov_relay_process_guc2vf(&iov->relay, tc->msg, tc->len);
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", tc->name, err, ERR_PTR(err));
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "%s was not rejected\n", tc->name);
+			ret = -ENOTSOCK;
+			break;
+		}
+		tc++;
+	}
+
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+
+	return ret;
+}
+
+static int mock_ignores_unexpected_guc2pf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_bad_origin[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY),
+	};
+	u32 msg_success[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS),
+	};
+	u32 msg_failure[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_FAILURE),
+	};
+	u32 msg_retry[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_NO_RESPONSE_RETRY),
+	};
+	u32 msg_busy[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_NO_RESPONSE_BUSY),
+	};
+	u32 msg_reserved2[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, 2),
+	};
+	u32 msg_reserved4[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(1),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, 4),
+	};
+	struct {
+		const char *name;
+		const u32 *msg;
+		u32 len;
+	} testcases[] = {
+#define TC(X) { #X, X, ARRAY_SIZE(X) }
+		TC(msg_bad_origin),
+		TC(msg_success),
+		TC(msg_failure),
+		TC(msg_retry),
+		TC(msg_busy),
+		TC(msg_reserved2),
+		TC(msg_reserved4),
+#undef TC
+		{ }
+	}, *tc = testcases;
+	int err, ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+
+	while (tc->name) {
+		err = intel_iov_relay_process_guc2pf(&iov->relay, tc->msg, tc->len);
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", tc->name, err, ERR_PTR(err));
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "%s was not rejected\n", tc->name);
+			ret = -ENOTSOCK;
+			break;
+		}
+		tc++;
+	}
+
+	iov->relay.selftest.disable_strict = 0;
+
+	return ret;
+}
+
+static int mock_ignores_unexpected_guc2vf(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_bad_origin[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY),
+	};
+	u32 msg_success[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS),
+	};
+	u32 msg_failure[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_FAILURE),
+	};
+	u32 msg_retry[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_NO_RESPONSE_RETRY),
+	};
+	u32 msg_busy[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_NO_RESPONSE_BUSY),
+	};
+	u32 msg_reserved2[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, 2),
+	};
+	u32 msg_reserved4[] = {
+		MSG_GUC2VF_RELAY_FROM_PF,
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, 4),
+	};
+	struct {
+		const char *name;
+		const u32 *msg;
+		u32 len;
+	} testcases[] = {
+#define TC(X) { #X, X, ARRAY_SIZE(X) }
+		TC(msg_bad_origin),
+		TC(msg_success),
+		TC(msg_failure),
+		TC(msg_retry),
+		TC(msg_busy),
+		TC(msg_reserved2),
+		TC(msg_reserved4),
+#undef TC
+		{ }
+	}, *tc = testcases;
+	int err, ret = 0;
+
+	iov->relay.selftest.disable_strict = 1;
+
+	while (tc->name) {
+		err = intel_iov_relay_process_guc2vf(&iov->relay, tc->msg, tc->len);
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", tc->name, err, ERR_PTR(err));
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "%s was not rejected\n", tc->name);
+			ret = -ENOTSOCK;
+			break;
+		}
+		tc++;
+	}
+
+	iov->relay.selftest.disable_strict = 0;
+
+	return ret;
+}
+
+struct payload_params {
+	u32 vfid;
+	u32 relayid;
+	const u32 *data;
+	u32 len;
+};
+
+static int pf2guc_payload_checker(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	struct payload_params *expected = relay->selftest.data;
+
+	host2guc_success(relay, msg, len);
+
+	if (len < PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN)
+		return -EPROTO;
+
+	if (len > PF2GUC_RELAY_TO_VF_REQUEST_MSG_MAX_LEN)
+		return -EMSGSIZE;
+
+	if (FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_HOST)
+		return -EPROTO;
+
+	/* XXX use FAST REQUEST */
+	if (FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_REQUEST)
+		return -EPROTO;
+
+	if (FIELD_GET(GUC_HXG_REQUEST_MSG_0_ACTION, msg[0]) != GUC_ACTION_PF2GUC_RELAY_TO_VF)
+		return -ENOTTY;
+
+	if (FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, msg[1]) != expected->vfid)
+		return -ENOTTY;
+
+	if (FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, msg[2]) != expected->relayid)
+		if (expected->relayid)
+			return -ENOTTY;
+
+	if (expected->len > PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA)
+		return -EINVAL;
+
+	if (len > PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN + expected->len)
+		return -EMSGSIZE;
+
+	if (len != PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN + expected->len)
+		return -ENOMSG;
+
+	if (memcmp(msg + PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN, expected->data,
+		   sizeof(u32) * expected->len))
+		return -EBADMSG;
+
+	return 0;
+}
+
+static int mock_prepares_pf2guc(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 vfid = VFID(1);
+	u32 msg[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, ~SELFTEST_RELAY_DATA),
+	};
+	u32 buf[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA];
+	struct payload_params params;
+	unsigned int n;
+	int err = 0;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		params.relayid = 0; /* don't check */
+		params.vfid = vfid;
+		params.data = msg;
+		params.len = n;
+
+		iov->relay.selftest.disable_strict = 1;
+		iov->relay.selftest.enable_loopback = 1;
+		iov->relay.selftest.data = &params;
+		iov->relay.selftest.host2guc = pf2guc_payload_checker;
+
+		err = intel_iov_relay_send_to_vf(&iov->relay, vfid, msg, n,
+						 buf, ARRAY_SIZE(buf));
+
+		if (err < 0) {
+			IOV_SELFTEST_ERROR(iov, "failed to send msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.host2guc)), 200);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "didn't send msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = PTR_ERR_OR_ZERO(iov->relay.selftest.host2guc);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "invalid msg len=%u, %d\n", n, err);
+			break;
+		}
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+	iov->relay.selftest.data = NULL;
+
+	return err;
+}
+
+static int pf2guc_auto_reply_success(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	u32 reply[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(0),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS),
+	};
+	int err = pf2guc_payload_checker(relay, msg, len);
+
+	if (err)
+		return err;
+
+	reply[1] = FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID,
+			      FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, msg[1]));
+	reply[2] = FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID,
+			      FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, msg[2]));
+
+	intel_iov_relay_process_guc2pf(relay, reply, ARRAY_SIZE(reply));
+	return 0;
+}
+
+static int mock_prepares_pf2guc_and_waits(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 vfid = VFID(1);
+	u32 msg[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+	};
+	u32 buf[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA];
+	struct payload_params params;
+	unsigned int n;
+	int err = 0;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		params.relayid = 0; /* don't check */
+		params.vfid = vfid;
+		params.data = msg;
+		params.len = n;
+
+		iov->relay.selftest.disable_strict = 1;
+		iov->relay.selftest.enable_loopback = 1;
+		iov->relay.selftest.data = &params;
+		iov->relay.selftest.host2guc = pf2guc_auto_reply_success;
+
+		err = intel_iov_relay_send_to_vf(&iov->relay, vfid, msg, n,
+						 buf, ARRAY_SIZE(buf));
+
+		if (err < 0) {
+			IOV_SELFTEST_ERROR(iov, "failed to send msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.host2guc)), 200);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "didn't send msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = PTR_ERR_OR_ZERO(iov->relay.selftest.host2guc);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "invalid msg len=%u, %d\n", n, err);
+			break;
+		}
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+	iov->relay.selftest.data = NULL;
+
+	return err;
+}
+
+static int pf2guc_auto_reply_failure(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	u32 reply[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(0),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_FAILURE) |
+		FIELD_PREP(GUC_HXG_FAILURE_MSG_0_ERROR, IOV_ERROR_NO_DATA_AVAILABLE),
+	};
+	int err = pf2guc_payload_checker(relay, msg, len);
+
+	if (err)
+		return err;
+
+	reply[1] = FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID,
+			      FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, msg[1]));
+	reply[2] = FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID,
+			      FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, msg[2]));
+
+	intel_iov_relay_process_guc2pf(relay, reply, ARRAY_SIZE(reply));
+	return 0;
+}
+
+static int mock_prepares_pf2guc_and_fails(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 vfid = VFID(1);
+	u32 msg[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+	};
+	u32 buf[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA];
+	struct payload_params params;
+	unsigned int n;
+	int err = 0;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		params.relayid = 0; /* don't check */
+		params.vfid = vfid;
+		params.data = msg;
+		params.len = n;
+
+		iov->relay.selftest.disable_strict = 1;
+		iov->relay.selftest.enable_loopback = 1;
+		iov->relay.selftest.data = &params;
+		iov->relay.selftest.host2guc = pf2guc_auto_reply_failure;
+
+		err = intel_iov_relay_send_to_vf(&iov->relay, vfid, msg, n,
+						 buf, ARRAY_SIZE(buf));
+
+		if (err > 0) {
+			IOV_SELFTEST_ERROR(iov, "unexpected success msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.host2guc)), 200);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "didn't send msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = PTR_ERR_OR_ZERO(iov->relay.selftest.host2guc);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "invalid msg len=%u, %d\n", n, err);
+			break;
+		}
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+	iov->relay.selftest.data = NULL;
+
+	return err;
+}
+
+static int pf2guc_auto_reply_retry(struct intel_iov_relay *relay, const u32 *msg, u32 len)
+{
+	u32 reply[] = {
+		MSG_GUC2PF_RELAY_FROM_VF(0),
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_NO_RESPONSE_RETRY) |
+		FIELD_PREP(GUC_HXG_RETRY_MSG_0_REASON, GUC_HXG_RETRY_REASON_UNSPECIFIED),
+	};
+	int err = pf2guc_payload_checker(relay, msg, len);
+
+	if (err)
+		return err;
+
+	reply[1] = FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID,
+			      FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, msg[1]));
+	reply[2] = FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID,
+			      FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, msg[2]));
+
+	intel_iov_relay_process_guc2pf(relay, reply, ARRAY_SIZE(reply));
+	return 0;
+}
+
+static int mock_prepares_pf2guc_and_retries(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 vfid = VFID(1);
+	u32 msg[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+	};
+	u32 buf[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA];
+	struct payload_params params;
+	unsigned int n;
+	int err = 0;
+
+	return 0;
+
+	for (n = GUC_HXG_MSG_MIN_LEN; n <= ARRAY_SIZE(msg); n++) {
+
+		params.relayid = 0; /* don't check */
+		params.vfid = vfid;
+		params.data = msg;
+		params.len = n;
+
+		iov->relay.selftest.disable_strict = 1;
+		iov->relay.selftest.enable_loopback = 1;
+		iov->relay.selftest.data = &params;
+		iov->relay.selftest.host2guc = pf2guc_auto_reply_retry;
+
+		err = intel_iov_relay_send_to_vf(&iov->relay, vfid, msg, n,
+						 buf, ARRAY_SIZE(buf));
+
+		if (err > 0) {
+			IOV_SELFTEST_ERROR(iov, "unexpected success msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = wait_for(IS_ERR_OR_NULL(READ_ONCE(iov->relay.selftest.host2guc)), 200);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "didn't send msg len=%u, %d\n", n, err);
+			break;
+		}
+
+		err = PTR_ERR_OR_ZERO(iov->relay.selftest.host2guc);
+		if (err) {
+			IOV_SELFTEST_ERROR(iov, "invalid msg len=%u, %d\n", n, err);
+			break;
+		}
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+	iov->relay.selftest.host2guc = NULL;
+	iov->relay.selftest.data = NULL;
+
+	return err;
+}
+
+int selftest_mock_iov_relay(void)
+{
+	static const struct i915_subtest mock_tests[] = {
+		SUBTEST(mock_accepts_min_msg),
+		SUBTEST(mock_drops_msg_if_native),
+		SUBTEST(mock_drops_malformed_guc2pf),
+		SUBTEST(mock_drops_malformed_guc2vf),
+		SUBTEST(mock_ignores_unexpected_guc2pf),
+		SUBTEST(mock_ignores_unexpected_guc2vf),
+		SUBTEST(mock_prepares_pf2guc),
+		SUBTEST(mock_prepares_pf2guc_and_waits),
+		SUBTEST(mock_prepares_pf2guc_and_fails),
+		SUBTEST(mock_prepares_pf2guc_and_retries),
+	};
+	struct drm_i915_private *i915;
+	struct intel_iov *iov;
+	int err;
+
+	i915 = mock_gem_device();
+	if (!i915)
+		return -ENOMEM;
+
+	iov = &to_gt(i915)->iov;
+	intel_iov_relay_init_early(&iov->relay);
+
+	err = i915_subtests(mock_tests, iov);
+
+	mock_destroy_device(i915);
+	return err;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_service.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_service.c
new file mode 100644
index 000000000000..dc0f5824c6ef
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_mock_iov_service.c
@@ -0,0 +1,232 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#include "gt/intel_gt.h"
+#include "gt/iov/abi/iov_actions_abi.h"
+#include "gt/iov/abi/iov_version_abi.h"
+#include "selftests/mock_gem_device.h"
+
+#define SELFTEST_RELAY_ID       0x76543210
+#define SELFTEST_VF_ID		1
+
+static int mock_drop_malformed_handshake_msg(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg_invalid_mbz[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_0_MBZ, 1) | /* non zero MBZ */
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, IOV_VERSION_LATEST_MAJOR) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, IOV_VERSION_LATEST_MINOR)
+	};
+	u32 msg_invalid_version[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, 0) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, 1)
+	};
+	u32 msg_too_short[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+	};
+	u32 msg_too_long[VF2PF_HANDSHAKE_REQUEST_MSG_LEN + 1] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, IOV_VERSION_LATEST_MAJOR) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, IOV_VERSION_LATEST_MINOR),
+	};
+	struct {
+		const char *name;
+		const u32 *msg;
+		u32 len;
+	} testcases[] = {
+#define TC(X) { #X, X, ARRAY_SIZE(X) }
+		TC(msg_invalid_mbz),
+		TC(msg_invalid_version),
+		TC(msg_too_short),
+		TC(msg_too_long),
+#undef TC
+		{ }
+	}, *tc = testcases;
+	int err, ret = 0;
+
+	while (tc->name) {
+		err = intel_iov_service_process_msg(iov, SELFTEST_VF_ID, SELFTEST_RELAY_ID, tc->msg,
+						    tc->len);
+		IOV_DEBUG(iov, "processing %s returned %d (%pe)\n", tc->name, err, ERR_PTR(err));
+
+		if (!err) {
+			IOV_SELFTEST_ERROR(iov, "%s was not rejected\n", tc->name);
+			ret = -ENOTSOCK;
+			break;
+		}
+		tc++;
+	}
+
+	return ret;
+}
+
+static int host2guc_success(struct intel_iov_relay *relay, const u32 *msg_recvd, u32 len)
+{
+	u32 *expected_response = relay->selftest.data;
+	const u32 *relay_msg;
+
+	GEM_BUG_ON(len < PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN);
+	GEM_BUG_ON(len > PF2GUC_RELAY_TO_VF_REQUEST_MSG_MAX_LEN);
+	GEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg_recvd[0]) != GUC_HXG_TYPE_REQUEST);
+	GEM_BUG_ON(FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, msg_recvd[1]) !=
+			     SELFTEST_VF_ID);
+	GEM_BUG_ON(FIELD_GET(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, msg_recvd[2]) !=
+			     SELFTEST_RELAY_ID);
+
+	/* msg_recvd is full H2G, extract IOV message */
+	relay_msg = msg_recvd + PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN;
+	if (!memcmp(expected_response, relay_msg, sizeof(*expected_response)))
+		return 0;
+
+	return -ENOTTY;
+}
+
+static int mock_try_handshake(struct intel_iov *iov, u32 major_wanted, u32 minor_wanted,
+			      u32 major, u32 minor)
+{
+	u32 msg[VF2PF_HANDSHAKE_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_VF2PF_HANDSHAKE),
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MAJOR, major_wanted) |
+		FIELD_PREP(VF2PF_HANDSHAKE_REQUEST_MSG_1_MINOR, minor_wanted),
+	};
+	u32 response[] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_RESPONSE_SUCCESS) |
+		FIELD_PREP(GUC_HXG_RESPONSE_MSG_0_DATA0, 0),
+		FIELD_PREP(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MAJOR, major) |
+		FIELD_PREP(VF2PF_HANDSHAKE_RESPONSE_MSG_1_MINOR, minor),
+	};
+	int ret = 0;
+
+	iov->relay.selftest.data = &response;
+	iov->relay.selftest.host2guc = host2guc_success;
+
+	ret = intel_iov_service_process_msg(iov, SELFTEST_VF_ID, SELFTEST_RELAY_ID, msg,
+					    ARRAY_SIZE(msg));
+
+	iov->relay.selftest.host2guc = NULL;
+	iov->relay.selftest.data = NULL;
+
+	return ret;
+}
+
+static int mock_handshake_baseline(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err, ret = 0;
+
+	err = mock_try_handshake(iov, 1, 0, 1, 0);
+
+	if (err) {
+		IOV_SELFTEST_ERROR(iov, "Service message rejected %d (%pe)\n", err, ERR_PTR(err));
+		ret = -ENOTSOCK;
+	}
+
+	return ret;
+}
+
+static int mock_handshake_full_match(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err, ret = 0;
+
+	err = mock_try_handshake(iov, IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR,
+				 IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR);
+
+	if (err) {
+		IOV_SELFTEST_ERROR(iov, "Service message rejected %d (%pe)\n", err, ERR_PTR(err));
+		ret = -ENOTSOCK;
+	}
+
+	return ret;
+}
+
+static int mock_handshake_with_newer(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err, ret = 0;
+
+	err = mock_try_handshake(iov, IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR + 1,
+				 IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR);
+
+	if (err) {
+		IOV_SELFTEST_ERROR(iov, "Service message rejected %d (%pe)\n", err, ERR_PTR(err));
+		ret = -ENOTSOCK;
+	}
+
+	return ret;
+}
+
+static int mock_handshake_latest_pf_support(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err, ret = 0;
+
+	err = mock_try_handshake(iov, 0, 0, IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR);
+
+	if (err) {
+		IOV_SELFTEST_ERROR(iov, "Service message rejected %d (%pe)\n", err, ERR_PTR(err));
+		ret = -ENOTSOCK;
+	}
+
+	return ret;
+}
+
+static int mock_handshake_reject_invalid(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err, ret = 0;
+
+	err = mock_try_handshake(iov, 0, 1, IOV_VERSION_LATEST_MAJOR, IOV_VERSION_LATEST_MINOR);
+
+	if (err != -EINVAL) {
+		IOV_SELFTEST_ERROR(iov, "Service message rejected %d (%pe)\n", err, ERR_PTR(err));
+		ret = -ENOTSOCK;
+	}
+
+	return ret;
+}
+
+int selftest_mock_iov_service(void)
+{
+	static const struct i915_subtest mock_tests[] = {
+		SUBTEST(mock_drop_malformed_handshake_msg),
+		SUBTEST(mock_handshake_baseline),
+		SUBTEST(mock_handshake_full_match),
+		SUBTEST(mock_handshake_with_newer),
+		SUBTEST(mock_handshake_latest_pf_support),
+		SUBTEST(mock_handshake_reject_invalid),
+	};
+	struct drm_i915_private *i915;
+	struct intel_iov *iov;
+	int err;
+
+	i915 = mock_gem_device();
+	if (!i915)
+		return -ENOMEM;
+
+	i915->__mode = I915_IOV_MODE_SRIOV_PF;
+	iov = &to_gt(i915)->iov;
+	intel_iov_relay_init_early(&iov->relay);
+
+	err = i915_subtests(mock_tests, iov);
+
+	i915->__mode = I915_IOV_MODE_NONE;
+	mock_destroy_device(i915);
+
+	return err;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_perf_iov_relay.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_perf_iov_relay.c
new file mode 100644
index 000000000000..2416d3389a3d
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_perf_iov_relay.c
@@ -0,0 +1,248 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#define SELFTEST_RELAY_PERF_LOOP		100
+#define SELFTEST_RELAY_PERF_TIME_MS		100
+
+static int pf_loopback_to_vf_delay(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg[] = {
+		MSG_IOV_SELFTEST_RELAY(SELFTEST_RELAY_OPCODE_NOP),
+	};
+	u32 buf[GUC_HXG_MSG_MIN_LEN];
+	s64 delay_total = 0, delay_min = 0, delay_max = 0;
+	unsigned int n;
+	int ret = 0;
+
+	iov->relay.selftest.enable_loopback = 1;
+
+	for (n = 0; n < SELFTEST_RELAY_PERF_LOOP; n++) {
+		ktime_t start = ktime_get();
+		s64 delta;
+
+		ret = intel_iov_relay_send_to_vf(&iov->relay, PFID, msg, ARRAY_SIZE(msg),
+						 buf, ARRAY_SIZE(buf));
+
+		if (ret != GUC_HXG_MSG_MIN_LEN)
+			break;
+
+		delta = ktime_us_delta(ktime_get(), start);
+		delay_total += delta;
+
+		delay_min = n ? min(delta, delay_min) : delta;
+		delay_max = max(delta, delay_max);
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+
+	if (n < SELFTEST_RELAY_PERF_LOOP)
+		return -ENODATA;
+
+	dev_info(iov_to_dev(iov), "delay %llu us (min %llu max %llu over %u iterations)\n",
+		 div_s64(delay_total, SELFTEST_RELAY_PERF_LOOP),
+		 delay_min, delay_max, SELFTEST_RELAY_PERF_LOOP);
+
+	return 0;
+}
+
+static int pf_loopback_to_vf_throughput(void *arg)
+{
+	struct intel_iov *iov = arg;
+	u32 msg[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		MSG_IOV_SELFTEST_RELAY(SELFTEST_RELAY_OPCODE_NOP),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		/* ... */
+	};
+	u32 buf[GUC_HXG_MSG_MIN_LEN];
+	ktime_t finish;
+	u64 throughput = 0;
+	u64 counter = 0;
+	int ret = 0;
+
+	iov->relay.selftest.enable_loopback = 1;
+	finish = ktime_add_ms(ktime_get(), SELFTEST_RELAY_PERF_TIME_MS);
+
+	while (ktime_before(ktime_get(), finish)) {
+		ret = intel_iov_relay_send_to_vf(&iov->relay, PFID, msg, ARRAY_SIZE(msg),
+						 buf, ARRAY_SIZE(buf));
+
+		if (ret != GUC_HXG_MSG_MIN_LEN)
+			break;
+
+		throughput += ARRAY_SIZE(msg);
+		throughput += ret;
+		counter++;
+	}
+
+	iov->relay.selftest.enable_loopback = 0;
+
+	if (ret != GUC_HXG_MSG_MIN_LEN)
+		return -ENODATA;
+
+	dev_info(iov_to_dev(iov), "throughput %llu bytes/s (%llu relays/s)\n",
+		 div_s64(throughput * sizeof(u32) * MSEC_PER_SEC,
+			 SELFTEST_RELAY_PERF_TIME_MS), counter);
+
+	return 0;
+}
+
+static int relay_to_pf_delay(struct intel_iov *iov)
+{
+	u32 msg[] = {
+		MSG_IOV_SELFTEST_RELAY(SELFTEST_RELAY_OPCODE_NOP),
+	};
+	u32 buf[GUC_HXG_MSG_MIN_LEN];
+	s64 delay_total = 0, delay_min = 0, delay_max = 0;
+	unsigned int n;
+	int ret = 0;
+
+	for (n = 0; n < SELFTEST_RELAY_PERF_LOOP; n++) {
+		ktime_t start = ktime_get();
+		s64 delta;
+
+		ret = intel_iov_relay_send_to_pf(&iov->relay, msg, ARRAY_SIZE(msg),
+						 buf, ARRAY_SIZE(buf));
+
+		if (ret != GUC_HXG_MSG_MIN_LEN)
+			break;
+
+		delta = ktime_us_delta(ktime_get(), start);
+		delay_total += delta;
+
+		delay_min = n ? min(delta, delay_min) : delta;
+		delay_max = max(delta, delay_max);
+	}
+
+	if (n < SELFTEST_RELAY_PERF_LOOP)
+		return -ENODATA;
+
+	dev_info(iov_to_dev(iov), "delay %llu us (min %llu max %llu over %u iterations)\n",
+		 div_s64(delay_total, SELFTEST_RELAY_PERF_LOOP),
+		 delay_min, delay_max, SELFTEST_RELAY_PERF_LOOP);
+
+	return 0;
+}
+
+static int pf_loopback_to_pf_delay(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.enable_loopback = 1;
+
+	err = relay_to_pf_delay(iov);
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+
+	return err;
+}
+
+static int relay_to_pf_throughput(struct intel_iov *iov)
+{
+	u32 msg[PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA] = {
+		MSG_IOV_SELFTEST_RELAY(SELFTEST_RELAY_OPCODE_NOP),
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA),
+		/* ... */
+	};
+	u32 buf[GUC_HXG_MSG_MIN_LEN];
+	ktime_t finish;
+	u64 throughput = 0;
+	u64 counter = 0;
+	int ret = 0;
+
+	finish = ktime_add_ms(ktime_get(), SELFTEST_RELAY_PERF_TIME_MS);
+
+	while (ktime_before(ktime_get(), finish)) {
+		ret = intel_iov_relay_send_to_pf(&iov->relay, msg, ARRAY_SIZE(msg),
+						 buf, ARRAY_SIZE(buf));
+
+		if (ret != GUC_HXG_MSG_MIN_LEN)
+			break;
+
+		throughput += ARRAY_SIZE(msg);
+		throughput += ret;
+		counter++;
+	}
+
+	if (ret != GUC_HXG_MSG_MIN_LEN)
+		return -ENODATA;
+
+	dev_info(iov_to_dev(iov), "throughput %llu bytes/s (%llu relays/s)\n",
+		 div_s64(throughput * sizeof(u32) * MSEC_PER_SEC,
+			 SELFTEST_RELAY_PERF_TIME_MS), counter);
+
+	return 0;
+}
+
+static int pf_loopback_to_pf_throughput(void *arg)
+{
+	struct intel_iov *iov = arg;
+	int err;
+
+	iov->relay.selftest.disable_strict = 1;
+	iov->relay.selftest.enable_loopback = 1;
+
+	err = relay_to_pf_throughput(iov);
+
+	iov->relay.selftest.enable_loopback = 0;
+	iov->relay.selftest.disable_strict = 0;
+
+	return err;
+}
+
+static int vf_to_pf_delay(void *arg)
+{
+	return relay_to_pf_delay(arg);
+}
+
+static int vf_to_pf_throughput(void *arg)
+{
+	return relay_to_pf_throughput(arg);
+}
+
+int selftest_perf_iov_relay(struct drm_i915_private *i915)
+{
+	static const struct i915_subtest pf_tests[] = {
+		SUBTEST(pf_loopback_to_vf_delay),
+		SUBTEST(pf_loopback_to_vf_throughput),
+		SUBTEST(pf_loopback_to_pf_delay),
+		SUBTEST(pf_loopback_to_pf_throughput),
+	};
+	static const struct i915_subtest vf_tests[] = {
+		SUBTEST(vf_to_pf_delay),
+		SUBTEST(vf_to_pf_throughput),
+	};
+	intel_wakeref_t wakeref;
+	int err = 0;
+
+	if (!IS_SRIOV(i915))
+		return 0;
+
+	if (IS_SRIOV_PF(i915) && i915_sriov_pf_status(i915) < 0)
+		return -EHOSTDOWN;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id) {
+			struct intel_iov *iov = &gt->iov;
+
+			if (IS_SRIOV_PF(i915)) {
+				intel_iov_provisioning_force_vgt_mode(iov);
+				err = intel_iov_live_subtests(pf_tests, iov);
+			} else if (IS_SRIOV_VF(i915)) {
+				err = intel_iov_live_subtests(vf_tests, iov);
+			}
+			if (err)
+				break;
+		}
+	}
+
+	return err;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/selftests/selftest_util_iov_relay.c b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_util_iov_relay.c
new file mode 100644
index 000000000000..aad34f540dcd
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/selftests/selftest_util_iov_relay.c
@@ -0,0 +1,104 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright(c) 2022 Intel Corporation. All rights reserved.
+ */
+
+#define SELFTEST_RELAY_ID	0x76543210
+#define SELFTEST_RELAY_DATA	0xDDDAAAA0
+
+#define MSG_PF2GUC_RELAY_TO_VF(N)							\
+	FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |				\
+	FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |				\
+	FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_PF2GUC_RELAY_TO_VF),	\
+	FIELD_PREP(PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID, VFID(N)),			\
+	FIELD_PREP(PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID, SELFTEST_RELAY_ID)	\
+	/* ...     PF2GUC_RELAY_TO_VF_REQUEST_MSG_n_RELAY_DATAx */
+
+#define MSG_GUC2PF_RELAY_FROM_VF(N)							\
+	FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |				\
+	FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |				\
+	FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2PF_RELAY_FROM_VF),	\
+	FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID, VFID(N)),			\
+	FIELD_PREP(GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID, SELFTEST_RELAY_ID)	\
+	/* ...     GUC2PF_RELAY_FROM_VF_EVENT_MSG_n_RELAY_DATAx */
+
+#define MSG_GUC2VF_RELAY_FROM_PF							\
+	FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_GUC) |				\
+	FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |				\
+	FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, GUC_ACTION_GUC2VF_RELAY_FROM_PF),	\
+	FIELD_PREP(GUC2VF_RELAY_FROM_PF_EVENT_MSG_1_RELAY_ID, SELFTEST_RELAY_ID)	\
+	/* ...     GUC2VF_RELAY_FROM_PF_EVENT_MSG_n_RELAY_DATAx */
+
+#define MSG_VF2GUC_RELAY_TO_PF								\
+	FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |				\
+	FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |				\
+	FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_RELAY_TO_PF),	\
+	FIELD_PREP(VF2GUC_RELAY_TO_PF_REQUEST_MSG_1_RELAY_ID, SELFTEST_RELAY_ID)	\
+	/* ...     VF2GUC_RELAY_TO_PF_REQUEST_MSG_n_RELAY_DATAx */
+
+#define MSG_IOV_SELFTEST_RELAY(OPCODE)							\
+	FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |				\
+	FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |				\
+	FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY) |		\
+	FIELD_PREP(GUC_HXG_REQUEST_MSG_0_DATA0, (OPCODE))				\
+	/* ...     GUC_HXG_REQUEST_MSG_n_DATAn, SELFTEST_RELAY_DATA */
+
+#define MSG_IOV_SELFTEST_RELAY_EVENT(OPCODE)						\
+	FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |				\
+	FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_EVENT) |				\
+	FIELD_PREP(GUC_HXG_EVENT_MSG_0_ACTION, IOV_ACTION_SELFTEST_RELAY) |		\
+	FIELD_PREP(GUC_HXG_EVENT_MSG_0_DATA0, (OPCODE))					\
+	/* ...     GUC_HXG_EVENT_MSG_n_DATAn, SELFTEST_RELAY_DATA */
+
+static int relay_selftest_process_msg(struct intel_iov_relay *relay, u32 origin,
+				      u32 relay_id, const u32 *msg, u32 len)
+{
+	/* during selftests we do allow empty relay message */
+	if (unlikely(len < GUC_HXG_MSG_MIN_LEN))
+		return 0;
+
+	/* but it still has to be H2H */
+	if (FIELD_GET(GUC_HXG_MSG_0_ORIGIN, msg[0]) != GUC_HXG_ORIGIN_HOST)
+		return -EPROTO;
+
+	/* only (FAST)REQUEST/EVENT are supported */
+	if (FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_REQUEST &&
+	    FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_FAST_REQUEST &&
+	    FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_EVENT)
+		return -ENOTTY;
+
+	/* only our action */
+	if (FIELD_GET(GUC_HXG_REQUEST_MSG_0_ACTION, msg[0]) != IOV_ACTION_SELFTEST_RELAY)
+		return -ENOTTY;
+
+	RELAY_DEBUG(relay, "received selftest %s.%u from %u = opcode %u\n",
+		    hxg_type_to_string(GUC_HXG_TYPE_REQUEST), relay_id, origin,
+		    FIELD_GET(GUC_HXG_REQUEST_MSG_0_DATA0, msg[0]));
+
+	if (FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0]) != GUC_HXG_TYPE_REQUEST)
+		return 0;
+
+	if (origin) {
+		if (intel_iov_is_pf(relay_to_iov(relay)))
+			return intel_iov_relay_reply_ack_to_vf(relay, origin, relay_id, 0);
+		return -EPROTO;
+	}
+
+	return relay_send_success(relay, origin, relay_id, 0);
+}
+
+static int relay_selftest_guc_send_nb(struct intel_guc *guc, const u32 *msg, u32 len, u32 g2h)
+{
+	struct intel_iov_relay *relay = &guc_to_gt(guc)->iov.relay;
+
+	if (unlikely(!IS_ERR_OR_NULL(relay->selftest.host2guc))) {
+		int ret = relay->selftest.host2guc(relay, msg, len);
+
+		if (ret != -ENOTTY) {
+			relay->selftest.host2guc = ERR_PTR(ret < 0 ? ret : 0);
+			return ret;
+		}
+	}
+
+	return intel_guc_send_nb(guc, msg, len, g2h);
+}
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
new file mode 100644
index 000000000000..c38a1e247525
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
@@ -0,0 +1,449 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2021 Intel Corporation
+ */
+
+#ifndef __GUC_ACTIONS_PF_ABI_H__
+#define __GUC_ACTIONS_PF_ABI_H__
+
+#include "guc_communication_ctb_abi.h"
+
+/**
+ * DOC: PF2GUC_UPDATE_VGT_POLICY
+ *
+ * This message is optionaly used by the PF to set `GuC VGT Policy KLVs`_.
+ *
+ * This message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_UPDATE_VGT_POLICY` = 0x5502     |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **CFG_ADDR_LO** - dword aligned GGTT offset that             |
+ *  |   |       | represents the start of `GuC VGT Policy KLVs`_ list.         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **CFG_ADDR_HI** - upper 32 bits of above offset.             |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **CFG_SIZE** - size (in dwords) of the config buffer         |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | **COUNT** - number of KLVs successfully applied              |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_UPDATE_VGT_POLICY			0x5502
+
+#define PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_LEN		(GUC_HXG_REQUEST_MSG_MIN_LEN + 3u)
+#define PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+#define PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_1_CFG_ADDR_LO	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_2_CFG_ADDR_HI	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_UPDATE_VGT_POLICY_REQUEST_MSG_3_CFG_SIZE		GUC_HXG_REQUEST_MSG_n_DATAn
+
+#define PF2GUC_UPDATE_VGT_POLICY_RESPONSE_MSG_LEN		GUC_HXG_RESPONSE_MSG_MIN_LEN
+#define PF2GUC_UPDATE_VGT_POLICY_RESPONSE_MSG_0_COUNT		GUC_HXG_RESPONSE_MSG_0_DATA0
+
+/**
+ * DOC: PF2GUC_UPDATE_VF_CFG
+ *
+ * The `PF2GUC_UPDATE_VF_CFG`_ message is used by PF to provision single VF in GuC.
+ *
+ * This message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_UPDATE_VF_CFG` = 0x5503         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VFID** - identifier of the VF that the KLV                 |
+ *  |   |       | configurations are being applied to                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **CFG_ADDR_LO** - dword aligned GGTT offset that represents  |
+ *  |   |       | the start of a list of virtualization related KLV configs    |
+ *  |   |       | that are to be applied to the VF.                            |
+ *  |   |       | If this parameter is zero, the list is not parsed.           |
+ *  |   |       | If full configs address parameter is zero and configs_size is|
+ *  |   |       | zero associated VF config shall be reset to its default state|
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **CFG_ADDR_HI** - upper 32 bits of configs address.          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 4 |  31:0 | **CFG_SIZE** - size (in dwords) of the config buffer         |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | **COUNT** - number of KLVs successfully applied              |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_UPDATE_VF_CFG			0x5503
+
+#define PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_LEN		(GUC_HXG_REQUEST_MSG_MIN_LEN + 4u)
+#define PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+#define PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_1_VFID		GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_2_CFG_ADDR_LO	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_3_CFG_ADDR_HI	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_UPDATE_VF_CFG_REQUEST_MSG_4_CFG_SIZE	GUC_HXG_REQUEST_MSG_n_DATAn
+
+#define PF2GUC_UPDATE_VF_CFG_RESPONSE_MSG_LEN		GUC_HXG_RESPONSE_MSG_MIN_LEN
+#define PF2GUC_UPDATE_VF_CFG_RESPONSE_MSG_0_COUNT	GUC_HXG_RESPONSE_MSG_0_DATA0
+
+/**
+ * DOC: GUC2PF_RELAY_FROM_VF
+ *
+ * The `GUC2PF_RELAY_FROM_VF`_ message is used by the GuC to forward VF/PF IOV
+ * messages received from the VF to the PF.
+ *
+ * This H2G message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_EVENT_                                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_GUC2PF_RELAY_FROM_VF` = 0x5100         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VFID** - source VF identifier                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **RELAY_ID** - VF/PF message ID                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **RELAY_DATA1** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|       |                                                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | n |  31:0 | **RELAY_DATAx** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_GUC2PF_RELAY_FROM_VF			0x5100
+
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 2u)
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_MAX_LEN		(GUC2PF_RELAY_FROM_VF_EVENT_MSG_MIN_LEN + 60u)
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_0_MBZ		GUC_HXG_EVENT_MSG_0_DATA0
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_1_VFID		GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_2_RELAY_ID	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_3_RELAY_DATA1	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_n_RELAY_DATAx	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_RELAY_FROM_VF_EVENT_MSG_NUM_RELAY_DATA	60u
+
+/**
+ * DOC: PF2GUC_RELAY_TO_VF
+ *
+ * The `PF2GUC_RELAY_TO_VF`_ message is used by the PF to send VF/PF IOV messages
+ * to the VF.
+ *
+ * This action message must be sent over CTB as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = `GUC_HXG_TYPE_FAST_REQUEST`_                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_RELAY_TO_VF` = 0x5101           |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VFID** - target VF identifier                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **RELAY_ID** - VF/PF message ID                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **RELAY_DATA1** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|       |                                                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | n |  31:0 | **RELAY_DATAx** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_RELAY_TO_VF			0x5101
+
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN		(GUC_HXG_REQUEST_MSG_MIN_LEN + 2u)
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_MAX_LEN		(PF2GUC_RELAY_TO_VF_REQUEST_MSG_MIN_LEN + 60u)
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_1_VFID		GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_2_RELAY_ID	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_3_RELAY_DATA1	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_n_RELAY_DATAx	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_RELAY_TO_VF_REQUEST_MSG_NUM_RELAY_DATA	60u
+
+/**
+ * DOC: GUC2PF_MMIO_RELAY_SERVICE
+ *
+ * The `GUC2PF_MMIO_RELAY_SERVICE`_ message is used by the GuC to forward data
+ * from `VF2GUC_MMIO_RELAY_SERVICE`_ request message that was sent by the VF.
+ *
+ * To reply to `VF2GUC_MMIO_RELAY_SERVICE`_ request message PF must be either
+ * `PF2GUC_MMIO_RELAY_SUCCESS`_ or `PF2GUC_MMIO_RELAY_FAILURE`_.
+ *
+ * This G2H message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_EVENT_                                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_GUC2PF_MMIO_RELAY_SERVICE` = 0x5006    |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VFID** - identifier of the VF which sent this message      |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 | 31:28 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | **MAGIC** - see `VF2GUC_MMIO_RELAY_SERVICE`_ request         |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 23:16 | **OPCODE** - see `VF2GUC_MMIO_RELAY_SERVICE`_ request        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | MBZ                                                          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **DATA1** - see `VF2GUC_MMIO_RELAY_SERVICE`_ request         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 4 |  31:0 | **DATA2** - see `VF2GUC_MMIO_RELAY_SERVICE`_ request         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 5 |  31:0 | **DATA3** - see `VF2GUC_MMIO_RELAY_SERVICE`_ request         |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_GUC2PF_MMIO_RELAY_SERVICE		0x5006
+
+#define GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 5u)
+#define GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_1_VFID	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_2_MAGIC	(0xf << 24)
+#define GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_2_OPCODE	(0xff << 16)
+#define GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_n_DATAx	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_MMIO_RELAY_SERVICE_EVENT_MSG_NUM_DATA	3u
+
+/**
+ * DOC: PF2GUC_MMIO_RELAY_SUCCESS
+ *
+ * The `PF2GUC_MMIO_RELAY_SUCCESS`_ message is used by the PF to send success
+ * response data related to `VF2GUC_MMIO_RELAY_SERVICE`_ request message that
+ * was received in `GUC2PF_MMIO_RELAY_SERVICE`_.
+ *
+ * This message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_FAST_REQUEST_                            |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_MMIO_RELAY_SUCCESS` = 0x5007    |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VFID** - identifier of the VF where to send this reply     |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 | 31:28 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | **MAGIC** - see `VF2GUC_MMIO_RELAY_SERVICE`_ response        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  23:0 | **DATA0** - see `VF2GUC_MMIO_RELAY_SERVICE`_ response        |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **DATA1** - see `VF2GUC_MMIO_RELAY_SERVICE`_ response        |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 4 |  31:0 | **DATA2** - see `VF2GUC_MMIO_RELAY_SERVICE`_ response        |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 5 |  31:0 | **DATA3** - see `VF2GUC_MMIO_RELAY_SERVICE`_ response        |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_MMIO_RELAY_SUCCESS		0x5007
+
+#define PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_LEN	(GUC_HXG_REQUEST_MSG_MIN_LEN + 5u)
+#define PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_1_VFID	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_2_MAGIC	(0xf << 24)
+#define PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_2_DATA0	(0xffffff << 0)
+#define PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_n_DATAx	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_MMIO_RELAY_SUCCESS_REQUEST_MSG_NUM_DATA	3u
+
+/**
+ * DOC: PF2GUC_MMIO_RELAY_FAILURE
+ *
+ * The `PF2GUC_MMIO_RELAY_FAILURE`_ message is used by PF to send error response
+ * data related to `VF2GUC_MMIO_RELAY_SERVICE`_ request message that
+ * was received in `GUC2PF_MMIO_RELAY_SERVICE`_.
+ *
+ * This message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_FAST_REQUEST_                            |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_MMIO_RELAY_FAILURE` = 0x5008    |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VFID** - identifier of the VF where to send reply          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 | 31:28 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | **MAGIC** - see `VF2GUC_MMIO_RELAY_SERVICE`_ request         |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  23:8 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |   7:0 | **FAULT** - see `IOV Error Codes`_                           |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_MMIO_RELAY_FAILURE		0x5008
+
+#define PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_LEN	(GUC_HXG_REQUEST_MSG_MIN_LEN + 2u)
+#define PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_1_VFID	GUC_HXG_REQUEST_MSG_n_DATAn
+#define PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_2_MAGIC	(0xf << 24)
+#define PF2GUC_MMIO_RELAY_FAILURE_REQUEST_MSG_2_FAULT	(0xff << 0)
+
+/**
+ * DOC: GUC2PF_ADVERSE_EVENT
+ *
+ * This message is used by the GuC to notify PF about adverse events.
+ *
+ * This G2H message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_EVENT_                                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_GUC2PF_ADVERSE_EVENT` = 0x5104         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | DATA1 = **VFID** - VF identifier                             |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | DATA2 = **THRESHOLD** - key of the exceeded threshold        |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_GUC2PF_ADVERSE_EVENT			0x5104
+
+#define GUC2PF_ADVERSE_EVENT_EVENT_MSG_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 2u)
+#define GUC2PF_ADVERSE_EVENT_EVENT_MSG_0_MBZ		GUC_HXG_EVENT_MSG_0_DATA0
+#define GUC2PF_ADVERSE_EVENT_EVENT_MSG_1_VFID		GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_ADVERSE_EVENT_EVENT_MSG_2_THRESHOLD	GUC_HXG_EVENT_MSG_n_DATAn
+
+/**
+ * DOC: GUC2PF_VF_STATE_NOTIFY
+ *
+ * The GUC2PF_VF_STATE_NOTIFY message is used by the GuC to notify PF about change
+ * of the VF state.
+ *
+ * This G2H message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_EVENT_                                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_GUC2PF_VF_STATE_NOTIFY` = 0x5106       |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | DATA1 = **VFID** - VF identifier                             |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | DATA2 = **EVENT** - notification event:                      |
+ *  |   |       |                                                              |
+ *  |   |       |   - _`GUC_PF_NOTIFY_VF_ENABLE` = 1 (only if VFID = 0)        |
+ *  |   |       |   - _`GUC_PF_NOTIFY_VF_FLR` = 1                              |
+ *  |   |       |   - _`GUC_PF_NOTIFY_VF_FLR_DONE` = 2                         |
+ *  |   |       |   - _`GUC_PF_NOTIFY_VF_PAUSE_DONE` = 3                       |
+ *  |   |       |   - _`GUC_PF_NOTIFY_VF_FIXUP_DONE` = 4                       |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_GUC2PF_VF_STATE_NOTIFY		0x5106
+
+#define GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 2u)
+#define GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_0_MBZ		GUC_HXG_EVENT_MSG_0_DATA0
+#define GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_1_VFID		GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2PF_VF_STATE_NOTIFY_EVENT_MSG_2_EVENT	GUC_HXG_EVENT_MSG_n_DATAn
+#define   GUC_PF_NOTIFY_VF_ENABLE			1
+#define   GUC_PF_NOTIFY_VF_FLR				1
+#define   GUC_PF_NOTIFY_VF_FLR_DONE			2
+#define   GUC_PF_NOTIFY_VF_PAUSE_DONE			3
+#define   GUC_PF_NOTIFY_VF_FIXUP_DONE			4
+
+/**
+ * DOC: PF2GUC_VF_CONTROL
+ *
+ * The PF2GUC_VF_CONTROL message is used by the PF to trigger VF state change
+ * maintained by the GuC.
+ *
+ * This H2G message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_VF_CONTROL_CMD` = 0x5506        |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | DATA1 = **VFID** - VF identifier                             |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | DATA2 = **COMMAND** - control command:                       |
+ *  |   |       |                                                              |
+ *  |   |       |   - _`GUC_PF_TRIGGER_VF_PAUSE` = 1                           |
+ *  |   |       |   - _`GUC_PF_TRIGGER_VF_RESUME` = 2                          |
+ *  |   |       |   - _`GUC_PF_TRIGGER_VF_STOP` = 3                            |
+ *  |   |       |   - _`GUC_PF_TRIGGER_VF_FLR_START` = 4                       |
+ *  |   |       |   - _`GUC_PF_TRIGGER_VF_FLR_FINISH` = 5                      |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = MBZ                                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_VF_CONTROL			0x5506
+
+#define PF2GUC_VF_CONTROL_REQUEST_MSG_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 2u)
+#define PF2GUC_VF_CONTROL_REQUEST_MSG_0_MBZ		GUC_HXG_EVENT_MSG_0_DATA0
+#define PF2GUC_VF_CONTROL_REQUEST_MSG_1_VFID		GUC_HXG_EVENT_MSG_n_DATAn
+#define PF2GUC_VF_CONTROL_REQUEST_MSG_2_COMMAND		GUC_HXG_EVENT_MSG_n_DATAn
+#define   GUC_PF_TRIGGER_VF_PAUSE			1
+#define   GUC_PF_TRIGGER_VF_RESUME			2
+#define   GUC_PF_TRIGGER_VF_STOP			3
+#define   GUC_PF_TRIGGER_VF_FLR_START			4
+#define   GUC_PF_TRIGGER_VF_FLR_FINISH			5
+
+#endif /* __GUC_ACTIONS_PF_ABI_H__ */
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
new file mode 100644
index 000000000000..34492462a7bb
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
@@ -0,0 +1,315 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2021 Intel Corporation
+ */
+
+#ifndef _ABI_GUC_ACTIONS_VF_ABI_H
+#define _ABI_GUC_ACTIONS_VF_ABI_H
+
+#include "guc_communication_mmio_abi.h"
+#include "guc_communication_ctb_abi.h"
+
+/**
+ * DOC: VF2GUC_MATCH_VERSION
+ *
+ * This action is used to match VF interface version used by VF and GuC.
+ *
+ * This action must be sent over MMIO.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_VF2GUC_MATCH_VERSION` = 0x5500         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:24 | **BRANCH** - branch ID of the VF interface                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 23:16 | **MAJOR** - major version of the VF interface                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:8 | **MINOR** - minor version of the VF interface                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |   7:0 | **MBZ**                                                      |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = MBZ                                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:24 | **BRANCH** - branch ID of the VF interface                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 23:16 | **MAJOR** - major version of the VF interface                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:8 | **MINOR** - minor version of the VF interface                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |   7:0 | **PATCH** - patch version of the VF interface                |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_VF2GUC_MATCH_VERSION			0x5500
+
+#define VF2GUC_MATCH_VERSION_REQUEST_MSG_LEN		(GUC_HXG_REQUEST_MSG_MIN_LEN + 1u)
+#define VF2GUC_MATCH_VERSION_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+#define VF2GUC_MATCH_VERSION_REQUEST_MSG_1_BRANCH	(0xff << 24)
+#define   GUC_VERSION_BRANCH_ANY			0
+#define VF2GUC_MATCH_VERSION_REQUEST_MSG_1_MAJOR	(0xff << 16)
+#define   GUC_VERSION_MAJOR_ANY				0
+#define VF2GUC_MATCH_VERSION_REQUEST_MSG_1_MINOR	(0xff << 8)
+#define   GUC_VERSION_MINOR_ANY				0
+#define VF2GUC_MATCH_VERSION_REQUEST_MSG_1_MBZ		(0xff << 0)
+
+#define VF2GUC_MATCH_VERSION_RESPONSE_MSG_LEN		(GUC_HXG_RESPONSE_MSG_MIN_LEN + 1u)
+#define VF2GUC_MATCH_VERSION_RESPONSE_MSG_0_MBZ		GUC_HXG_RESPONSE_MSG_0_DATA0
+#define VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_BRANCH	(0xff << 24)
+#define VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_MAJOR	(0xff << 16)
+#define VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_MINOR	(0xff << 8)
+#define VF2GUC_MATCH_VERSION_RESPONSE_MSG_1_PATCH	(0xff << 0)
+
+/**
+ * DOC: VF2GUC_VF_RESET
+ *
+ * This action is used by VF to reset GuC's VF state.
+ *
+ * This message must be sent as `MMIO HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_VF2GUC_VF_RESET` = 0x5507              |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = MBZ                                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_VF2GUC_VF_RESET			0x5507
+
+#define VF2GUC_VF_RESET_REQUEST_MSG_LEN			GUC_HXG_REQUEST_MSG_MIN_LEN
+#define VF2GUC_VF_RESET_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+
+#define VF2GUC_VF_RESET_RESPONSE_MSG_LEN		GUC_HXG_RESPONSE_MSG_MIN_LEN
+#define VF2GUC_VF_RESET_RESPONSE_MSG_0_MBZ		GUC_HXG_RESPONSE_MSG_0_DATA0
+
+/**
+ * DOC: VF2GUC_QUERY_SINGLE_KLV
+ *
+ * This action is used by VF to query value of the single KLV data.
+ *
+ * This action must be sent over MMIO.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_VF2GUC_QUERY_SINGLE_KLV` = 0x5509      |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 | 31:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | **KEY** - key for which value is requested                   |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | **LENGTH** - length of data in dwords                        |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **VALUE32** - bits 31:0 of value if **LENGTH** >= 1          |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **VALUE64** - bits 63:32 of value if **LENGTH** >= 2         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **VALUE96** - bits 95:64 of value if **LENGTH** >= 3         |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_VF2GUC_QUERY_SINGLE_KLV		0x5509
+
+#define VF2GUC_QUERY_SINGLE_KLV_REQUEST_MSG_LEN		(GUC_HXG_REQUEST_MSG_MIN_LEN + 1u)
+#define VF2GUC_QUERY_SINGLE_KLV_REQUEST_MSG_0_MBZ	GUC_HXG_REQUEST_MSG_0_DATA0
+#define VF2GUC_QUERY_SINGLE_KLV_REQUEST_MSG_1_MBZ	(0xffff << 16)
+#define VF2GUC_QUERY_SINGLE_KLV_REQUEST_MSG_1_KEY	(0xffff << 0)
+
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_MIN_LEN	GUC_HXG_RESPONSE_MSG_MIN_LEN
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_MAX_LEN	(GUC_HXG_RESPONSE_MSG_MIN_LEN + 3u)
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_0_MBZ	(0xfff << 16)
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_0_LENGTH	(0xffff << 0)
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_1_VALUE32	GUC_HXG_REQUEST_MSG_n_DATAn
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_2_VALUE64	GUC_HXG_REQUEST_MSG_n_DATAn
+#define VF2GUC_QUERY_SINGLE_KLV_RESPONSE_MSG_3_VALUE96	GUC_HXG_REQUEST_MSG_n_DATAn
+
+/**
+ * DOC: VF2GUC_RELAY_TO_PF
+ *
+ * The `VF2GUC_RELAY_TO_PF`_ message is used to send VF/PF messages to the PF.
+ *
+ * This message must be sent over CTB.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_ or GUC_HXG_TYPE_FAST_REQUEST_   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_VF2GUC_RELAY_TO_PF` = 0x5103           |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **RELAY_ID** - VF/PF message ID                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **RELAY_DATA1** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|       |                                                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | n |  31:0 | **RELAY_DATAx** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_VF2GUC_RELAY_TO_PF			0x5103
+
+#define VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN		(GUC_HXG_REQUEST_MSG_MIN_LEN + 1u)
+#define VF2GUC_RELAY_TO_PF_REQUEST_MSG_MAX_LEN		(VF2GUC_RELAY_TO_PF_REQUEST_MSG_MIN_LEN + 60u)
+#define VF2GUC_RELAY_TO_PF_REQUEST_MSG_0_MBZ		GUC_HXG_REQUEST_MSG_0_DATA0
+#define VF2GUC_RELAY_TO_PF_REQUEST_MSG_1_RELAY_ID	GUC_HXG_REQUEST_MSG_n_DATAn
+#define VF2GUC_RELAY_TO_PF_REQUEST_MSG_n_RELAY_DATAx	GUC_HXG_REQUEST_MSG_n_DATAn
+#define VF2GUC_RELAY_TO_PF_REQUEST_MSG_NUM_RELAY_DATA	60u
+
+/**
+ * DOC: GUC2VF_RELAY_FROM_PF
+ *
+ * The `GUC2VF_RELAY_FROM_PF`_ message is used by GuC to forward VF/PF messages
+ * received from the PF.
+ *
+ * This message must be sent over CTB.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_EVENT_                                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | MBZ                                                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_GUC2VF_RELAY_FROM_PF` = 0x5102         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **RELAY_ID** - VF/PF message ID                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **RELAY_DATA1** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|       |                                                              |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | n |  31:0 | **RELAY_DATAx** - VF/PF message payload data                 |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_GUC2VF_RELAY_FROM_PF			0x5102
+
+#define GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 1u)
+#define GUC2VF_RELAY_FROM_PF_EVENT_MSG_MAX_LEN		(GUC2VF_RELAY_FROM_PF_EVENT_MSG_MIN_LEN + 60u)
+#define GUC2VF_RELAY_FROM_PF_EVENT_MSG_0_MBZ		GUC_HXG_EVENT_MSG_0_DATA0
+#define GUC2VF_RELAY_FROM_PF_EVENT_MSG_1_RELAY_ID	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2VF_RELAY_FROM_PF_EVENT_MSG_n_RELAY_DATAx	GUC_HXG_EVENT_MSG_n_DATAn
+#define GUC2VF_RELAY_FROM_PF_EVENT_MSG_NUM_RELAY_DATA	60u
+
+/**
+ * DOC: VF2GUC_MMIO_RELAY_SERVICE
+ *
+ * The VF2GUC_MMIO_RELAY_SERVICE action allows to send early MMIO VF/PF messages
+ * from the VF to the PF.
+ *
+ * Note that support for the sending such messages to the PF is not guaranteed
+ * and might be disabled or blocked in the future releases.
+ *
+ * The value of **MAGIC** used in the GUC_HXG_TYPE_REQUEST_ shall be generated
+ * by the VF and value of **MAGIC** included in GUC_HXG_TYPE_RESPONSE_SUCCESS_
+ * shall be the same.
+ *
+ * In case of GUC_HXG_TYPE_RESPONSE_FAILURE_, **MAGIC** shall be encoded in upper
+ * bits of **HINT** field.
+ *
+ * This action may take longer time to completion and VFs should expect intermediate
+ * `HXG Busy`_ response message.
+ *
+ * This action is only available over MMIO.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | **MAGIC** - MMIO VF/PF message magic number (like CRC)       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 23:16 | **OPCODE** - MMIO VF/PF message opcode                       |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_VF2GUC_MMIO_RELAY_SERVICE` = 0x5005    |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **DATA1** - optional MMIO VF/PF payload data (or zero)       |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **DATA2** - optional MMIO VF/PF payload data (or zero)       |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **DATA3** - optional MMIO VF/PF payload data (or zero)       |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:24 | **MAGIC** - must match value from the REQUEST                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  23:0 | **DATA0** - MMIO VF/PF response data                         |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | **DATA1** - MMIO VF/PF response data (or zero)               |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | **DATA2** - MMIO VF/PF response data (or zero)               |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | **DATA3** - MMIO VF/PF response data (or zero)               |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_VF2GUC_MMIO_RELAY_SERVICE		0x5005
+
+#define VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_MIN_LEN	(GUC_HXG_REQUEST_MSG_MIN_LEN)
+#define VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_MAX_LEN	(GUC_HXG_REQUEST_MSG_MIN_LEN + 3u)
+#define VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_0_MAGIC	(0xf << 24)
+#define VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_0_OPCODE	(0xff << 16)
+#define VF2GUC_MMIO_RELAY_SERVICE_REQUEST_MSG_n_DATAn	GUC_HXG_REQUEST_MSG_n_DATAn
+
+#define VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_MIN_LEN	(GUC_HXG_RESPONSE_MSG_MIN_LEN)
+#define VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_MAX_LEN	(GUC_HXG_RESPONSE_MSG_MIN_LEN + 3u)
+#define VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_0_MAGIC	(0xf << 24)
+#define VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_0_DATA0	(0xffffff << 0)
+#define VF2GUC_MMIO_RELAY_SERVICE_RESPONSE_MSG_n_DATAn	GUC_HXG_RESPONSE_MSG_n_DATAn
+
+#endif /* _ABI_GUC_ACTIONS_VF_ABI_H */
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
index 58012edd4eb0..1167c0c62ae4 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
@@ -16,6 +16,8 @@
  *  +===+=======+==============================================================+
  *  | 0 | 31:16 | **KEY** - KLV key identifier                                 |
  *  |   |       |   - `GuC Self Config KLVs`_                                  |
+ *  |   |       |   - `GuC VGT Policy KLVs`_                                   |
+ *  |   |       |   - `GuC VF Configuration KLVs`_                             |
  *  |   |       |                                                              |
  *  |   +-------+--------------------------------------------------------------+
  *  |   |  15:0 | **LEN** - length of VALUE (in 32bit dwords)                  |
@@ -38,6 +40,14 @@
  *
  * `GuC KLV`_ keys available for use with HOST2GUC_SELF_CFG_.
  *
+ * _`GUC_KLV_SELF_CFG_MEMIRQ_STATUS_ADDR` : 0x0900
+ *      Refers to 64 bit Global Gfx address (in bytes) of memory based interrupts
+ *      status vector for use by the GuC.
+ *
+ * _`GUC_KLV_SELF_CFG_MEMIRQ_SOURCE_ADDR` : 0x0901
+ *      Refers to 64 bit Global Gfx address (in bytes) of memory based interrupts
+ *      source vector for use by the GuC.
+ *
  * _`GUC_KLV_SELF_CFG_H2G_CTB_ADDR` : 0x0902
  *      Refers to 64 bit Global Gfx address of H2G `CT Buffer`_.
  *      Should be above WOPCM address but below APIC base address for native mode.
@@ -63,6 +73,12 @@
  *      Should be a multiple of 4K.
  */
 
+#define GUC_KLV_SELF_CFG_MEMIRQ_STATUS_ADDR_KEY		0x0900
+#define GUC_KLV_SELF_CFG_MEMIRQ_STATUS_ADDR_LEN		2u
+
+#define GUC_KLV_SELF_CFG_MEMIRQ_SOURCE_ADDR_KEY		0x0901
+#define GUC_KLV_SELF_CFG_MEMIRQ_SOURCE_ADDR_LEN		2u
+
 #define GUC_KLV_SELF_CFG_H2G_CTB_ADDR_KEY		0x0902
 #define GUC_KLV_SELF_CFG_H2G_CTB_ADDR_LEN		2u
 
@@ -101,4 +117,195 @@ enum {
 	GUC_CONTEXT_POLICIES_KLV_NUM_IDS = 5,
 };
 
+/**
+ * DOC: GuC VGT Policy KLVs
+ *
+ * `GuC KLV`_ keys available for use with PF2GUC_UPDATE_VGT_POLICY.
+ *
+ * _`GUC_KLV_VGT_POLICY_SCHED_IF_IDLE` : 0x8001
+ *      This config sets whether strict scheduling is enabled whereby any VF
+ *      that doesnt have work to submit is still allocated a fixed execution
+ *      time-slice to ensure active VFs execution is always consitent even
+ *      during other VF reprovisiong / rebooting events. Changing this KLV
+ *      impacts all VFs and takes effect on the next VF-Switch event.
+ *
+ *      :0: don't schedule idle (default)
+ *      :1: schedule if idle
+ *
+ * _`GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD` : 0x8002
+ *      This config sets the sample period for tracking adverse event counters.
+ *       A sample period is the period in millisecs during which events are counted.
+ *       This is applicable for all the VFs.
+ *
+ *      :0: adverse events are not counted (default)
+ *      :n: sample period in milliseconds
+ *
+ * _`GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH` : 0x8D00
+ *      This enum is to reset utilized HW engine after VF Switch (i.e to clean
+ *      up Stale HW register left behind by previous VF)
+ *
+ *      :0: don't reset (default)
+ *      :1: reset
+ */
+
+#define GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_KEY		0x8001
+#define GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_LEN		1u
+
+#define GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD_KEY	0x8002
+#define GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD_LEN	1u
+
+#define GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH_KEY	0x8D00
+#define GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH_LEN	1u
+
+/**
+ * DOC: GuC VF Configuration KLVs
+ *
+ * `GuC KLV`_ keys available for use with PF2GUC_UPDATE_VF_CFG.
+ *
+ * _`GUC_KLV_VF_CFG_GGTT_START` : 0x0001
+ *      A 4K aligned start GTT address/offset assigned to VF.
+ *      Value is 64 bits.
+ *
+ * _`GUC_KLV_VF_CFG_GGTT_SIZE` : 0x0002
+ *      A 4K aligned size of GGTT assigned to VF.
+ *      Value is 64 bits.
+ *
+ * _`GUC_KLV_VF_CFG_NUM_CONTEXTS` : 0x0004
+ *      Refers to the number of contexts allocated to this VF.
+ *
+ *      :0: no contexts (default)
+ *      :1-65535: number of contexts (Gen12)
+ *
+ * _`GUC_KLV_VF_CFG_NUM_DOORBELLS` : 0x0006
+ *      Refers to the number of doorbells allocated to this VF.
+ *
+ *      :0: no doorbells (default)
+ *      :1-255: number of doorbells (Gen12)
+ *
+ * _`GUC_KLV_VF_CFG_EXEC_QUANTUM` : 0x8A01
+ *      This config sets the VFs-execution-quantum in milliseconds.
+ *      GUC will attempt to obey the maximum values as much as HW is capable
+ *      of and this will never be perfectly-exact (accumulated nano-second
+ *      granularity) since the GPUs clock time runs off a different crystal
+ *      from the CPUs clock. Changing this KLV on a VF that is currently
+ *      running a context wont take effect until a new context is scheduled in.
+ *      That said, when the PF is changing this value from 0xFFFFFFFF to
+ *      something else, it might never take effect if the VF is running an
+ *      inifinitely long compute or shader kernel. In such a scenario, the
+ *      PF would need to trigger a VM PAUSE and then change the KLV to force
+ *      it to take effect. Such cases might typically happen on a 1PF+1VF
+ *      Virtualization config enabled for heavier workloads like AI/ML.
+ *
+ *      :0: infinite exec quantum (default)
+ *
+ * _`GUC_KLV_VF_CFG_PREEMPT_TIMEOUT` : 0x8A02
+ *      This config sets the VF-preemption-timeout in microseconds.
+ *      GUC will attempt to obey the minimum and maximum values as much as
+ *      HW is capable and this will never be perfectly-exact (accumulated
+ *      nano-second granularity) since the GPUs clock time runs off a
+ *      different crystal from the CPUs clock. Changing this KLV on a VF
+ *      that is currently running a context wont take effect until a new
+ *      context is scheduled in.
+ *      That said, when the PF is changing this value from 0xFFFFFFFF to
+ *      something else, it might never take effect if the VF is running an
+ *      inifinitely long compute or shader kernel.
+ *      In this case, the PF would need to trigger a VM PAUSE and then change
+ *      the KLV to force it to take effect. Such cases might typically happen
+ *      on a 1PF+1VF Virtualization config enabled for heavier workloads like
+ *      AI/ML.
+ *
+ *      :0: no preemption timeout (default)
+ *
+ * _`GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR` : 0x8A03
+ *      This config sets threshold for CAT errors caused by the VF.
+ *
+ *      :0: adverse events or error will not be reported (default)
+ *      :n: event occurrence count per sampling interval
+ *
+ * _`GUC_KLV_VF_CFG_THRESHOLD_ENGINE_RESET` : 0x8A04
+ *      This config sets threshold for engine reset caused by the VF.
+ *
+ *      :0: adverse events or error will not be reported (default)
+ *      :n: event occurrence count per sampling interval
+ *
+ * _`GUC_KLV_VF_CFG_THRESHOLD_PAGE_FAULT` : 0x8A05
+ *      This config sets threshold for page fault errors caused by the VF.
+ *
+ *      :0: adverse events or error will not be reported (default)
+ *      :n: event occurrence count per sampling interval
+ *
+ * _`GUC_KLV_VF_CFG_THRESHOLD_H2G_STORM` : 0x8A06
+ *      This config sets threshold for H2G interrupts triggered by the VF.
+ *
+ *      :0: adverse events or error will not be reported (default)
+ *      :n: time (us) per sampling interval
+ *
+ * _`GUC_KLV_VF_CFG_THRESHOLD_IRQ_STORM` : 0x8A07
+ *      This config sets threshold for GT interrupts triggered by the VF's
+ *      workloads.
+ *
+ *      :0: adverse events or error will not be reported (default)
+ *      :n: time (us) per sampling interval
+ *
+ * _`GUC_KLV_VF_CFG_THRESHOLD_DOORBELL_STORM` : 0x8A08
+ *      This config sets threshold for doorbell's ring triggered by the VF.
+ *
+ *      :0: adverse events or error will not be reported (default)
+ *      :n: time (us) per sampling interval
+ *
+ * _`GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID` : 0x8A0A
+ *      Refers to the start index of doorbell assigned to this VF.
+ *
+ *      :0: (default)
+ *      :1-255: number of doorbells (Gen12)
+ *
+ * _`GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID` : 0x8A0B
+ *      Refers to the start index in context array allocated to this VFs use.
+ *
+ *      :0: (default)
+ *      :1-65535: number of contexts (Gen12)
+ */
+
+#define GUC_KLV_VF_CFG_GGTT_START_KEY		0x0001
+#define GUC_KLV_VF_CFG_GGTT_START_LEN		2u
+
+#define GUC_KLV_VF_CFG_GGTT_SIZE_KEY		0x0002
+#define GUC_KLV_VF_CFG_GGTT_SIZE_LEN		2u
+
+#define GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY		0x0004
+#define GUC_KLV_VF_CFG_NUM_CONTEXTS_LEN		1u
+
+#define GUC_KLV_VF_CFG_NUM_DOORBELLS_KEY	0x0006
+#define GUC_KLV_VF_CFG_NUM_DOORBELLS_LEN	1u
+
+#define GUC_KLV_VF_CFG_EXEC_QUANTUM_KEY		0x8a01
+#define GUC_KLV_VF_CFG_EXEC_QUANTUM_LEN		1u
+
+#define GUC_KLV_VF_CFG_PREEMPT_TIMEOUT_KEY	0x8a02
+#define GUC_KLV_VF_CFG_PREEMPT_TIMEOUT_LEN	1u
+
+#define GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR_KEY		0x8a03
+#define GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR_LEN		1u
+
+#define GUC_KLV_VF_CFG_THRESHOLD_ENGINE_RESET_KEY	0x8a04
+#define GUC_KLV_VF_CFG_THRESHOLD_ENGINE_RESET_LEN	1u
+
+#define GUC_KLV_VF_CFG_THRESHOLD_PAGE_FAULT_KEY		0x8a05
+#define GUC_KLV_VF_CFG_THRESHOLD_PAGE_FAULT_LEN		1u
+
+#define GUC_KLV_VF_CFG_THRESHOLD_H2G_STORM_KEY		0x8a06
+#define GUC_KLV_VF_CFG_THRESHOLD_H2G_STORM_LEN		1u
+
+#define GUC_KLV_VF_CFG_THRESHOLD_IRQ_STORM_KEY		0x8a07
+#define GUC_KLV_VF_CFG_THRESHOLD_IRQ_STORM_LEN		1u
+
+#define GUC_KLV_VF_CFG_THRESHOLD_DOORBELL_STORM_KEY	0x8a08
+#define GUC_KLV_VF_CFG_THRESHOLD_DOORBELL_STORM_LEN	1u
+
+#define GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID_KEY	0x8a0a
+#define GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID_LEN	1u
+
+#define GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID_KEY	0x8a0b
+#define GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID_LEN	1u
+
 #endif /* _ABI_GUC_KLVS_ABI_H */
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h
index 7d5ba4d97d70..98eb4f46572b 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h
@@ -24,6 +24,7 @@
  *  |   | 30:28 | **TYPE** - message type                                      |
  *  |   |       |   - _`GUC_HXG_TYPE_REQUEST` = 0                              |
  *  |   |       |   - _`GUC_HXG_TYPE_EVENT` = 1                                |
+ *  |   |       |   - _`GUC_HXG_TYPE_FAST_REQUEST` = 2                         |
  *  |   |       |   - _`GUC_HXG_TYPE_NO_RESPONSE_BUSY` = 3                     |
  *  |   |       |   - _`GUC_HXG_TYPE_NO_RESPONSE_RETRY` = 5                    |
  *  |   |       |   - _`GUC_HXG_TYPE_RESPONSE_FAILURE` = 6                     |
@@ -46,6 +47,7 @@
 #define GUC_HXG_MSG_0_TYPE			(0x7 << 28)
 #define   GUC_HXG_TYPE_REQUEST			0u
 #define   GUC_HXG_TYPE_EVENT			1u
+#define   GUC_HXG_TYPE_FAST_REQUEST		2u
 #define   GUC_HXG_TYPE_NO_RESPONSE_BUSY		3u
 #define   GUC_HXG_TYPE_NO_RESPONSE_RETRY	5u
 #define   GUC_HXG_TYPE_RESPONSE_FAILURE		6u
@@ -89,6 +91,34 @@
 #define GUC_HXG_REQUEST_MSG_0_ACTION		(0xffff << 0)
 #define GUC_HXG_REQUEST_MSG_n_DATAn		GUC_HXG_MSG_n_PAYLOAD
 
+/**
+ * DOC: HXG Fast Request
+ *
+ * The `HXG Request`_ message should be used to initiate asynchronous activity
+ * for which confirmation or return data is not expected.
+ *
+ * If confirmation is required then `HXG Request`_ shall be used instead.
+ *
+ * The recipient of this message may only use `HXG Failure`_ message if it was
+ * unable to accept this request (like invalid data).
+ *
+ * Format of `HXG Fast Request`_ message is same as `HXG Request`_ except @TYPE.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN - see `HXG Message`_                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = `GUC_HXG_TYPE_FAST_REQUEST`_                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 - see `HXG Request`_                                   |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION - see `HXG Request`_                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ *  |...|       | DATAn - see `HXG Request`_                                   |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+
 /**
  * DOC: HXG Event
  *
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_version_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_version_abi.h
new file mode 100644
index 000000000000..22315fecf9d7
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_version_abi.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef _ABI_GUC_VERSION_ABI_H
+#define _ABI_GUC_VERSION_ABI_H
+
+#define GUC_VF_VERSION_LATEST_MAJOR	1
+#define GUC_VF_VERSION_LATEST_MINOR	1
+
+#endif /* _ABI_GUC_VERSION_ABI_H */
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_gsc_uc.h b/drivers/gpu/drm/i915/gt/uc/intel_gsc_uc.h
index d3667ff40507..877177ca8ed6 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_gsc_uc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_gsc_uc.h
@@ -43,20 +43,21 @@ void intel_gsc_uc_fini(struct intel_gsc_uc *gsc);
 void intel_gsc_uc_flush_work(struct intel_gsc_uc *gsc);
 void intel_gsc_uc_load_start(struct intel_gsc_uc *gsc);
 
-static inline bool intel_gsc_uc_is_supported(struct intel_gsc_uc *gsc)
+static inline bool intel_gsc_uc_is_supported(const struct intel_gsc_uc *gsc)
 {
 	return intel_uc_fw_is_supported(&gsc->fw);
 }
 
-static inline bool intel_gsc_uc_is_wanted(struct intel_gsc_uc *gsc)
+static inline bool intel_gsc_uc_is_wanted(const struct intel_gsc_uc *gsc)
 {
 	return intel_uc_fw_is_enabled(&gsc->fw);
 }
 
-static inline bool intel_gsc_uc_is_used(struct intel_gsc_uc *gsc)
+static inline bool intel_gsc_uc_is_used(const struct intel_gsc_uc *gsc)
 {
 	GEM_BUG_ON(__intel_uc_fw_status(&gsc->fw) == INTEL_UC_FIRMWARE_SELECTED);
-	return intel_uc_fw_is_available(&gsc->fw);
+	return intel_uc_fw_is_available(&gsc->fw) ||
+	       intel_uc_fw_is_preloaded(&gsc->fw);
 }
 
 #endif
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.c b/drivers/gpu/drm/i915/gt/uc/intel_guc.c
index f9bddaa876d9..1670344cf946 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.c
@@ -17,6 +17,15 @@
 #include "i915_drv.h"
 #include "i915_irq.h"
 
+#ifdef CONFIG_DRM_I915_DEBUG_GUC
+#define GUC_DEBUG(_guc, _fmt, ...) guc_dbg(_guc, _fmt, ##__VA_ARGS__)
+#else
+#define GUC_DEBUG(_guc, _fmt, ...) typecheck(struct intel_guc *, _guc)
+#endif
+
+static const struct intel_guc_ops guc_ops_default;
+static const struct intel_guc_ops guc_ops_vf;
+
 /**
  * DOC: GuC
  *
@@ -75,6 +84,10 @@ void intel_guc_init_send_regs(struct intel_guc *guc)
 					FW_REG_READ | FW_REG_WRITE);
 	}
 	guc->send_regs.fw_domains = fw_domains;
+
+	/* XXX: move to init_early when safe to call IS_SRIOV_VF */
+	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		guc->ops = &guc_ops_vf;
 }
 
 static void gen9_reset_guc_interrupts(struct intel_guc *guc)
@@ -198,6 +211,8 @@ void intel_guc_init_early(struct intel_guc *guc)
 
 	intel_guc_enable_msg(guc, INTEL_GUC_RECV_MSG_EXCEPTION |
 				  INTEL_GUC_RECV_MSG_CRASH_DUMP_POSTED);
+
+	guc->ops = &guc_ops_default;
 }
 
 void intel_guc_init_late(struct intel_guc *guc)
@@ -388,7 +403,7 @@ void intel_guc_dump_time_info(struct intel_guc *guc, struct drm_printer *p)
 		   gt->clock_frequency, gt->clock_period_ns);
 }
 
-int intel_guc_init(struct intel_guc *guc)
+static int __guc_init(struct intel_guc *guc)
 {
 	int ret;
 
@@ -455,7 +470,7 @@ int intel_guc_init(struct intel_guc *guc)
 	return ret;
 }
 
-void intel_guc_fini(struct intel_guc *guc)
+static void __guc_fini(struct intel_guc *guc)
 {
 	if (!intel_uc_fw_is_loadable(&guc->fw))
 		return;
@@ -474,6 +489,50 @@ void intel_guc_fini(struct intel_guc *guc)
 	intel_uc_fw_fini(&guc->fw);
 }
 
+static int __vf_guc_init(struct intel_guc *guc)
+{
+	struct intel_gt *gt = guc_to_gt(guc);
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_VF(gt->i915));
+
+	err = intel_guc_ct_init(&guc->ct);
+	if (err)
+		return err;
+
+	/* GuC submission is mandatory for VFs */
+	err = intel_guc_submission_init(guc);
+	if (err)
+		goto err_ct;
+
+	/*
+	 * Disable slpc controls for VF. This cannot be done in
+	 * __guc_slpc_selected since the VF probe is not complete
+	 * at that point.
+	 */
+	guc->slpc.supported = false;
+	guc->slpc.selected = false;
+
+	/* Disable GUCRC for VF */
+	guc->rc_supported = false;
+
+	return 0;
+
+err_ct:
+	intel_guc_ct_fini(&guc->ct);
+	return err;
+}
+
+static void __vf_guc_fini(struct intel_guc *guc)
+{
+	struct intel_gt *gt = guc_to_gt(guc);
+
+	GEM_BUG_ON(!IS_SRIOV_VF(gt->i915));
+
+	intel_guc_submission_fini(guc);
+	intel_guc_ct_fini(&guc->ct);
+}
+
 /*
  * This function implements the MMIO based host to GuC interface.
  */
@@ -494,6 +553,8 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 	mutex_lock(&guc->send_mutex);
 	intel_uncore_forcewake_get(uncore, guc->send_regs.fw_domains);
 
+	GUC_DEBUG(guc, "mmio sending %*ph\n", len * 4, request);
+
 retry:
 	for (i = 0; i < len; i++)
 		intel_uncore_write(uncore, guc_send_reg(guc, i), request[i]);
@@ -520,11 +581,19 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 	}
 
 	if (FIELD_GET(GUC_HXG_MSG_0_TYPE, header) == GUC_HXG_TYPE_NO_RESPONSE_BUSY) {
+		int loop = IS_SRIOV_VF(guc_to_gt(guc)->i915) ? 20 : 1;
+
 #define done ({ header = intel_uncore_read(uncore, guc_send_reg(guc, 0)); \
 		FIELD_GET(GUC_HXG_MSG_0_ORIGIN, header) != GUC_HXG_ORIGIN_GUC || \
 		FIELD_GET(GUC_HXG_MSG_0_TYPE, header) != GUC_HXG_TYPE_NO_RESPONSE_BUSY; })
 
+busy_loop:
 		ret = wait_for(done, 1000);
+		if (unlikely(ret && --loop)) {
+			guc_dbg(guc, "mmio request %#x: still busy, countdown %u\n",
+				request[0], loop);
+			goto busy_loop;
+		}
 		if (unlikely(ret))
 			goto timeout;
 		if (unlikely(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, header) !=
@@ -569,10 +638,13 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 		for (i = 1; i < count; i++)
 			response_buf[i] = intel_uncore_read(uncore,
 							    guc_send_reg(guc, i));
+		GUC_DEBUG(guc, "mmio received %*ph\n", count * 4, response_buf);
 
 		/* Use number of copied dwords as our return value */
 		ret = count;
 	} else {
+		GUC_DEBUG(guc, "mmio received %*ph\n", 4, &header);
+
 		/* Use data from the GuC response as our return value */
 		ret = FIELD_GET(GUC_HXG_RESPONSE_MSG_0_DATA0, header);
 	}
@@ -878,6 +950,9 @@ void intel_guc_load_status(struct intel_guc *guc, struct drm_printer *p)
 
 	intel_uc_fw_dump(&guc->fw, p);
 
+	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		return;
+
 	with_intel_runtime_pm(uncore->rpm, wakeref) {
 		u32 status = intel_uncore_read(uncore, GUC_STATUS);
 		u32 i;
@@ -925,3 +1000,13 @@ void intel_guc_write_barrier(struct intel_guc *guc)
 		wmb();
 	}
 }
+
+static const struct intel_guc_ops guc_ops_default = {
+	.init = __guc_init,
+	.fini = __guc_fini,
+};
+
+static const struct intel_guc_ops guc_ops_vf = {
+	.init = __vf_guc_init,
+	.fini = __vf_guc_fini,
+};
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
index e46aac1a41e6..e60014f3c6f9 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
@@ -22,8 +22,14 @@
 #include "i915_vma.h"
 
 struct __guc_ads_blob;
+struct intel_guc;
 struct intel_guc_state_capture;
 
+struct intel_guc_ops {
+	int (*init)(struct intel_guc *guc);
+	void (*fini)(struct intel_guc *guc);
+};
+
 /**
  * struct intel_guc - Top level structure of GuC.
  *
@@ -31,6 +37,8 @@ struct intel_guc_state_capture;
  * i915_sched_engine for submission.
  */
 struct intel_guc {
+	/** @ops: Operations to init / fini the GuC */
+	struct intel_guc_ops const *ops;
 	/** @fw: the GuC firmware */
 	struct intel_uc_fw fw;
 	/** @log: sub-structure containing GuC log related data and objects */
@@ -103,7 +111,8 @@ struct intel_guc {
 		struct ida guc_ids;
 		/**
 		 * @num_guc_ids: Number of guc_ids, selftest feature to be able
-		 * to reduce this number while testing.
+		 * to reduce this number while testing. Also used on VFs to
+		 * reduce the pool of guc_ids.
 		 */
 		int num_guc_ids;
 		/**
@@ -378,6 +387,19 @@ static inline u32 intel_guc_ggtt_offset(struct intel_guc *guc,
 	return offset;
 }
 
+static inline int intel_guc_init(struct intel_guc *guc)
+{
+	if (guc->ops->init)
+		return guc->ops->init(guc);
+	return 0;
+}
+
+static inline void intel_guc_fini(struct intel_guc *guc)
+{
+	if (guc->ops->fini)
+		guc->ops->fini(guc);
+}
+
 void intel_guc_init_early(struct intel_guc *guc);
 void intel_guc_init_late(struct intel_guc *guc);
 void intel_guc_init_send_regs(struct intel_guc *guc);
@@ -398,28 +420,29 @@ int intel_guc_allocate_and_map_vma(struct intel_guc *guc, u32 size,
 int intel_guc_self_cfg32(struct intel_guc *guc, u16 key, u32 value);
 int intel_guc_self_cfg64(struct intel_guc *guc, u16 key, u64 value);
 
-static inline bool intel_guc_is_supported(struct intel_guc *guc)
+static inline bool intel_guc_is_supported(const struct intel_guc *guc)
 {
 	return intel_uc_fw_is_supported(&guc->fw);
 }
 
-static inline bool intel_guc_is_wanted(struct intel_guc *guc)
+static inline bool intel_guc_is_wanted(const struct intel_guc *guc)
 {
 	return intel_uc_fw_is_enabled(&guc->fw);
 }
 
-static inline bool intel_guc_is_used(struct intel_guc *guc)
+static inline bool intel_guc_is_used(const struct intel_guc *guc)
 {
 	GEM_BUG_ON(__intel_uc_fw_status(&guc->fw) == INTEL_UC_FIRMWARE_SELECTED);
-	return intel_uc_fw_is_available(&guc->fw);
+	return intel_uc_fw_is_available(&guc->fw) ||
+	       intel_uc_fw_is_preloaded(&guc->fw);
 }
 
-static inline bool intel_guc_is_fw_running(struct intel_guc *guc)
+static inline bool intel_guc_is_fw_running(const struct intel_guc *guc)
 {
 	return intel_uc_fw_is_running(&guc->fw);
 }
 
-static inline bool intel_guc_is_ready(struct intel_guc *guc)
+static inline bool intel_guc_is_ready(const struct intel_guc *guc)
 {
 	return intel_guc_is_fw_running(guc) && intel_guc_ct_enabled(&guc->ct);
 }
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c
index 69ce06faf8cd..9f480b97ad06 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c
@@ -602,6 +602,9 @@ static void guc_init_golden_context(struct intel_guc *guc)
 	if (!intel_uc_uses_guc_submission(&gt->uc))
 		return;
 
+	if (IS_SRIOV_VF(gt->i915))
+		return;
+
 	GEM_BUG_ON(iosys_map_is_null(&guc->ads_map));
 
 	/*
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
index 48f868281195..4b43f2875742 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
@@ -12,6 +12,10 @@
 #include "i915_drv.h"
 #include "intel_guc_ct.h"
 #include "intel_guc_print.h"
+#include "gt/iov/intel_iov_event.h"
+#include "gt/iov/intel_iov_relay.h"
+#include "gt/iov/intel_iov_service.h"
+#include "gt/iov/intel_iov_state.h"
 
 static inline struct intel_guc *ct_to_guc(struct intel_guc_ct *ct)
 {
@@ -827,6 +831,9 @@ int intel_guc_ct_send(struct intel_guc_ct *ct, const u32 *action, u32 len,
 		return ct_send_nb(ct, action, len, flags);
 
 	ret = ct_send(ct, action, len, response_buf, response_buf_size, &status);
+	if (I915_SELFTEST_ONLY(flags & INTEL_GUC_CT_SEND_SELFTEST))
+		return ret;
+
 	if (unlikely(ret < 0)) {
 		if (ret != -ENODEV)
 			CT_ERROR(ct, "Sending action %#x failed (%pe) status=%#X\n",
@@ -1018,9 +1025,9 @@ static int ct_handle_response(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 		break;
 	}
 	if (!found) {
-		CT_ERROR(ct, "Unsolicited response (fence %u)\n", fence);
-		CT_ERROR(ct, "Could not find fence=%u, last_fence=%u\n", fence,
-			 ct->requests.last_fence);
+		CT_ERROR(ct, "Unsolicited response message %#x (fence %u len %u)\n",
+			 hxg[0], fence, len);
+		CT_ERROR(ct, "Last used fence was %u\n", ct->requests.last_fence);
 		list_for_each_entry(req, &ct->requests.pending, link)
 			CT_ERROR(ct, "request %u awaits response\n",
 				 req->fence);
@@ -1038,6 +1045,8 @@ static int ct_handle_response(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 static int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *request)
 {
 	struct intel_guc *guc = ct_to_guc(ct);
+	struct intel_gt *gt = ct_to_gt(ct);
+	struct intel_iov *iov = &gt->iov;
 	const u32 *hxg;
 	const u32 *payload;
 	u32 hxg_len, action, len;
@@ -1074,6 +1083,21 @@ static int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 	case INTEL_GUC_ACTION_ENGINE_FAILURE_NOTIFICATION:
 		ret = intel_guc_engine_failure_process_msg(guc, payload, len);
 		break;
+	case GUC_ACTION_GUC2PF_VF_STATE_NOTIFY:
+		ret = intel_iov_state_process_guc2pf(iov, hxg, hxg_len);
+		break;
+	case GUC_ACTION_GUC2PF_ADVERSE_EVENT:
+		ret = intel_iov_event_process_guc2pf(iov, hxg, hxg_len);
+		break;
+	case GUC_ACTION_GUC2PF_RELAY_FROM_VF:
+		ret = intel_iov_relay_process_guc2pf(&iov->relay, hxg, hxg_len);
+		break;
+	case GUC_ACTION_GUC2VF_RELAY_FROM_PF:
+		ret = intel_iov_relay_process_guc2vf(&iov->relay, hxg, hxg_len);
+		break;
+	case GUC_ACTION_GUC2PF_MMIO_RELAY_SERVICE:
+		ret = intel_iov_service_process_mmio_relay(iov, hxg, hxg_len);
+		break;
 	case INTEL_GUC_ACTION_NOTIFY_FLUSH_LOG_BUFFER_TO_FILE:
 		intel_guc_log_handle_flush_event(&guc->log);
 		ret = 0;
@@ -1271,7 +1295,15 @@ static void ct_receive_tasklet_func(struct tasklet_struct *t)
 void intel_guc_ct_event_handler(struct intel_guc_ct *ct)
 {
 	if (unlikely(!ct->enabled)) {
-		WARN(1, "Unexpected GuC event received while CT disabled!\n");
+		/*
+		 * We are unable to mask memory based interrupt from GuC,
+		 * so there is a chance that an GuC CT event for VF will come
+		 * just as CT will be already disabled. As we are not able to
+		 * handle such an event properly, we should abandon it.
+		 * In this case, calling WARN is not recommended.
+		 */
+		WARN(!HAS_MEMORY_IRQ_STATUS(ct_to_i915(ct)),
+		     "Unexpected GuC event received while CT disabled!\n");
 		return;
 	}
 
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
index 6599cee3a3c4..edd2a4c5778a 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
@@ -107,12 +107,13 @@ int intel_guc_ct_enable(struct intel_guc_ct *ct);
 void intel_guc_ct_disable(struct intel_guc_ct *ct);
 void intel_guc_ct_sanitize(struct intel_guc_ct *ct);
 
-static inline bool intel_guc_ct_enabled(struct intel_guc_ct *ct)
+static inline bool intel_guc_ct_enabled(const struct intel_guc_ct *ct)
 {
 	return ct->enabled;
 }
 
 #define INTEL_GUC_CT_SEND_NB		BIT(31)
+#define INTEL_GUC_CT_SEND_SELFTEST	BIT(30)
 #define INTEL_GUC_CT_SEND_G2H_DW_SHIFT	0
 #define INTEL_GUC_CT_SEND_G2H_DW_MASK	(0xff << INTEL_GUC_CT_SEND_G2H_DW_SHIFT)
 #define MAKE_SEND_FLAGS(len) ({ \
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c
index 7269eb0bbedf..c9f8f628d7c5 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c
@@ -5,6 +5,7 @@
 
 #include <drm/drm_print.h>
 
+#include "gt/intel_gt.h"
 #include "gt/intel_gt_debugfs.h"
 #include "gt/uc/intel_guc_ads.h"
 #include "gt/uc/intel_guc_ct.h"
@@ -13,6 +14,7 @@
 #include "intel_guc.h"
 #include "intel_guc_debugfs.h"
 #include "intel_guc_log_debugfs.h"
+#include "i915_sriov.h"
 
 static int guc_info_show(struct seq_file *m, void *data)
 {
@@ -24,7 +26,9 @@ static int guc_info_show(struct seq_file *m, void *data)
 
 	intel_guc_load_status(guc, &p);
 	drm_puts(&p, "\n");
-	intel_guc_log_info(&guc->log, &p);
+
+	if (!IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		intel_guc_log_info(&guc->log, &p);
 
 	if (!intel_guc_submission_is_used(guc))
 		return 0;
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
index 4ae5fc2f6002..40331f2e1a06 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
@@ -12,13 +12,35 @@
 #include "gt/intel_engine_types.h"
 
 #include "abi/guc_actions_abi.h"
+#include "abi/guc_actions_pf_abi.h"
 #include "abi/guc_actions_slpc_abi.h"
+#include "abi/guc_actions_vf_abi.h"
 #include "abi/guc_errors_abi.h"
 #include "abi/guc_communication_mmio_abi.h"
 #include "abi/guc_communication_ctb_abi.h"
 #include "abi/guc_klvs_abi.h"
 #include "abi/guc_messages_abi.h"
 
+static inline const char *hxg_type_to_string(u32 type)
+{
+	switch (type) {
+	case GUC_HXG_TYPE_REQUEST:
+		return "request";
+	case GUC_HXG_TYPE_EVENT:
+		return "event";
+	case GUC_HXG_TYPE_NO_RESPONSE_BUSY:
+		return "busy";
+	case GUC_HXG_TYPE_NO_RESPONSE_RETRY:
+		return "retry";
+	case GUC_HXG_TYPE_RESPONSE_FAILURE:
+		return "failure";
+	case GUC_HXG_TYPE_RESPONSE_SUCCESS:
+		return "response";
+	default:
+		return "<invalid>";
+	}
+}
+
 /* Payload length only i.e. don't include G2H header length */
 #define G2H_LEN_DW_SCHED_CONTEXT_MODE_SET	2
 #define G2H_LEN_DW_DEREGISTER_CONTEXT		1
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_rc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_rc.h
index 57e86c337838..3645a3484b5f 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_rc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_rc.h
@@ -10,17 +10,17 @@
 
 void intel_guc_rc_init_early(struct intel_guc *guc);
 
-static inline bool intel_guc_rc_is_supported(struct intel_guc *guc)
+static inline bool intel_guc_rc_is_supported(const struct intel_guc *guc)
 {
 	return guc->rc_supported;
 }
 
-static inline bool intel_guc_rc_is_wanted(struct intel_guc *guc)
+static inline bool intel_guc_rc_is_wanted(const struct intel_guc *guc)
 {
 	return guc->submission_selected && intel_guc_rc_is_supported(guc);
 }
 
-static inline bool intel_guc_rc_is_used(struct intel_guc *guc)
+static inline bool intel_guc_rc_is_used(const struct intel_guc *guc)
 {
 	return intel_guc_submission_is_used(guc) && intel_guc_rc_is_wanted(guc);
 }
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_slpc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_slpc.h
index 17ed515f6a85..f81cd30d1f49 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_slpc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_slpc.h
@@ -14,17 +14,17 @@
 struct intel_gt;
 struct drm_printer;
 
-static inline bool intel_guc_slpc_is_supported(struct intel_guc *guc)
+static inline bool intel_guc_slpc_is_supported(const struct intel_guc *guc)
 {
 	return guc->slpc.supported;
 }
 
-static inline bool intel_guc_slpc_is_wanted(struct intel_guc *guc)
+static inline bool intel_guc_slpc_is_wanted(const struct intel_guc *guc)
 {
 	return guc->slpc.selected;
 }
 
-static inline bool intel_guc_slpc_is_used(struct intel_guc *guc)
+static inline bool intel_guc_slpc_is_used(const struct intel_guc *guc)
 {
 	return intel_guc_submission_is_used(guc) && intel_guc_slpc_is_wanted(guc);
 }
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index bbb04f26b9d0..075c4f6c8b65 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -149,17 +149,6 @@ guc_create_parallel(struct intel_engine_cs **engines,
 
 #define GUC_REQUEST_SIZE 64 /* bytes */
 
-/*
- * We reserve 1/16 of the guc_ids for multi-lrc as these need to be contiguous
- * per the GuC submission interface. A different allocation algorithm is used
- * (bitmap vs. ida) between multi-lrc and single-lrc hence the reason to
- * partition the guc_id space. We believe the number of multi-lrc contexts in
- * use should be low and 1/16 should be sufficient. Minimum of 32 guc_ids for
- * multi-lrc.
- */
-#define NUMBER_MULTI_LRC_GUC_ID(guc)	\
-	((guc)->submission_state.num_guc_ids / 16)
-
 /*
  * Below is a set of functions which control the GuC scheduling state which
  * require a lock.
@@ -1325,7 +1314,8 @@ static ktime_t guc_engine_busyness(struct intel_engine_cs *engine, ktime_t *now)
 	 * start_gt_clk is derived from GuC state. To get a consistent
 	 * view of activity, we query the GuC state only if gt is awake.
 	 */
-	if (!in_reset && (wakeref = intel_gt_pm_get_if_awake(gt))) {
+	if (!in_reset && !IS_SRIOV_VF(gt->i915) &&
+	    (wakeref = intel_gt_pm_get_if_awake(gt))) {
 		stats_saved = *stats;
 		gt_stamp_saved = guc->timestamp.gt_stamp;
 		/*
@@ -1453,6 +1443,9 @@ void intel_guc_busyness_park(struct intel_gt *gt)
 {
 	struct intel_guc *guc = &gt->uc.guc;
 
+	if (IS_SRIOV_VF(gt->i915))
+		return;
+
 	if (!guc_submission_initialized(guc))
 		return;
 
@@ -1482,6 +1475,9 @@ void intel_guc_busyness_unpark(struct intel_gt *gt)
 	unsigned long flags;
 	ktime_t unused;
 
+	if (IS_SRIOV_VF(gt->i915))
+		return;
+
 	if (!guc_submission_initialized(guc))
 		return;
 
@@ -1552,7 +1548,9 @@ void intel_guc_submission_reset_prepare(struct intel_guc *guc)
 	intel_gt_park_heartbeats(guc_to_gt(guc));
 	disable_submission(guc);
 	guc->interrupts.disable(guc);
-	__reset_guc_busyness_stats(guc);
+
+	if (!IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		__reset_guc_busyness_stats(guc);
 
 	/* Flush IRQ handler */
 	spin_lock_irq(guc_to_gt(guc)->irq_lock);
@@ -1881,6 +1879,7 @@ void intel_guc_submission_reset_finish(struct intel_guc *guc)
 
 static void destroyed_worker_func(struct work_struct *w);
 static void reset_fail_worker_func(struct work_struct *w);
+static int number_mlrc_guc_id(struct intel_guc *guc);
 
 /*
  * Set up the memory resources to be shared with the GuC (via the GGTT)
@@ -1901,7 +1900,7 @@ int intel_guc_submission_init(struct intel_guc *guc)
 	}
 
 	guc->submission_state.guc_ids_bitmap =
-		bitmap_zalloc(NUMBER_MULTI_LRC_GUC_ID(guc), GFP_KERNEL);
+		bitmap_zalloc(number_mlrc_guc_id(guc), GFP_KERNEL);
 	if (!guc->submission_state.guc_ids_bitmap) {
 		ret = -ENOMEM;
 		goto destroy_pool;
@@ -1995,6 +1994,69 @@ static void guc_submit_request(struct i915_request *rq)
 	spin_unlock_irqrestore(&sched_engine->lock, flags);
 }
 
+/*
+ * We reserve 1/16 of the guc_ids for multi-lrc as these need to be contiguous
+ * per the GuC submission interface. A different allocation algorithm is used
+ * (bitmap vs. ida) between multi-lrc and single-lrc hence the reason to
+ * partition the guc_id space. We believe the number of multi-lrc contexts in
+ * use should be low and 1/16 should be sufficient.
+ */
+#define MLRC_GUC_ID_RATIO	16
+
+static int number_mlrc_guc_id(struct intel_guc *guc)
+{
+	return guc->submission_state.num_guc_ids / MLRC_GUC_ID_RATIO;
+}
+
+static int number_slrc_guc_id(struct intel_guc *guc)
+{
+	return guc->submission_state.num_guc_ids - number_mlrc_guc_id(guc);
+}
+
+static int mlrc_guc_id_base(struct intel_guc *guc)
+{
+	return number_slrc_guc_id(guc);
+}
+
+static int new_mlrc_guc_id(struct intel_guc *guc, struct intel_context *ce)
+{
+	int ret;
+
+	GEM_BUG_ON(!intel_context_is_parent(ce));
+	GEM_BUG_ON(!guc->submission_state.guc_ids_bitmap);
+
+	ret =  bitmap_find_free_region(guc->submission_state.guc_ids_bitmap,
+				       number_mlrc_guc_id(guc),
+				       order_base_2(ce->parallel.number_children
+						    + 1));
+	if (unlikely(ret < 0))
+		return ret;
+
+	return ret + mlrc_guc_id_base(guc);
+}
+
+static int new_slrc_guc_id(struct intel_guc *guc, struct intel_context *ce)
+{
+	GEM_BUG_ON(intel_context_is_parent(ce));
+
+	return ida_simple_get(&guc->submission_state.guc_ids,
+			      0, number_slrc_guc_id(guc),
+			      GFP_KERNEL | __GFP_RETRY_MAYFAIL |
+			      __GFP_NOWARN);
+}
+
+int intel_guc_submission_limit_ids(struct intel_guc *guc, u32 limit)
+{
+	if (limit > GUC_MAX_CONTEXT_ID)
+		return -E2BIG;
+
+	if (!ida_is_empty(&guc->submission_state.guc_ids))
+		return -ETXTBSY;
+
+	guc->submission_state.num_guc_ids = limit;
+	return 0;
+}
+
 static int new_guc_id(struct intel_guc *guc, struct intel_context *ce)
 {
 	int ret;
@@ -2002,16 +2064,10 @@ static int new_guc_id(struct intel_guc *guc, struct intel_context *ce)
 	GEM_BUG_ON(intel_context_is_child(ce));
 
 	if (intel_context_is_parent(ce))
-		ret = bitmap_find_free_region(guc->submission_state.guc_ids_bitmap,
-					      NUMBER_MULTI_LRC_GUC_ID(guc),
-					      order_base_2(ce->parallel.number_children
-							   + 1));
+		ret = new_mlrc_guc_id(guc, ce);
 	else
-		ret = ida_simple_get(&guc->submission_state.guc_ids,
-				     NUMBER_MULTI_LRC_GUC_ID(guc),
-				     guc->submission_state.num_guc_ids,
-				     GFP_KERNEL | __GFP_RETRY_MAYFAIL |
-				     __GFP_NOWARN);
+		ret = new_slrc_guc_id(guc, ce);
+
 	if (unlikely(ret < 0))
 		return ret;
 
@@ -2029,7 +2085,7 @@ static void __release_guc_id(struct intel_guc *guc, struct intel_context *ce)
 	if (!context_guc_id_invalid(ce)) {
 		if (intel_context_is_parent(ce)) {
 			bitmap_release_region(guc->submission_state.guc_ids_bitmap,
-					      ce->guc_id.id,
+					      ce->guc_id.id - mlrc_guc_id_base(guc),
 					      order_base_2(ce->parallel.number_children
 							   + 1));
 		} else {
@@ -4100,6 +4156,12 @@ static bool guc_sched_engine_disabled(struct i915_sched_engine *sched_engine)
 	return !sched_engine->tasklet.callback;
 }
 
+static int vf_guc_resume(struct intel_engine_cs *engine)
+{
+	intel_breadcrumbs_reset(engine->breadcrumbs);
+	return 0;
+}
+
 static void guc_set_default_submission(struct intel_engine_cs *engine)
 {
 	engine->submit_request = guc_submit_request;
@@ -4187,6 +4249,8 @@ static void guc_default_vfuncs(struct intel_engine_cs *engine)
 	engine->sched_engine->schedule = i915_schedule;
 
 	engine->reset.prepare = guc_engine_reset_prepare;
+	if (IS_SRIOV_VF(engine->i915))
+		engine->reset.prepare = guc_reset_nop;
 	engine->reset.rewind = guc_rewind_nop;
 	engine->reset.cancel = guc_reset_nop;
 	engine->reset.finish = guc_reset_nop;
@@ -4296,6 +4360,9 @@ int intel_guc_submission_setup(struct intel_engine_cs *engine)
 	if (engine->flags & I915_ENGINE_HAS_RCS_REG_STATE)
 		rcs_submission_override(engine);
 
+	if (IS_SRIOV_VF(engine->i915))
+		engine->resume = vf_guc_resume;
+
 	lrc_init_wa_ctx(engine);
 
 	/* Finally, take ownership and responsibility for cleanup! */
@@ -4407,7 +4474,8 @@ void intel_guc_submission_enable(struct intel_guc *guc)
 				   GUC_SEM_INTR_ENABLE_ALL);
 
 	guc_init_lrc_mapping(guc);
-	guc_init_engine_stats(guc);
+	if (!IS_SRIOV_VF(gt->i915))
+		guc_init_engine_stats(guc);
 	guc_init_global_schedule_policy(guc);
 }
 
@@ -4441,7 +4509,7 @@ static bool __guc_submission_selected(struct intel_guc *guc)
 
 int intel_guc_sched_disable_gucid_threshold_max(struct intel_guc *guc)
 {
-	return guc->submission_state.num_guc_ids - NUMBER_MULTI_LRC_GUC_ID(guc);
+	return guc->submission_state.num_guc_ids - number_mlrc_guc_id(guc);
 }
 
 /*
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
index 5a95a9f0a8e3..e31cc83e74b9 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
@@ -14,6 +14,7 @@ struct drm_printer;
 struct intel_engine_cs;
 
 void intel_guc_submission_init_early(struct intel_guc *guc);
+int intel_guc_submission_limit_ids(struct intel_guc *guc, u32 limit);
 int intel_guc_submission_init(struct intel_guc *guc);
 void intel_guc_submission_enable(struct intel_guc *guc);
 void intel_guc_submission_disable(struct intel_guc *guc);
@@ -38,19 +39,24 @@ int intel_guc_wait_for_pending_msg(struct intel_guc *guc,
 				   bool interruptible,
 				   long timeout);
 
-static inline bool intel_guc_submission_is_supported(struct intel_guc *guc)
+static inline bool intel_guc_submission_is_supported(const struct intel_guc *guc)
 {
 	return guc->submission_supported;
 }
 
-static inline bool intel_guc_submission_is_wanted(struct intel_guc *guc)
+static inline bool intel_guc_submission_is_wanted(const struct intel_guc *guc)
 {
 	return guc->submission_selected;
 }
 
-static inline bool intel_guc_submission_is_used(struct intel_guc *guc)
+static inline bool intel_guc_submission_is_used(const struct intel_guc *guc)
 {
 	return intel_guc_is_used(guc) && intel_guc_submission_is_wanted(guc);
 }
 
+static inline u16 intel_guc_submission_ids_in_use(struct intel_guc *guc)
+{
+	return guc->submission_state.guc_ids_in_use;
+}
+
 #endif
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_huc.h b/drivers/gpu/drm/i915/gt/uc/intel_huc.h
index 52db03620c60..b1bb231321b9 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_huc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_huc.h
@@ -60,20 +60,21 @@ static inline int intel_huc_sanitize(struct intel_huc *huc)
 	return 0;
 }
 
-static inline bool intel_huc_is_supported(struct intel_huc *huc)
+static inline bool intel_huc_is_supported(const struct intel_huc *huc)
 {
 	return intel_uc_fw_is_supported(&huc->fw);
 }
 
-static inline bool intel_huc_is_wanted(struct intel_huc *huc)
+static inline bool intel_huc_is_wanted(const struct intel_huc *huc)
 {
 	return intel_uc_fw_is_enabled(&huc->fw);
 }
 
-static inline bool intel_huc_is_used(struct intel_huc *huc)
+static inline bool intel_huc_is_used(const struct intel_huc *huc)
 {
 	GEM_BUG_ON(__intel_uc_fw_status(&huc->fw) == INTEL_UC_FIRMWARE_SELECTED);
-	return intel_uc_fw_is_available(&huc->fw);
+	return intel_uc_fw_is_available(&huc->fw) ||
+	       intel_uc_fw_is_preloaded(&huc->fw);
 }
 
 static inline bool intel_huc_is_loaded_by_gsc(const struct intel_huc *huc)
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc.c b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
index 45cff40a06af..cf2bd95ec41e 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
@@ -8,6 +8,8 @@
 #include "gt/intel_gt.h"
 #include "gt/intel_gt_print.h"
 #include "gt/intel_reset.h"
+#include "gt/iov/intel_iov_memirq.h"
+#include "gt/iov/intel_iov_query.h"
 #include "intel_gsc_fw.h"
 #include "intel_gsc_uc.h"
 #include "intel_guc.h"
@@ -21,6 +23,7 @@
 
 static const struct intel_uc_ops uc_ops_off;
 static const struct intel_uc_ops uc_ops_on;
+static const struct intel_uc_ops uc_ops_vf;
 
 static void uc_expand_default_options(struct intel_uc *uc)
 {
@@ -134,7 +137,9 @@ void intel_uc_init_early(struct intel_uc *uc)
 
 	__confirm_options(uc);
 
-	if (intel_uc_wants_guc(uc))
+	if (IS_SRIOV_VF(uc_to_gt(uc)->i915))
+		uc->ops = &uc_ops_vf;
+	else if (intel_uc_wants_guc(uc))
 		uc->ops = &uc_ops_on;
 	else
 		uc->ops = &uc_ops_off;
@@ -192,6 +197,9 @@ void intel_uc_driver_remove(struct intel_uc *uc)
  */
 static void guc_clear_mmio_msg(struct intel_guc *guc)
 {
+	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		return;
+
 	intel_uncore_write(guc_to_gt(guc)->uncore, SOFT_SCRATCH(15), 0);
 }
 
@@ -199,6 +207,9 @@ static void guc_get_mmio_msg(struct intel_guc *guc)
 {
 	u32 val;
 
+	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		return;
+
 	spin_lock_irq(&guc->irq_lock);
 
 	val = intel_uncore_read(guc_to_gt(guc)->uncore, SOFT_SCRATCH(15));
@@ -594,6 +605,113 @@ static void __uc_fini_hw(struct intel_uc *uc)
 	__uc_sanitize(uc);
 }
 
+static int __vf_uc_sanitize(struct intel_uc *uc)
+{
+	intel_huc_sanitize(&uc->huc);
+	intel_guc_sanitize(&uc->guc);
+
+	return 0;
+}
+
+static void __vf_uc_init_fw(struct intel_uc *uc)
+{
+	struct intel_gt *gt = uc_to_gt(uc);
+	unsigned int major = gt->iov.vf.config.guc_abi.major;
+	unsigned int minor = gt->iov.vf.config.guc_abi.minor;
+
+	intel_uc_fw_set_preloaded(&uc->guc.fw, major, minor);
+}
+
+static int __vf_uc_init(struct intel_uc *uc)
+{
+	return intel_guc_init(&uc->guc);
+}
+
+static void __vf_uc_fini(struct intel_uc *uc)
+{
+	intel_guc_fini(&uc->guc);
+}
+
+static int __vf_uc_init_hw(struct intel_uc *uc)
+{
+	struct intel_gt *gt = uc_to_gt(uc);
+	struct drm_i915_private *i915 = gt->i915;
+	struct intel_guc *guc = &uc->guc;
+	struct intel_huc *huc = &uc->huc;
+	int err;
+
+	GEM_BUG_ON(!HAS_GT_UC(i915));
+	GEM_BUG_ON(!IS_SRIOV_VF(i915));
+	GEM_BUG_ON(!intel_uc_uses_guc_submission(&gt->uc));
+
+	err = intel_iov_query_bootstrap(&gt->iov);
+	if (unlikely(err))
+		goto err_out;
+
+	if (!intel_uc_fw_is_running(&guc->fw)) {
+		err = intel_uc_fw_status_to_error(guc->fw.status);
+		goto err_out;
+	}
+
+	intel_guc_reset_interrupts(guc);
+
+	if (HAS_MEMORY_IRQ_STATUS(i915)) {
+		err = intel_iov_memirq_prepare_guc(&gt->iov);
+		if (unlikely(err))
+			goto err_out;
+	}
+
+	err = guc_enable_communication(guc);
+	if (unlikely(err))
+		goto err_out;
+
+	err = intel_iov_query_version(&gt->iov);
+	if (unlikely(err))
+		goto err_out;
+
+	/*
+	 * pretend that HuC is running if it is supported
+	 * for status rely on runtime reg shared by PF
+	 */
+	if (intel_uc_fw_is_supported(&huc->fw)) {
+		if (intel_huc_check_status(huc) > 0)
+			/* XXX: We don't know how to get the HuC version yet */
+			intel_uc_fw_set_preloaded(&huc->fw, 0, 0);
+		else
+			intel_uc_fw_change_status(&huc->fw, INTEL_UC_FIRMWARE_DISABLED);
+	}
+
+	intel_guc_submission_enable(guc);
+
+	dev_info(i915->drm.dev, "%s firmware %s version %u.%u %s:%s\n",
+		 intel_uc_fw_type_repr(INTEL_UC_FW_TYPE_GUC), guc->fw.file_selected.path,
+		 guc->fw.file_selected.ver.major, guc->fw.file_selected.ver.minor,
+		 "submission", i915_iov_mode_to_string(IOV_MODE(i915)));
+
+	dev_info(i915->drm.dev, "%s firmware %s\n",
+		 intel_uc_fw_type_repr(INTEL_UC_FW_TYPE_HUC),
+		 intel_uc_fw_status_repr(__intel_uc_fw_status(&huc->fw)));
+
+	return 0;
+
+err_out:
+	__vf_uc_sanitize(uc);
+	i915_probe_error(i915, "GuC initialization failed (%pe)\n", ERR_PTR(err));
+	return -EIO;
+}
+
+static void __vf_uc_fini_hw(struct intel_uc *uc)
+{
+	struct intel_guc *guc = &uc->guc;
+
+	intel_guc_submission_disable(guc);
+
+	if (intel_guc_ct_enabled(&guc->ct))
+		guc_disable_communication(guc);
+
+	__vf_uc_sanitize(uc);
+}
+
 /**
  * intel_uc_reset_prepare - Prepare for reset
  * @uc: the intel_uc structure
@@ -618,7 +736,7 @@ void intel_uc_reset_prepare(struct intel_uc *uc)
 		intel_guc_submission_reset_prepare(guc);
 
 sanitize:
-	__uc_sanitize(uc);
+	intel_uc_sanitize(uc);
 }
 
 void intel_uc_reset(struct intel_uc *uc, intel_engine_mask_t stalled)
@@ -698,6 +816,9 @@ static int __uc_resume(struct intel_uc *uc, bool enable_communication)
 	struct intel_gt *gt = guc_to_gt(guc);
 	int err;
 
+	if (intel_gt_terminally_wedged(gt))
+		return 0;
+
 	if (!intel_guc_is_fw_running(guc))
 		return 0;
 
@@ -758,3 +879,12 @@ static const struct intel_uc_ops uc_ops_on = {
 	.init_hw = __uc_init_hw,
 	.fini_hw = __uc_fini_hw,
 };
+
+static const struct intel_uc_ops uc_ops_vf = {
+	.sanitize = __vf_uc_sanitize,
+	.init_fw = __vf_uc_init_fw,
+	.init = __vf_uc_init,
+	.fini = __vf_uc_fini,
+	.init_hw = __vf_uc_init_hw,
+	.fini_hw = __vf_uc_fini_hw,
+};
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc.h b/drivers/gpu/drm/i915/gt/uc/intel_uc.h
index 5d0f1bcc381e..99c2077e545b 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc.h
@@ -74,7 +74,7 @@ int intel_uc_runtime_resume(struct intel_uc *uc);
  */
 
 #define __uc_state_checker(x, func, state, required) \
-static inline bool intel_uc_##state##_##func(struct intel_uc *uc) \
+static inline bool intel_uc_##state##_##func(const struct intel_uc *uc) \
 { \
 	return intel_##func##_is_##required(&uc->x); \
 }
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
index b462378e4142..f53141eac658 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
@@ -452,6 +452,32 @@ void intel_uc_fw_init_early(struct intel_uc_fw *uc_fw,
 				  INTEL_UC_FIRMWARE_NOT_SUPPORTED);
 }
 
+/**
+ * intel_uc_fw_set_preloaded() - set uC firmware as pre-loaded
+ * @uc_fw: uC firmware structure
+ * @major: major version of the pre-loaded firmware
+ * @minor: minor version of the pre-loaded firmware
+ *
+ * If the uC firmware was loaded to h/w by other entity, just
+ * mark it as loaded.
+ */
+void intel_uc_fw_set_preloaded(struct intel_uc_fw *uc_fw, u16 major, u16 minor)
+{
+	uc_fw->file_selected.path = "PRELOADED";
+	uc_fw->file_selected.ver.major = major;
+	uc_fw->file_selected.ver.minor = minor;
+
+	if (uc_fw->type == INTEL_UC_FW_TYPE_GUC) {
+		struct intel_guc *guc = container_of(uc_fw, struct intel_guc, fw);
+
+		guc->submission_version.major = major;
+		guc->submission_version.minor = minor;
+		guc->submission_version.patch = 0;
+	}
+
+	intel_uc_fw_change_status(uc_fw, INTEL_UC_FIRMWARE_PRELOADED);
+}
+
 static void __force_fw_fetch_failures(struct intel_uc_fw *uc_fw, int e)
 {
 	struct drm_i915_private *i915 = __uc_fw_to_gt(uc_fw)->i915;
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h
index 6ba00e6b3975..cdf7c82ba5fc 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h
@@ -56,7 +56,8 @@ enum intel_uc_fw_status {
 	INTEL_UC_FIRMWARE_LOADABLE, /* all fw-required objects are ready */
 	INTEL_UC_FIRMWARE_LOAD_FAIL, /* failed to xfer or init/auth the fw */
 	INTEL_UC_FIRMWARE_TRANSFERRED, /* dma xfer done */
-	INTEL_UC_FIRMWARE_RUNNING /* init/auth done */
+	INTEL_UC_FIRMWARE_RUNNING, /* init/auth done */
+	INTEL_UC_FIRMWARE_PRELOADED, /* already pre-loaded */
 };
 
 enum intel_uc_fw_type {
@@ -167,6 +168,8 @@ const char *intel_uc_fw_status_repr(enum intel_uc_fw_status status)
 		return "TRANSFERRED";
 	case INTEL_UC_FIRMWARE_RUNNING:
 		return "RUNNING";
+	case INTEL_UC_FIRMWARE_PRELOADED:
+		return "PRELOADED";
 	}
 	return "<invalid>";
 }
@@ -193,6 +196,7 @@ static inline int intel_uc_fw_status_to_error(enum intel_uc_fw_status status)
 	case INTEL_UC_FIRMWARE_LOADABLE:
 	case INTEL_UC_FIRMWARE_TRANSFERRED:
 	case INTEL_UC_FIRMWARE_RUNNING:
+	case INTEL_UC_FIRMWARE_PRELOADED:
 		return 0;
 	}
 	return -EINVAL;
@@ -212,41 +216,48 @@ static inline const char *intel_uc_fw_type_repr(enum intel_uc_fw_type type)
 }
 
 static inline enum intel_uc_fw_status
-__intel_uc_fw_status(struct intel_uc_fw *uc_fw)
+__intel_uc_fw_status(const struct intel_uc_fw *uc_fw)
 {
 	/* shouldn't call this before checking hw/blob availability */
 	GEM_BUG_ON(uc_fw->status == INTEL_UC_FIRMWARE_UNINITIALIZED);
 	return uc_fw->status;
 }
 
-static inline bool intel_uc_fw_is_supported(struct intel_uc_fw *uc_fw)
+static inline bool intel_uc_fw_is_supported(const struct intel_uc_fw *uc_fw)
 {
 	return __intel_uc_fw_status(uc_fw) != INTEL_UC_FIRMWARE_NOT_SUPPORTED;
 }
 
-static inline bool intel_uc_fw_is_enabled(struct intel_uc_fw *uc_fw)
+static inline bool intel_uc_fw_is_enabled(const struct intel_uc_fw *uc_fw)
 {
 	return __intel_uc_fw_status(uc_fw) > INTEL_UC_FIRMWARE_DISABLED;
 }
 
-static inline bool intel_uc_fw_is_available(struct intel_uc_fw *uc_fw)
+static inline bool intel_uc_fw_is_available(const struct intel_uc_fw *uc_fw)
 {
-	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_AVAILABLE;
+	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_AVAILABLE &&
+	       __intel_uc_fw_status(uc_fw) != INTEL_UC_FIRMWARE_PRELOADED;
 }
 
-static inline bool intel_uc_fw_is_loadable(struct intel_uc_fw *uc_fw)
+static inline bool intel_uc_fw_is_loadable(const struct intel_uc_fw *uc_fw)
 {
-	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_LOADABLE;
+	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_LOADABLE &&
+	       __intel_uc_fw_status(uc_fw) != INTEL_UC_FIRMWARE_PRELOADED;
 }
 
-static inline bool intel_uc_fw_is_loaded(struct intel_uc_fw *uc_fw)
+static inline bool intel_uc_fw_is_loaded(const struct intel_uc_fw *uc_fw)
 {
 	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_TRANSFERRED;
 }
 
-static inline bool intel_uc_fw_is_running(struct intel_uc_fw *uc_fw)
+static inline bool intel_uc_fw_is_running(const struct intel_uc_fw *uc_fw)
 {
-	return __intel_uc_fw_status(uc_fw) == INTEL_UC_FIRMWARE_RUNNING;
+	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_RUNNING;
+}
+
+static inline bool intel_uc_fw_is_preloaded(const struct intel_uc_fw *uc_fw)
+{
+	return __intel_uc_fw_status(uc_fw) == INTEL_UC_FIRMWARE_PRELOADED;
 }
 
 static inline bool intel_uc_fw_is_overridden(const struct intel_uc_fw *uc_fw)
@@ -256,11 +267,11 @@ static inline bool intel_uc_fw_is_overridden(const struct intel_uc_fw *uc_fw)
 
 static inline void intel_uc_fw_sanitize(struct intel_uc_fw *uc_fw)
 {
-	if (intel_uc_fw_is_loaded(uc_fw))
+	if (intel_uc_fw_is_loadable(uc_fw))
 		intel_uc_fw_change_status(uc_fw, INTEL_UC_FIRMWARE_LOADABLE);
 }
 
-static inline u32 __intel_uc_fw_get_upload_size(struct intel_uc_fw *uc_fw)
+static inline u32 __intel_uc_fw_get_upload_size(const struct intel_uc_fw *uc_fw)
 {
 	return sizeof(struct uc_css_header) + uc_fw->ucode_size;
 }
@@ -273,7 +284,7 @@ static inline u32 __intel_uc_fw_get_upload_size(struct intel_uc_fw *uc_fw)
  *
  * Return: Upload firmware size, or zero on firmware fetch failure.
  */
-static inline u32 intel_uc_fw_get_upload_size(struct intel_uc_fw *uc_fw)
+static inline u32 intel_uc_fw_get_upload_size(const struct intel_uc_fw *uc_fw)
 {
 	if (!intel_uc_fw_is_available(uc_fw))
 		return 0;
@@ -283,6 +294,7 @@ static inline u32 intel_uc_fw_get_upload_size(struct intel_uc_fw *uc_fw)
 
 void intel_uc_fw_init_early(struct intel_uc_fw *uc_fw,
 			    enum intel_uc_fw_type type);
+void intel_uc_fw_set_preloaded(struct intel_uc_fw *uc_fw, u16 major, u16 minor);
 int intel_uc_fw_fetch(struct intel_uc_fw *uc_fw);
 void intel_uc_fw_cleanup_fetch(struct intel_uc_fw *uc_fw);
 int intel_uc_fw_upload(struct intel_uc_fw *uc_fw, u32 offset, u32 dma_flags);
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index 90663f251fd1..c8ea189ed268 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -78,6 +78,16 @@ static int i915_capabilities(struct seq_file *m, void *data)
 	return 0;
 }
 
+static int sriov_info_show(struct seq_file *m, void *data)
+{
+	struct drm_i915_private *i915 = node_to_i915(m->private);
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	i915_sriov_print_info(i915, &p);
+
+	return 0;
+}
+
 static char get_tiling_flag(struct drm_i915_gem_object *obj)
 {
 	switch (i915_gem_object_get_tiling(obj)) {
@@ -792,6 +802,14 @@ static const struct drm_info_list i915_debugfs_list[] = {
 	{"i915_wa_registers", i915_wa_registers, 0},
 	{"i915_sseu_status", i915_sseu_status, 0},
 	{"i915_rps_boost_info", i915_rps_boost_info, 0},
+	{"i915_sriov_info", sriov_info_show, 0},
+};
+
+static const struct drm_info_list i915_vf_debugfs_list[] = {
+	{"i915_capabilities", i915_capabilities, 0},
+	{"i915_gem_objects", i915_gem_object_info, 0},
+	{"i915_engine_info", i915_engine_info, 0},
+	{"i915_sriov_info", sriov_info_show, 0},
 };
 
 static const struct i915_debugfs_files {
@@ -805,26 +823,48 @@ static const struct i915_debugfs_files {
 	{"i915_error_state", &i915_error_state_fops},
 	{"i915_gpu_info", &i915_gpu_info_fops},
 #endif
+}, i915_vf_debugfs_files[] = {
+	{"i915_wedged", &i915_wedged_fops},
+	{"i915_gem_drop_caches", &i915_drop_caches_fops},
 };
 
 void i915_debugfs_register(struct drm_i915_private *dev_priv)
 {
 	struct drm_minor *minor = dev_priv->drm.primary;
+	const struct drm_info_list *debugfs_list;
+	const struct i915_debugfs_files *debugfs_files;
+	size_t debugfs_files_size;
+	size_t debugfs_list_size;
 	int i;
 
 	i915_debugfs_params(dev_priv);
 
 	debugfs_create_file("i915_forcewake_user", S_IRUSR, minor->debugfs_root,
 			    to_i915(minor->dev), &i915_forcewake_fops);
-	for (i = 0; i < ARRAY_SIZE(i915_debugfs_files); i++) {
-		debugfs_create_file(i915_debugfs_files[i].name,
+
+	if (IS_SRIOV_VF(dev_priv)) {
+		debugfs_files = i915_vf_debugfs_files;
+		debugfs_list = i915_vf_debugfs_list;
+
+		debugfs_files_size = ARRAY_SIZE(i915_vf_debugfs_files);
+		debugfs_list_size = ARRAY_SIZE(i915_vf_debugfs_list);
+	} else {
+		debugfs_files = i915_debugfs_files;
+		debugfs_list = i915_debugfs_list;
+
+		debugfs_files_size = ARRAY_SIZE(i915_debugfs_files);
+		debugfs_list_size = ARRAY_SIZE(i915_debugfs_list);
+	}
+
+	for (i = 0; i < debugfs_files_size; i++) {
+		debugfs_create_file(debugfs_files[i].name,
 				    S_IRUGO | S_IWUSR,
 				    minor->debugfs_root,
 				    to_i915(minor->dev),
-				    i915_debugfs_files[i].fops);
+				    debugfs_files[i].fops);
 	}
 
-	drm_debugfs_create_files(i915_debugfs_list,
-				 ARRAY_SIZE(i915_debugfs_list),
+	drm_debugfs_create_files(debugfs_list,
+				 debugfs_list_size,
 				 minor->debugfs_root, minor);
 }
diff --git a/drivers/gpu/drm/i915/i915_debugfs_params.c b/drivers/gpu/drm/i915/i915_debugfs_params.c
index 614bde321589..e1537c6ccf06 100644
--- a/drivers/gpu/drm/i915/i915_debugfs_params.c
+++ b/drivers/gpu/drm/i915/i915_debugfs_params.c
@@ -40,6 +40,17 @@ static int notify_guc(struct drm_i915_private *i915)
 {
 	int ret = 0;
 
+	/*
+	 * FIXME: This needs to return -EPERM to userland to indicate
+	 * that a VF is not allowed to change the scheduling policies.
+	 * However, doing so will currently 'break' a whole bunch of IGT
+	 * tests that rely on disabling engine reset. Although, they are
+	 * already broken as they will not correctly detect hang failures
+	 * and are potentially returning false successes.
+	 */
+	if (IS_SRIOV_VF(i915))
+		return 0;
+
 	if (intel_uc_uses_guc_submission(&to_gt(i915)->uc))
 		ret = intel_guc_global_policies_update(&to_gt(i915)->uc.guc);
 
diff --git a/drivers/gpu/drm/i915/i915_driver.c b/drivers/gpu/drm/i915/i915_driver.c
index 6bc0718895d2..caed6658fffb 100644
--- a/drivers/gpu/drm/i915/i915_driver.c
+++ b/drivers/gpu/drm/i915/i915_driver.c
@@ -92,6 +92,7 @@
 #include "i915_memcpy.h"
 #include "i915_perf.h"
 #include "i915_query.h"
+#include "i915_sriov.h"
 #include "i915_suspend.h"
 #include "i915_switcheroo.h"
 #include "i915_sysfs.h"
@@ -507,6 +508,10 @@ static int i915_driver_hw_probe(struct drm_i915_private *dev_priv)
 
 	pci_set_master(pdev);
 
+	/* Assume that VF is up, otherwise we may end with unknown state */
+	if (IS_SRIOV_VF(dev_priv))
+		ret = pci_set_power_state(pdev, PCI_D0);
+
 	/* On the 945G/GM, the chipset reports the MSI capability on the
 	 * integrated graphics even though the support isn't actually there
 	 * according to the published specs.  It doesn't appear to function
@@ -535,7 +540,8 @@ static int i915_driver_hw_probe(struct drm_i915_private *dev_priv)
 	if (ret)
 		goto err_msi;
 
-	intel_opregion_setup(dev_priv);
+	if (!IS_SRIOV_VF(dev_priv))
+		intel_opregion_setup(dev_priv);
 
 	ret = i915_pcode_init(dev_priv);
 	if (ret)
@@ -593,6 +599,12 @@ static void i915_driver_hw_remove(struct drm_i915_private *dev_priv)
 		pci_d3cold_enable(root_pdev);
 }
 
+static void i915_virtualization_commit(struct drm_i915_private *i915)
+{
+	if (IS_SRIOV_PF(i915))
+		i915_sriov_pf_confirm(i915);
+}
+
 /**
  * i915_driver_register - register the driver with the rest of the system
  * @dev_priv: device private
@@ -628,7 +640,8 @@ static void i915_driver_register(struct drm_i915_private *dev_priv)
 
 	intel_pxp_debugfs_register(dev_priv->pxp);
 
-	i915_hwmon_register(dev_priv);
+	if (!IS_SRIOV_VF(dev_priv))
+		i915_hwmon_register(dev_priv);
 
 	intel_display_driver_register(dev_priv);
 
@@ -702,6 +715,8 @@ static void i915_welcome_messages(struct drm_i915_private *dev_priv)
 		i915_print_iommu_status(dev_priv, &p);
 		for_each_gt(gt, dev_priv, i)
 			intel_gt_info_print(&gt->info, &p);
+
+		drm_printf(&p, "mode: %s\n", i915_iov_mode_to_string(IOV_MODE(dev_priv)));
 	}
 
 	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG))
@@ -744,6 +759,23 @@ i915_driver_create(struct pci_dev *pdev, const struct pci_device_id *ent)
 	return i915;
 }
 
+static void i915_virtualization_probe(struct drm_i915_private *i915)
+{
+	GEM_BUG_ON(i915->__mode);
+
+	intel_vgpu_detect(i915);
+	if (intel_vgpu_active(i915))
+		i915->__mode = I915_IOV_MODE_GVT_VGPU;
+	else
+		i915->__mode = i915_sriov_probe(i915);
+
+	GEM_BUG_ON(!i915->__mode);
+
+	if (IS_IOV_ACTIVE(i915))
+		dev_info(i915->drm.dev, "Running in %s mode\n",
+			 i915_iov_mode_to_string(IOV_MODE(i915)));
+}
+
 /**
  * i915_driver_probe - setup chip and create an initial config
  * @pdev: PCI device
@@ -768,13 +800,18 @@ int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	if (ret)
 		goto out_fini;
 
+	/* This must be called before any calls to IS/IOV_MODE() macros */
+	i915_virtualization_probe(i915);
+
 	ret = i915_driver_early_probe(i915);
 	if (ret < 0)
 		goto out_pci_disable;
 
 	disable_rpm_wakeref_asserts(&i915->runtime_pm);
 
-	intel_vgpu_detect(i915);
+	ret = i915_sriov_early_tweaks(i915);
+	if (ret < 0)
+		goto out_runtime_pm_put;
 
 	ret = intel_gt_probe_all(i915);
 	if (ret < 0)
@@ -814,6 +851,8 @@ int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	enable_rpm_wakeref_asserts(&i915->runtime_pm);
 
+	i915_virtualization_commit(i915);
+
 	i915_welcome_messages(i915);
 
 	i915->do_release = true;
@@ -1129,6 +1168,8 @@ static int i915_drm_suspend_late(struct drm_device *dev, bool hibernation)
 
 	intel_pxp_suspend(dev_priv->pxp);
 
+	i915_sriov_suspend_late(dev_priv);
+
 	i915_gem_suspend_late(dev_priv);
 
 	for_each_gt(gt, dev_priv, i)
@@ -1334,6 +1375,8 @@ static int i915_drm_resume_early(struct drm_device *dev)
 
 	intel_power_domains_resume(dev_priv);
 
+	i915_sriov_resume_early(dev_priv);
+
 	enable_rpm_wakeref_asserts(&dev_priv->runtime_pm);
 
 	return ret;
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index a7263ff7b333..c5c26afefcd6 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -57,7 +57,10 @@
 #include "i915_params.h"
 #include "i915_perf_types.h"
 #include "i915_scheduler.h"
+#include "i915_sriov.h"
+#include "i915_sriov_types.h"
 #include "i915_utils.h"
+#include "i915_virtualization.h"
 #include "intel_device_info.h"
 #include "intel_memory_region.h"
 #include "intel_runtime_pm.h"
@@ -203,6 +206,14 @@ struct drm_i915_private {
 	/* i915 device parameters */
 	struct i915_params params;
 
+	/* i915 virtualization mode, use IOV_MODE() to access */
+	enum i915_iov_mode __mode;
+#define IOV_MODE(i915) ({				\
+	BUILD_BUG_ON(!I915_IOV_MODE_NONE);		\
+	GEM_BUG_ON(!(i915)->__mode);			\
+	(i915)->__mode;					\
+})
+
 	const struct intel_device_info __info; /* Use INTEL_INFO() to access. */
 	struct intel_runtime_info __runtime; /* Use RUNTIME_INFO() to access. */
 	struct intel_driver_caps caps;
@@ -212,6 +223,7 @@ struct drm_i915_private {
 	struct intel_uncore uncore;
 	struct intel_uncore_mmio_debug mmio_debug;
 
+	struct i915_sriov sriov;
 	struct i915_virtual_gpu vgpu;
 
 	struct intel_gvt *gvt;
@@ -883,6 +895,12 @@ IS_SUBPLATFORM(const struct drm_i915_private *i915,
 
 #define HAS_EXTRA_GT_LIST(dev_priv)   (INTEL_INFO(dev_priv)->extra_gt_list)
 
+#define HAS_SRIOV(dev_priv)	(INTEL_INFO(dev_priv)->has_sriov)
+
+#define HAS_MEMORY_IRQ(dev_priv) (INTEL_INFO(dev_priv)->has_memirq)
+
+#define HAS_MEMORY_IRQ_STATUS(dev_priv) (HAS_MEMORY_IRQ(dev_priv) && IS_SRIOV_VF(dev_priv))
+
 /*
  * Platform has the dedicated compression control state for each lmem surfaces
  * stored in lmem to support the 3D and media compression formats.
diff --git a/drivers/gpu/drm/i915/i915_gpu_error.c b/drivers/gpu/drm/i915/i915_gpu_error.c
index 904f21e1380c..f1785ce1970e 100644
--- a/drivers/gpu/drm/i915/i915_gpu_error.c
+++ b/drivers/gpu/drm/i915/i915_gpu_error.c
@@ -737,6 +737,10 @@ static void err_print_gt_global_nonguc(struct drm_i915_error_state_buf *m,
 	int i;
 
 	err_printf(m, "GT awake: %s\n", str_yes_no(gt->awake));
+
+	if (IS_SRIOV_VF(gt->_gt->i915))
+		return;
+
 	err_printf(m, "CS timestamp frequency: %u Hz, %d ns\n",
 		   gt->clock_frequency, gt->clock_period_ns);
 	err_printf(m, "EIR: 0x%08x\n", gt->eir);
@@ -1219,6 +1223,9 @@ static void engine_record_registers(struct intel_engine_coredump *ee)
 	const struct intel_engine_cs *engine = ee->engine;
 	struct drm_i915_private *i915 = engine->i915;
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	if (GRAPHICS_VER(i915) >= 6) {
 		ee->rc_psmi = ENGINE_READ(engine, RING_PSMI_CTL);
 
@@ -1723,9 +1730,11 @@ gt_record_uc(struct intel_gt_coredump *gt,
 	 * log times to system times (in conjunction with the error->boottime and
 	 * gt->clock_frequency fields saved elsewhere).
 	 */
-	error_uc->guc.timestamp = intel_uncore_read(gt->_gt->uncore, GUCPMTIMESTAMP);
-	error_uc->guc.vma_log = create_vma_coredump(gt->_gt, uc->guc.log.vma,
-						    "GuC log buffer", compress);
+	if (!IS_SRIOV_VF(gt->_gt->i915)) {
+		error_uc->guc.timestamp = intel_uncore_read(gt->_gt->uncore, GUCPMTIMESTAMP);
+		error_uc->guc.vma_log = create_vma_coredump(gt->_gt, uc->guc.log.vma,
+							    "GuC log buffer", compress);
+	}
 	error_uc->guc.vma_ctb = create_vma_coredump(gt->_gt, uc->guc.ct.vma,
 						    "GuC CT buffer", compress);
 	error_uc->guc.last_fence = uc->guc.ct.requests.last_fence;
@@ -1796,6 +1805,9 @@ static void gt_record_global_nonguc_regs(struct intel_gt_coredump *gt)
 		gt->ngtier = 1;
 	}
 
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	gt->eir = intel_uncore_read(uncore, EIR);
 	gt->pgtbl_er = intel_uncore_read(uncore, PGTBL_ER);
 }
@@ -2013,6 +2025,10 @@ intel_gt_coredump_alloc(struct intel_gt *gt, gfp_t gfp, u32 dump_flags)
 	gc->_gt = gt;
 	gc->awake = intel_gt_pm_is_awake(gt);
 
+	/* We can't record anything more on VF */
+	if (IS_SRIOV_VF(gt->i915))
+		return gc;
+
 	gt_record_display_regs(gc);
 	gt_record_global_nonguc_regs(gc);
 
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index 7cfab1b17259..5b3265c835fb 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -48,6 +48,7 @@
 #include "gt/intel_gt_pm_irq.h"
 #include "gt/intel_gt_regs.h"
 #include "gt/intel_rps.h"
+#include "gt/iov/intel_iov_memirq.h"
 
 #include "i915_driver.h"
 #include "i915_drv.h"
@@ -2479,6 +2480,43 @@ static irqreturn_t dg1_irq_handler(int irq, void *arg)
 	return IRQ_HANDLED;
 }
 
+static irqreturn_t vf_mem_irq_handler(int irq, void *arg)
+{
+	struct drm_i915_private * const i915 = arg;
+	struct intel_gt *gt;
+	unsigned int i;
+
+	if (!intel_irqs_enabled(i915))
+		return IRQ_NONE;
+
+	for_each_gt(gt, i915, i)
+		intel_iov_memirq_handler(&gt->iov);
+
+	pmu_irq_stats(i915, IRQ_HANDLED);
+
+	return IRQ_HANDLED;
+}
+
+static void vf_mem_irq_reset(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int i;
+
+	for_each_gt(gt, i915, i)
+		intel_iov_memirq_reset(&gt->iov);
+}
+
+static int vf_mem_irq_postinstall(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int i;
+
+	for_each_gt(gt, i915, i)
+		intel_iov_memirq_postinstall(&gt->iov);
+
+	return 0;
+}
+
 /* Called from drm generic code, passed 'crtc' which
  * we use as a pipe index
  */
@@ -2846,8 +2884,10 @@ static void gen11_irq_reset(struct drm_i915_private *dev_priv)
 	gen11_gt_irq_reset(gt);
 	gen11_display_irq_reset(dev_priv);
 
-	GEN3_IRQ_RESET(uncore, GEN11_GU_MISC_);
-	GEN3_IRQ_RESET(uncore, GEN8_PCU_);
+	if (!IS_SRIOV_VF(dev_priv)) {
+		GEN3_IRQ_RESET(uncore, GEN11_GU_MISC_);
+		GEN3_IRQ_RESET(uncore, GEN8_PCU_);
+	}
 }
 
 static void dg1_irq_reset(struct drm_i915_private *dev_priv)
@@ -3660,7 +3700,8 @@ static void gen11_irq_postinstall(struct drm_i915_private *dev_priv)
 	gen11_gt_irq_postinstall(gt);
 	gen11_de_irq_postinstall(dev_priv);
 
-	GEN3_IRQ_INIT(uncore, GEN11_GU_MISC_, ~gu_misc_masked, gu_misc_masked);
+	if (!IS_SRIOV_VF(dev_priv))
+		GEN3_IRQ_INIT(uncore, GEN11_GU_MISC_, ~gu_misc_masked, gu_misc_masked);
 
 	gen11_master_intr_enable(uncore->regs);
 	intel_uncore_posting_read(&dev_priv->uncore, GEN11_GFX_MSTR_IRQ);
@@ -4246,7 +4287,9 @@ static irq_handler_t intel_irq_handler(struct drm_i915_private *dev_priv)
 		else
 			return i8xx_irq_handler;
 	} else {
-		if (GRAPHICS_VER_FULL(dev_priv) >= IP_VER(12, 10))
+		if (HAS_MEMORY_IRQ_STATUS(dev_priv))
+			return vf_mem_irq_handler;
+		else if (GRAPHICS_VER_FULL(dev_priv) >= IP_VER(12, 10))
 			return dg1_irq_handler;
 		else if (GRAPHICS_VER(dev_priv) >= 11)
 			return gen11_irq_handler;
@@ -4271,7 +4314,9 @@ static void intel_irq_reset(struct drm_i915_private *dev_priv)
 		else
 			i8xx_irq_reset(dev_priv);
 	} else {
-		if (GRAPHICS_VER_FULL(dev_priv) >= IP_VER(12, 10))
+		if (HAS_MEMORY_IRQ_STATUS(dev_priv))
+			vf_mem_irq_reset(dev_priv);
+		else if (GRAPHICS_VER_FULL(dev_priv) >= IP_VER(12, 10))
 			dg1_irq_reset(dev_priv);
 		else if (GRAPHICS_VER(dev_priv) >= 11)
 			gen11_irq_reset(dev_priv);
@@ -4296,7 +4341,9 @@ static void intel_irq_postinstall(struct drm_i915_private *dev_priv)
 		else
 			i8xx_irq_postinstall(dev_priv);
 	} else {
-		if (GRAPHICS_VER_FULL(dev_priv) >= IP_VER(12, 10))
+		if (HAS_MEMORY_IRQ_STATUS(dev_priv))
+			vf_mem_irq_postinstall(dev_priv);
+		else if (GRAPHICS_VER_FULL(dev_priv) >= IP_VER(12, 10))
 			dg1_irq_postinstall(dev_priv);
 		else if (GRAPHICS_VER(dev_priv) >= 11)
 			gen11_irq_postinstall(dev_priv);
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 85ad27d16030..5837b4c8ae60 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -226,6 +226,10 @@ i915_param_named_unsafe(lmem_size, uint, 0400,
 i915_param_named_unsafe(lmem_bar_size, uint, 0400,
 			"Set the lmem bar size(in MiB).");
 
+i915_param_named(max_vfs, uint, 0400,
+	"Limit number of virtual functions to allocate. "
+	"(0 = no VFs [default]; N = allow up to N VFs)");
+
 static void _param_print_bool(struct drm_printer *p, const char *name,
 			      bool val)
 {
diff --git a/drivers/gpu/drm/i915/i915_params.h b/drivers/gpu/drm/i915/i915_params.h
index 3a2e147df02b..145cc4efbe59 100644
--- a/drivers/gpu/drm/i915/i915_params.h
+++ b/drivers/gpu/drm/i915/i915_params.h
@@ -77,6 +77,7 @@ struct drm_printer;
 	param(unsigned int, request_timeout_ms, CONFIG_DRM_I915_REQUEST_TIMEOUT, CONFIG_DRM_I915_REQUEST_TIMEOUT ? 0600 : 0) \
 	param(unsigned int, lmem_size, 0, 0400) \
 	param(unsigned int, lmem_bar_size, 0, 0400) \
+	param(unsigned int, max_vfs, 0, 0400) \
 	/* leave bools at the end to not create holes */ \
 	param(bool, enable_hangcheck, true, 0600) \
 	param(bool, load_detect_test, false, 0600) \
diff --git a/drivers/gpu/drm/i915/i915_pci.c b/drivers/gpu/drm/i915/i915_pci.c
index aeb705389eb8..d9053a0fe3b9 100644
--- a/drivers/gpu/drm/i915/i915_pci.c
+++ b/drivers/gpu/drm/i915/i915_pci.c
@@ -899,6 +899,7 @@ static const struct intel_device_info tgl_info = {
 	.display.has_modular_fia = 1,
 	.__runtime.platform_engine_mask =
 		BIT(RCS0) | BIT(BCS0) | BIT(VECS0) | BIT(VCS0) | BIT(VCS2),
+	.has_sriov = 1,
 };
 
 static const struct intel_device_info rkl_info = {
@@ -945,6 +946,7 @@ static const struct intel_device_info adl_s_info = {
 	.__runtime.platform_engine_mask =
 		BIT(RCS0) | BIT(BCS0) | BIT(VECS0) | BIT(VCS0) | BIT(VCS2),
 	.dma_mask_size = 39,
+	.has_sriov = 1,
 };
 
 #define XE_LPD_FEATURES \
@@ -1002,6 +1004,7 @@ static const struct intel_device_info adl_p_info = {
 		BIT(RCS0) | BIT(BCS0) | BIT(VECS0) | BIT(VCS0) | BIT(VCS2),
 	.__runtime.ppgtt_size = 48,
 	.dma_mask_size = 39,
+	.has_sriov = 1,
 };
 
 #undef GEN
@@ -1149,8 +1152,10 @@ static const struct intel_device_info mtl_info = {
 	.has_gmd_id = 1,
 	.has_guc_deprivilege = 1,
 	.has_llc = 0,
+	.has_memirq = 1,
 	.has_mslice_steering = 0,
 	.has_snoop = 1,
+	.has_sriov = 1,
 	.has_pxp = 1,
 	.__runtime.memory_regions = REGION_SMEM | REGION_STOLEN_LMEM,
 	.__runtime.platform_engine_mask = BIT(RCS0) | BIT(BCS0) | BIT(CCS0),
@@ -1252,6 +1257,9 @@ static void i915_pci_remove(struct pci_dev *pdev)
 	if (!i915) /* driver load aborted, nothing to cleanup */
 		return;
 
+	if (IS_SRIOV_PF(i915))
+		i915_sriov_pf_disable_vfs(i915);
+
 	i915_driver_remove(i915);
 	pci_set_drvdata(pdev, NULL);
 }
@@ -1346,12 +1354,13 @@ static int i915_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		return -ENODEV;
 	}
 
-	/* Only bind to function 0 of the device. Early generations
-	 * used function 1 as a placeholder for multi-head. This causes
-	 * us confusion instead, especially on the systems where both
-	 * functions have the same PCI-ID!
+	/*
+	 * Don't bind to non-zero function, unless it is a virtual function.
+	 * Early generations used function 1 as a placeholder for multi-head.
+	 * This causes us confusion instead, especially on the systems where
+	 * both functions have the same PCI-ID!
 	 */
-	if (PCI_FUNC(pdev->devfn))
+	if (PCI_FUNC(pdev->devfn) && !pdev->is_virtfn)
 		return -ENODEV;
 
 	if (!intel_mmio_bar_valid(pdev, intel_info))
@@ -1389,9 +1398,47 @@ static void i915_pci_shutdown(struct pci_dev *pdev)
 {
 	struct drm_i915_private *i915 = pci_get_drvdata(pdev);
 
+	if (IS_SRIOV_PF(i915))
+		i915_sriov_pf_disable_vfs(i915);
+
 	i915_driver_shutdown(i915);
 }
 
+/**
+ * i915_pci_sriov_configure - Configure SR-IOV (enable/disable VFs).
+ * @pdev: pci_dev struct
+ * @num_vfs: number of VFs to enable (or zero to disable all)
+ *
+ * This function will be called when user requests SR-IOV configuration via the
+ * sysfs interface. Note that VFs configuration can be done only on the PF and
+ * after successful PF initialization.
+ *
+ * Return: number of configured VFs or a negative error code on failure.
+ */
+static int i915_pci_sriov_configure(struct pci_dev *pdev, int num_vfs)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+	struct drm_i915_private *i915 = to_i915(dev);
+	int ret;
+
+	/* handled in drivers/pci/pci-sysfs.c */
+	GEM_BUG_ON(num_vfs < 0);
+	GEM_BUG_ON(num_vfs > U16_MAX);
+	GEM_BUG_ON(num_vfs > pci_sriov_get_totalvfs(pdev));
+	GEM_BUG_ON(num_vfs && pci_num_vf(pdev));
+	GEM_BUG_ON(!num_vfs && !pci_num_vf(pdev));
+
+	if (!IS_SRIOV_PF(i915))
+		return -ENODEV;
+
+	if (num_vfs > 0)
+		ret = i915_sriov_pf_enable_vfs(i915, num_vfs);
+	else
+		ret = i915_sriov_pf_disable_vfs(i915);
+
+	return ret;
+}
+
 static struct pci_driver i915_pci_driver = {
 	.name = DRIVER_NAME,
 	.id_table = pciidlist,
@@ -1399,6 +1446,7 @@ static struct pci_driver i915_pci_driver = {
 	.remove = i915_pci_remove,
 	.shutdown = i915_pci_shutdown,
 	.driver.pm = &i915_pm_ops,
+	.sriov_configure = i915_pci_sriov_configure,
 };
 
 int i915_pci_register_driver(void)
diff --git a/drivers/gpu/drm/i915/i915_perf.c b/drivers/gpu/drm/i915/i915_perf.c
index 824a34ec0b83..1488d64b21dd 100644
--- a/drivers/gpu/drm/i915/i915_perf.c
+++ b/drivers/gpu/drm/i915/i915_perf.c
@@ -4830,6 +4830,10 @@ void i915_perf_init(struct drm_i915_private *i915)
 {
 	struct i915_perf *perf = &i915->perf;
 
+	/* XXX const struct i915_perf_ops! */
+	if (IS_SRIOV_VF(i915))
+		return;
+
 	perf->oa_formats = oa_formats;
 	if (IS_HASWELL(i915)) {
 		perf->ops.is_valid_b_counter_reg = gen7_is_valid_b_counter_addr;
diff --git a/drivers/gpu/drm/i915/i915_pmu.c b/drivers/gpu/drm/i915/i915_pmu.c
index 737d45d11d9f..aa25074dffcf 100644
--- a/drivers/gpu/drm/i915/i915_pmu.c
+++ b/drivers/gpu/drm/i915/i915_pmu.c
@@ -1147,7 +1147,7 @@ void i915_pmu_register(struct drm_i915_private *i915)
 
 	int ret = -ENOMEM;
 
-	if (GRAPHICS_VER(i915) <= 2) {
+	if (GRAPHICS_VER(i915) <= 2 || IS_SRIOV_VF(i915)) {
 		drm_info(&i915->drm, "PMU not supported for this GPU.");
 		return;
 	}
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 5ad0db45631e..ee9956acb19c 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -5590,6 +5590,14 @@
 #define GEN12_DCPR_STATUS_1				_MMIO(0x46440)
 #define  XELPDP_PMDEMAND_INFLIGHT_STATUS		REG_BIT(26)
 
+/* VF_CAPABILITY_REGISTER */
+#define GEN12_VF_CAP_REG		_MMIO(0x1901f8)
+#define   GEN12_VF			REG_BIT(0)
+
+/* VIRTUALIZATION CONTROL REGISTER */
+#define GEN12_VIRTUAL_CTRL_REG		_MMIO(0x10108C)
+#define   GEN12_GUEST_GTT_UPDATE_EN	REG_BIT(8)
+
 #define ILK_DISPLAY_CHICKEN2	_MMIO(0x42004)
 /* Required on all Ironlake and Sandybridge according to the B-Spec. */
 #define  ILK_ELPIN_409_SELECT	(1 << 25)
diff --git a/drivers/gpu/drm/i915/i915_request.c b/drivers/gpu/drm/i915/i915_request.c
index 7503dcb9043b..36d129c04aa8 100644
--- a/drivers/gpu/drm/i915/i915_request.c
+++ b/drivers/gpu/drm/i915/i915_request.c
@@ -2260,7 +2260,7 @@ enum i915_request_state i915_test_request_state(struct i915_request *rq)
 	if (!i915_request_started(rq))
 		return I915_REQUEST_PENDING;
 
-	if (match_ring(rq))
+	if (IS_SRIOV_VF(rq->engine->i915) ? i915_request_is_active(rq) : match_ring(rq))
 		return I915_REQUEST_ACTIVE;
 
 	return I915_REQUEST_QUEUED;
diff --git a/drivers/gpu/drm/i915/i915_sriov.c b/drivers/gpu/drm/i915/i915_sriov.c
new file mode 100644
index 000000000000..719892974082
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_sriov.c
@@ -0,0 +1,732 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2023 Intel Corporation
+ */
+
+#include "i915_sriov.h"
+#include "i915_sriov_sysfs.h"
+#include "i915_drv.h"
+#include "i915_pci.h"
+#include "i915_reg.h"
+#include "intel_pci_config.h"
+
+#include "gt/intel_gt.h"
+#include "gt/intel_gt_pm.h"
+#include "gt/iov/intel_iov_provisioning.h"
+#include "gt/iov/intel_iov_state.h"
+#include "gt/iov/intel_iov_utils.h"
+
+/* safe for use before register access via uncore is completed */
+static u32 pci_peek_mmio_read32(struct pci_dev *pdev, i915_reg_t reg)
+{
+	unsigned long offset = i915_mmio_reg_offset(reg);
+	void __iomem *addr;
+	u32 value;
+
+	addr = pci_iomap_range(pdev, 0, offset, sizeof(u32));
+	if (WARN(!addr, "Failed to map MMIO at %#lx\n", offset))
+		return 0;
+
+	value = readl(addr);
+	pci_iounmap(pdev, addr);
+
+	return value;
+}
+
+static bool gen12_pci_capability_is_vf(struct pci_dev *pdev)
+{
+	u32 value = pci_peek_mmio_read32(pdev, GEN12_VF_CAP_REG);
+
+	/*
+	 * Bugs in PCI programming (or failing hardware) can occasionally cause
+	 * lost access to the MMIO BAR.  When this happens, register reads will
+	 * come back with 0xFFFFFFFF for every register, including VF_CAP, and
+	 * then we may wrongly claim that we are running on the VF device.
+	 * Since VF_CAP has only one bit valid, make sure no other bits are set.
+	 */
+	if (WARN(value & ~GEN12_VF, "MMIO BAR malfunction, %#x returned %#x\n",
+		 i915_mmio_reg_offset(GEN12_VF_CAP_REG), value))
+		return false;
+
+	return value & GEN12_VF;
+}
+
+#ifdef CONFIG_PCI_IOV
+
+static unsigned int wanted_max_vfs(struct drm_i915_private *i915)
+{
+	return i915->params.max_vfs;
+}
+
+static int pf_reduce_totalvfs(struct drm_i915_private *i915, int limit)
+{
+	int err;
+
+	err = pci_sriov_set_totalvfs(to_pci_dev(i915->drm.dev), limit);
+	drm_WARN(&i915->drm, err, "Failed to set number of VFs to %d (%pe)\n",
+		 limit, ERR_PTR(err));
+	return err;
+}
+
+static bool pf_has_valid_vf_bars(struct drm_i915_private *i915)
+{
+	struct pci_dev *pdev = to_pci_dev(i915->drm.dev);
+
+	if (!i915_pci_resource_valid(pdev, GEN12_VF_GTTMMADR_BAR))
+		return false;
+
+	if (HAS_LMEM(i915) && !i915_pci_resource_valid(pdev, GEN12_VF_LMEM_BAR))
+		return false;
+
+	return true;
+}
+
+static bool pf_continue_as_native(struct drm_i915_private *i915, const char *why)
+{
+#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)
+	drm_dbg(&i915->drm, "PF: %s, continuing as native\n", why);
+#endif
+	pf_reduce_totalvfs(i915, 0);
+	return false;
+}
+
+static bool pf_verify_readiness(struct drm_i915_private *i915)
+{
+	struct device *dev = i915->drm.dev;
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int totalvfs = pci_sriov_get_totalvfs(pdev);
+	int newlimit = min_t(u16, wanted_max_vfs(i915), totalvfs);
+
+	GEM_BUG_ON(!dev_is_pf(dev));
+	GEM_WARN_ON(totalvfs > U16_MAX);
+
+	if (!newlimit)
+		return pf_continue_as_native(i915, "all VFs disabled");
+
+	if (!pf_has_valid_vf_bars(i915))
+		return pf_continue_as_native(i915, "VFs BAR not ready");
+
+	pf_reduce_totalvfs(i915, newlimit);
+
+	i915->sriov.pf.device_vfs = totalvfs;
+	i915->sriov.pf.driver_vfs = newlimit;
+
+	return true;
+}
+
+#else
+
+static int pf_reduce_totalvfs(struct drm_i915_private *i915, int limit)
+{
+	return 0;
+}
+
+#endif
+
+/**
+ * i915_sriov_probe - Probe I/O Virtualization mode.
+ * @i915: the i915 struct
+ *
+ * This function should be called once and as soon as possible during
+ * driver probe to detect whether we are driving a PF or a VF device.
+ * SR-IOV PF mode detection is based on PCI @dev_is_pf() function.
+ * SR-IOV VF mode detection is based on MMIO register read.
+ */
+enum i915_iov_mode i915_sriov_probe(struct drm_i915_private *i915)
+{
+	struct device *dev = i915->drm.dev;
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	if (!HAS_SRIOV(i915))
+		return I915_IOV_MODE_NONE;
+
+	if (gen12_pci_capability_is_vf(pdev))
+		return I915_IOV_MODE_SRIOV_VF;
+
+#ifdef CONFIG_PCI_IOV
+	if (dev_is_pf(dev) && pf_verify_readiness(i915))
+		return I915_IOV_MODE_SRIOV_PF;
+#endif
+
+	return I915_IOV_MODE_NONE;
+}
+
+static int vf_check_guc_submission_support(struct drm_i915_private *i915)
+{
+	if (!intel_guc_submission_is_wanted(&to_gt(i915)->uc.guc)) {
+		drm_err(&i915->drm, "GuC submission disabled\n");
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+static void vf_tweak_device_info(struct drm_i915_private *i915)
+{
+	struct intel_device_info *info = mkwrite_device_info(i915);
+	struct intel_runtime_info *rinfo = RUNTIME_INFO(i915);
+
+	/* Force PCH_NOOP. We have no access to display */
+	i915->pch_type = PCH_NOP;
+	memset(&info->display, 0, sizeof(info->display));
+	rinfo->memory_regions &= ~(REGION_STOLEN_SMEM | REGION_STOLEN_LMEM);
+	rinfo->pipe_mask = 0;
+}
+
+/**
+ * i915_sriov_early_tweaks - Perform early tweaks needed for SR-IOV.
+ * @i915: the i915 struct
+ *
+ * This function should be called once and as soon as possible during
+ * driver probe to perform early checks and required tweaks to
+ * the driver data.
+ */
+int i915_sriov_early_tweaks(struct drm_i915_private *i915)
+{
+	int err;
+
+	if (IS_SRIOV_VF(i915)) {
+		err = vf_check_guc_submission_support(i915);
+		if (unlikely(err))
+			return err;
+		vf_tweak_device_info(i915);
+	}
+
+	return 0;
+}
+
+int i915_sriov_pf_get_device_totalvfs(struct drm_i915_private *i915)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	return i915->sriov.pf.device_vfs;
+}
+
+int i915_sriov_pf_get_totalvfs(struct drm_i915_private *i915)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	return i915->sriov.pf.driver_vfs;
+}
+
+static void pf_set_status(struct drm_i915_private *i915, int status)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	GEM_BUG_ON(!status);
+	GEM_WARN_ON(i915->sriov.pf.__status);
+
+	i915->sriov.pf.__status = status;
+}
+
+static bool pf_checklist(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	for_each_gt(gt, i915, id) {
+		if (intel_gt_has_unrecoverable_error(gt)) {
+			pf_update_status(&gt->iov, -EIO, "GT wedged");
+			return false;
+		}
+	}
+
+	return true;
+}
+
+/**
+ * i915_sriov_pf_confirm - Confirm that PF is ready to enable VFs.
+ * @i915: the i915 struct
+ *
+ * This function shall be called by the PF when all necessary
+ * initialization steps were successfully completed and PF is
+ * ready to enable VFs.
+ */
+void i915_sriov_pf_confirm(struct drm_i915_private *i915)
+{
+	struct device *dev = i915->drm.dev;
+	int totalvfs = i915_sriov_pf_get_totalvfs(i915);
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	if (i915_sriov_pf_aborted(i915) || !pf_checklist(i915)) {
+		dev_notice(dev, "No VFs could be associated with this PF!\n");
+		pf_reduce_totalvfs(i915, 0);
+		return;
+	}
+
+	dev_info(dev, "%d VFs could be associated with this PF\n", totalvfs);
+	pf_set_status(i915, totalvfs);
+}
+
+/**
+ * i915_sriov_pf_abort - Abort PF initialization.
+ * @i915: the i915 struct
+ *
+ * This function should be called by the PF when some of the necessary
+ * initialization steps failed and PF won't be able to manage VFs.
+ */
+void i915_sriov_pf_abort(struct drm_i915_private *i915, int err)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	GEM_BUG_ON(err >= 0);
+
+	__i915_printk(i915, KERN_NOTICE, "PF aborted (%pe) %pS\n",
+		      ERR_PTR(err), (void *)_RET_IP_);
+
+	pf_set_status(i915, err);
+}
+
+/**
+ * i915_sriov_pf_aborted - Check if PF initialization was aborted.
+ * @i915: the i915 struct
+ *
+ * This function may be called by the PF to check if any previous
+ * initialization step has failed.
+ *
+ * Return: true if already aborted
+ */
+bool i915_sriov_pf_aborted(struct drm_i915_private *i915)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	return i915->sriov.pf.__status < 0;
+}
+
+/**
+ * i915_sriov_pf_status - Status of the PF initialization.
+ * @i915: the i915 struct
+ *
+ * This function may be called by the PF to get its status.
+ *
+ * Return: number of supported VFs if PF is ready or
+ *         a negative error code on failure (-EBUSY if
+ *         PF initialization is still in progress).
+ */
+int i915_sriov_pf_status(struct drm_i915_private *i915)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	return i915->sriov.pf.__status ?: -EBUSY;
+}
+
+bool i915_sriov_pf_is_auto_provisioning_enabled(struct drm_i915_private *i915)
+{
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	return !i915->sriov.pf.disable_auto_provisioning;
+}
+
+int i915_sriov_pf_set_auto_provisioning(struct drm_i915_private *i915, bool enable)
+{
+	u16 num_vfs = i915_sriov_pf_get_totalvfs(i915);
+	struct intel_gt *gt;
+	unsigned int id;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	if (enable == i915_sriov_pf_is_auto_provisioning_enabled(i915))
+		return 0;
+
+	/* disabling is always allowed */
+	if (!enable)
+		goto set;
+
+	/* enabling is only allowed if all provisioning is empty */
+	for_each_gt(gt, i915, id) {
+		err = intel_iov_provisioning_verify(&gt->iov, num_vfs);
+		if (err == -ENODATA)
+			continue;
+		return -ESTALE;
+	}
+
+set:
+	dev_info(i915->drm.dev, "VFs auto-provisioning was turned %s\n",
+		 str_on_off(enable));
+
+	i915->sriov.pf.disable_auto_provisioning = !enable;
+	return 0;
+}
+
+/**
+ * i915_sriov_print_info - Print SR-IOV information.
+ * @iov: the i915 struct
+ * @p: the DRM printer
+ *
+ * Print SR-IOV related info into provided DRM printer.
+ */
+void i915_sriov_print_info(struct drm_i915_private *i915, struct drm_printer *p)
+{
+	struct device *dev = i915->drm.dev;
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	drm_printf(p, "supported: %s\n", str_yes_no(HAS_SRIOV(i915)));
+	drm_printf(p, "enabled: %s\n", str_yes_no(IS_SRIOV(i915)));
+
+	if (!IS_SRIOV(i915))
+		return;
+
+	drm_printf(p, "mode: %s\n", i915_iov_mode_to_string(IOV_MODE(i915)));
+
+	if (IS_SRIOV_PF(i915)) {
+		int status = i915_sriov_pf_status(i915);
+
+		drm_printf(p, "status: %s\n", str_on_off(status > 0));
+		if (status < 0)
+			drm_printf(p, "error: %d (%pe)\n",
+				   status, ERR_PTR(status));
+
+		drm_printf(p, "device vfs: %u\n", i915_sriov_pf_get_device_totalvfs(i915));
+		drm_printf(p, "driver vfs: %u\n", i915_sriov_pf_get_totalvfs(i915));
+		drm_printf(p, "supported vfs: %u\n", pci_sriov_get_totalvfs(pdev));
+		drm_printf(p, "enabled vfs: %u\n", pci_num_vf(pdev));
+	}
+}
+
+static int pf_update_guc_clients(struct intel_iov *iov, unsigned int num_vfs)
+{
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	err = intel_iov_provisioning_push(iov, num_vfs);
+	if (unlikely(err))
+		IOV_DEBUG(iov, "err=%d", err);
+
+	return err;
+}
+
+/**
+ * i915_sriov_pf_enable_vfs - Enable VFs.
+ * @i915: the i915 struct
+ * @num_vfs: number of VFs to enable (shall not be zero)
+ *
+ * This function will enable specified number of VFs. Note that VFs can be
+ * enabled only after successful PF initialization.
+ * This function shall be called only on PF.
+ *
+ * Return: number of configured VFs or a negative error code on failure.
+ */
+int i915_sriov_pf_enable_vfs(struct drm_i915_private *i915, int num_vfs)
+{
+	bool auto_provisioning = i915_sriov_pf_is_auto_provisioning_enabled(i915);
+	struct device *dev = i915->drm.dev;
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct intel_gt *gt;
+	unsigned int id;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	GEM_BUG_ON(num_vfs < 0);
+	drm_dbg(&i915->drm, "enabling %d VFs\n", num_vfs);
+
+	/* verify that all initialization was successfully completed */
+	err = i915_sriov_pf_status(i915);
+	if (err < 0)
+		goto fail;
+
+	/* hold the reference to runtime pm as long as VFs are enabled */
+	for_each_gt(gt, i915, id)
+		intel_gt_pm_get_untracked(gt);
+
+	for_each_gt(gt, i915, id) {
+		err = intel_iov_provisioning_verify(&gt->iov, num_vfs);
+		if (err == -ENODATA) {
+			if (auto_provisioning)
+				err = intel_iov_provisioning_auto(&gt->iov, num_vfs);
+			else
+				err = 0; /* trust late provisioning */
+		}
+		if (unlikely(err))
+			goto fail_pm;
+	}
+
+	for_each_gt(gt, i915, id) {
+		err = pf_update_guc_clients(&gt->iov, num_vfs);
+		if (unlikely(err < 0))
+			goto fail_pm;
+	}
+
+	err = pci_enable_sriov(pdev, num_vfs);
+	if (err < 0)
+		goto fail_guc;
+
+	i915_sriov_sysfs_update_links(i915, true);
+
+	dev_info(dev, "Enabled %u VFs\n", num_vfs);
+	return num_vfs;
+
+fail_guc:
+	for_each_gt(gt, i915, id)
+		pf_update_guc_clients(&gt->iov, 0);
+fail_pm:
+	for_each_gt(gt, i915, id) {
+		intel_iov_provisioning_auto(&gt->iov, 0);
+		intel_gt_pm_put_untracked(gt);
+	}
+fail:
+	drm_err(&i915->drm, "Failed to enable %u VFs (%pe)\n",
+		num_vfs, ERR_PTR(err));
+	return err;
+}
+
+static void pf_start_vfs_flr(struct intel_iov *iov, unsigned int num_vfs)
+{
+	unsigned int n;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	for (n = 1; n <= num_vfs; n++)
+		intel_iov_state_start_flr(iov, n);
+}
+
+#define I915_VF_FLR_TIMEOUT_MS 500
+
+static void pf_wait_vfs_flr(struct intel_iov *iov, unsigned int num_vfs)
+{
+	unsigned int timeout_ms = I915_VF_FLR_TIMEOUT_MS;
+	unsigned int n;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	for (n = 1; n <= num_vfs; n++) {
+		if (wait_for(intel_iov_state_no_flr(iov, n), timeout_ms)) {
+			IOV_ERROR(iov, "VF%u FLR didn't complete within %u ms\n",
+				  n, timeout_ms);
+			timeout_ms /= 2;
+		}
+	}
+}
+
+/**
+ * i915_sriov_pf_disable_vfs - Disable VFs.
+ * @i915: the i915 struct
+ *
+ * This function will disable all previously enabled VFs.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_pf_disable_vfs(struct drm_i915_private *i915)
+{
+	struct device *dev = i915->drm.dev;
+	struct pci_dev *pdev = to_pci_dev(dev);
+	u16 num_vfs = pci_num_vf(pdev);
+	u16 vfs_assigned = pci_vfs_assigned(pdev);
+	struct intel_gt *gt;
+	unsigned int id;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	drm_dbg(&i915->drm, "disabling %u VFs\n", num_vfs);
+
+	if (vfs_assigned) {
+		dev_warn(dev, "Can't disable %u VFs, %u are still assigned\n",
+			 num_vfs, vfs_assigned);
+		return -EPERM;
+	}
+
+	if (!num_vfs)
+		return 0;
+
+	i915_sriov_sysfs_update_links(i915, false);
+
+	pci_disable_sriov(pdev);
+
+	for_each_gt(gt, i915, id)
+		pf_start_vfs_flr(&gt->iov, num_vfs);
+	for_each_gt(gt, i915, id)
+		pf_wait_vfs_flr(&gt->iov, num_vfs);
+
+	for_each_gt(gt, i915, id) {
+		pf_update_guc_clients(&gt->iov, 0);
+		intel_iov_provisioning_auto(&gt->iov, 0);
+	}
+
+	for_each_gt(gt, i915, id)
+		intel_gt_pm_put_untracked(gt);
+
+	dev_info(dev, "Disabled %u VFs\n", num_vfs);
+	return 0;
+}
+
+/**
+ * i915_sriov_pf_stop_vf - Stop VF.
+ * @i915: the i915 struct
+ * @vfid: VF identifier
+ *
+ * This function will stop VF on all tiles.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_pf_stop_vf(struct drm_i915_private *i915, unsigned int vfid)
+{
+	struct device *dev = i915->drm.dev;
+	struct intel_gt *gt;
+	unsigned int id;
+	int result = 0;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	for_each_gt(gt, i915, id) {
+		err = intel_iov_state_stop_vf(&gt->iov, vfid);
+		if (unlikely(err)) {
+			dev_warn(dev, "Failed to stop VF%u on gt%u (%pe)\n",
+				 vfid, id, ERR_PTR(err));
+			result = result ?: err;
+		}
+	}
+
+	return result;
+}
+
+/**
+ * i915_sriov_pf_pause_vf - Pause VF.
+ * @i915: the i915 struct
+ * @vfid: VF identifier
+ *
+ * This function will pause VF on all tiles.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_pf_pause_vf(struct drm_i915_private *i915, unsigned int vfid)
+{
+	struct device *dev = i915->drm.dev;
+	struct intel_gt *gt;
+	unsigned int id;
+	int result = 0;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	for_each_gt(gt, i915, id) {
+		err = intel_iov_state_pause_vf(&gt->iov, vfid);
+		if (unlikely(err)) {
+			dev_warn(dev, "Failed to pause VF%u on gt%u (%pe)\n",
+				 vfid, id, ERR_PTR(err));
+			result = result ?: err;
+		}
+	}
+
+	return result;
+}
+
+/**
+ * i915_sriov_pf_resume_vf - Resume VF.
+ * @i915: the i915 struct
+ * @vfid: VF identifier
+ *
+ * This function will resume VF on all tiles.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_pf_resume_vf(struct drm_i915_private *i915, unsigned int vfid)
+{
+	struct device *dev = i915->drm.dev;
+	struct intel_gt *gt;
+	unsigned int id;
+	int result = 0;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	for_each_gt(gt, i915, id) {
+		err = intel_iov_state_resume_vf(&gt->iov, vfid);
+		if (unlikely(err)) {
+			dev_warn(dev, "Failed to resume VF%u on gt%u (%pe)\n",
+				 vfid, id, ERR_PTR(err));
+			result = result ?: err;
+		}
+	}
+
+	return result;
+}
+
+/**
+ * i915_sriov_pf_clear_vf - Unprovision VF.
+ * @i915: the i915 struct
+ * @vfid: VF identifier
+ *
+ * This function will uprovision VF on all tiles.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_pf_clear_vf(struct drm_i915_private *i915, unsigned int vfid)
+{
+	struct device *dev = i915->drm.dev;
+	struct intel_gt *gt;
+	unsigned int id;
+	int result = 0;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	for_each_gt(gt, i915, id) {
+		err = intel_iov_provisioning_clear(&gt->iov, vfid);
+		if (unlikely(err)) {
+			dev_warn(dev, "Failed to unprovision VF%u on gt%u (%pe)\n",
+				 vfid, id, ERR_PTR(err));
+			result = result ?: err;
+		}
+	}
+
+	return result;
+}
+
+/**
+ * i915_sriov_suspend_late - Suspend late SR-IOV.
+ * @i915: the i915 struct
+ *
+ * The function is called in a callback suspend_late.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_suspend_late(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	if (IS_SRIOV_PF(i915)) {
+		/*
+		 * When we're enabling the VFs in i915_sriov_pf_enable_vfs(), we also get
+		 * a GT PM wakeref which we hold for the whole VFs life cycle.
+		 * However for the time of suspend this wakeref must be put back.
+		 * We'll get it back during the resume in i915_sriov_resume_early().
+		 */
+		if (pci_num_vf(to_pci_dev(i915->drm.dev)) != 0) {
+			for_each_gt(gt, i915, id)
+				intel_gt_pm_put_untracked(gt);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * i915_sriov_resume_early - Resume early SR-IOV.
+ * @i915: the i915 struct
+ *
+ * The function is called in a callback resume_early.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_resume_early(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	if (IS_SRIOV_PF(i915)) {
+		/*
+		 * When we're enabling the VFs in i915_sriov_pf_enable_vfs(), we also get
+		 * a GT PM wakeref which we hold for the whole VFs life cycle.
+		 * However for the time of suspend this wakeref must be put back.
+		 * If we have VFs enabled, now is the moment at which we get back this wakeref.
+		 */
+		if (pci_num_vf(to_pci_dev(i915->drm.dev)) != 0) {
+			for_each_gt(gt, i915, id)
+				intel_gt_pm_get_untracked(gt);
+		}
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/i915_sriov.h b/drivers/gpu/drm/i915/i915_sriov.h
new file mode 100644
index 000000000000..ee03d43ae6e0
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_sriov.h
@@ -0,0 +1,47 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __I915_SRIOV_H__
+#define __I915_SRIOV_H__
+
+#include "i915_drv.h"
+#include "i915_virtualization.h"
+
+struct drm_i915_private;
+struct drm_printer;
+
+#ifdef CONFIG_PCI_IOV
+#define IS_SRIOV_PF(i915) (IOV_MODE(i915) == I915_IOV_MODE_SRIOV_PF)
+#else
+#define IS_SRIOV_PF(i915) false
+#endif
+#define IS_SRIOV_VF(i915) (IOV_MODE(i915) == I915_IOV_MODE_SRIOV_VF)
+
+#define IS_SRIOV(i915) (IS_SRIOV_PF(i915) || IS_SRIOV_VF(i915))
+
+enum i915_iov_mode i915_sriov_probe(struct drm_i915_private *i915);
+int i915_sriov_early_tweaks(struct drm_i915_private *i915);
+void i915_sriov_print_info(struct drm_i915_private *i915, struct drm_printer *p);
+
+/* PF only */
+void i915_sriov_pf_confirm(struct drm_i915_private *i915);
+void i915_sriov_pf_abort(struct drm_i915_private *i915, int err);
+bool i915_sriov_pf_aborted(struct drm_i915_private *i915);
+int i915_sriov_pf_status(struct drm_i915_private *i915);
+int i915_sriov_pf_get_device_totalvfs(struct drm_i915_private *i915);
+int i915_sriov_pf_get_totalvfs(struct drm_i915_private *i915);
+int i915_sriov_pf_enable_vfs(struct drm_i915_private *i915, int numvfs);
+int i915_sriov_pf_disable_vfs(struct drm_i915_private *i915);
+int i915_sriov_pf_stop_vf(struct drm_i915_private *i915, unsigned int vfid);
+int i915_sriov_pf_pause_vf(struct drm_i915_private *i915, unsigned int vfid);
+int i915_sriov_pf_resume_vf(struct drm_i915_private *i915, unsigned int vfid);
+int i915_sriov_pf_clear_vf(struct drm_i915_private *i915, unsigned int vfid);
+
+bool i915_sriov_pf_is_auto_provisioning_enabled(struct drm_i915_private *i915);
+int i915_sriov_pf_set_auto_provisioning(struct drm_i915_private *i915, bool enable);
+
+int i915_sriov_suspend_late(struct drm_i915_private *i915);
+int i915_sriov_resume_early(struct drm_i915_private *i915);
+#endif /* __I915_SRIOV_H__ */
diff --git a/drivers/gpu/drm/i915/i915_sriov_sysfs.c b/drivers/gpu/drm/i915/i915_sriov_sysfs.c
new file mode 100644
index 000000000000..94217e4c3228
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_sriov_sysfs.c
@@ -0,0 +1,616 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "i915_drv.h"
+#include "i915_sriov_sysfs.h"
+#include "i915_sriov_sysfs_types.h"
+#include "i915_sysfs.h"
+
+/*
+ * /sys/class/drm/card*
+ *  iov/
+ *      ...
+ *      pf/
+ *       ...
+ *      vf1/
+ *       ...
+ */
+
+#define SRIOV_PRELIMINARY "prelim_"
+#define SRIOV_KOBJ_HOME_NAME SRIOV_PRELIMINARY "iov"
+#define SRIOV_EXT_KOBJ_PF_NAME "pf"
+#define SRIOV_EXT_KOBJ_VFn_NAME "vf%u"
+#define SRIOV_DEVICE_LINK_NAME "device"
+
+struct drm_i915_private *sriov_kobj_to_i915(struct i915_sriov_kobj *kobj)
+{
+	struct device *kdev = kobj_to_dev(kobj->base.parent);
+	struct drm_i915_private *i915 = kdev_minor_to_i915(kdev);
+
+	return i915;
+}
+
+struct drm_i915_private *sriov_ext_kobj_to_i915(struct i915_sriov_ext_kobj *kobj)
+{
+	return sriov_kobj_to_i915(to_sriov_kobj(kobj->base.parent));
+}
+
+static inline bool sriov_ext_kobj_is_pf(struct i915_sriov_ext_kobj *kobj)
+{
+	return !kobj->id;
+}
+
+/* core SR-IOV attributes */
+
+static ssize_t mode_sriov_attr_show(struct drm_i915_private *i915, char *buf)
+{
+	return sysfs_emit(buf, "%s\n", i915_iov_mode_to_string(IOV_MODE(i915)));
+}
+
+I915_SRIOV_ATTR_RO(mode);
+
+static struct attribute *sriov_attrs[] = {
+	&mode_sriov_attr.attr,
+	NULL
+};
+
+static const struct attribute_group sriov_attr_group = {
+	.attrs = sriov_attrs,
+};
+
+static const struct attribute_group *default_sriov_attr_groups[] = {
+	&sriov_attr_group,
+	NULL
+};
+
+/* extended (PF and VFs) SR-IOV attributes */
+
+static ssize_t auto_provisioning_sriov_ext_attr_show(struct drm_i915_private *i915,
+						     unsigned int id, char *buf)
+{
+	int value = i915_sriov_pf_is_auto_provisioning_enabled(i915);
+
+	return sysfs_emit(buf, "%d\n", value);
+}
+
+static ssize_t auto_provisioning_sriov_ext_attr_store(struct drm_i915_private *i915,
+						      unsigned int id,
+						      const char *buf, size_t count)
+{
+	bool value;
+	int err;
+
+	err = kstrtobool(buf, &value);
+	if (err)
+		return err;
+
+	err = i915_sriov_pf_set_auto_provisioning(i915, value);
+	return err ?: count;
+}
+
+I915_SRIOV_EXT_ATTR(auto_provisioning);
+
+static ssize_t id_sriov_ext_attr_show(struct drm_i915_private *i915,
+				      unsigned int id, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", id);
+}
+
+#define CONTROL_STOP "stop"
+#define CONTROL_PAUSE "pause"
+#define CONTROL_RESUME "resume"
+#define CONTROL_CLEAR "clear"
+
+static ssize_t control_sriov_ext_attr_store(struct drm_i915_private *i915,
+					    unsigned int id,
+					    const char *buf, size_t count)
+{
+	int err = -EPERM;
+
+	if (sysfs_streq(buf, CONTROL_STOP)) {
+		err = i915_sriov_pf_stop_vf(i915, id);
+	} else if (sysfs_streq(buf, CONTROL_PAUSE)) {
+		err = i915_sriov_pf_pause_vf(i915, id);
+	} else if (sysfs_streq(buf, CONTROL_RESUME)) {
+		err = i915_sriov_pf_resume_vf(i915, id);
+	} else if (sysfs_streq(buf, CONTROL_CLEAR)) {
+		err = i915_sriov_pf_clear_vf(i915, id);
+	} else {
+		err = -EINVAL;
+	}
+
+	return err ?: count;
+}
+
+I915_SRIOV_EXT_ATTR_RO(id);
+I915_SRIOV_EXT_ATTR_WO(control);
+
+static struct attribute *sriov_ext_attrs[] = {
+	NULL
+};
+
+static const struct attribute_group sriov_ext_attr_group = {
+	.attrs = sriov_ext_attrs,
+};
+
+static struct attribute *pf_ext_attrs[] = {
+	&auto_provisioning_sriov_ext_attr.attr,
+	NULL
+};
+
+static umode_t pf_ext_attr_is_visible(struct kobject *kobj,
+				      struct attribute *attr, int index)
+{
+	struct i915_sriov_ext_kobj *sriov_kobj = to_sriov_ext_kobj(kobj);
+
+	if (!sriov_ext_kobj_is_pf(sriov_kobj))
+		return 0;
+
+	return attr->mode;
+}
+
+static const struct attribute_group pf_ext_attr_group = {
+	.attrs = pf_ext_attrs,
+	.is_visible = pf_ext_attr_is_visible,
+};
+
+static struct attribute *vf_ext_attrs[] = {
+	&id_sriov_ext_attr.attr,
+	&control_sriov_ext_attr.attr,
+	NULL
+};
+
+static umode_t vf_ext_attr_is_visible(struct kobject *kobj,
+				      struct attribute *attr, int index)
+{
+	struct i915_sriov_ext_kobj *sriov_kobj = to_sriov_ext_kobj(kobj);
+
+	if (sriov_ext_kobj_is_pf(sriov_kobj))
+		return 0;
+
+	return attr->mode;
+}
+
+static const struct attribute_group vf_ext_attr_group = {
+	.attrs = vf_ext_attrs,
+	.is_visible = vf_ext_attr_is_visible,
+};
+
+static const struct attribute_group *default_sriov_ext_attr_groups[] = {
+	&sriov_ext_attr_group,
+	&pf_ext_attr_group,
+	&vf_ext_attr_group,
+	NULL,
+};
+
+/* no user serviceable parts below */
+
+static ssize_t sriov_attr_show(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	struct drm_i915_private *i915 = sriov_kobj_to_i915(to_sriov_kobj(kobj));
+	struct i915_sriov_attr *sriov_attr = to_sriov_attr(attr);
+
+	return sriov_attr->show ? sriov_attr->show(i915, buf) : -EIO;
+}
+
+static ssize_t sriov_attr_store(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	struct drm_i915_private *i915 = sriov_kobj_to_i915(to_sriov_kobj(kobj));
+	struct i915_sriov_attr *sriov_attr = to_sriov_attr(attr);
+
+	return sriov_attr->store ? sriov_attr->store(i915, buf, count) : -EIO;
+}
+
+static const struct sysfs_ops sriov_sysfs_ops = {
+	.show = sriov_attr_show,
+	.store = sriov_attr_store,
+};
+
+static void sriov_kobj_release(struct kobject *kobj)
+{
+	struct i915_sriov_kobj *sriov_kobj = to_sriov_kobj(kobj);
+
+	kfree(sriov_kobj);
+}
+
+static struct kobj_type sriov_ktype = {
+	.release = sriov_kobj_release,
+	.sysfs_ops = &sriov_sysfs_ops,
+	.default_groups = default_sriov_attr_groups,
+};
+
+static ssize_t sriov_ext_attr_show(struct kobject *kobj, struct attribute *attr,
+				   char *buf)
+{
+	struct i915_sriov_ext_kobj *sriov_kobj = to_sriov_ext_kobj(kobj);
+	struct i915_sriov_ext_attr *sriov_attr = to_sriov_ext_attr(attr);
+	struct drm_i915_private *i915 = sriov_ext_kobj_to_i915(sriov_kobj);
+	unsigned int id = sriov_kobj->id;
+
+	return sriov_attr->show ? sriov_attr->show(i915, id, buf) : -EIO;
+}
+
+static ssize_t sriov_ext_attr_store(struct kobject *kobj, struct attribute *attr,
+				    const char *buf, size_t count)
+{
+	struct i915_sriov_ext_kobj *sriov_kobj = to_sriov_ext_kobj(kobj);
+	struct i915_sriov_ext_attr *sriov_attr = to_sriov_ext_attr(attr);
+	struct drm_i915_private *i915 = sriov_ext_kobj_to_i915(sriov_kobj);
+	unsigned int id = sriov_kobj->id;
+
+	return sriov_attr->store ? sriov_attr->store(i915, id, buf, count) : -EIO;
+}
+
+static const struct sysfs_ops sriov_ext_sysfs_ops = {
+	.show = sriov_ext_attr_show,
+	.store = sriov_ext_attr_store,
+};
+
+static void sriov_ext_kobj_release(struct kobject *kobj)
+{
+	struct i915_sriov_ext_kobj *sriov_kobj = to_sriov_ext_kobj(kobj);
+
+	kfree(sriov_kobj);
+}
+
+static struct kobj_type sriov_ext_ktype = {
+	.release = sriov_ext_kobj_release,
+	.sysfs_ops = &sriov_ext_sysfs_ops,
+	.default_groups = default_sriov_ext_attr_groups,
+};
+
+static unsigned int pf_nodes_count(struct drm_i915_private *i915)
+{
+	/* 1 x PF + n x VFs */
+	return 1 + i915_sriov_pf_get_totalvfs(i915);
+}
+
+static int pf_setup_failed(struct drm_i915_private *i915, int err, const char *what)
+{
+	i915_probe_error(i915, "Failed to setup SR-IOV sysfs %s (%pe)\n",
+			 what, ERR_PTR(err));
+	return err;
+}
+
+static int pf_setup_home(struct drm_i915_private *i915)
+{
+	struct device *kdev = i915->drm.primary->kdev;
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_kobj *home = pf->sysfs.home;
+	int err;
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+	GEM_BUG_ON(home);
+
+	err = i915_inject_probe_error(i915, -ENOMEM);
+	if (unlikely(err))
+		goto failed;
+
+	home = kzalloc(sizeof(*home), GFP_KERNEL);
+	if (unlikely(!home)) {
+		err = -ENOMEM;
+		goto failed;
+	}
+
+	err = kobject_init_and_add(&home->base, &sriov_ktype, &kdev->kobj, SRIOV_KOBJ_HOME_NAME);
+	if (unlikely(err)) {
+		goto failed_init;
+	}
+
+	GEM_BUG_ON(pf->sysfs.home);
+	pf->sysfs.home = home;
+	return 0;
+
+failed_init:
+	kobject_put(&home->base);
+failed:
+	return pf_setup_failed(i915, err, "home");
+}
+
+static void pf_teardown_home(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_kobj *home = fetch_and_zero(&pf->sysfs.home);
+
+	if (home)
+		kobject_put(&home->base);
+}
+
+static int pf_setup_tree(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_kobj *home = pf->sysfs.home;
+	struct i915_sriov_ext_kobj **kobjs;
+	struct i915_sriov_ext_kobj *kobj;
+	unsigned int count = pf_nodes_count(i915);
+	unsigned int n;
+	int err;
+
+	err = i915_inject_probe_error(i915, -ENOMEM);
+	if (unlikely(err))
+		goto failed;
+
+	kobjs = kcalloc(count, sizeof(*kobjs), GFP_KERNEL);
+	if (unlikely(!kobjs)) {
+		err = -ENOMEM;
+		goto failed;
+	}
+
+	for (n = 0; n < count; n++) {
+		kobj = kzalloc(sizeof(*kobj), GFP_KERNEL);
+		if (!kobj) {
+			err = -ENOMEM;
+			goto failed_kobj_n;
+		}
+
+		kobj->id = n;
+		if (n) {
+			err = kobject_init_and_add(&kobj->base, &sriov_ext_ktype,
+						   &home->base, SRIOV_EXT_KOBJ_VFn_NAME, n);
+		} else {
+			err = kobject_init_and_add(&kobj->base, &sriov_ext_ktype,
+						   &home->base, SRIOV_EXT_KOBJ_PF_NAME);
+		}
+		if (unlikely(err))
+			goto failed_kobj_n;
+
+		err = i915_inject_probe_error(i915, -EEXIST);
+		if (unlikely(err))
+			goto failed_kobj_n;
+
+		kobjs[n] = kobj;
+	}
+
+	GEM_BUG_ON(pf->sysfs.kobjs);
+	pf->sysfs.kobjs = kobjs;
+	return 0;
+
+failed_kobj_n:
+	if (kobj)
+		kobject_put(&kobj->base);
+	while (n--)
+		kobject_put(&kobjs[n]->base);
+failed:
+	return pf_setup_failed(i915, err, "tree");
+}
+
+static void pf_teardown_tree(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_ext_kobj **kobjs = fetch_and_zero(&pf->sysfs.kobjs);
+	unsigned int count = pf_nodes_count(i915);
+	unsigned int n;
+
+	if (!kobjs)
+		return;
+
+	for (n = 0; n < count; n++)
+		kobject_put(&kobjs[n]->base);
+
+	kfree(kobjs);
+}
+
+static int pf_setup_device_link(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_ext_kobj **kobjs = pf->sysfs.kobjs;
+	int err;
+
+	err = i915_inject_probe_error(i915, -EEXIST);
+	if (unlikely(err))
+		goto failed;
+
+	err = sysfs_create_link(&kobjs[0]->base, &i915->drm.dev->kobj, SRIOV_DEVICE_LINK_NAME);
+	if (unlikely(err))
+		goto failed;
+
+	return 0;
+
+failed:
+	return pf_setup_failed(i915, err, "link");
+}
+
+static void pf_teardown_device_link(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_ext_kobj **kobjs = pf->sysfs.kobjs;
+
+	sysfs_remove_link(&kobjs[0]->base, SRIOV_DEVICE_LINK_NAME);
+}
+
+static void pf_welcome(struct drm_i915_private *i915)
+{
+#if IS_ENABLED(CONFIG_DRM_I915_DEBUG)
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	const char *path = kobject_get_path(&pf->sysfs.home->base, GFP_KERNEL);
+
+	drm_dbg(&i915->drm, "SR-IOV sysfs available at /sys%s\n", path);
+	kfree(path);
+#endif
+	GEM_BUG_ON(!i915->sriov.pf.sysfs.kobjs);
+}
+
+static void pf_goodbye(struct drm_i915_private *i915)
+{
+	GEM_WARN_ON(i915->sriov.pf.sysfs.kobjs);
+	GEM_WARN_ON(i915->sriov.pf.sysfs.home);
+}
+
+static bool pf_initialized(struct drm_i915_private *i915)
+{
+	GEM_WARN_ON(i915->sriov.pf.sysfs.home && !i915->sriov.pf.sysfs.kobjs);
+	GEM_WARN_ON(!i915->sriov.pf.sysfs.home && i915->sriov.pf.sysfs.kobjs);
+	return i915->sriov.pf.sysfs.home;
+}
+
+/**
+ * i915_sriov_sysfs_setup - Setup SR-IOV sysfs tree.
+ * @i915: the i915 struct
+ *
+ * On SR-IOV PF this function will setup dedicated sysfs tree
+ * with PF and VFs attributes.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_sysfs_setup(struct drm_i915_private *i915)
+{
+	int err;
+
+	if (!IS_SRIOV_PF(i915))
+		return 0;
+
+	if (i915_sriov_pf_aborted(i915))
+		return 0;
+
+	err = pf_setup_home(i915);
+	if (unlikely(err))
+		goto failed;
+
+	err = pf_setup_tree(i915);
+	if (unlikely(err))
+		goto failed_tree;
+
+	err = pf_setup_device_link(i915);
+	if (unlikely(err))
+		goto failed_link;
+
+	pf_welcome(i915);
+	return 0;
+
+failed_link:
+	pf_teardown_tree(i915);
+failed_tree:
+	pf_teardown_home(i915);
+failed:
+	return pf_setup_failed(i915, err, "");
+}
+
+/**
+ * i915_sriov_sysfs_teardown - Cleanup SR-IOV sysfs tree.
+ * @i915: the i915 struct
+ *
+ * Cleanup data initialized by @i915_sriov_sysfs_setup.
+ */
+void i915_sriov_sysfs_teardown(struct drm_i915_private *i915)
+{
+	if (!IS_SRIOV_PF(i915))
+		return;
+
+	if (!pf_initialized(i915))
+		return;
+
+	pf_teardown_device_link(i915);
+	pf_teardown_tree(i915);
+	pf_teardown_home(i915);
+	pf_goodbye(i915);
+}
+
+/* our Gen12 SR-IOV platforms are simple */
+#define GEN12_VF_OFFSET 1
+#define GEN12_VF_STRIDE 1
+#define GEN12_VF_ROUTING_OFFSET(id) (GEN12_VF_OFFSET + ((id) - 1) * GEN12_VF_STRIDE)
+
+static struct pci_dev *pf_get_vf_pci_dev(struct drm_i915_private *i915, unsigned int id)
+{
+	struct pci_dev *pdev = to_pci_dev(i915->drm.dev);
+	u16 vf_devid = pci_dev_id(pdev) + GEN12_VF_ROUTING_OFFSET(id);
+
+	GEM_BUG_ON(!dev_is_pf(&pdev->dev));
+	GEM_BUG_ON(!id);
+
+	/* caller must use pci_dev_put() */
+	return pci_get_domain_bus_and_slot(pci_domain_nr(pdev->bus),
+					   PCI_BUS_NUM(vf_devid),
+					   PCI_DEVFN(PCI_SLOT(vf_devid),
+						     PCI_FUNC(vf_devid)));
+}
+
+static int pf_add_vfs_device_links(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_ext_kobj **kobjs = pf->sysfs.kobjs;
+	struct pci_dev *pf_pdev = to_pci_dev(i915->drm.dev);
+	struct pci_dev *vf_pdev = NULL;
+	unsigned int numvfs = pci_num_vf(pf_pdev);
+	unsigned int n;
+	int err;
+
+	if (!kobjs)
+		return 0;
+
+	GEM_BUG_ON(numvfs > pf_nodes_count(i915));
+
+	for (n = 1; n <= numvfs; n++) {
+
+		err = i915_inject_probe_error(i915, -ENODEV);
+		if (unlikely(err)) {
+			vf_pdev = NULL;
+			goto failed_n;
+		}
+
+		vf_pdev = pf_get_vf_pci_dev(i915, n);
+		if (unlikely(!vf_pdev)) {
+			err = -ENODEV;
+			goto failed_n;
+		}
+
+		err = i915_inject_probe_error(i915, -EEXIST);
+		if (unlikely(err))
+			goto failed_n;
+
+		err = sysfs_create_link(&kobjs[n]->base, &vf_pdev->dev.kobj,
+					SRIOV_DEVICE_LINK_NAME);
+		if (unlikely(err))
+			goto failed_n;
+
+		/* balance pf_get_vf_pci_dev() */
+		pci_dev_put(vf_pdev);
+	}
+
+	return 0;
+
+failed_n:
+	if (vf_pdev)
+		pci_dev_put(vf_pdev);
+	while (n-- > 1)
+		sysfs_remove_link(&kobjs[n]->base, SRIOV_DEVICE_LINK_NAME);
+
+	return pf_setup_failed(i915, err, "links");
+}
+
+static void pf_remove_vfs_device_links(struct drm_i915_private *i915)
+{
+	struct i915_sriov_pf *pf = &i915->sriov.pf;
+	struct i915_sriov_ext_kobj **kobjs = pf->sysfs.kobjs;
+	struct pci_dev *pf_pdev = to_pci_dev(i915->drm.dev);
+	unsigned int numvfs = pci_num_vf(pf_pdev);
+	unsigned int n;
+
+	if (!kobjs)
+		return;
+
+	GEM_BUG_ON(numvfs > pf_nodes_count(i915));
+
+	for (n = 1; n <= numvfs; n++)
+		sysfs_remove_link(&kobjs[n]->base, SRIOV_DEVICE_LINK_NAME);
+}
+
+/**
+ * i915_sriov_sysfs_update_links - Update links in SR-IOV sysfs tree.
+ * @i915: the i915 struct
+ *
+ * On PF this function will add or remove PCI device links from VFs.
+ */
+void i915_sriov_sysfs_update_links(struct drm_i915_private *i915, bool add)
+{
+	if (!IS_SRIOV_PF(i915))
+		return;
+
+	if (add)
+		pf_add_vfs_device_links(i915);
+	else
+		pf_remove_vfs_device_links(i915);
+}
diff --git a/drivers/gpu/drm/i915/i915_sriov_sysfs.h b/drivers/gpu/drm/i915/i915_sriov_sysfs.h
new file mode 100644
index 000000000000..7fab9f795fba
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_sriov_sysfs.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __I915_SRIOV_SYSFS_H__
+#define __I915_SRIOV_SYSFS_H__
+
+#include "i915_sriov_sysfs_types.h"
+
+int i915_sriov_sysfs_setup(struct drm_i915_private *i915);
+void i915_sriov_sysfs_teardown(struct drm_i915_private *i915);
+void i915_sriov_sysfs_update_links(struct drm_i915_private *i915, bool add);
+
+struct drm_i915_private *sriov_kobj_to_i915(struct i915_sriov_kobj *kobj);
+struct drm_i915_private *sriov_ext_kobj_to_i915(struct i915_sriov_ext_kobj *kobj);
+
+#endif /* __I915_SRIOV_SYSFS_H__ */
diff --git a/drivers/gpu/drm/i915/i915_sriov_sysfs_types.h b/drivers/gpu/drm/i915/i915_sriov_sysfs_types.h
new file mode 100644
index 000000000000..aabfadd3c374
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_sriov_sysfs_types.h
@@ -0,0 +1,59 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __I915_SRIOV_SYSFS_TYPES_H__
+#define __I915_SRIOV_SYSFS_TYPES_H__
+
+#include <linux/kobject.h>
+
+struct drm_i915_private;
+
+struct i915_sriov_kobj {
+	struct kobject base;
+};
+#define to_sriov_kobj(x) container_of(x, struct i915_sriov_kobj, base)
+
+struct i915_sriov_attr {
+	struct attribute attr;
+	ssize_t (*show)(struct drm_i915_private *i915, char *buf);
+	ssize_t (*store)(struct drm_i915_private *i915, const char *buf, size_t count);
+};
+#define to_sriov_attr(x) container_of(x, struct i915_sriov_attr, attr)
+
+#define I915_SRIOV_ATTR(name) \
+static struct i915_sriov_attr name##_sriov_attr = \
+	__ATTR(name, 0644, name##_sriov_attr_show, name##_sriov_attr_store)
+
+#define I915_SRIOV_ATTR_RO(name) \
+static struct i915_sriov_attr name##_sriov_attr = \
+	__ATTR(name, 0444, name##_sriov_attr_show, NULL)
+
+struct i915_sriov_ext_kobj {
+	struct kobject base;
+	unsigned int id;
+};
+#define to_sriov_ext_kobj(x) container_of(x, struct i915_sriov_ext_kobj, base)
+
+struct i915_sriov_ext_attr {
+	struct attribute attr;
+	ssize_t (*show)(struct drm_i915_private *i915, unsigned int id, char *buf);
+	ssize_t (*store)(struct drm_i915_private *i915, unsigned int id,
+			 const char *buf, size_t count);
+};
+#define to_sriov_ext_attr(x) container_of(x, struct i915_sriov_ext_attr, attr)
+
+#define I915_SRIOV_EXT_ATTR(name) \
+static struct i915_sriov_ext_attr name##_sriov_ext_attr = \
+	__ATTR(name, 0644, name##_sriov_ext_attr_show, name##_sriov_ext_attr_store)
+
+#define I915_SRIOV_EXT_ATTR_RO(name) \
+static struct i915_sriov_ext_attr name##_sriov_ext_attr = \
+	__ATTR(name, 0644, name##_sriov_ext_attr_show, NULL)
+
+#define I915_SRIOV_EXT_ATTR_WO(name) \
+static struct i915_sriov_ext_attr name##_sriov_ext_attr = \
+	__ATTR(name, 0644, NULL, name##_sriov_ext_attr_store)
+
+#endif /* __I915_SRIOV_SYSFS_TYPES_H__ */
diff --git a/drivers/gpu/drm/i915/i915_sriov_types.h b/drivers/gpu/drm/i915/i915_sriov_types.h
new file mode 100644
index 000000000000..0d1bfa9ef513
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_sriov_types.h
@@ -0,0 +1,41 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __I915_SRIOV_TYPES_H__
+#define __I915_SRIOV_TYPES_H__
+
+#include <linux/types.h>
+#include "i915_sriov_sysfs_types.h"
+
+/**
+ * struct i915_sriov_pf - i915 SR-IOV PF data.
+ * @__status: Status of the PF. Don't access directly!
+ * @device_vfs: Number of VFs supported by the device.
+ * @driver_vfs: Number of VFs supported by the driver.
+ * @sysfs.home: Home object for all entries in sysfs.
+ * @sysfs.kobjs: Array with PF and VFs objects exposed in sysfs.
+ */
+struct i915_sriov_pf {
+	int __status;
+	u16 device_vfs;
+	u16 driver_vfs;
+	struct {
+		struct i915_sriov_kobj *home;
+		struct i915_sriov_ext_kobj **kobjs;
+	} sysfs;
+
+	/** @disable_auto_provisioning: flag to control VFs auto-provisioning */
+	bool disable_auto_provisioning;
+};
+
+/**
+ * struct i915_sriov - i915 SR-IOV data.
+ * @pf: PF only data.
+ */
+struct i915_sriov {
+	struct i915_sriov_pf pf;
+};
+
+#endif /* __I915_SRIOV_TYPES_H__ */
diff --git a/drivers/gpu/drm/i915/i915_sysfs.c b/drivers/gpu/drm/i915/i915_sysfs.c
index 595e8b574990..eaeacb5f3142 100644
--- a/drivers/gpu/drm/i915/i915_sysfs.c
+++ b/drivers/gpu/drm/i915/i915_sysfs.c
@@ -36,6 +36,7 @@
 #include "gt/sysfs_engines.h"
 
 #include "i915_drv.h"
+#include "i915_sriov_sysfs.h"
 #include "i915_sysfs.h"
 #include "intel_pm.h"
 
@@ -256,6 +257,8 @@ void i915_setup_sysfs(struct drm_i915_private *dev_priv)
 		drm_warn(&dev_priv->drm,
 			 "failed to register GT sysfs directory\n");
 
+	i915_sriov_sysfs_setup(dev_priv);
+
 	i915_setup_error_capture(kdev);
 
 	intel_engines_add_sysfs(dev_priv);
@@ -267,6 +270,8 @@ void i915_teardown_sysfs(struct drm_i915_private *dev_priv)
 
 	i915_teardown_error_capture(kdev);
 
+	i915_sriov_sysfs_teardown(dev_priv);
+
 	device_remove_bin_file(kdev,  &dpf_attrs_1);
 	device_remove_bin_file(kdev,  &dpf_attrs);
 
diff --git a/drivers/gpu/drm/i915/i915_utils.h b/drivers/gpu/drm/i915/i915_utils.h
index 2c430c0c3bad..c297da339a6f 100644
--- a/drivers/gpu/drm/i915/i915_utils.h
+++ b/drivers/gpu/drm/i915/i915_utils.h
@@ -111,6 +111,17 @@ bool i915_error_injected(void);
 #define range_overflows_end_t(type, start, size, max) \
 	range_overflows_end((type)(start), (type)(size), (type)(max))
 
+#ifndef check_round_up_overflow
+#define check_round_up_overflow(a, b, d) __must_check_overflow(({		\
+	typeof(a) __a = (a);							\
+	typeof(b) __b = (b);							\
+	typeof(d) __d = (d);							\
+	(void) (&__a == &__b);							\
+	(void) (&__a == __d);							\
+	(*__d = __a) && __builtin_add_overflow((__a-1) | (__b-1), 1, __d);	\
+}))
+#endif
+
 #define ptr_mask_bits(ptr, n) ({					\
 	unsigned long __v = (unsigned long)(ptr);			\
 	(typeof(ptr))(__v & -BIT(n));					\
@@ -391,4 +402,6 @@ static inline bool i915_run_as_guest(void)
 
 bool i915_vtd_active(struct drm_i915_private *i915);
 
+#define make_u64(hi__, low__) ((u64)(hi__) << 32 | (low__))
+
 #endif /* !__I915_UTILS_H */
diff --git a/drivers/gpu/drm/i915/i915_vgpu.c b/drivers/gpu/drm/i915/i915_vgpu.c
index c97323973f9b..bfe0b6df9b63 100644
--- a/drivers/gpu/drm/i915/i915_vgpu.c
+++ b/drivers/gpu/drm/i915/i915_vgpu.c
@@ -74,7 +74,10 @@ void intel_vgpu_detect(struct drm_i915_private *dev_priv)
 	 * we do not support VGT on older gens, return early so we don't have
 	 * to consider differently numbered or sized MMIO bars
 	 */
-	if (GRAPHICS_VER(dev_priv) < 6)
+
+	/* don't use GRAPHICS_VER() as it might be not ready yet */
+	if (INTEL_INFO(dev_priv)->__runtime.graphics.ip.ver < 6 ||
+	    INTEL_INFO(dev_priv)->__runtime.graphics.ip.ver > 11)
 		return;
 
 	shared_area = pci_iomap_range(pdev, 0, VGT_PVINFO_PAGE, VGT_PVINFO_SIZE);
diff --git a/drivers/gpu/drm/i915/i915_virtualization.h b/drivers/gpu/drm/i915/i915_virtualization.h
new file mode 100644
index 000000000000..1e80e66eec16
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_virtualization.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __I915_VIRTUALIZATION_H__
+#define __I915_VIRTUALIZATION_H__
+
+#include <linux/build_bug.h>
+
+#include "i915_gem.h"
+#include "i915_virtualization_types.h"
+
+static inline const char *i915_iov_mode_to_string(enum i915_iov_mode mode)
+{
+	switch (mode) {
+	case I915_IOV_MODE_NONE:
+		return "non virtualized";
+	case I915_IOV_MODE_GVT_VGPU:
+		return "GVT VGPU";
+	case I915_IOV_MODE_SRIOV_PF:
+		return "SR-IOV PF";
+	case I915_IOV_MODE_SRIOV_VF:
+		return "SR-IOV VF";
+	default:
+		return "<invalid>";
+	}
+}
+
+#define IS_IOV_ACTIVE(i915) (IOV_MODE(i915) != I915_IOV_MODE_NONE)
+
+#endif /* __I915_VIRTUALIZATION_H__ */
diff --git a/drivers/gpu/drm/i915/i915_virtualization_types.h b/drivers/gpu/drm/i915/i915_virtualization_types.h
new file mode 100644
index 000000000000..243a24b7c9af
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_virtualization_types.h
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __I915_VIRTUALIZATION_TYPES_H__
+#define __I915_VIRTUALIZATION_TYPES_H__
+
+/**
+ * enum i915_iov_mode - I/O Virtualization mode.
+ */
+enum i915_iov_mode {
+	I915_IOV_MODE_NONE = 1,
+	I915_IOV_MODE_GVT_VGPU,
+	I915_IOV_MODE_SRIOV_PF,
+	I915_IOV_MODE_SRIOV_VF,
+};
+
+#endif /* __I915_VIRTUALIZATION_TYPES_H__ */
diff --git a/drivers/gpu/drm/i915/intel_device_info.c b/drivers/gpu/drm/i915/intel_device_info.c
index 98769e5f2c3d..254659b1dc33 100644
--- a/drivers/gpu/drm/i915/intel_device_info.c
+++ b/drivers/gpu/drm/i915/intel_device_info.c
@@ -342,6 +342,15 @@ static void intel_ipver_early_init(struct drm_i915_private *i915)
 		return;
 	}
 
+	/* VF can't access IPVER registers directly */
+	if (IS_SRIOV_VF(i915)) {
+		/* 14018060378 not ready yet, use hardcoded values from INTEL_INFO */
+		drm_info(&i915->drm, "Beware, driver is using hardcoded IPVER values!\n");
+		drm_WARN_ON(&i915->drm, RUNTIME_INFO(i915)->graphics.ip.ver < 12);
+		drm_WARN_ON(&i915->drm, !RUNTIME_INFO(i915)->media.ip.ver);
+		return;
+	}
+
 	ip_ver_read(i915, i915_mmio_reg_offset(GMD_ID_GRAPHICS),
 		    &runtime->graphics.ip);
 	/* Wa_22012778468 */
@@ -507,8 +516,10 @@ void intel_device_info_runtime_init(struct drm_i915_private *dev_priv)
 		runtime->ppgtt_type = INTEL_PPGTT_NONE;
 	}
 
-	runtime->rawclk_freq = intel_read_rawclk(dev_priv);
-	drm_dbg(&dev_priv->drm, "rawclk rate: %d kHz\n", runtime->rawclk_freq);
+	if (!IS_SRIOV_VF(dev_priv)) {
+		runtime->rawclk_freq = intel_read_rawclk(dev_priv);
+		drm_dbg(&dev_priv->drm, "rawclk rate: %d kHz\n", runtime->rawclk_freq);
+	}
 
 	if (!HAS_DISPLAY(dev_priv)) {
 		dev_priv->drm.driver_features &= ~(DRIVER_MODESET |
diff --git a/drivers/gpu/drm/i915/intel_device_info.h b/drivers/gpu/drm/i915/intel_device_info.h
index 80bda653d61b..68ba24ee7af5 100644
--- a/drivers/gpu/drm/i915/intel_device_info.h
+++ b/drivers/gpu/drm/i915/intel_device_info.h
@@ -163,6 +163,7 @@ enum intel_ppgtt_type {
 	func(has_logical_ring_contexts); \
 	func(has_logical_ring_elsq); \
 	func(has_media_ratio_mode); \
+	func(has_memirq); \
 	func(has_mslice_steering); \
 	func(has_oa_bpc_reporting); \
 	func(has_oa_slice_contrib_limits); \
@@ -173,6 +174,7 @@ enum intel_ppgtt_type {
 	func(has_rps); \
 	func(has_runtime_pm); \
 	func(has_snoop); \
+	func(has_sriov); \
 	func(has_coherent_ggtt); \
 	func(tuning_thread_rr_after_dep); \
 	func(unfenced_needs_alignment); \
diff --git a/drivers/gpu/drm/i915/intel_pci_config.h b/drivers/gpu/drm/i915/intel_pci_config.h
index 23b8e519f333..9751ba441d07 100644
--- a/drivers/gpu/drm/i915/intel_pci_config.h
+++ b/drivers/gpu/drm/i915/intel_pci_config.h
@@ -31,6 +31,11 @@ static inline int intel_mmio_bar(int graphics_ver)
 	}
 }
 
+#ifdef CONFIG_PCI_IOV
+#define GEN12_VF_GTTMMADR_BAR			(PCI_IOV_RESOURCES + GEN4_GTTMMADR_BAR)
+#define GEN12_VF_LMEM_BAR			(PCI_IOV_RESOURCES + GEN12_LMEM_BAR)
+#endif
+
 /* BSM in include/drm/i915_drm.h */
 
 #define MCHBAR_I915				0x44
diff --git a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c
index 9680b47d09b6..85843a0dcd96 100644
--- a/drivers/gpu/drm/i915/intel_pm.c
+++ b/drivers/gpu/drm/i915/intel_pm.c
@@ -5092,7 +5092,9 @@ CG_FUNCS(nop);
  */
 void intel_init_clock_gating_hooks(struct drm_i915_private *dev_priv)
 {
-	if (IS_METEORLAKE(dev_priv))
+	if (IS_SRIOV_VF(dev_priv))
+		dev_priv->clock_gating_funcs = &nop_clock_gating_funcs;
+	else if (IS_METEORLAKE(dev_priv))
 		dev_priv->clock_gating_funcs = &nop_clock_gating_funcs;
 	else if (IS_PONTEVECCHIO(dev_priv))
 		dev_priv->clock_gating_funcs = &pvc_clock_gating_funcs;
@@ -5192,6 +5194,12 @@ static const struct intel_wm_funcs nop_funcs = {
 /* Set up chip specific power management-related functions */
 void intel_init_pm(struct drm_i915_private *dev_priv)
 {
+	if (IS_SRIOV_VF(dev_priv)) {
+		/* XXX */
+		dev_priv->display.funcs.wm = &nop_funcs;
+		return;
+	}
+
 	if (DISPLAY_VER(dev_priv) >= 9) {
 		skl_wm_init(dev_priv);
 		return;
diff --git a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
index 8dee9e62a73e..39c431ca06f7 100644
--- a/drivers/gpu/drm/i915/intel_uncore.c
+++ b/drivers/gpu/drm/i915/intel_uncore.c
@@ -1952,6 +1952,26 @@ __vgpu_read(16)
 __vgpu_read(32)
 __vgpu_read(64)
 
+#define __early_read(x) \
+static u##x \
+early_read##x(struct intel_uncore *uncore, i915_reg_t reg, bool trace) { \
+	return __raw_uncore_read##x(uncore, reg); \
+}
+
+#define __early_write(x) \
+static void \
+early_write##x(struct intel_uncore *uncore, i915_reg_t reg, u##x val, bool trace) { \
+	__raw_uncore_write##x(uncore, reg, val); \
+}
+
+__early_read(8)
+__early_read(16)
+__early_read(32)
+__early_read(64)
+__early_write(8)
+__early_write(16)
+__early_write(32)
+
 #define GEN2_READ_HEADER(x) \
 	u##x val = 0; \
 	assert_rpm_wakelock_held(uncore->rpm);
@@ -2157,6 +2177,76 @@ __vgpu_write(8)
 __vgpu_write(16)
 __vgpu_write(32)
 
+static const struct i915_range vf_accessible_regs[] = {
+	{ .start = 0x190010, .end = 0x190010 },
+	{ .start = 0x190018, .end = 0x19001C },
+	{ .start = 0x190030, .end = 0x190048 },
+	{ .start = 0x190060, .end = 0x190064 },
+	{ .start = 0x190070, .end = 0x190074 },
+	{ .start = 0x190090, .end = 0x190090 },
+	{ .start = 0x1900a0, .end = 0x1900a0 },
+	{ .start = 0x1900a8, .end = 0x1900ac },
+	{ .start = 0x1900b0, .end = 0x1900b4 },
+	{ .start = 0x1900d0, .end = 0x1900d4 },
+	{ .start = 0x1900e8, .end = 0x1900ec },
+	{ .start = 0x1900F0, .end = 0x1900F4 },
+	{ .start = 0x190100, .end = 0x190100 },
+	{ .start = 0x1901f0, .end = 0x1901f0 },
+	{ .start = 0x1901f8, .end = 0x1901f8 },
+	{ .start = 0x190240, .end = 0x19024c },
+	{ .start = 0x190300, .end = 0x190304 },
+	{ .start = 0x19030c, .end = 0x19031c },
+};
+
+static bool reg_is_vf_accessible(u32 offset)
+{
+	return BSEARCH(offset, &vf_accessible_regs[0], ARRAY_SIZE(vf_accessible_regs), mmio_range_cmp);
+}
+
+static int __vf_runtime_reg_cmp(u32 key, const struct vf_runtime_reg *reg)
+{
+	u32 offset = reg->offset;
+
+	if (key < offset)
+		return -1;
+	else if (key > offset)
+		return 1;
+	else
+		return 0;
+}
+
+static const struct vf_runtime_reg *
+__vf_runtime_reg_find(struct intel_gt *gt, u32 offset)
+{
+	const struct vf_runtime_reg *regs = gt->iov.vf.runtime.regs;
+	u32 regs_num = gt->iov.vf.runtime.regs_size;
+
+	return BSEARCH(offset, regs, regs_num, __vf_runtime_reg_cmp);
+}
+
+#define __vf_read(x) \
+static u##x vf_read##x(struct intel_uncore *uncore, \
+		       i915_reg_t reg, bool trace) \
+{ \
+	u32 offset = i915_mmio_reg_offset(reg); \
+	const struct vf_runtime_reg *vf_reg = __vf_runtime_reg_find(uncore->gt, offset); \
+	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV) && vf_reg) \
+		drm_dbg(&uncore->i915->drm, "runtime MMIO %#04x = %#x\n", \
+			offset, vf_reg->value); \
+	if (vf_reg) \
+		return vf_reg->value; \
+	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV) && !reg_is_vf_accessible(offset)) { \
+		WARN(1, "rejected read MMIO %#04x\n", offset); \
+		return 0; \
+	} \
+	return gen2_read##x(uncore, reg, trace); \
+}
+
+__vf_read(8)
+__vf_read(16)
+__vf_read(32)
+#define vf_read64 gen2_read64 /* no support for 64 */
+
 #define ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, x) \
 do { \
 	(uncore)->funcs.mmio_writeb = x##_write8; \
@@ -2501,6 +2591,9 @@ void intel_uncore_init_early(struct intel_uncore *uncore,
 	uncore->i915 = gt->i915;
 	uncore->gt = gt;
 	uncore->rpm = &gt->i915->runtime_pm;
+
+	ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, early);
+	ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, early);
 }
 
 static void uncore_raw_init(struct intel_uncore *uncore)
@@ -2513,6 +2606,9 @@ static void uncore_raw_init(struct intel_uncore *uncore)
 	} else if (GRAPHICS_VER(uncore->i915) == 5) {
 		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen5);
 		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, gen5);
+	} else if (IS_SRIOV_VF(uncore->i915)) {
+		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen2);
+		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, vf);
 	} else {
 		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen2);
 		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, gen2);
@@ -2619,7 +2715,7 @@ int intel_uncore_init_mmio(struct intel_uncore *uncore)
 		return -ENODEV;
 	}
 
-	if (GRAPHICS_VER(i915) > 5 && !intel_vgpu_active(i915))
+	if (GRAPHICS_VER(i915) > 5 && !IS_SRIOV_VF(i915) && !intel_vgpu_active(i915))
 		uncore->flags |= UNCORE_HAS_FORCEWAKE;
 
 	if (!intel_uncore_has_forcewake(uncore)) {
diff --git a/drivers/gpu/drm/i915/selftests/i915_live_selftests.h b/drivers/gpu/drm/i915/selftests/i915_live_selftests.h
index 5aee6c9a8295..094afcad87fe 100644
--- a/drivers/gpu/drm/i915/selftests/i915_live_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_live_selftests.h
@@ -51,5 +51,8 @@ selftest(slpc, intel_slpc_live_selftests)
 selftest(guc, intel_guc_live_selftests)
 selftest(guc_multi_lrc, intel_guc_multi_lrc_live_selftests)
 selftest(guc_hang, intel_guc_hang_check)
+selftest(iov_provisioning, selftest_live_iov_provisioning)
+selftest(iov_relay, selftest_live_iov_relay)
+selftest(iov_service, selftest_live_iov_service)
 /* Here be dragons: keep last to run last! */
 selftest(late_gt_pm, intel_gt_pm_late_selftests)
diff --git a/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h b/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
index 0c22e0fc9059..4c0eed4aedfe 100644
--- a/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
@@ -33,3 +33,5 @@ selftest(evict, i915_gem_evict_mock_selftests)
 selftest(gtt, i915_gem_gtt_mock_selftests)
 selftest(hugepages, i915_gem_huge_page_mock_selftests)
 selftest(memory_region, intel_memory_region_mock_selftests)
+selftest(iov_relay, selftest_mock_iov_relay)
+selftest(iov_service, selftest_mock_iov_service)
diff --git a/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h b/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h
index 058450d351f7..6dbde97ea17b 100644
--- a/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h
@@ -19,3 +19,4 @@ selftest(engine_cs, intel_engine_cs_perf_selftests)
 selftest(request, i915_request_perf_selftests)
 selftest(migrate, intel_migrate_perf_selftests)
 selftest(region, intel_memory_region_perf_selftests)
+selftest(iov_relay, selftest_perf_iov_relay)
diff --git a/drivers/gpu/drm/i915/selftests/mock_gem_device.c b/drivers/gpu/drm/i915/selftests/mock_gem_device.c
index f6a7c0bd2955..2eefb1d9d90e 100644
--- a/drivers/gpu/drm/i915/selftests/mock_gem_device.c
+++ b/drivers/gpu/drm/i915/selftests/mock_gem_device.c
@@ -162,6 +162,7 @@ struct drm_i915_private *mock_gem_device(void)
 	if (pm_runtime_enabled(&pdev->dev))
 		WARN_ON(pm_runtime_get_sync(&pdev->dev));
 
+	i915->__mode = I915_IOV_MODE_NONE;
 
 	i915_params_copy(&i915->params, &i915_modparams);
 
-- 
2.25.1

