From daa381030b4a15e9db7bb5f54773303655a07b2b Mon Sep 17 00:00:00 2001
From: Junxiao Chang <junxiao.chang@intel.com>
Date: Wed, 24 Nov 2021 13:16:43 +0800
Subject: [PATCH 068/888] Revert "drm/i915/guc: Workaround reset G2H is
 received after schedule done G2H"

This reverts commit 096bab6ef85d4e525f1cf850fb12475f54a80113.
---
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c | 43 +++----------------
 1 file changed, 6 insertions(+), 37 deletions(-)

diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index 8f7a11e65ef5..e4a099f8f820 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -832,35 +832,17 @@ __unwind_incomplete_requests(struct intel_context *ce)
 static void __guc_reset_context(struct intel_context *ce, bool stalled)
 {
 	struct i915_request *rq;
-	unsigned long flags;
 	u32 head;
-	bool skip = false;
 
 	intel_context_get(ce);
 
 	/*
-	 * GuC will implicitly mark the context as non-schedulable when it sends
-	 * the reset notification. Make sure our state reflects this change. The
-	 * context will be marked enabled on resubmission.
-	 *
-	 * XXX: If the context is reset as a result of the request cancellation
-	 * this G2H is received after the schedule disable complete G2H which is
-	 * likely wrong as this creates a race between the request cancellation
-	 * code re-submitting the context and this G2H handler. This likely
-	 * should be fixed in the GuC but until if / when that gets fixed we
-	 * need to workaround this. Convert this function to a NOP if a pending
-	 * enable is in flight as this indicates that a request cancellation has
-	 * occurred.
+	 * GuC will implicitly mark the context as non-schedulable
+	 * when it sends the reset notification. Make sure our state
+	 * reflects this change. The context will be marked enabled
+	 * on resubmission.
 	 */
-	spin_lock_irqsave(&ce->guc_state.lock, flags);
-	if (likely(!context_pending_enable(ce))) {
-		clr_context_enabled(ce);
-	} else {
-		skip = true;
-	}
-	spin_unlock_irqrestore(&ce->guc_state.lock, flags);
-	if (unlikely(skip))
-		goto out_put;
+	clr_context_enabled(ce);
 
 	rq = intel_context_find_active_request(ce);
 	if (!rq) {
@@ -879,7 +861,6 @@ static void __guc_reset_context(struct intel_context *ce, bool stalled)
 out_replay:
 	guc_reset_state(ce, head, stalled);
 	__unwind_incomplete_requests(ce);
-out_put:
 	intel_context_put(ce);
 }
 
@@ -1624,13 +1605,6 @@ static void guc_context_cancel_request(struct intel_context *ce,
 			guc_reset_state(ce, intel_ring_wrap(ce->ring, rq->head),
 					true);
 		}
-
-		/*
-		 * XXX: Racey if context is reset, see comment in
-		 * __guc_reset_context().
-		 */
-		flush_work(&ce_to_guc(ce)->ct.requests.worker);
-
 		guc_context_unblock(ce);
 	}
 }
@@ -2745,12 +2719,7 @@ static void guc_handle_context_reset(struct intel_guc *guc,
 {
 	trace_intel_context_reset(ce);
 
-	/*
-	 * XXX: Racey if request cancellation has occurred, see comment in
-	 * __guc_reset_context().
-	 */
-	if (likely(!intel_context_is_banned(ce) &&
-		   !context_blocked(ce))) {
+	if (likely(!intel_context_is_banned(ce))) {
 		capture_error_state(guc, ce);
 		guc_context_replay(ce);
 	}
-- 
2.25.1

