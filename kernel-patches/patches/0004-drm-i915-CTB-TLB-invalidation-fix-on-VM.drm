From c75552e4fc1b556c4d33ec20ec1e5c99f666068e Mon Sep 17 00:00:00 2001
From: SyazwanHalim <mohd.syazwan.abdul.halim@intel.com>
Date: Mon, 11 Mar 2024 23:19:38 +0800
Subject: [PATCH 04/86] drm/i915: CTB TLB invalidation fix on VM

port over delta changes between existing patches with
latest patches https://patchwork.freedesktop.org/series/125245/

The GuC firmware had defined the interface for Translation Look-Aside
Buffer (TLB) invalidation.  We should use this interface when
invalidating the engine and GuC TLBs.
Add additional functionality to intel_gt_invalidate_tlb, invalidating
the GuC TLBs and falling back to GT invalidation when the GuC is
disabled.
The invalidation is done by sending a request directly to the GuC
tlb_lookup that invalidates the table.  The invalidation is submitted as
a wait request and is performed in the CT event handler.  This means we
cannot perform this TLB invalidation path if the CT is not enabled.
If the request isn't fulfilled in two seconds, this would constitute
an error in the invalidation as that would constitute either a lost
request or a severe GuC overload.

With this new invalidation routine, we can perform GuC-based GGTT
invalidations.  GuC-based GGTT invalidation is incompatible with
MMIO invalidation so we should not perform MMIO invalidation when
GuC-based GGTT invalidation is expected.

The additional complexity incurred in this patch will be necessary for
range-based tlb invalidations, which will be platformed in the future.

Signed-off-by: Prathap Kumar Valsan <prathap.kumar.valsan@intel.com>
Signed-off-by: Bruce Chang <yu.bruce.chang@intel.com>
Signed-off-by: Chris Wilson <chris.p.wilson@intel.com>
Signed-off-by: Umesh Nerlige Ramappa <umesh.nerlige.ramappa@intel.com>
Signed-off-by: Jonathan Cavitt <jonathan.cavitt@intel.com>
Signed-off-by: Aravind Iddamsetty <aravind.iddamsetty@intel.com>
Signed-off-by: Fei Yang <fei.yang@intel.com>
Signed-off-by: SyazwanHalim <mohd.syazwan.abdul.halim@intel.com>
CC: Andi Shyti <andi.shyti@linux.intel.com>
Reviewed-by: Andi Shyti <andi.shyti@linux.intel.com>
Acked-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Acked-by: Nirmoy Das <nirmoy.das@intel.com>
Reviewed-by: John Harrison <John.C.Harrison@Intel.com>
---
 drivers/gpu/drm/i915/gt/intel_ggtt.c          |   7 +-
 drivers/gpu/drm/i915/gt/intel_gt.h            |   8 -
 drivers/gpu/drm/i915/gt/intel_tlb.c           |  14 +-
 drivers/gpu/drm/i915/gt/selftest_tlb.c        |  11 +-
 .../gpu/drm/i915/gt/uc/abi/guc_actions_abi.h  |   2 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc.h        |  12 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c     |   7 +-
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c | 178 ++++++++----------
 drivers/gpu/drm/i915/gt/uc/intel_uc.c         |   7 +
 drivers/gpu/drm/i915/i915_driver.c            |   5 -
 10 files changed, 118 insertions(+), 133 deletions(-)

diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index 0d68021d722b..585b221fd028 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -238,7 +238,7 @@ static void guc_ggtt_ct_invalidate(struct intel_gt *gt)
 	with_intel_runtime_pm_if_active(uncore->rpm, wakeref) {
 		struct intel_guc *guc = &gt->uc.guc;
 
-		intel_guc_invalidate_tlb(guc);
+		intel_guc_invalidate_tlb_guc(guc);
 	}
 }
 
@@ -276,7 +276,7 @@ static void gen12vf_ggtt_invalidate(struct i915_ggtt *ggtt)
 			continue;
 
 		with_intel_runtime_pm(gt->uncore->rpm, wakeref)
-			intel_guc_invalidate_tlb(guc);
+			intel_guc_invalidate_tlb_guc(guc);
 	}
 }
 
@@ -1292,8 +1292,7 @@ static int gen8_gmch_probe(struct i915_ggtt *ggtt)
 		ggtt->vm.raw_insert_page = gen8_ggtt_insert_page;
 	}
 
-	if (intel_uc_wants_guc(&ggtt->vm.gt->uc) &&
-	    intel_uc_wants_guc_submission(&ggtt->vm.gt->uc))
+	if (intel_uc_wants_guc_submission(&ggtt->vm.gt->uc))
 		ggtt->invalidate = guc_ggtt_invalidate;
 	else
 		ggtt->invalidate = gen8_ggtt_invalidate;
diff --git a/drivers/gpu/drm/i915/gt/intel_gt.h b/drivers/gpu/drm/i915/gt/intel_gt.h
index 6f7e007bb14b..3ceba1e0e872 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt.h
@@ -162,14 +162,6 @@ static inline bool intel_gt_is_wedged(const struct intel_gt *gt)
 	return unlikely(test_bit(I915_WEDGED, &gt->reset.flags));
 }
 
-static inline bool intel_gt_is_enabled(const struct intel_gt *gt)
-{
-	/* Check if GT is wedged or suspended */
-	if (intel_gt_is_wedged(gt) || !intel_irqs_enabled(gt->i915))
-		return false;
-	return true;
-}
-
 int intel_gt_probe_all(struct drm_i915_private *i915);
 int intel_gt_tiles_init(struct drm_i915_private *i915);
 void intel_gt_release_all(struct drm_i915_private *i915);
diff --git a/drivers/gpu/drm/i915/gt/intel_tlb.c b/drivers/gpu/drm/i915/gt/intel_tlb.c
index 018aeb6bacc6..4bb13d1890e3 100644
--- a/drivers/gpu/drm/i915/gt/intel_tlb.c
+++ b/drivers/gpu/drm/i915/gt/intel_tlb.c
@@ -138,14 +138,16 @@ void intel_gt_invalidate_tlb_full(struct intel_gt *gt, u32 seqno)
 		if (tlb_seqno_passed(gt, seqno))
 			goto unlock;
 
-		if (intel_guc_submission_is_used(guc)) {
-			if (intel_guc_is_ready(guc))
-				intel_guc_invalidate_tlb_full(guc);
-		} else {
+		if (HAS_GUC_TLB_INVALIDATION(gt->i915)) {
 			/*
-			 * Fall back to old path if GuC is disabled.
-			 * This is safe because GuC is not enabled and not writing to MMIO.
+			 * Only perform GuC TLB invalidation if GuC is ready.
+			 * The only time GuC could not be ready is on GT reset,
+			 * which would clobber all the TLBs anyways, making
+			 * any TLB invalidation path here unnecessary.
 			 */
+			if (intel_guc_is_ready(guc))
+				intel_guc_invalidate_tlb_engines(guc);
+		} else {
 			mmio_invalidate_full(gt);
 		}
 
diff --git a/drivers/gpu/drm/i915/gt/selftest_tlb.c b/drivers/gpu/drm/i915/gt/selftest_tlb.c
index 1be9bddd2e34..00b872b6380b 100644
--- a/drivers/gpu/drm/i915/gt/selftest_tlb.c
+++ b/drivers/gpu/drm/i915/gt/selftest_tlb.c
@@ -137,11 +137,14 @@ pte_tlbinv(struct intel_context *ce,
 	i915_request_add(rq);
 
 	/*
-	 * Short sleep to sanitycheck the batch is spinning before we begin
-	 * FIXME: needs updating to wait until the request has started
-	 * rather than waiting for a fixed amount of time.
+	 * Short sleep to sanitycheck the batch is spinning before we begin.
+	 * FIXME: Why is GSC so slow?
 	 */
-	msleep(200);
+	if (ce->engine->class == OTHER_CLASS)
+		msleep(200);
+	else
+		msleep(10);
+
 	if (va == vb) {
 		if (!i915_request_completed(rq)) {
 			pr_err("%s(%s): Semaphore sanitycheck failed %llx, with alignment %llx, using PTE size %x (phys %x, sg %x)\n",
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
index 194294ee2f31..e44255c00f1d 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
@@ -277,7 +277,7 @@ enum intel_guc_state_capture_event_status {
 #define INTEL_GUC_TLB_INVAL_FLUSH_CACHE REG_BIT(31)
 
 enum intel_guc_tlb_invalidation_type {
-	INTEL_GUC_TLB_INVAL_FULL = 0x0,
+	INTEL_GUC_TLB_INVAL_ENGINES = 0x0,
 	INTEL_GUC_TLB_INVAL_GUC = 0x3,
 };
 
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
index 6a663531fd47..f48af942e127 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
@@ -322,8 +322,8 @@ struct intel_guc {
 
 struct intel_guc_tlb_wait {
 	struct wait_queue_head wq;
-	u8 status;
-} __aligned(4);
+	bool busy;
+};
 
 static inline struct intel_guc *log_to_guc(struct intel_guc_log *log)
 {
@@ -455,8 +455,6 @@ int intel_guc_allocate_and_map_vma(struct intel_guc *guc, u32 size,
 int intel_guc_self_cfg32(struct intel_guc *guc, u16 key, u32 value);
 int intel_guc_self_cfg64(struct intel_guc *guc, u16 key, u64 value);
 
-int intel_guc_invalidate_tlb_full(struct intel_guc *guc);
-int intel_guc_invalidate_tlb(struct intel_guc *guc);
 int intel_guc_tlb_invalidation_done(struct intel_guc *guc, const u32 *hxg,
 				    u32 size);
 
@@ -562,6 +560,12 @@ void intel_guc_dump_time_info(struct intel_guc *guc, struct drm_printer *p);
 
 int intel_guc_sched_disable_gucid_threshold_max(struct intel_guc *guc);
 
+bool intel_guc_tlb_invalidation_is_available(struct intel_guc *guc);
+int intel_guc_invalidate_tlb_engines(struct intel_guc *guc);
+int intel_guc_invalidate_tlb_guc(struct intel_guc *guc);
+int intel_guc_tlb_invalidation_done(struct intel_guc *guc,
+				    const u32 *payload, u32 len);
+
 int intel_guc_enable_gsc_engine(struct intel_guc *guc);
 int intel_guc_disable_gsc_engine(struct intel_guc *guc);
 
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
index 0c2747659830..e0de1d93e432 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
@@ -1232,6 +1232,9 @@ static int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 	case INTEL_GUC_ACTION_NOTIFY_EXCEPTION:
 		ret = intel_guc_crash_process_msg(guc, action);
 		break;
+	case INTEL_GUC_ACTION_TLB_INVALIDATION_DONE:
+		ret = intel_guc_tlb_invalidation_done(guc, payload, len);
+		break;
 	default:
 		ret = -EOPNOTSUPP;
 		break;
@@ -1310,9 +1313,7 @@ static int ct_handle_event(struct intel_guc_ct *ct, struct ct_incoming_msg *requ
 
 	/* Handle tlb invalidation response in interrupt context */
 	if (action == INTEL_GUC_ACTION_TLB_INVALIDATION_DONE) {
-		int ret = intel_guc_tlb_invalidation_done(ct_to_guc(ct), hxg, request->size);
-		ct_free_msg(request);
-		return ret;
+		return ct_process_request(ct, request);
 	}
 
 	spin_lock_irqsave(&ct->requests.lock, flags);
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index 7634beb68440..a84c717981d0 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -1795,20 +1795,18 @@ static void __guc_reset_context(struct intel_context *ce, intel_engine_mask_t st
 	intel_context_put(parent);
 }
 
-static void wake_up_tlb_invalidate(struct intel_guc_tlb_wait *wait)
-{
-	/* Barrier to ensure the store is observed by the woken thread */
-	smp_store_mb(wait->status, 0);
-	wake_up(&wait->wq);
-}
-
 void wake_up_all_tlb_invalidate(struct intel_guc *guc)
 {
 	struct intel_guc_tlb_wait *wait;
 	unsigned long i;
 
+	if (!intel_guc_tlb_invalidation_is_available(guc))
+		return;
+
+	xa_lock_irq(&guc->tlb_lookup);
 	xa_for_each(&guc->tlb_lookup, i, wait)
-		wake_up_tlb_invalidate(wait);
+		wake_up(&wait->wq);
+	xa_unlock_irq(&guc->tlb_lookup);
 }
 
 void intel_guc_submission_reset(struct intel_guc *guc, intel_engine_mask_t stalled)
@@ -1970,17 +1968,32 @@ void intel_guc_submission_reset_finish(struct intel_guc *guc)
 	intel_guc_global_policies_update(guc);
 	enable_submission(guc);
 	intel_gt_unpark_heartbeats(guc_to_gt(guc));
+
+	/*
+	 * The full GT reset will have cleared the TLB caches and flushed the
+	 * G2H message queue; we can release all the blocked waiters.
+	 */
+	wake_up_all_tlb_invalidate(guc);
 }
 
 static void destroyed_worker_func(struct work_struct *w);
 static void reset_fail_worker_func(struct work_struct *w);
 static int number_mlrc_guc_id(struct intel_guc *guc);
 
+bool intel_guc_tlb_invalidation_is_available(struct intel_guc *guc)
+{
+	return HAS_GUC_TLB_INVALIDATION(guc_to_gt(guc)->i915) &&
+		intel_guc_is_ready(guc);
+}
+
 static int init_tlb_lookup(struct intel_guc *guc)
 {
 	struct intel_guc_tlb_wait *wait;
 	int err;
 
+	if (!HAS_GUC_TLB_INVALIDATION(guc_to_gt(guc)->i915))
+		return 0;
+
 	xa_init_flags(&guc->tlb_lookup, XA_FLAGS_ALLOC);
 
 	wait = kzalloc(sizeof(*wait), GFP_KERNEL);
@@ -1988,9 +2001,11 @@ static int init_tlb_lookup(struct intel_guc *guc)
 		return -ENOMEM;
 
 	init_waitqueue_head(&wait->wq);
+
+	/* Preallocate a shared id for use under memory pressure. */
 	err = xa_alloc_cyclic_irq(&guc->tlb_lookup, &guc->serial_slot, wait,
 				  xa_limit_32b, &guc->next_seqno, GFP_KERNEL);
-	if (err == -ENOMEM) {
+	if (err < 0) {
 		kfree(wait);
 		return err;
 	}
@@ -2002,11 +2017,13 @@ static void fini_tlb_lookup(struct intel_guc *guc)
 {
 	struct intel_guc_tlb_wait *wait;
 
+	if (!HAS_GUC_TLB_INVALIDATION(guc_to_gt(guc)->i915))
+		return;
+
 	wait = xa_load(&guc->tlb_lookup, guc->serial_slot);
-	if (wait) {
-		GEM_BUG_ON(wait->status);
-		kfree(wait);
-	}
+	if (wait && wait->busy)
+		guc_err(guc, "Unexpected busy item in tlb_lookup on fini\n");
+	kfree(wait);
 
 	xa_destroy(&guc->tlb_lookup);
 }
@@ -4813,33 +4830,22 @@ static void wait_wake_outstanding_tlb_g2h(struct intel_guc *guc, u32 seqno)
 	xa_lock_irqsave(&guc->tlb_lookup, flags);
 	wait = xa_load(&guc->tlb_lookup, seqno);
 
-	/* We received a response after the waiting task did exit with a timeout */
-	if (unlikely(!wait))
-		drm_dbg(&guc_to_gt(guc)->i915->drm,
-			"Stale TLB invalidation response with seqno %d\n", seqno);
-
 	if (wait)
-		wake_up_tlb_invalidate(wait);
+		wake_up(&wait->wq);
+	else
+		guc_dbg(guc,
+			"Stale TLB invalidation response with seqno %d\n", seqno);
 
 	xa_unlock_irqrestore(&guc->tlb_lookup, flags);
 }
 
-int intel_guc_tlb_invalidation_done(struct intel_guc *guc, const u32 *hxg, u32 size)
+int intel_guc_tlb_invalidation_done(struct intel_guc *guc,
+				    const u32 *payload, u32 len)
 {
-	u32 seqno, hxg_len, len;
-
-	/*
-	 * FIXME: these calculations would be better done signed. That
-	 * way underflow can be detected as well.
-	 */
-	hxg_len = size - GUC_CTB_MSG_MIN_LEN;
-	len = hxg_len - GUC_HXG_MSG_MIN_LEN;
-
-	if (unlikely(len < 1))
+	if (len < 1)
 		return -EPROTO;
 
-	seqno = hxg[GUC_HXG_MSG_MIN_LEN];
-	wait_wake_outstanding_tlb_g2h(guc, seqno);
+	wait_wake_outstanding_tlb_g2h(guc, payload[0]);
 	return 0;
 }
 
@@ -4850,12 +4856,7 @@ static long must_wait_woken(struct wait_queue_entry *wq_entry, long timeout)
 	 * we do not wake up early if the kthread task has been completed.
 	 * As we are called from page reclaim in any task context,
 	 * we may be invoked from stopped kthreads, but we *must*
-	 * complete the wait from the HW .
-	 *
-	 * A second problem is that since we are called under reclaim
-	 * and wait_woken() inspected the thread state, it makes an invalid
-	 * assumption that all PF_KTHREAD tasks have set_kthread_struct()
-	 * called upon them, and will trigger a GPF in is_kthread_should_stop().
+	 * complete the wait from the HW.
 	 */
 	do {
 		set_current_state(TASK_UNINTERRUPTIBLE);
@@ -4864,34 +4865,45 @@ static long must_wait_woken(struct wait_queue_entry *wq_entry, long timeout)
 
 		timeout = schedule_timeout(timeout);
 	} while (timeout);
-	__set_current_state(TASK_RUNNING);
 
 	/* See wait_woken() and woken_wake_function() */
+	__set_current_state(TASK_RUNNING);
 	smp_store_mb(wq_entry->flags, wq_entry->flags & ~WQ_FLAG_WOKEN);
 
 	return timeout;
 }
 
-static int guc_send_invalidate_tlb(struct intel_guc *guc, u32 type)
+static bool intel_gt_is_enabled(const struct intel_gt *gt)
+{
+	/* Check if GT is wedged or suspended */
+	if (intel_gt_is_wedged(gt) || !intel_irqs_enabled(gt->i915))
+		return false;
+	return true;
+}
+
+static int guc_send_invalidate_tlb(struct intel_guc *guc,
+				   enum intel_guc_tlb_invalidation_type type)
 {
 	struct intel_guc_tlb_wait _wq, *wq = &_wq;
-	DEFINE_WAIT_FUNC(wait, woken_wake_function);
 	struct intel_gt *gt = guc_to_gt(guc);
-	long timeout = 0;
-	u32 elapsed = 0;
-	int err = 0;
+	DEFINE_WAIT_FUNC(wait, woken_wake_function);
+	int err;
 	u32 seqno;
 	u32 action[] = {
 		INTEL_GUC_ACTION_TLB_INVALIDATION,
 		0,
 		REG_FIELD_PREP(INTEL_GUC_TLB_INVAL_TYPE_MASK, type) |
-			REG_FIELD_PREP(INTEL_GUC_TLB_INVAL_MODE_MASK, INTEL_GUC_TLB_INVAL_MODE_HEAVY) |
+			REG_FIELD_PREP(INTEL_GUC_TLB_INVAL_MODE_MASK,
+				       INTEL_GUC_TLB_INVAL_MODE_HEAVY) |
 			INTEL_GUC_TLB_INVAL_FLUSH_CACHE,
 	};
 	u32 size = ARRAY_SIZE(action);
 
-	if (!intel_guc_ct_enabled(&guc->ct) ||
-	    !intel_gt_is_enabled(gt))
+	/*
+	 * Early guard against GT enablement.  TLB invalidation should not be
+	 * attempted if the GT is disabled due to suspend/wedge.
+	 */
+	if (!intel_gt_is_enabled(gt))
 		return -EINVAL;
 
 	init_waitqueue_head(&_wq.wq);
@@ -4903,15 +4915,15 @@ static int guc_send_invalidate_tlb(struct intel_guc *guc, u32 type)
 		xa_lock_irq(&guc->tlb_lookup);
 		wq = xa_load(&guc->tlb_lookup, guc->serial_slot);
 		wait_event_lock_irq(wq->wq,
-				    !READ_ONCE(wq->status),
+				    !READ_ONCE(wq->busy),
 				    guc->tlb_lookup.xa_lock);
 		/*
-		 * Update wq->status under lock to ensure only one waiter can
+		 * Update wq->busy under lock to ensure only one waiter can
 		 * issue the TLB invalidation command using the serial slot at a
-		 * time. The condition is set to false before releasing the lock
+		 * time. The condition is set to true before releasing the lock
 		 * so that other caller continue to wait until woken up again.
 		 */
-		wq->status = 1;
+		wq->busy = true;
 		xa_unlock_irq(&guc->tlb_lookup);
 
 		seqno = guc->serial_slot;
@@ -4921,52 +4933,23 @@ static int guc_send_invalidate_tlb(struct intel_guc *guc, u32 type)
 
 	add_wait_queue(&wq->wq, &wait);
 
+	/* This is a critical reclaim path and thus we must loop here. */
 	err = intel_guc_send_busy_loop(guc, action, size, G2H_LEN_DW_INVALIDATE_TLB, true);
-	if (err) {
+	if (err)
 		goto out;
-	}
+
 	/*
-	 * GuC has a timeout of 1ms for a TLB invalidation response from GAM. On a
-	 * timeout GuC drops the request and has no mechanism to notify the host about
-	 * the timeout. So keep a larger timeout that accounts for this individual
-	 * timeout and max number of outstanding invalidation requests that can be
-	 * queued in CT buffer.
-	 *
-	 * Although the invalidation request itself should complete within 1ms, the
-	 * request might be in a long queue of other, slower, CTB requests. If the
-	 * CTB buffer is fully backed up, a multi-second delay is possible. Make a
-	 * while loop of two 1-second-waits with debug prints to catch attention for
-	 * potential issues.
-	 *
-	 * FIXME: Add a check for the CTB buffer processing to have passed the TLB
-	 * invalidation request before starting the timeout. Blindly waiting for 2
-	 * seconds doesn't really ensure the failure is not just a consequence of
-	 * slow GuC processing.
+	 * Late guard against GT enablement.  It is not an error for the TLB
+	 * invalidation to time out if the GT is disabled during the process
+	 * due to suspend/wedge.  In fact, the TLB invalidation is cancelled
+	 * in this case.
 	 */
-#define OUTSTANDING_GUC_TIMEOUT_PERIOD  (HZ)
-	while (!timeout) {
-		timeout = must_wait_woken(&wait, OUTSTANDING_GUC_TIMEOUT_PERIOD);
-		if (timeout)
-			break;
-		gt_dbg(gt, "TLB invalidation (seqno=%u) pending for %us\n", seqno, ++elapsed);
-		if (elapsed >= 2) {
-			/*
-			 * FIXME: Real TLB invalidation timeout is critical and warrants a GT
-			 * reset. However, it's possible that after this long wait the GT could
-			 * just come out from a reset thus it appears to be enabled, then this
-			 * code here wedges the GT again.
-			 */
-			if (intel_gt_is_enabled(gt)) {
-				gt_err(gt,
-				       "TLB invalidation response timed out for seqno %u\n",
-				       seqno);
-				intel_gt_set_wedged(gt);
-				err = -ETIME;
-			}
-			break;
-		}
+	if (!must_wait_woken(&wait, intel_guc_ct_max_queue_time_jiffies()) &&
+	    intel_gt_is_enabled(gt)) {
+		guc_err(guc,
+			"TLB invalidation response timed out for seqno %u\n", seqno);
+		err = -ETIME;
 	}
-
 out:
 	remove_wait_queue(&wq->wq, &wait);
 	if (seqno != guc->serial_slot)
@@ -4975,19 +4958,18 @@ static int guc_send_invalidate_tlb(struct intel_guc *guc, u32 type)
 	return err;
 }
 
-/* Full TLB invalidation */
-int intel_guc_invalidate_tlb_full(struct intel_guc *guc)
+/* Send a H2G command to invalidate the TLBs at engine level and beyond. */
+int intel_guc_invalidate_tlb_engines(struct intel_guc *guc)
 {
-	return guc_send_invalidate_tlb(guc, INTEL_GUC_TLB_INVAL_FULL);
+	return guc_send_invalidate_tlb(guc, INTEL_GUC_TLB_INVAL_ENGINES);
 }
 
-/* GuC TLB Invalidation: Invalidate the TLB's of GuC itself. */
-int intel_guc_invalidate_tlb(struct intel_guc *guc)
+/* Send a H2G command to invalidate the GuC's internal TLB. */
+int intel_guc_invalidate_tlb_guc(struct intel_guc *guc)
 {
 	return guc_send_invalidate_tlb(guc, INTEL_GUC_TLB_INVAL_GUC);
 }
 
-
 int intel_guc_deregister_done_process_msg(struct intel_guc *guc,
 					  const u32 *msg,
 					  u32 len)
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc.c b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
index af49c560283b..f41195d202bf 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
@@ -821,6 +821,8 @@ void intel_uc_suspend(struct intel_uc *uc)
 	/* flush the GSC worker */
 	intel_gsc_uc_flush_work(&uc->gsc);
 
+	wake_up_all_tlb_invalidate(guc);
+
 	if (!intel_guc_is_ready(guc)) {
 		guc->interrupts.enabled = false;
 		return;
@@ -872,6 +874,11 @@ static int __uc_resume(struct intel_uc *uc, bool enable_communication)
 
 	intel_gsc_uc_resume(&uc->gsc);
 
+	if (intel_guc_tlb_invalidation_is_available(guc)) {
+		intel_guc_invalidate_tlb_engines(guc);
+		intel_guc_invalidate_tlb_guc(guc);
+	}
+
 	return 0;
 }
 
diff --git a/drivers/gpu/drm/i915/i915_driver.c b/drivers/gpu/drm/i915/i915_driver.c
index 41214db459cb..ba140adb748c 100644
--- a/drivers/gpu/drm/i915/i915_driver.c
+++ b/drivers/gpu/drm/i915/i915_driver.c
@@ -1355,11 +1355,6 @@ static int i915_drm_resume(struct drm_device *dev)
 
 	intel_gvt_resume(dev_priv);
 
-	for_each_gt(gt, dev_priv, i) {
-		intel_guc_invalidate_tlb_full(&gt->uc.guc);
-		intel_guc_invalidate_tlb(&gt->uc.guc);
-	}
-
 	enable_rpm_wakeref_asserts(&dev_priv->runtime_pm);
 	intel_gt_bind_context_set_ready(to_gt(dev_priv), true);
 
-- 
2.25.1

