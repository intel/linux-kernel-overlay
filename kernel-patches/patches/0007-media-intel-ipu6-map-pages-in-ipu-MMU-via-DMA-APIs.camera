From 6d0eca8d59120d097d1cdae928b30e125832837e Mon Sep 17 00:00:00 2001
From: Ng Khai Wen <khai.wen.ng@intel.com>
Date: Wed, 14 Jul 2021 13:34:12 +0800
Subject: [PATCH 07/14] media: intel-ipu6: map pages in ipu MMU via DMA APIs

Change Description:
As the page table will be used by IPU DMA, so driver need map
all the physical pages by DMA APIs and fill the DMA address in
page table and register instead of use the physical addresses.

Signed-off-by: Bingbu Cao <bingbu.cao@intel.com>
Signed-off-by: Ng Khai Wen <khai.wen.ng@intel.com>
---
 drivers/media/pci/intel/ipu-mmu.c | 334 ++++++++++++++++++++++--------
 drivers/media/pci/intel/ipu-mmu.h |  15 +-
 2 files changed, 259 insertions(+), 90 deletions(-)

diff --git a/drivers/media/pci/intel/ipu-mmu.c b/drivers/media/pci/intel/ipu-mmu.c
index 3e51dea8980b..7d6e19278ee9 100644
--- a/drivers/media/pci/intel/ipu-mmu.c
+++ b/drivers/media/pci/intel/ipu-mmu.c
@@ -41,7 +41,6 @@
 #define MMUV2_REG_L2_STREAMID(i)	(0x4c + ((i) * 4))
 
 #define TBL_PHYS_ADDR(a)	((phys_addr_t)(a) << ISP_PADDR_SHIFT)
-#define TBL_VIRT_ADDR(a)	phys_to_virt(TBL_PHYS_ADDR(a))
 
 static void tlb_invalidate(struct ipu_mmu *mmu)
 {
@@ -77,36 +76,151 @@ static void page_table_dump(struct ipu_mmu_info *mmu_info)
 {
 	u32 l1_idx;
 
-	pr_debug("begin IOMMU page table dump\n");
+	dev_dbg(mmu_info->dev, "begin IOMMU page table dump\n");
 
 	for (l1_idx = 0; l1_idx < ISP_L1PT_PTES; l1_idx++) {
 		u32 l2_idx;
 		u32 iova = (phys_addr_t)l1_idx << ISP_L1PT_SHIFT;
 
-		if (mmu_info->pgtbl[l1_idx] == mmu_info->dummy_l2_tbl)
+		if (mmu_info->l1_pt[l1_idx] == mmu_info->dummy_l2_pteval)
 			continue;
-		pr_debug("l1 entry %u; iovas 0x%8.8x--0x%8.8x, at %p\n",
-			 l1_idx, iova, iova + ISP_PAGE_SIZE,
-			 (void *)TBL_PHYS_ADDR(mmu_info->pgtbl[l1_idx]));
+		dev_dbg(mmu_info->dev,
+			"l1 entry %u; iovas 0x%8.8x-0x%8.8x, at %p\n",
+			l1_idx, iova, iova + ISP_PAGE_SIZE,
+			(void *)TBL_PHYS_ADDR(mmu_info->l1_pt[l1_idx]));
 
 		for (l2_idx = 0; l2_idx < ISP_L2PT_PTES; l2_idx++) {
-			u32 *l2_pt = TBL_VIRT_ADDR(mmu_info->pgtbl[l1_idx]);
+			u32 *l2_pt = mmu_info->l2_pts[l1_idx];
 			u32 iova2 = iova + (l2_idx << ISP_L2PT_SHIFT);
 
-			if (l2_pt[l2_idx] == mmu_info->dummy_page)
+			if (l2_pt[l2_idx] == mmu_info->dummy_page_pteval)
 				continue;
 
-			pr_debug("\tl2 entry %u; iova 0x%8.8x, phys %p\n",
-				 l2_idx, iova2,
-				 (void *)TBL_PHYS_ADDR(l2_pt[l2_idx]));
+			dev_dbg(mmu_info->dev,
+				"\tl2 entry %u; iova 0x%8.8x, phys %p\n",
+				l2_idx, iova2,
+				(void *)TBL_PHYS_ADDR(l2_pt[l2_idx]));
 		}
 	}
 
-	pr_debug("end IOMMU page table dump\n");
+	dev_dbg(mmu_info->dev, "end IOMMU page table dump\n");
 }
 #endif /* DEBUG */
 
-static u32 *alloc_page_table(struct ipu_mmu_info *mmu_info, bool l1)
+static dma_addr_t map_single(struct ipu_mmu_info *mmu_info, void *ptr)
+{
+	dma_addr_t dma;
+
+	dma = dma_map_single(mmu_info->dev, ptr, PAGE_SIZE, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(mmu_info->dev, dma))
+		return 0;
+
+	return dma;
+}
+
+static int get_dummy_page(struct ipu_mmu_info *mmu_info)
+{
+	dma_addr_t dma;
+	void *pt = (void *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+
+	if (!pt)
+		return -ENOMEM;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	dma = map_single(mmu_info, pt);
+	if (!dma) {
+		dev_err(mmu_info->dev, "Failed to map dummy page\n");
+		goto err_free_page;
+	}
+
+	mmu_info->dummy_page = pt;
+	mmu_info->dummy_page_pteval = dma >> ISP_PAGE_SHIFT;
+
+	return 0;
+
+err_free_page:
+	free_page((unsigned long)pt);
+	return -ENOMEM;
+}
+
+static void free_dummy_page(struct ipu_mmu_info *mmu_info)
+{
+	dma_unmap_single(mmu_info->dev,
+			 TBL_PHYS_ADDR(mmu_info->dummy_page_pteval),
+			 PAGE_SIZE, DMA_BIDIRECTIONAL);
+	free_page((unsigned long)mmu_info->dummy_page);
+}
+
+static int alloc_dummy_l2_pt(struct ipu_mmu_info *mmu_info)
+{
+	dma_addr_t dma;
+	u32 *pt = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+	int i;
+
+	if (!pt)
+		return -ENOMEM;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	dma = map_single(mmu_info, pt);
+	if (!dma) {
+		dev_err(mmu_info->dev, "Failed to map l2pt page\n");
+		goto err_free_page;
+	}
+
+	for (i = 0; i < ISP_L2PT_PTES; i++)
+		pt[i] = mmu_info->dummy_page_pteval;
+
+	mmu_info->dummy_l2_pt = pt;
+	mmu_info->dummy_l2_pteval = dma >> ISP_PAGE_SHIFT;
+
+	return 0;
+
+err_free_page:
+	free_page((unsigned long)pt);
+	return -ENOMEM;
+}
+
+static void free_dummy_l2_pt(struct ipu_mmu_info *mmu_info)
+{
+	dma_unmap_single(mmu_info->dev,
+			 TBL_PHYS_ADDR(mmu_info->dummy_l2_pteval),
+			 PAGE_SIZE, DMA_BIDIRECTIONAL);
+	free_page((unsigned long)mmu_info->dummy_l2_pt);
+}
+
+static u32 *alloc_l1_pt(struct ipu_mmu_info *mmu_info)
+{
+	dma_addr_t dma;
+	u32 *pt = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+	int i;
+
+	if (!pt)
+		return NULL;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	for (i = 0; i < ISP_L1PT_PTES; i++)
+		pt[i] = mmu_info->dummy_l2_pteval;
+
+	dma = map_single(mmu_info, pt);
+	if (!dma) {
+		dev_err(mmu_info->dev, "Failed to map l1pt page\n");
+		goto err_free_page;
+	}
+
+	mmu_info->l1_pt_dma = dma >> ISP_PADDR_SHIFT;
+	dev_dbg(mmu_info->dev, "l1 pt %p mapped at %llx\n", pt, dma);
+
+	return pt;
+
+err_free_page:
+	free_page((unsigned long)pt);
+	return NULL;
+}
+
+static u32 *alloc_l2_pt(struct ipu_mmu_info *mmu_info)
 {
 	u32 *pt = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
 	int i;
@@ -114,10 +228,10 @@ static u32 *alloc_page_table(struct ipu_mmu_info *mmu_info, bool l1)
 	if (!pt)
 		return NULL;
 
-	pr_debug("get_zeroed_page() == %p\n", pt);
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
 
 	for (i = 0; i < ISP_L1PT_PTES; i++)
-		pt[i] = l1 ? mmu_info->dummy_l2_tbl : mmu_info->dummy_page;
+		pt[i] = mmu_info->dummy_page_pteval;
 
 	return pt;
 }
@@ -127,42 +241,57 @@ static int l2_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 {
 	u32 l1_idx = iova >> ISP_L1PT_SHIFT;
 	u32 l1_entry;
-	u32 *l2_pt;
+	u32 *l2_pt, *l2_virt;
 	u32 iova_start = iova;
 	unsigned int l2_idx;
 	unsigned long flags;
+	dma_addr_t dma;
 
-	pr_debug("mapping l2 page table for l1 index %u (iova %8.8x)\n",
-		 l1_idx, (u32)iova);
+	dev_dbg(mmu_info->dev,
+		"mapping l2 page table for l1 index %u (iova %8.8x)\n",
+		l1_idx, (u32)iova);
 
 	spin_lock_irqsave(&mmu_info->lock, flags);
-	l1_entry = mmu_info->pgtbl[l1_idx];
-	if (l1_entry == mmu_info->dummy_l2_tbl) {
-		u32 *l2_virt = alloc_page_table(mmu_info, false);
+	l1_entry = mmu_info->l1_pt[l1_idx];
+	if (l1_entry == mmu_info->dummy_l2_pteval) {
+		l2_virt = mmu_info->l2_pts[l1_idx];
+		if (likely(!l2_virt)) {
+			l2_virt = alloc_l2_pt(mmu_info);
+			if (!l2_virt) {
+				spin_unlock_irqrestore(&mmu_info->lock, flags);
+				return -ENOMEM;
+			}
+		}
 
-		if (!l2_virt) {
+		dma = map_single(mmu_info, l2_virt);
+		if (!dma) {
+			dev_err(mmu_info->dev, "Failed to map l2pt page\n");
+			free_page((unsigned long)l2_virt);
 			spin_unlock_irqrestore(&mmu_info->lock, flags);
-			return -ENOMEM;
+			return -EINVAL;
 		}
 
-		l1_entry = virt_to_phys(l2_virt) >> ISP_PADDR_SHIFT;
-		pr_debug("allocated page for l1_idx %u\n", l1_idx);
+		l1_entry = dma >> ISP_PADDR_SHIFT;
 
-		mmu_info->pgtbl[l1_idx] = l1_entry;
-		clflush_cache_range(&mmu_info->pgtbl[l1_idx],
-				    sizeof(mmu_info->pgtbl[l1_idx]));
+		dev_dbg(mmu_info->dev, "page for l1_idx %u %p allocated\n",
+			l1_idx, l2_virt);
+		mmu_info->l1_pt[l1_idx] = l1_entry;
+		mmu_info->l2_pts[l1_idx] = l2_virt;
+		clflush_cache_range(&mmu_info->l1_pt[l1_idx],
+				    sizeof(mmu_info->l1_pt[l1_idx]));
 	}
 
-	l2_pt = TBL_VIRT_ADDR(l1_entry);
+	l2_pt = mmu_info->l2_pts[l1_idx];
 
-	pr_debug("l2_pt at %p\n", l2_pt);
+	dev_dbg(mmu_info->dev, "l2_pt at %p with dma 0x%x\n", l2_pt, l1_entry);
 
 	paddr = ALIGN(paddr, ISP_PAGE_SIZE);
 
 	l2_idx = (iova_start & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
 
-	pr_debug("l2_idx %u, phys 0x%8.8x\n", l2_idx, l2_pt[l2_idx]);
-	if (l2_pt[l2_idx] != mmu_info->dummy_page) {
+	dev_dbg(mmu_info->dev, "l2_idx %u, phys 0x%8.8x\n", l2_idx,
+		l2_pt[l2_idx]);
+	if (l2_pt[l2_idx] != mmu_info->dummy_page_pteval) {
 		spin_unlock_irqrestore(&mmu_info->lock, flags);
 		return -EINVAL;
 	}
@@ -172,7 +301,8 @@ static int l2_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 	clflush_cache_range(&l2_pt[l2_idx], sizeof(l2_pt[l2_idx]));
 	spin_unlock_irqrestore(&mmu_info->lock, flags);
 
-	pr_debug("l2 index %u mapped as 0x%8.8x\n", l2_idx, l2_pt[l2_idx]);
+	dev_dbg(mmu_info->dev, "l2 index %u mapped as 0x%8.8x\n", l2_idx,
+		l2_pt[l2_idx]);
 
 	return 0;
 }
@@ -183,9 +313,9 @@ static int __ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 	u32 iova_start = round_down(iova, ISP_PAGE_SIZE);
 	u32 iova_end = ALIGN(iova + size, ISP_PAGE_SIZE);
 
-	pr_debug
-	    ("mapping iova 0x%8.8x--0x%8.8x, size %zu at paddr 0x%10.10llx\n",
-	     iova_start, iova_end, size, paddr);
+	dev_dbg(mmu_info->dev,
+		"mapping iova 0x%8.8x--0x%8.8x, size %zu at paddr 0x%10.10llx\n",
+		iova_start, iova_end, size, paddr);
 
 	return l2_map(mmu_info, iova_start, paddr, size);
 }
@@ -200,23 +330,26 @@ static size_t l2_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
 	size_t unmapped = 0;
 	unsigned long flags;
 
-	pr_debug("unmapping l2 page table for l1 index %u (iova 0x%8.8lx)\n",
-		 l1_idx, iova);
+	dev_dbg(mmu_info->dev, "unmapping l2 page table for l1 index %u (iova 0x%8.8lx)\n",
+		l1_idx, iova);
 
 	spin_lock_irqsave(&mmu_info->lock, flags);
-	if (mmu_info->pgtbl[l1_idx] == mmu_info->dummy_l2_tbl) {
+	if (mmu_info->l1_pt[l1_idx] == mmu_info->dummy_l2_pteval) {
 		spin_unlock_irqrestore(&mmu_info->lock, flags);
-		return -EINVAL;
+		dev_err(mmu_info->dev,
+			"unmap iova 0x%8.8lx l1 idx %u which was not mapped\n",
+			iova, l1_idx);
+		return 0;
 	}
 
 	for (l2_idx = (iova_start & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
 	     (iova_start & ISP_L1PT_MASK) + (l2_idx << ISP_PAGE_SHIFT)
 	     < iova_start + size && l2_idx < ISP_L2PT_PTES; l2_idx++) {
-		l2_pt = TBL_VIRT_ADDR(mmu_info->pgtbl[l1_idx]);
-		l2_pt[l2_idx] = mmu_info->dummy_page;
-
-		pr_debug("l2 index %u unmapped, was 0x%10.10llx\n",
-			 l2_idx, TBL_PHYS_ADDR(l2_pt[l2_idx]));
+		l2_pt = mmu_info->l2_pts[l1_idx];
+		dev_dbg(mmu_info->dev,
+			"unmap l2 index %u with pteval 0x%10.10llx\n",
+			l2_idx, TBL_PHYS_ADDR(l2_pt[l2_idx]));
+		l2_pt[l2_idx] = mmu_info->dummy_page_pteval;
 
 		clflush_cache_range(&l2_pt[l2_idx], sizeof(l2_pt[l2_idx]));
 		unmapped++;
@@ -238,6 +371,7 @@ static int allocate_trash_buffer(struct ipu_mmu *mmu)
 	struct iova *iova;
 	u32 iova_addr;
 	unsigned int i;
+	dma_addr_t dma;
 	int ret;
 
 	/* Allocate 8MB in iova range */
@@ -248,6 +382,16 @@ static int allocate_trash_buffer(struct ipu_mmu *mmu)
 		return -ENOMEM;
 	}
 
+	dma = dma_map_page(mmu->dmap->mmu_info->dev, mmu->trash_page, 0,
+			   PAGE_SIZE, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(mmu->dmap->mmu_info->dev, dma)) {
+		dev_err(mmu->dmap->mmu_info->dev, "Failed to map trash page\n");
+		ret = -ENOMEM;
+		goto out_free_iova;
+	}
+
+	mmu->trash_page_dma = dma;
+
 	/*
 	 * Map the 8MB iova address range to the same physical trash page
 	 * mmu->trash_page which is already reserved at the probe
@@ -255,7 +399,7 @@ static int allocate_trash_buffer(struct ipu_mmu *mmu)
 	iova_addr = iova->pfn_lo;
 	for (i = 0; i < n_pages; i++) {
 		ret = ipu_mmu_map(mmu->dmap->mmu_info, iova_addr << PAGE_SHIFT,
-				  page_to_phys(mmu->trash_page), PAGE_SIZE);
+				  mmu->trash_page_dma, PAGE_SIZE);
 		if (ret) {
 			dev_err(mmu->dev,
 				"mapping trash buffer range failed\n");
@@ -273,6 +417,9 @@ static int allocate_trash_buffer(struct ipu_mmu *mmu)
 out_unmap:
 	ipu_mmu_unmap(mmu->dmap->mmu_info, iova->pfn_lo << PAGE_SHIFT,
 		      (iova->pfn_hi - iova->pfn_lo + 1) << PAGE_SHIFT);
+	dma_unmap_page(mmu->dmap->mmu_info->dev, mmu->trash_page_dma,
+		       PAGE_SIZE, DMA_BIDIRECTIONAL);
+out_free_iova:
 	__free_iova(&mmu->dmap->iovad, iova);
 	return ret;
 }
@@ -294,9 +441,8 @@ int ipu_mmu_hw_init(struct ipu_mmu *mmu)
 		u16 block_addr;
 
 		/* Write page table address per MMU */
-		writel((phys_addr_t)virt_to_phys(mmu_info->pgtbl)
-			   >> ISP_PADDR_SHIFT,
-			   mmu->mmu_hw[i].base + REG_L1_PHYS);
+		writel((phys_addr_t)mmu_info->l1_pt_dma,
+		       mmu->mmu_hw[i].base + REG_L1_PHYS);
 
 		/* Set info bits per MMU */
 		writel(mmu->mmu_hw[i].info_bits,
@@ -357,7 +503,7 @@ EXPORT_SYMBOL(ipu_mmu_hw_init);
 static struct ipu_mmu_info *ipu_mmu_alloc(struct ipu_device *isp)
 {
 	struct ipu_mmu_info *mmu_info;
-	void *ptr;
+	int ret;
 
 	mmu_info = kzalloc(sizeof(*mmu_info), GFP_KERNEL);
 	if (!mmu_info)
@@ -365,40 +511,44 @@ static struct ipu_mmu_info *ipu_mmu_alloc(struct ipu_device *isp)
 
 	mmu_info->aperture_start = 0;
 	mmu_info->aperture_end = DMA_BIT_MASK(isp->secure_mode ?
-				      IPU_MMU_ADDRESS_BITS :
-				      IPU_MMU_ADDRESS_BITS_NON_SECURE);
+					      IPU_MMU_ADDRESS_BITS :
+					      IPU_MMU_ADDRESS_BITS_NON_SECURE);
 	mmu_info->pgsize_bitmap = SZ_4K;
+	mmu_info->dev = &isp->pdev->dev;
 
-	ptr = (void *)get_zeroed_page(GFP_KERNEL | GFP_DMA32);
-	if (!ptr)
-		goto err_mem;
-
-	mmu_info->dummy_page = virt_to_phys(ptr) >> ISP_PAGE_SHIFT;
+	ret = get_dummy_page(mmu_info);
+	if (ret)
+		goto err_free_info;
 
-	ptr = alloc_page_table(mmu_info, false);
-	if (!ptr)
-		goto err;
+	ret = alloc_dummy_l2_pt(mmu_info);
+	if (ret)
+		goto err_free_dummy_page;
 
-	mmu_info->dummy_l2_tbl = virt_to_phys(ptr) >> ISP_PAGE_SHIFT;
+	mmu_info->l2_pts = vzalloc(ISP_L2PT_PTES * sizeof(*mmu_info->l2_pts));
+	if (!mmu_info->l2_pts)
+		goto err_free_dummy_l2_pt;
 
 	/*
 	 * We always map the L1 page table (a single page as well as
 	 * the L2 page tables).
 	 */
-	mmu_info->pgtbl = alloc_page_table(mmu_info, true);
-	if (!mmu_info->pgtbl)
-		goto err;
+	mmu_info->l1_pt = alloc_l1_pt(mmu_info);
+	if (!mmu_info->l1_pt)
+		goto err_free_l2_pts;
 
 	spin_lock_init(&mmu_info->lock);
 
-	pr_debug("domain initialised\n");
+	dev_dbg(mmu_info->dev, "domain initialised\n");
 
 	return mmu_info;
 
-err:
-	free_page((unsigned long)TBL_VIRT_ADDR(mmu_info->dummy_page));
-	free_page((unsigned long)TBL_VIRT_ADDR(mmu_info->dummy_l2_tbl));
-err_mem:
+err_free_l2_pts:
+	vfree(mmu_info->l2_pts);
+err_free_dummy_l2_pt:
+	free_dummy_l2_pt(mmu_info);
+err_free_dummy_page:
+	free_dummy_page(mmu_info);
+err_free_info:
 	kfree(mmu_info);
 
 	return NULL;
@@ -434,7 +584,7 @@ static struct ipu_dma_mapping *alloc_dma_mapping(struct ipu_device *isp)
 
 	kref_init(&dmap->ref);
 
-	pr_debug("alloc mapping\n");
+	dev_dbg(&isp->pdev->dev, "alloc mapping\n");
 
 	iova_cache_get();
 
@@ -449,7 +599,7 @@ phys_addr_t ipu_mmu_iova_to_phys(struct ipu_mmu_info *mmu_info,
 	phys_addr_t phy_addr;
 
 	spin_lock_irqsave(&mmu_info->lock, flags);
-	l2_pt = TBL_VIRT_ADDR(mmu_info->pgtbl[iova >> ISP_L1PT_SHIFT]);
+	l2_pt = mmu_info->l2_pts[iova >> ISP_L1PT_SHIFT];
 	phy_addr = (phys_addr_t)l2_pt[(iova & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT];
 	phy_addr <<= ISP_PAGE_SHIFT;
 	spin_unlock_irqrestore(&mmu_info->lock, flags);
@@ -527,7 +677,7 @@ size_t ipu_mmu_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
 		if (!unmapped_page)
 			break;
 
-		dev_dbg(NULL, "unmapped: iova 0x%lx size 0x%zx\n",
+		dev_dbg(mmu_info->dev, "unmapped: iova 0x%lx size 0x%zx\n",
 			iova, unmapped_page);
 
 		iova += unmapped_page;
@@ -558,19 +708,22 @@ int ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
 	 * size of the smallest page supported by the hardware
 	 */
 	if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
-		pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
-		       iova, &paddr, size, min_pagesz);
+		dev_err(mmu_info->dev,
+			"unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
+			iova, &paddr, size, min_pagesz);
 		return -EINVAL;
 	}
 
-	pr_debug("map: iova 0x%lx pa %pa size 0x%zx\n", iova, &paddr, size);
+	dev_dbg(mmu_info->dev, "map: iova 0x%lx pa %pa size 0x%zx\n",
+		iova, &paddr, size);
 
 	while (size) {
 		size_t pgsize = ipu_mmu_pgsize(mmu_info->pgsize_bitmap,
 					       iova | paddr, size);
 
-		pr_debug("mapping: iova 0x%lx pa %pa pgsize 0x%zx\n",
-			 iova, &paddr, pgsize);
+		dev_dbg(mmu_info->dev,
+			"mapping: iova 0x%lx pa %pa pgsize 0x%zx\n",
+			iova, &paddr, pgsize);
 
 		ret = __ipu_mmu_map(mmu_info, iova, paddr, pgsize);
 		if (ret)
@@ -609,19 +762,26 @@ static void ipu_mmu_destroy(struct ipu_mmu *mmu)
 		}
 
 		mmu->iova_trash_page = 0;
-	}
-
-	if (mmu->trash_page)
+		dma_unmap_page(mmu_info->dev, mmu->trash_page_dma,
+			       PAGE_SIZE, DMA_BIDIRECTIONAL);
+		mmu->trash_page_dma = 0;
 		__free_page(mmu->trash_page);
+	}
 
-	for (l1_idx = 0; l1_idx < ISP_L1PT_PTES; l1_idx++)
-		if (mmu_info->pgtbl[l1_idx] != mmu_info->dummy_l2_tbl)
-			free_page((unsigned long)
-				  TBL_VIRT_ADDR(mmu_info->pgtbl[l1_idx]));
+	for (l1_idx = 0; l1_idx < ISP_L1PT_PTES; l1_idx++) {
+		if (mmu_info->l1_pt[l1_idx] != mmu_info->dummy_l2_pteval) {
+			dma_unmap_single(mmu_info->dev,
+					 TBL_PHYS_ADDR(mmu_info->l1_pt[l1_idx]),
+					 PAGE_SIZE, DMA_BIDIRECTIONAL);
+			free_page((unsigned long)mmu_info->l2_pts[l1_idx]);
+		}
+	}
 
-	free_page((unsigned long)TBL_VIRT_ADDR(mmu_info->dummy_page));
-	free_page((unsigned long)TBL_VIRT_ADDR(mmu_info->dummy_l2_tbl));
-	free_page((unsigned long)mmu_info->pgtbl);
+	free_dummy_page(mmu_info);
+	dma_unmap_single(mmu_info->dev, mmu_info->l1_pt_dma,
+			 PAGE_SIZE, DMA_BIDIRECTIONAL);
+	free_page((unsigned long)mmu_info->dummy_l2_pt);
+	free_page((unsigned long)mmu_info->l1_pt);
 	kfree(mmu_info);
 }
 
diff --git a/drivers/media/pci/intel/ipu-mmu.h b/drivers/media/pci/intel/ipu-mmu.h
index 40f3cb4b220c..ea9b54392aa4 100644
--- a/drivers/media/pci/intel/ipu-mmu.h
+++ b/drivers/media/pci/intel/ipu-mmu.h
@@ -16,15 +16,23 @@
  * @pgtbl: virtual address of the l1 page table (one page)
  */
 struct ipu_mmu_info {
-	u32 __iomem *pgtbl;
+	struct device *dev;
+
+	u32 __iomem *l1_pt;
+	u32 l1_pt_dma;
+	u32 **l2_pts;
+
+	u32 *dummy_l2_pt;
+	u32 dummy_l2_pteval;
+	void *dummy_page;
+	u32 dummy_page_pteval;
+
 	dma_addr_t aperture_start;
 	dma_addr_t aperture_end;
 	unsigned long pgsize_bitmap;
 
 	spinlock_t lock;	/* Serialize access to users */
 	struct ipu_dma_mapping *dmap;
-	u32 dummy_l2_tbl;
-	u32 dummy_page;
 };
 
 /*
@@ -44,6 +52,7 @@ struct ipu_mmu {
 	struct list_head vma_list;
 
 	struct page *trash_page;
+	dma_addr_t trash_page_dma;
 	dma_addr_t iova_trash_page;
 
 	bool ready;
-- 
2.25.1

