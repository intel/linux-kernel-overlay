From fb028184a9716cb00db31f5efedc2a0dbdbafb25 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Piotr=20Pi=C3=B3rkowski?= <piotr.piorkowski@intel.com>
Date: Fri, 26 Apr 2024 09:24:10 +0200
Subject: [PATCH 02/14] drm/i915/iov: Introduce VFs shadow copy of GGTT on PF
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Due to problems accessing GGTT after through BAR0, we need to have
an alternative way to GGTT save/restore GGTT during migration.
The solution is to keep track of all GGTT changes that VFs make and
store them on PF so that we can do a GGTT save/restore based on it
during the migration.
Add shadow copy of GGTT to storing up-to-date VFs GGTT on PF.

HSDES: 22018453856

Signed-off-by: Piotr Pi√≥rkowski <piotr.piorkowski@intel.com>
Signed-off-by: Zawawi, Muhammad Zul Husni <muhammad.zul.husni.zawawi@intel.com>
---
 drivers/gpu/drm/i915/gt/intel_ggtt.c          |  41 ++-
 drivers/gpu/drm/i915/gt/intel_gtt.h           |   5 +-
 drivers/gpu/drm/i915/gt/iov/intel_iov.c       |  14 +-
 drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.c  | 324 +++++++++++++++++-
 drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.h  |  16 +
 .../drm/i915/gt/iov/intel_iov_provisioning.c  |   6 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.c |   1 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_types.h |  21 ++
 8 files changed, 419 insertions(+), 9 deletions(-)

diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index 179dfc37a1f8..30bada161b49 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -1847,8 +1847,29 @@ static int sgtable_update_ptes_via_cpu(struct i915_ggtt *ggtt, u32 ggtt_addr, st
 	return n;
 }
 
-int i915_ggtt_sgtable_update_ptes(struct i915_ggtt *ggtt, u32 ggtt_addr, struct sg_table *st,
-				  u32 num_entries, const gen8_pte_t pte_pattern)
+static void sgtable_update_shadow_ggtt(struct i915_ggtt *ggtt, unsigned int vfid,
+					    u64 ggtt_addr, struct sg_table *st, u32 num_entries,
+					    const gen8_pte_t pte_pattern)
+{
+	struct intel_iov *iov = &ggtt->vm.gt->iov;
+	dma_addr_t addr;
+	struct sgt_iter iter;
+
+	if (!st) {
+		while (num_entries--) {
+			intel_iov_ggtt_shadow_set_pte(iov, vfid, ggtt_addr, pte_pattern);
+			ggtt_addr += I915_GTT_PAGE_SIZE_4K;
+		}
+		return;
+	}
+
+	for_each_sgt_daddr(addr, iter, st)
+		intel_iov_ggtt_shadow_set_pte(iov, vfid, ggtt_addr, pte_pattern | addr);
+}
+
+int i915_ggtt_sgtable_update_ptes(struct i915_ggtt *ggtt, unsigned int vfid, u64 ggtt_addr,
+				  struct sg_table *st, u32 num_entries,
+				  const gen8_pte_t pte_pattern)
 {
 	int ret;
 
@@ -1858,6 +1879,18 @@ int i915_ggtt_sgtable_update_ptes(struct i915_ggtt *ggtt, u32 ggtt_addr, struct
 	else
 		ret = sgtable_update_ptes_via_cpu(ggtt, ggtt_addr, st, num_entries, pte_pattern);
 
+	if (ret <= 0)
+		goto out;
+
+	/*
+	 * If we update the GGTT for PF in this function, it means that we
+	 * release the GGTT owned by VF. In this case, we don't need to update
+	 * the GGTT shadow, because it should be removed immediately.
+	 */
+	if (vfid != PFID)
+		sgtable_update_shadow_ggtt(ggtt, vfid, ggtt_addr, st, num_entries, pte_pattern);
+
+out:
 	return (ret) ? 0 : -EIO;
 }
 
@@ -1891,8 +1924,8 @@ void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
 	/* Wa_22018453856 */
 	if (i915_ggtt_require_binder(ggtt->vm.i915) &&
 	    should_update_ggtt_with_bind(ggtt) &&
-	    gen8_ggtt_bind_ptes(ggtt, base >> PAGE_SHIFT, NULL, size / PAGE_SIZE, pte))
-		goto invalidate;
+	    i915_ggtt_sgtable_update_ptes(ggtt, vfid, base, NULL, size / PAGE_SIZE, pte))
+			goto invalidate;
 
 	gtt_entries += base >> PAGE_SHIFT;
 	while (size) {
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.h b/drivers/gpu/drm/i915/gt/intel_gtt.h
index 88645355b620..c91023e55b14 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.h
@@ -627,8 +627,9 @@ int i915_ggtt_balloon(struct i915_ggtt *ggtt, u64 start, u64 end,
 		      struct drm_mm_node *node);
 void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node);
 
-int i915_ggtt_sgtable_update_ptes(struct i915_ggtt *ggtt, u32 offset, struct sg_table *st,
-				  u32 num_entries, const gen8_pte_t pte_pattern);
+int i915_ggtt_sgtable_update_ptes(struct i915_ggtt *ggtt, unsigned int vfid, u64 ggtt_addr,
+				  struct sg_table *st, u32 num_entries,
+				  const gen8_pte_t pte_pattern);
 gen8_pte_t i915_ggtt_prepare_vf_pte(u16 vfid);
 void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
 			       const struct drm_mm_node *node);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov.c b/drivers/gpu/drm/i915/gt/iov/intel_iov.c
index ecd483ea93d2..edcd76b7b33d 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov.c
@@ -199,7 +199,15 @@ int intel_iov_init_ggtt(struct intel_iov *iov)
 {
 	int err;
 
-	if (intel_iov_is_vf(iov)) {
+	if (intel_iov_is_pf(iov)) {
+		/* Wa_22018453856 */
+		if (i915_ggtt_require_binder(iov_to_i915(iov)) &&
+		    iov_to_gt(iov)->type != GT_MEDIA) {
+			err = intel_iov_ggtt_shadow_init(iov);
+			if (unlikely(err))
+				return err;
+		}
+	} else if (intel_iov_is_vf(iov)) {
 		err = vf_balloon_ggtt(iov);
 		if (unlikely(err))
 			return err;
@@ -214,7 +222,9 @@ int intel_iov_init_ggtt(struct intel_iov *iov)
  */
 void intel_iov_fini_ggtt(struct intel_iov *iov)
 {
-	if (intel_iov_is_vf(iov))
+	if (intel_iov_is_pf(iov))
+		intel_iov_ggtt_shadow_fini(iov);
+	else if (intel_iov_is_vf(iov))
 		vf_deballoon_ggtt(iov);
 }
 
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.c
index 54dc6c31eeb8..a24b27d4dd66 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.c
@@ -118,7 +118,7 @@ int intel_iov_ggtt_pf_update_vf_ptes(struct intel_iov *iov, u32 vfid, u32 pte_of
 		goto cleanup;
 	}
 
-	err = i915_ggtt_sgtable_update_ptes(iov_to_gt(iov)->ggtt, ggtt_addr, st, n_ptes,
+	err = i915_ggtt_sgtable_update_ptes(iov_to_gt(iov)->ggtt, vfid, ggtt_addr, st, n_ptes,
 					    pte_pattern);
 cleanup:
 	sg_free_table(st);
@@ -262,3 +262,325 @@ void intel_iov_ggtt_vf_flush_ptes(struct intel_iov *iov)
 	intel_iov_query_update_ggtt_ptes(iov);
 	buffer->count = 0;
 }
+
+/**
+ * intel_iov_ggtt_shadow_init - allocate general shadow GGTT resources
+ * @iov: the &struct intel_iov
+ *
+ * Return: 0 if success. Negative error code on error.
+ */
+int intel_iov_ggtt_shadow_init(struct intel_iov *iov)
+{
+	struct intel_iov_ggtt_shadow *vfs_shadows_ggtt;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(iov->pf.ggtt.shadows_ggtt);
+
+	vfs_shadows_ggtt = kcalloc(1 + pf_get_totalvfs(iov), sizeof(*vfs_shadows_ggtt), GFP_KERNEL);
+	if (unlikely(!vfs_shadows_ggtt))
+		return -ENOMEM;
+
+	iov->pf.ggtt.shadows_ggtt = vfs_shadows_ggtt;
+
+	return 0;
+}
+
+/**
+ * intel_iov_ggtt_shadow_fini - free general shadow  resources
+ * @iov: the &struct intel_iov
+ */
+void intel_iov_ggtt_shadow_fini(struct intel_iov *iov)
+{
+	kfree(iov->pf.ggtt.shadows_ggtt);
+	iov->pf.ggtt.shadows_ggtt = NULL;
+}
+
+/**
+ * intel_iov_ggtt_shadow_vf_alloc - allocate VF shadow GGTT resources
+ * @iov: the &struct intel_iov
+ * @vfid: VF ID
+ * @ggtt_region: pointer to VF GGTT region
+ *
+ * Return: 0 if success or shadow GGTT not initialized. Negative error code on error.
+ */
+int intel_iov_ggtt_shadow_vf_alloc(struct intel_iov *iov, unsigned int vfid,
+				   struct drm_mm_node *ggtt_region)
+{
+	gen8_pte_t *ptes;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return 0;
+
+	GEM_BUG_ON(!drm_mm_node_allocated(ggtt_region));
+	GEM_BUG_ON(iov->pf.ggtt.shadows_ggtt[vfid].ptes);
+
+	ptes = kvzalloc(ggtt_size_to_ptes_size(ggtt_region->size), GFP_KERNEL);
+	if (unlikely(!ptes))
+		return -ENOMEM;
+
+	iov->pf.ggtt.shadows_ggtt[vfid].ptes = ptes;
+	iov->pf.ggtt.shadows_ggtt[vfid].ggtt_region = ggtt_region;
+	iov->pf.ggtt.shadows_ggtt[vfid].vfid = vfid;
+
+	return 0;
+}
+
+/**
+ * intel_iov_ggtt_shadow_vf_free - free shadow GGTT resources allocated for VF
+ * @iov: the &struct intel_iov
+ * @vfid: VF ID
+ *
+ * Skip if shadow GGTT not initialized.
+ */
+void intel_iov_ggtt_shadow_vf_free(struct intel_iov *iov, unsigned int vfid)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return;
+
+	kvfree(iov->pf.ggtt.shadows_ggtt[vfid].ptes);
+	iov->pf.ggtt.shadows_ggtt[vfid].ptes = NULL;
+}
+
+static u64 ggtt_addr_to_pte_offset(u64 ggtt_addr)
+{
+	GEM_BUG_ON(!IS_ALIGNED(ggtt_addr, I915_GTT_PAGE_SIZE_4K));
+
+	return (ggtt_addr / I915_GTT_PAGE_SIZE_4K) * sizeof(gen8_pte_t);
+}
+
+static u64 pf_ggtt_addr_to_vf_pte_offset(struct intel_iov *iov, unsigned int vfid, u64 ggtt_addr)
+{
+	struct drm_mm_node *ggtt_region;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	ggtt_region = iov->pf.ggtt.shadows_ggtt[vfid].ggtt_region;
+
+	GEM_BUG_ON(ggtt_region->start > ggtt_addr ||
+		   ggtt_region->start + ggtt_region->size <= ggtt_addr);
+
+	return ggtt_addr_to_pte_offset(ggtt_addr - ggtt_region->start);
+}
+
+static gen8_pte_t *ggtt_shadow_get_pte_ptr(struct intel_iov *iov, unsigned int vfid, u64 ggtt_addr)
+{
+	u32 pte_idx;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(!IS_ALIGNED(ggtt_addr, sizeof(gen8_pte_t)));
+
+	pte_idx = pf_ggtt_addr_to_vf_pte_offset(iov, vfid, ggtt_addr) / sizeof(gen8_pte_t);
+	return &iov->pf.ggtt.shadows_ggtt[vfid].ptes[pte_idx];
+}
+
+/**
+ * intel_iov_ggtt_shadow_set_pte - set VF GGTT PTE in shadow GGTT
+ * @iov: the &struct intel_iov
+ * @vfid: VF id
+ * @ggtt_addr: GGTT address
+ * @pte: PTE value to save
+ *
+ */
+void intel_iov_ggtt_shadow_set_pte(struct intel_iov *iov, unsigned int vfid, u64 ggtt_addr,
+					gen8_pte_t pte)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return;
+
+	memset64(ggtt_shadow_get_pte_ptr(iov, vfid, ggtt_addr), pte, 1);
+}
+
+/**
+ * intel_iov_ggtt_shadow_get_pte - get VF GGTT PTE from shadow GGTT
+ * @iov: the &struct intel_iov
+ * @vfid: VF ID
+ * @ggtt_addr: GGTT address
+ *
+ * Returns: PTE value for given @ggtt_addr, or 0 if shadow GGTT not initialized.
+ */
+gen8_pte_t intel_iov_ggtt_shadow_get_pte(struct intel_iov *iov, unsigned int vfid, u64 ggtt_addr)
+{
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return 0;
+
+	return *(ggtt_shadow_get_pte_ptr(iov, vfid, ggtt_addr));
+}
+
+/**
+ * intel_iov_ggtt_shadow_save - copy VF GGTT PTEs to preallocated buffer
+ * @iov: the &struct intel_iov
+ * @vfid: VF id
+ * @buf: preallocated buffer in which PTEs will be saved
+ * @size: size of preallocated buffer (in bytes)
+ *        - must be sizeof(gen8_pte_t) aligned
+ * @flags: function flags:
+ *         - #I915_GGTT_SAVE_PTES_NO_VFID BIT - save PTEs without VFID
+ *
+ * Returns: size of the buffer used (or needed if both @buf and @size are (0)) to store all PTEs
+ *          for a given vfid, -EINVAL if one of @buf or @size is 0.
+ */
+int intel_iov_ggtt_shadow_save(struct intel_iov *iov, unsigned int vfid, void *buf, size_t size,
+			       unsigned int flags)
+{
+	struct drm_mm_node *ggtt_region;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return 0;
+
+	ggtt_region = iov->pf.ggtt.shadows_ggtt[vfid].ggtt_region;
+
+	if (!buf && !size)
+		return ggtt_size_to_ptes_size(ggtt_region->size);
+
+	if (!buf || !size)
+		return -EINVAL;
+
+	if (size > ggtt_size_to_ptes_size(ggtt_region->size))
+		return -ENOSPC;
+
+	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
+
+	memcpy(buf, iov->pf.ggtt.shadows_ggtt[vfid].ptes, size);
+
+	if (flags & I915_GGTT_SAVE_PTES_NO_VFID)
+		ggtt_pte_clear_vfid(buf, size);
+
+	return size;
+}
+
+static int pf_ggtt_shadow_restore_ggtt(struct intel_iov *iov, unsigned int vfid)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	u32 pte_count = 0;
+	gen8_pte_t pte_flags = 0, new_pte_flags;
+	struct drm_mm_node *ggtt_region;
+	size_t size;
+	u64 ggtt_addr;
+	struct sg_table *st;
+	struct scatterlist *sg;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return 0;
+
+	ggtt_region = iov->pf.ggtt.shadows_ggtt[vfid].ggtt_region;
+	size = ggtt_size_to_ptes_size(ggtt_region->size);
+	ggtt_addr = ggtt_region->start;
+
+	st = kmalloc(sizeof(*st), GFP_KERNEL);
+	if (!st)
+		return -ENOMEM;
+
+	if (sg_alloc_table(st, size / sizeof(gen8_pte_t), GFP_KERNEL)) {
+		err = -ENOMEM;
+		goto out_free_table;
+	}
+
+	sg = st->sgl;
+	st->nents = 0;
+
+	while (size) {
+		gen8_pte_t pte = intel_iov_ggtt_shadow_get_pte(iov, vfid, ggtt_addr);
+
+		new_pte_flags = pte & ~GEN12_GGTT_PTE_ADDR_MASK;
+		if (pte_count && new_pte_flags != pte_flags) {
+			err = i915_ggtt_sgtable_update_ptes(ggtt, vfid, ggtt_addr, st, pte_count,
+							    pte_flags);
+			if (err < 0)
+				goto out_free_st;
+
+			sg_free_table(st);
+			if (sg_alloc_table(st, size / sizeof(gen8_pte_t), GFP_KERNEL)) {
+				err = -ENOMEM;
+				goto out_free_st;
+			}
+
+			pte_count = 0;
+		}
+		sg = sg_add_pte(st, sg, pte);
+
+		pte_count++;
+		pte_flags = new_pte_flags;
+		ggtt_addr += I915_GTT_PAGE_SIZE_4K;
+		size -= sizeof(gen8_pte_t);
+	}
+
+	err = i915_ggtt_sgtable_update_ptes(ggtt, vfid, ggtt_addr, st, pte_count, pte_flags);
+
+out_free_table:
+	sg_free_table(st);
+out_free_st:
+	kfree(st);
+	return err;
+}
+
+/**
+ * intel_iov_ggtt_shadow_restore() - restore GGTT PTEs from buffer
+ * @iov: the &struct intel_iov
+ * @vfid: VF id
+ * @buf: buffer from which PTEs will be restored
+ * @size: size of preallocated buffer (in bytes)
+ *        - must be sizeof(gen8_pte_t) aligned
+ * @flags: function flags:
+ *         - #I915_GGTT_RESTORE_PTES_VFID_MASK - VFID for restored PTEs
+ *         - #I915_GGTT_RESTORE_PTES_NEW_VFID - restore PTEs with new VFID
+ *           (from #I915_GGTT_RESTORE_PTES_VFID_MASK)
+ *
+ * Returns: size of restored PTES on success, or negative error code.
+ */
+int intel_iov_ggtt_shadow_restore(struct intel_iov *iov, unsigned int vfid, const void *buf,
+				       size_t size, unsigned int flags)
+{
+	struct drm_mm_node *ggtt_region;
+	u64 ggtt_addr;
+	size_t remain_size;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(flags & I915_GGTT_RESTORE_PTES_NEW_VFID &&
+		   vfid != FIELD_GET(I915_GGTT_RESTORE_PTES_VFID_MASK, flags));
+	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
+
+	if (!iov->pf.ggtt.shadows_ggtt)
+		return 0;
+
+	ggtt_region = iov->pf.ggtt.shadows_ggtt[vfid].ggtt_region;
+
+	if (size > ggtt_size_to_ptes_size(ggtt_region->size))
+		return -ENOSPC;
+
+	if (!buf || !size)
+		return -EINVAL;
+
+	ggtt_addr = ggtt_region->start;
+	remain_size = size;
+
+	while (remain_size) {
+		gen8_pte_t pte = *(gen8_pte_t *)buf;
+
+		if (flags & I915_GGTT_RESTORE_PTES_NEW_VFID)
+			pte |= i915_ggtt_prepare_vf_pte(vfid);
+
+		intel_iov_ggtt_shadow_set_pte(iov, vfid, ggtt_addr, pte);
+
+		ggtt_addr += I915_GTT_PAGE_SIZE_4K;
+		buf += sizeof(gen8_pte_t);
+		remain_size -= sizeof(gen8_pte_t);
+	}
+
+	err = pf_ggtt_shadow_restore_ggtt(iov, vfid);
+
+	return err ?: size;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.h
index 3de1a537e606..a66e6c322862 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_ggtt.h
@@ -19,4 +19,20 @@ void intel_iov_ggtt_vf_release(struct intel_iov *iov);
 void intel_iov_ggtt_vf_update_pte(struct intel_iov *iov, u32 offset, gen8_pte_t pte);
 void intel_iov_ggtt_vf_flush_ptes(struct intel_iov *iov);
 
+int intel_iov_ggtt_shadow_init(struct intel_iov *iov);
+void intel_iov_ggtt_shadow_fini(struct intel_iov *iov);
+
+int intel_iov_ggtt_shadow_vf_alloc(struct intel_iov *iov, unsigned int vfid,
+				   struct drm_mm_node *ggtt_region);
+void intel_iov_ggtt_shadow_vf_free(struct intel_iov *iov, unsigned int vfid);
+
+void intel_iov_ggtt_shadow_set_pte(struct intel_iov *iov, unsigned int vfid, u64 pte_offset,
+				   gen8_pte_t pte);
+gen8_pte_t intel_iov_ggtt_shadow_get_pte(struct intel_iov *iov, unsigned int vfid, u64 pte_offset);
+
+int intel_iov_ggtt_shadow_save(struct intel_iov *iov, unsigned int vfid, void *buf, size_t size,
+			       unsigned int flags);
+int intel_iov_ggtt_shadow_restore(struct intel_iov *iov, unsigned int vfid, const void *buf,
+				  size_t size, unsigned int flags);
+
 #endif /* __INTEL_IOV_GGTT_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
index ba67c0022ba9..afb0eb37899d 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
@@ -4,6 +4,7 @@
  */
 
 #include "intel_iov.h"
+#include "intel_iov_ggtt.h"
 #include "intel_iov_provisioning.h"
 #include "intel_iov_utils.h"
 #include "gt/intel_gt.h"
@@ -654,6 +655,7 @@ static int pf_provision_ggtt(struct intel_iov *iov, unsigned int id, u64 size)
 		err = pf_push_config_ggtt(iov, id, 0, 0);
 release:
 		i915_ggtt_set_space_owner(ggtt, 0, node);
+		intel_iov_ggtt_shadow_vf_free(iov, id);
 
 		mutex_lock(&ggtt->vm.mutex);
 		drm_mm_remove_node(node);
@@ -682,6 +684,10 @@ static int pf_provision_ggtt(struct intel_iov *iov, unsigned int id, u64 size)
 	if (unlikely(err))
 		return err;
 
+	err = intel_iov_ggtt_shadow_vf_alloc(iov, id, node);
+	if (unlikely(err))
+		goto release;
+
 	i915_ggtt_set_space_owner(ggtt, id, node);
 
 	err = pf_push_config_ggtt(iov, id, node->start, node->size);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
index 169a56b91ffd..17394d8725e8 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
@@ -6,6 +6,7 @@
 #include "i915_pci.h"
 #include "intel_iov.h"
 #include "intel_iov_event.h"
+#include "intel_iov_ggtt.h"
 #include "intel_iov_state.h"
 #include "intel_iov_utils.h"
 #include "gt/intel_gt.h"
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
index ee2884250df2..4a50af4566b2 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
@@ -179,6 +179,26 @@ struct intel_iov_vf_runtime {
 	} *regs;
 };
 
+/**
+ * struct intel_iov_ggtt_shadow - shadow GGTT data for single VF.
+ * @ptes: pointer to a buffer that stores the GGTT PTEs of a specific VF.
+ * @ggtt_region: pointer to the ggtt_region assigned to a specific VF during provisioning.
+ * @vfid: vfid VF, to which the data in this structure belongs.
+ */
+struct intel_iov_ggtt_shadow {
+	gen8_pte_t *ptes;
+	struct drm_mm_node *ggtt_region;
+	unsigned int vfid;
+};
+
+/**
+ * struct intel_iov_pf_ggtt - PF-specific GGTT data.
+ * @shadows_ggtt: shadow GGTT VFs array.
+ */
+struct intel_iov_pf_ggtt {
+	struct intel_iov_ggtt_shadow *shadows_ggtt;
+};
+
 /**
  * struct intel_iov_vf_ggtt_ptes - Placeholder for the VF PTEs data.
  * @ptes: an array of buffered GGTT PTEs awaiting update by PF.
@@ -273,6 +293,7 @@ struct intel_iov {
 			struct intel_iov_provisioning provisioning;
 			struct intel_iov_service service;
 			struct intel_iov_state state;
+			struct intel_iov_pf_ggtt ggtt;
 		} pf;
 
 		struct {
-- 
2.25.1

