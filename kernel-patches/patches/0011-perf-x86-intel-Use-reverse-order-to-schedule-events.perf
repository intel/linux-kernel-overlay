From 75b5380bf0a16d4879124047e5a24e0a9c62dee1 Mon Sep 17 00:00:00 2001
From: Kan Liang <kan.liang@linux.intel.com>
Date: Fri, 10 Sep 2021 10:55:19 -0700
Subject: [PATCH 11/12] perf/x86/intel: Use reverse order to schedule events
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The below case is the only scenario I can imagine in which the reversed
scheduling is better.

There must be three perf users on a SPR machine.
A: have a regular event globally.
B: have a regular event to monitor a task.
C: have a :ppp event monitor another task.

With the current scheduling,
- Counter 0 is assigned to A.
- When B is sched in, counter 1 is assigned to B.
- When B is sched out and C is sched in, :ppp event must use counter 0.
  So  in x86_pmu_enable(), perf has to stop the running A and re-assign
counter 1 to A.
   The x86_pmu_stop() should be expensive because it may writes two
registers and drain PEBS.
- When C is sched out and B is sched in, A is a global CPU event which
  is assigned first. So counter 0 will still be assigned to A. Counter 1
to B.
- When B is sched out and C is sched in again, we have call expensive
  x86_pmu_stop() again to take over the counter 0 from A.
Basically, every time C is sched in, the expensive x86_pmu_stop() will
be invoked, which will increase the overhead of the context switch.

With the reversed scheduling, counter 7 will be assigned to A. C doesnâ€™t
need to stop the running A.

Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
---
 arch/x86/events/core.c | 14 +++++++++-----
 1 file changed, 9 insertions(+), 5 deletions(-)

diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index c2b2dc04510f..7c367b59a86a 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -820,6 +820,7 @@ static void perf_sched_init(struct perf_sched *sched, struct event_constraint **
 	sched->state.event	= idx;		/* start with min weight */
 	sched->state.weight	= wmin;
 	sched->state.unassigned	= num;
+	sched->state.counter	= INTEL_PMC_IDX_FIXED;
 }
 
 static void perf_sched_save_state(struct perf_sched *sched)
@@ -844,7 +845,7 @@ static bool perf_sched_restore_state(struct perf_sched *sched)
 	sched->state.used &= ~BIT_ULL(sched->state.counter);
 
 	/* try the next one */
-	sched->state.counter++;
+	sched->state.counter--;
 
 	return true;
 }
@@ -856,6 +857,7 @@ static bool perf_sched_restore_state(struct perf_sched *sched)
 static bool __perf_sched_find_counter(struct perf_sched *sched)
 {
 	struct event_constraint *c;
+	u64 idxmsk;
 	int idx;
 
 	if (!sched->state.unassigned)
@@ -879,14 +881,16 @@ static bool __perf_sched_find_counter(struct perf_sched *sched)
 		}
 	}
 
-	/* Grab the first unused counter starting with idx */
-	idx = sched->state.counter;
-	for_each_set_bit_from(idx, c->idxmsk, INTEL_PMC_IDX_FIXED) {
+	/* Grab the first unused counter in a reverse order */
+	idxmsk = c->idxmsk64 & ((1ULL << INTEL_PMC_IDX_FIXED) - 1);
+	while ((idx = find_last_bit((unsigned long *)&idxmsk, sched->state.counter)) < sched->state.counter) {
 		u64 mask = BIT_ULL(idx);
 
 		if (c->flags & PERF_X86_EVENT_PAIR)
 			mask |= mask << 1;
 
+		idxmsk &= ~mask;
+
 		if (sched->state.used & mask)
 			continue;
 
@@ -942,7 +946,7 @@ static bool perf_sched_next_event(struct perf_sched *sched)
 		c = sched->constraints[sched->state.event];
 	} while (c->weight != sched->state.weight);
 
-	sched->state.counter = 0;	/* start with first counter */
+	sched->state.counter = INTEL_PMC_IDX_FIXED;
 
 	return true;
 }
-- 
2.32.0

