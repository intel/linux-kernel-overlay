From d929768267b41125b479b53e6c01b208b7e5bb15 Mon Sep 17 00:00:00 2001
From: Junxiao Chang <junxiao.chang@intel.com>
Date: Wed, 24 Nov 2021 13:16:37 +0800
Subject: [PATCH 025/888] Revert "drm/i915: Multi-BB execbuf"

This reverts commit 95eed6488ef2ab6d7947630f596a1764cbfa31bc.
---
 .../gpu/drm/i915/gem/i915_gem_execbuffer.c    | 767 ++++++------------
 drivers/gpu/drm/i915/gt/intel_context.h       |   8 +-
 drivers/gpu/drm/i915/gt/intel_context_types.h |  12 -
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c |   2 -
 drivers/gpu/drm/i915/i915_request.h           |   9 -
 drivers/gpu/drm/i915/i915_vma.c               |  21 +-
 drivers/gpu/drm/i915/i915_vma.h               |  13 +-
 7 files changed, 257 insertions(+), 575 deletions(-)

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
index 4f28c42c6dc4..1aa249908b64 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
@@ -244,23 +244,17 @@ struct i915_execbuffer {
 	struct drm_i915_gem_exec_object2 *exec; /** ioctl execobj[] */
 	struct eb_vma *vma;
 
-	struct intel_gt *gt; /* gt for the execbuf */
+	struct intel_engine_cs *engine; /** engine to queue the request to */
 	struct intel_context *context; /* logical state for the request */
 	struct i915_gem_context *gem_context; /** caller's context */
 
-	struct i915_request *requests[MAX_ENGINE_INSTANCE + 1]; /** our requests to build */
-	struct eb_vma *batches[MAX_ENGINE_INSTANCE + 1]; /** identity of the batch obj/vma */
+	struct i915_request *request; /** our request to build */
+	struct eb_vma *batch; /** identity of the batch obj/vma */
 	struct i915_vma *trampoline; /** trampoline used for chaining */
 
-	/** used for excl fence in dma_resv objects when > 1 BB submitted */
-	struct dma_fence *composite_fence;
-
 	/** actual size of execobj[] as we may extend it for the cmdparser */
 	unsigned int buffer_count;
 
-	/* number of batches in execbuf IOCTL */
-	unsigned int num_batches;
-
 	/** list of vma not yet bound during reservation phase */
 	struct list_head unbound;
 
@@ -287,7 +281,7 @@ struct i915_execbuffer {
 
 	u64 invalid_flags; /** Set of execobj.flags that are invalid */
 
-	u64 batch_len[MAX_ENGINE_INSTANCE + 1]; /** Length of batch within object */
+	u64 batch_len; /** Length of batch within object */
 	u32 batch_start_offset; /** Location within object of batch */
 	u32 batch_flags; /** Flags composed for emit_bb_start() */
 	struct intel_gt_buffer_pool_node *batch_pool; /** pool node for batch buffer */
@@ -305,13 +299,14 @@ struct i915_execbuffer {
 };
 
 static int eb_parse(struct i915_execbuffer *eb);
-static int eb_pin_engine(struct i915_execbuffer *eb, bool throttle);
+static struct i915_request *eb_pin_engine(struct i915_execbuffer *eb,
+					  bool throttle);
 static void eb_unpin_engine(struct i915_execbuffer *eb);
 
 static inline bool eb_use_cmdparser(const struct i915_execbuffer *eb)
 {
-	return intel_engine_requires_cmd_parser(eb->context->engine) ||
-		(intel_engine_using_cmd_parser(eb->context->engine) &&
+	return intel_engine_requires_cmd_parser(eb->engine) ||
+		(intel_engine_using_cmd_parser(eb->engine) &&
 		 eb->args->batch_len);
 }
 
@@ -538,21 +533,11 @@ eb_validate_vma(struct i915_execbuffer *eb,
 	return 0;
 }
 
-static inline bool
-is_batch_buffer(struct i915_execbuffer *eb, unsigned int buffer_idx)
-{
-	return eb->args->flags & I915_EXEC_BATCH_FIRST ?
-		buffer_idx < eb->num_batches :
-		buffer_idx >= eb->args->buffer_count - eb->num_batches;
-}
-
-static int
+static void
 eb_add_vma(struct i915_execbuffer *eb,
-	   unsigned int *current_batch,
-	   unsigned int i,
+	   unsigned int i, unsigned batch_idx,
 	   struct i915_vma *vma)
 {
-	struct drm_i915_private *i915 = eb->i915;
 	struct drm_i915_gem_exec_object2 *entry = &eb->exec[i];
 	struct eb_vma *ev = &eb->vma[i];
 
@@ -579,41 +564,15 @@ eb_add_vma(struct i915_execbuffer *eb,
 	 * Note that actual hangs have only been observed on gen7, but for
 	 * paranoia do it everywhere.
 	 */
-	if (is_batch_buffer(eb, i)) {
+	if (i == batch_idx) {
 		if (entry->relocation_count &&
 		    !(ev->flags & EXEC_OBJECT_PINNED))
 			ev->flags |= __EXEC_OBJECT_NEEDS_BIAS;
 		if (eb->reloc_cache.has_fence)
 			ev->flags |= EXEC_OBJECT_NEEDS_FENCE;
 
-		eb->batches[*current_batch] = ev;
-
-		if (unlikely(ev->flags & EXEC_OBJECT_WRITE)) {
-			drm_dbg(&i915->drm,
-				"Attempting to use self-modifying batch buffer\n");
-			return -EINVAL;
-		}
-
-		if (range_overflows_t(u64,
-				      eb->batch_start_offset,
-				      eb->args->batch_len,
-				      ev->vma->size)) {
-			drm_dbg(&i915->drm, "Attempting to use out-of-bounds batch\n");
-			return -EINVAL;
-		}
-
-		if (eb->args->batch_len == 0)
-			eb->batch_len[*current_batch] = ev->vma->size -
-				eb->batch_start_offset;
-		if (unlikely(eb->batch_len == 0)) { /* impossible! */
-			drm_dbg(&i915->drm, "Invalid batch length\n");
-			return -EINVAL;
-		}
-
-		++*current_batch;
+		eb->batch = ev;
 	}
-
-	return 0;
 }
 
 static inline int use_cpu_reloc(const struct reloc_cache *cache,
@@ -757,6 +716,14 @@ static int eb_reserve(struct i915_execbuffer *eb)
 	} while (1);
 }
 
+static unsigned int eb_batch_index(const struct i915_execbuffer *eb)
+{
+	if (eb->args->flags & I915_EXEC_BATCH_FIRST)
+		return 0;
+	else
+		return eb->buffer_count - 1;
+}
+
 static int eb_select_context(struct i915_execbuffer *eb)
 {
 	struct i915_gem_context *ctx;
@@ -865,7 +832,9 @@ static struct i915_vma *eb_lookup_vma(struct i915_execbuffer *eb, u32 handle)
 
 static int eb_lookup_vmas(struct i915_execbuffer *eb)
 {
-	unsigned int i, current_batch = 0;
+	struct drm_i915_private *i915 = eb->i915;
+	unsigned int batch = eb_batch_index(eb);
+	unsigned int i;
 	int err = 0;
 
 	INIT_LIST_HEAD(&eb->relocs);
@@ -885,9 +854,7 @@ static int eb_lookup_vmas(struct i915_execbuffer *eb)
 			goto err;
 		}
 
-		err = eb_add_vma(eb, &current_batch, i, vma);
-		if (err)
-			return err;
+		eb_add_vma(eb, i, batch, vma);
 
 		if (i915_gem_object_is_userptr(vma->obj)) {
 			err = i915_gem_object_userptr_submit_init(vma->obj);
@@ -910,6 +877,26 @@ static int eb_lookup_vmas(struct i915_execbuffer *eb)
 		}
 	}
 
+	if (unlikely(eb->batch->flags & EXEC_OBJECT_WRITE)) {
+		drm_dbg(&i915->drm,
+			"Attempting to use self-modifying batch buffer\n");
+		return -EINVAL;
+	}
+
+	if (range_overflows_t(u64,
+			      eb->batch_start_offset, eb->batch_len,
+			      eb->batch->vma->size)) {
+		drm_dbg(&i915->drm, "Attempting to use out-of-bounds batch\n");
+		return -EINVAL;
+	}
+
+	if (eb->batch_len == 0)
+		eb->batch_len = eb->batch->vma->size - eb->batch_start_offset;
+	if (unlikely(eb->batch_len == 0)) { /* impossible! */
+		drm_dbg(&i915->drm, "Invalid batch length\n");
+		return -EINVAL;
+	}
+
 	return 0;
 
 err:
@@ -1642,7 +1629,8 @@ static int eb_reinit_userptr(struct i915_execbuffer *eb)
 	return 0;
 }
 
-static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
+static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb,
+					   struct i915_request *rq)
 {
 	bool have_copy = false;
 	struct eb_vma *ev;
@@ -1658,6 +1646,21 @@ static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
 	eb_release_vmas(eb, false);
 	i915_gem_ww_ctx_fini(&eb->ww);
 
+	if (rq) {
+		/* nonblocking is always false */
+		if (i915_request_wait(rq, I915_WAIT_INTERRUPTIBLE,
+				      MAX_SCHEDULE_TIMEOUT) < 0) {
+			i915_request_put(rq);
+			rq = NULL;
+
+			err = -EINTR;
+			goto err_relock;
+		}
+
+		i915_request_put(rq);
+		rq = NULL;
+	}
+
 	/*
 	 * We take 3 passes through the slowpatch.
 	 *
@@ -1684,21 +1687,28 @@ static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
 	if (!err)
 		err = eb_reinit_userptr(eb);
 
+err_relock:
 	i915_gem_ww_ctx_init(&eb->ww, true);
 	if (err)
 		goto out;
 
 	/* reacquire the objects */
 repeat_validate:
-	err = eb_pin_engine(eb, false);
-	if (err)
+	rq = eb_pin_engine(eb, false);
+	if (IS_ERR(rq)) {
+		err = PTR_ERR(rq);
+		rq = NULL;
 		goto err;
+	}
+
+	/* We didn't throttle, should be NULL */
+	GEM_WARN_ON(rq);
 
 	err = eb_validate_vmas(eb);
 	if (err)
 		goto err;
 
-	GEM_BUG_ON(!eb->batches[0]);
+	GEM_BUG_ON(!eb->batch);
 
 	list_for_each_entry(ev, &eb->relocs, reloc_link) {
 		if (!have_copy) {
@@ -1762,23 +1772,46 @@ static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
 		}
 	}
 
+	if (rq)
+		i915_request_put(rq);
+
 	return err;
 }
 
 static int eb_relocate_parse(struct i915_execbuffer *eb)
 {
 	int err;
+	struct i915_request *rq = NULL;
 	bool throttle = true;
 
 retry:
-	err = eb_pin_engine(eb, throttle);
-	if (err) {
+	rq = eb_pin_engine(eb, throttle);
+	if (IS_ERR(rq)) {
+		err = PTR_ERR(rq);
+		rq = NULL;
 		if (err != -EDEADLK)
 			return err;
 
 		goto err;
 	}
 
+	if (rq) {
+		bool nonblock = eb->file->filp->f_flags & O_NONBLOCK;
+
+		/* Need to drop all locks now for throttling, take slowpath */
+		err = i915_request_wait(rq, I915_WAIT_INTERRUPTIBLE, 0);
+		if (err == -ETIME) {
+			if (nonblock) {
+				err = -EWOULDBLOCK;
+				i915_request_put(rq);
+				goto err;
+			}
+			goto slow;
+		}
+		i915_request_put(rq);
+		rq = NULL;
+	}
+
 	/* only throttle once, even if we didn't need to throttle */
 	throttle = false;
 
@@ -1818,7 +1851,7 @@ static int eb_relocate_parse(struct i915_execbuffer *eb)
 	return err;
 
 slow:
-	err = eb_relocate_parse_slow(eb);
+	err = eb_relocate_parse_slow(eb, rq);
 	if (err)
 		/*
 		 * If the user expects the execobject.offset and
@@ -1832,33 +1865,11 @@ static int eb_relocate_parse(struct i915_execbuffer *eb)
 	return err;
 }
 
-#define for_each_batch_create_order(_eb, _i) \
-	for (_i = 0; _i < _eb->num_batches; ++_i)
-#define for_each_batch_add_order(_eb, _i) \
-do { \
-	BUILD_BUG_ON(!typecheck(int, _i)); \
-	for (_i = _eb->num_batches - 1; _i >= 0; --_i) \
-} while (0)
-
-static struct i915_request *
-eb_find_first_request(struct i915_execbuffer *eb)
-{
-	int i;
-
-	for_each_batch_add_order(eb, i)
-		if (eb->requests[i])
-			return eb->requests[i];
-
-	GEM_BUG_ON("Request not found");
-
-	return NULL;
-}
-
 static int eb_move_to_gpu(struct i915_execbuffer *eb)
 {
 	const unsigned int count = eb->buffer_count;
 	unsigned int i = count;
-	int err = 0, j;
+	int err = 0;
 
 	while (i--) {
 		struct eb_vma *ev = &eb->vma[i];
@@ -1871,17 +1882,11 @@ static int eb_move_to_gpu(struct i915_execbuffer *eb)
 		if (flags & EXEC_OBJECT_CAPTURE) {
 			struct i915_capture_list *capture;
 
-			for_each_batch_create_order(eb, j) {
-				if (!eb->requests[j])
-					break;
-
-				capture = kmalloc(sizeof(*capture), GFP_KERNEL);
-				if (capture) {
-					capture->next =
-						eb->requests[j]->capture_list;
-					capture->vma = vma;
-					eb->requests[j]->capture_list = capture;
-				}
+			capture = kmalloc(sizeof(*capture), GFP_KERNEL);
+			if (capture) {
+				capture->next = eb->request->capture_list;
+				capture->vma = vma;
+				eb->request->capture_list = capture;
 			}
 		}
 
@@ -1902,26 +1907,14 @@ static int eb_move_to_gpu(struct i915_execbuffer *eb)
 				flags &= ~EXEC_OBJECT_ASYNC;
 		}
 
-		/* We only need to await on the first request */
 		if (err == 0 && !(flags & EXEC_OBJECT_ASYNC)) {
 			err = i915_request_await_object
-				(eb_find_first_request(eb), obj,
-				 flags & EXEC_OBJECT_WRITE);
+				(eb->request, obj, flags & EXEC_OBJECT_WRITE);
 		}
 
-		for_each_batch_add_order(eb, j) {
-			if (err)
-				break;
-			if (!eb->requests[j])
-				continue;
-
-			err = _i915_vma_move_to_active(vma, eb->requests[j],
-						       j ? NULL :
-						       eb->composite_fence ?
-						       eb->composite_fence :
-						       &eb->requests[j]->fence,
-						       flags | __EXEC_OBJECT_NO_RESERVE);
-		}
+		if (err == 0)
+			err = i915_vma_move_to_active(vma, eb->request,
+						      flags | __EXEC_OBJECT_NO_RESERVE);
 	}
 
 #ifdef CONFIG_MMU_NOTIFIER
@@ -1952,16 +1945,11 @@ static int eb_move_to_gpu(struct i915_execbuffer *eb)
 		goto err_skip;
 
 	/* Unconditionally flush any chipset caches (for streaming writes). */
-	intel_gt_chipset_flush(eb->gt);
+	intel_gt_chipset_flush(eb->engine->gt);
 	return 0;
 
 err_skip:
-	for_each_batch_create_order(eb, j) {
-		if (!eb->requests[j])
-			break;
-
-		i915_request_set_error_once(eb->requests[j], err);
-	}
+	i915_request_set_error_once(eb->request, err);
 	return err;
 }
 
@@ -2056,16 +2044,14 @@ static int eb_parse(struct i915_execbuffer *eb)
 	int err;
 
 	if (!eb_use_cmdparser(eb)) {
-		batch = eb_dispatch_secure(eb, eb->batches[0]->vma);
+		batch = eb_dispatch_secure(eb, eb->batch->vma);
 		if (IS_ERR(batch))
 			return PTR_ERR(batch);
 
 		goto secure_batch;
 	}
 
-	GEM_BUG_ON(intel_context_is_parallel(eb->context));
-
-	len = eb->batch_len[0];
+	len = eb->batch_len;
 	if (!CMDPARSER_USES_GGTT(eb->i915)) {
 		/*
 		 * ppGTT backed shadow buffers must be mapped RO, to prevent
@@ -2079,11 +2065,11 @@ static int eb_parse(struct i915_execbuffer *eb)
 	} else {
 		len += I915_CMD_PARSER_TRAMPOLINE_SIZE;
 	}
-	if (unlikely(len < eb->batch_len[0])) /* last paranoid check of overflow */
+	if (unlikely(len < eb->batch_len)) /* last paranoid check of overflow */
 		return -EINVAL;
 
 	if (!pool) {
-		pool = intel_gt_get_buffer_pool(eb->gt, len,
+		pool = intel_gt_get_buffer_pool(eb->engine->gt, len,
 						I915_MAP_WB);
 		if (IS_ERR(pool))
 			return PTR_ERR(pool);
@@ -2108,7 +2094,7 @@ static int eb_parse(struct i915_execbuffer *eb)
 		trampoline = shadow;
 
 		shadow = shadow_batch_pin(eb, pool->obj,
-					  &eb->gt->ggtt->vm,
+					  &eb->engine->gt->ggtt->vm,
 					  PIN_GLOBAL);
 		if (IS_ERR(shadow)) {
 			err = PTR_ERR(shadow);
@@ -2130,27 +2116,26 @@ static int eb_parse(struct i915_execbuffer *eb)
 	if (err)
 		goto err_trampoline;
 
-	err = intel_engine_cmd_parser(eb->context->engine,
-				      eb->batches[0]->vma,
+	err = intel_engine_cmd_parser(eb->engine,
+				      eb->batch->vma,
 				      eb->batch_start_offset,
-				      eb->batch_len[0],
+				      eb->batch_len,
 				      shadow, trampoline);
 	if (err)
 		goto err_unpin_batch;
 
-	eb->batches[0] = &eb->vma[eb->buffer_count++];
-	eb->batches[0]->vma = i915_vma_get(shadow);
-	eb->batches[0]->flags = __EXEC_OBJECT_HAS_PIN;
+	eb->batch = &eb->vma[eb->buffer_count++];
+	eb->batch->vma = i915_vma_get(shadow);
+	eb->batch->flags = __EXEC_OBJECT_HAS_PIN;
 
 	eb->trampoline = trampoline;
 	eb->batch_start_offset = 0;
 
 secure_batch:
 	if (batch) {
-		GEM_BUG_ON(intel_context_is_parallel(eb->context));
-		eb->batches[0] = &eb->vma[eb->buffer_count++];
-		eb->batches[0]->flags = __EXEC_OBJECT_HAS_PIN;
-		eb->batches[0]->vma = i915_vma_get(batch);
+		eb->batch = &eb->vma[eb->buffer_count++];
+		eb->batch->flags = __EXEC_OBJECT_HAS_PIN;
+		eb->batch->vma = i915_vma_get(batch);
 	}
 	return 0;
 
@@ -2166,18 +2151,19 @@ static int eb_parse(struct i915_execbuffer *eb)
 	return err;
 }
 
-static int eb_request_submit(struct i915_execbuffer *eb,
-			     struct i915_request *rq,
-			     struct i915_vma *batch,
-			     u64 batch_len)
+static int eb_submit(struct i915_execbuffer *eb, struct i915_vma *batch)
 {
 	int err;
 
-	if (intel_context_nopreempt(rq->context))
-		__set_bit(I915_FENCE_FLAG_NOPREEMPT, &rq->fence.flags);
+	if (intel_context_nopreempt(eb->context))
+		__set_bit(I915_FENCE_FLAG_NOPREEMPT, &eb->request->fence.flags);
+
+	err = eb_move_to_gpu(eb);
+	if (err)
+		return err;
 
 	if (eb->args->flags & I915_EXEC_GEN7_SOL_RESET) {
-		err = i915_reset_gen7_sol_offsets(rq);
+		err = i915_reset_gen7_sol_offsets(eb->request);
 		if (err)
 			return err;
 	}
@@ -2188,26 +2174,26 @@ static int eb_request_submit(struct i915_execbuffer *eb,
 	 * allows us to determine if the batch is still waiting on the GPU
 	 * or actually running by checking the breadcrumb.
 	 */
-	if (rq->context->engine->emit_init_breadcrumb) {
-		err = rq->context->engine->emit_init_breadcrumb(rq);
+	if (eb->engine->emit_init_breadcrumb) {
+		err = eb->engine->emit_init_breadcrumb(eb->request);
 		if (err)
 			return err;
 	}
 
-	err = rq->context->engine->emit_bb_start(rq,
-						 batch->node.start +
-						 eb->batch_start_offset,
-						 batch_len,
-						 eb->batch_flags);
+	err = eb->engine->emit_bb_start(eb->request,
+					batch->node.start +
+					eb->batch_start_offset,
+					eb->batch_len,
+					eb->batch_flags);
 	if (err)
 		return err;
 
 	if (eb->trampoline) {
-		GEM_BUG_ON(intel_context_is_parallel(rq->context));
 		GEM_BUG_ON(eb->batch_start_offset);
-		err = rq->context->engine->emit_bb_start(rq,
-							 eb->trampoline->node.start +
-							 batch_len, 0, 0);
+		err = eb->engine->emit_bb_start(eb->request,
+						eb->trampoline->node.start +
+						eb->batch_len,
+						0, 0);
 		if (err)
 			return err;
 	}
@@ -2215,27 +2201,6 @@ static int eb_request_submit(struct i915_execbuffer *eb,
 	return 0;
 }
 
-static int eb_submit(struct i915_execbuffer *eb)
-{
-	unsigned int i;
-	int err;
-
-	err = eb_move_to_gpu(eb);
-
-	for_each_batch_create_order(eb, i) {
-		if (!eb->requests[i])
-			break;
-
-		trace_i915_request_queue(eb->requests[i], eb->batch_flags);
-		if (!err)
-			err = eb_request_submit(eb, eb->requests[i],
-						eb->batches[i]->vma,
-						eb->batch_len[i]);
-	}
-
-	return err;
-}
-
 static int num_vcs_engines(const struct drm_i915_private *i915)
 {
 	return hweight_long(VDBOX_MASK(&i915->gt));
@@ -2301,62 +2266,17 @@ static struct i915_request *eb_throttle(struct i915_execbuffer *eb, struct intel
 	return i915_request_get(rq);
 }
 
-static int eb_pin_timeline(struct i915_execbuffer *eb, struct intel_context *ce,
-			   bool throttle)
+static struct i915_request *eb_pin_engine(struct i915_execbuffer *eb, bool throttle)
 {
+	struct intel_context *ce = eb->context;
 	struct intel_timeline *tl;
-	struct i915_request *rq;
-
-	/*
-	 * Take a local wakeref for preparing to dispatch the execbuf as
-	 * we expect to access the hardware fairly frequently in the
-	 * process, and require the engine to be kept awake between accesses.
-	 * Upon dispatch, we acquire another prolonged wakeref that we hold
-	 * until the timeline is idle, which in turn releases the wakeref
-	 * taken on the engine, and the parent device.
-	 */
-	tl = intel_context_timeline_lock(ce);
-	if (IS_ERR(tl))
-		return PTR_ERR(tl);
-
-	intel_context_enter(ce);
-	if (throttle)
-		rq = eb_throttle(eb, ce);
-	intel_context_timeline_unlock(tl);
-
-	if (rq) {
-		bool nonblock = eb->file->filp->f_flags & O_NONBLOCK;
-		long timeout = nonblock ? 0 : MAX_SCHEDULE_TIMEOUT;
-
-		if (i915_request_wait(rq, I915_WAIT_INTERRUPTIBLE,
-				      timeout) < 0) {
-			i915_request_put(rq);
-
-			tl = intel_context_timeline_lock(ce);
-			intel_context_exit(ce);
-			intel_context_timeline_unlock(tl);
-
-			if (nonblock)
-				return -EWOULDBLOCK;
-			else
-				return -EINTR;
-		}
-		i915_request_put(rq);
-	}
-
-	return 0;
-}
-
-static int eb_pin_engine(struct i915_execbuffer *eb, bool throttle)
-{
-	struct intel_context *ce = eb->context, *child;
+	struct i915_request *rq = NULL;
 	int err;
-	int i = 0, j = 0;
 
 	GEM_BUG_ON(eb->args->flags & __EXEC_ENGINE_PINNED);
 
 	if (unlikely(intel_context_is_banned(ce)))
-		return -EIO;
+		return ERR_PTR(-EIO);
 
 	/*
 	 * Pinning the contexts may generate requests in order to acquire
@@ -2365,59 +2285,44 @@ static int eb_pin_engine(struct i915_execbuffer *eb, bool throttle)
 	 */
 	err = intel_context_pin_ww(ce, &eb->ww);
 	if (err)
-		return err;
-	for_each_child(ce, child) {
-		err = intel_context_pin_ww(child, &eb->ww);
-		GEM_BUG_ON(err);	/* perma-pinned should incr a counter */
-	}
+		return ERR_PTR(err);
 
-	for_each_child(ce, child) {
-		err = eb_pin_timeline(eb, child, throttle);
-		if (err)
-			goto unwind;
-		++i;
+	/*
+	 * Take a local wakeref for preparing to dispatch the execbuf as
+	 * we expect to access the hardware fairly frequently in the
+	 * process, and require the engine to be kept awake between accesses.
+	 * Upon dispatch, we acquire another prolonged wakeref that we hold
+	 * until the timeline is idle, which in turn releases the wakeref
+	 * taken on the engine, and the parent device.
+	 */
+	tl = intel_context_timeline_lock(ce);
+	if (IS_ERR(tl)) {
+		intel_context_unpin(ce);
+		return ERR_CAST(tl);
 	}
-	err = eb_pin_timeline(eb, ce, throttle);
-	if (err)
-		goto unwind;
 
-	eb->args->flags |= __EXEC_ENGINE_PINNED;
-	return 0;
+	intel_context_enter(ce);
+	if (throttle)
+		rq = eb_throttle(eb, ce);
+	intel_context_timeline_unlock(tl);
 
-unwind:
-	for_each_child(ce, child) {
-		if (j++ < i) {
-			mutex_lock(&child->timeline->mutex);
-			intel_context_exit(child);
-			mutex_unlock(&child->timeline->mutex);
-		}
-	}
-	for_each_child(ce, child)
-		intel_context_unpin(child);
-	intel_context_unpin(ce);
-	return err;
+	eb->args->flags |= __EXEC_ENGINE_PINNED;
+	return rq;
 }
 
 static void eb_unpin_engine(struct i915_execbuffer *eb)
 {
-	struct intel_context *ce = eb->context, *child;
+	struct intel_context *ce = eb->context;
+	struct intel_timeline *tl = ce->timeline;
 
 	if (!(eb->args->flags & __EXEC_ENGINE_PINNED))
 		return;
 
 	eb->args->flags &= ~__EXEC_ENGINE_PINNED;
 
-	for_each_child(ce, child) {
-		mutex_lock(&child->timeline->mutex);
-		intel_context_exit(child);
-		mutex_unlock(&child->timeline->mutex);
-
-		intel_context_unpin(child);
-	}
-
-	mutex_lock(&ce->timeline->mutex);
+	mutex_lock(&tl->mutex);
 	intel_context_exit(ce);
-	mutex_unlock(&ce->timeline->mutex);
+	mutex_unlock(&tl->mutex);
 
 	intel_context_unpin(ce);
 }
@@ -2468,7 +2373,7 @@ eb_select_legacy_ring(struct i915_execbuffer *eb)
 static int
 eb_select_engine(struct i915_execbuffer *eb)
 {
-	struct intel_context *ce, *child;
+	struct intel_context *ce;
 	unsigned int idx;
 	int err;
 
@@ -2481,20 +2386,6 @@ eb_select_engine(struct i915_execbuffer *eb)
 	if (IS_ERR(ce))
 		return PTR_ERR(ce);
 
-	if (intel_context_is_parallel(ce)) {
-		if (eb->buffer_count < ce->guc_number_children + 1) {
-			intel_context_put(ce);
-			return -EINVAL;
-		}
-		if (eb->batch_start_offset || eb->args->batch_len) {
-			intel_context_put(ce);
-			return -EINVAL;
-		}
-	}
-	eb->num_batches = ce->guc_number_children + 1;
-
-	for_each_child(ce, child)
-		intel_context_get(child);
 	intel_gt_pm_get(ce->engine->gt);
 
 	if (!test_bit(CONTEXT_ALLOC_BIT, &ce->flags)) {
@@ -2502,13 +2393,6 @@ eb_select_engine(struct i915_execbuffer *eb)
 		if (err)
 			goto err;
 	}
-	for_each_child(ce, child) {
-		if (!test_bit(CONTEXT_ALLOC_BIT, &child->flags)) {
-			err = intel_context_alloc_state(child);
-			if (err)
-				goto err;
-		}
-	}
 
 	/*
 	 * ABI: Before userspace accesses the GPU (e.g. execbuffer), report
@@ -2519,7 +2403,7 @@ eb_select_engine(struct i915_execbuffer *eb)
 		goto err;
 
 	eb->context = ce;
-	eb->gt = ce->engine->gt;
+	eb->engine = ce->engine;
 
 	/*
 	 * Make sure engine pool stays alive even if we call intel_context_put
@@ -2530,8 +2414,6 @@ eb_select_engine(struct i915_execbuffer *eb)
 
 err:
 	intel_gt_pm_put(ce->engine->gt);
-	for_each_child(ce, child)
-		intel_context_put(child);
 	intel_context_put(ce);
 	return err;
 }
@@ -2539,11 +2421,7 @@ eb_select_engine(struct i915_execbuffer *eb)
 static void
 eb_put_engine(struct i915_execbuffer *eb)
 {
-	struct intel_context *child;
-
-	intel_gt_pm_put(eb->gt);
-	for_each_child(eb->context, child)
-		intel_context_put(child);
+	intel_gt_pm_put(eb->engine->gt);
 	intel_context_put(eb->context);
 }
 
@@ -2766,8 +2644,7 @@ static void put_fence_array(struct eb_fence *fences, int num_fences)
 }
 
 static int
-await_fence_array(struct i915_execbuffer *eb,
-		  struct i915_request *rq)
+await_fence_array(struct i915_execbuffer *eb)
 {
 	unsigned int n;
 	int err;
@@ -2781,7 +2658,8 @@ await_fence_array(struct i915_execbuffer *eb,
 		if (!eb->fences[n].dma_fence)
 			continue;
 
-		err = i915_request_await_dma_fence(rq, eb->fences[n].dma_fence);
+		err = i915_request_await_dma_fence(eb->request,
+						   eb->fences[n].dma_fence);
 		if (err < 0)
 			return err;
 	}
@@ -2789,9 +2667,9 @@ await_fence_array(struct i915_execbuffer *eb,
 	return 0;
 }
 
-static void signal_fence_array(const struct i915_execbuffer *eb,
-			       struct dma_fence * const fence)
+static void signal_fence_array(const struct i915_execbuffer *eb)
 {
+	struct dma_fence * const fence = &eb->request->fence;
 	unsigned int n;
 
 	for (n = 0; n < eb->num_fences; n++) {
@@ -2839,12 +2717,12 @@ static void retire_requests(struct intel_timeline *tl, struct i915_request *end)
 			break;
 }
 
-static int eb_request_add(struct i915_execbuffer *eb, struct i915_request *rq)
+static int eb_request_add(struct i915_execbuffer *eb, int err)
 {
+	struct i915_request *rq = eb->request;
 	struct intel_timeline * const tl = i915_request_timeline(rq);
 	struct i915_sched_attr attr = {};
 	struct i915_request *prev;
-	int err = 0;
 
 	lockdep_assert_held(&tl->mutex);
 	lockdep_unpin_lock(&tl->mutex, rq->cookie);
@@ -2856,6 +2734,11 @@ static int eb_request_add(struct i915_execbuffer *eb, struct i915_request *rq)
 	/* Check that the context wasn't destroyed before submission */
 	if (likely(!intel_context_is_closed(eb->context))) {
 		attr = eb->gem_context->sched;
+	} else {
+		/* Serialise with context_close via the add_to_timeline */
+		i915_request_set_error_once(rq, -ENOENT);
+		__i915_request_skip(rq);
+		err = -ENOENT; /* override any transient errors */
 	}
 
 	__i915_request_queue(rq, &attr);
@@ -2869,44 +2752,6 @@ static int eb_request_add(struct i915_execbuffer *eb, struct i915_request *rq)
 	return err;
 }
 
-static int eb_requests_add(struct i915_execbuffer *eb, int err)
-{
-	int i;
-
-	/*
-	* We iterate in reverse order of creation to release timeline mutexes in
-	* same order.
-	*/
-	for_each_batch_add_order(eb, i) {
-		struct i915_request *rq = eb->requests[i];
-
-		if (!rq)
-			continue;
-
-		if (unlikely(intel_context_is_closed(eb->context))) {
-			/* Serialise with context_close via the add_to_timeline */
-			i915_request_set_error_once(rq, -ENOENT);
-			__i915_request_skip(rq);
-			err = -ENOENT; /* override any transient errors */
-		}
-
-		if (intel_context_is_parallel(eb->context)) {
-			if (err) {
-				__i915_request_skip(rq);
-				set_bit(I915_FENCE_FLAG_SKIP_PARALLEL,
-					&rq->fence.flags);
-			}
-			if (i == 0)
-				set_bit(I915_FENCE_FLAG_SUBMIT_PARALLEL,
-					&rq->fence.flags);
-		}
-
-		err |= eb_request_add(eb, rq);
-	}
-
-	return err;
-}
-
 static const i915_user_extension_fn execbuf_extensions[] = {
 	[DRM_I915_GEM_EXECBUFFER_EXT_TIMELINE_FENCES] = parse_timeline_fences,
 };
@@ -2933,166 +2778,6 @@ parse_execbuf2_extensions(struct drm_i915_gem_execbuffer2 *args,
 				    eb);
 }
 
-static void eb_requests_get(struct i915_execbuffer *eb)
-{
-	unsigned int i;
-
-	for_each_batch_create_order(eb, i) {
-		if (!eb->requests[i])
-			break;
-
-		i915_request_get(eb->requests[i]);
-	}
-}
-
-static void eb_requests_put(struct i915_execbuffer *eb)
-{
-	unsigned int i;
-
-	for_each_batch_create_order(eb, i) {
-		if (!eb->requests[i])
-			break;
-
-		i915_request_put(eb->requests[i]);
-	}
-}
-static struct sync_file *
-eb_composite_fence_create(struct i915_execbuffer *eb, int out_fence_fd)
-{
-	struct sync_file *out_fence = NULL;
-	struct dma_fence_array *fence_array;
-	struct dma_fence **fences;
-	unsigned int i;
-
-	GEM_BUG_ON(!intel_context_is_parent(eb->context));
-
-	fences = kmalloc(eb->num_batches * sizeof(*fences), GFP_KERNEL);
-	if (!fences)
-		return ERR_PTR(-ENOMEM);
-
-	for_each_batch_create_order(eb, i)
-		fences[i] = &eb->requests[i]->fence;
-
-	fence_array = dma_fence_array_create(eb->num_batches,
-					     fences,
-					     eb->context->fence_context,
-					     eb->context->seqno,
-					     false);
-	if (!fence_array) {
-		kfree(fences);
-		return ERR_PTR(-ENOMEM);
-	}
-
-	/* Move ownership to the dma_fence_array created above */
-	for_each_batch_create_order(eb, i)
-		dma_fence_get(fences[i]);
-
-	if (out_fence_fd != -1) {
-		out_fence = sync_file_create(&fence_array->base);
-		/* sync_file now owns fence_arry, drop creation ref */
-		dma_fence_put(&fence_array->base);
-		if (!out_fence)
-			return ERR_PTR(-ENOMEM);
-	}
-
-	eb->composite_fence = &fence_array->base;
-
-	return out_fence;
-}
-
-static struct intel_context *
-eb_find_context(struct i915_execbuffer *eb, unsigned int context_number)
-{
-	struct intel_context *child;
-
-	if (likely(context_number == 0))
-		return eb->context;
-
-	for_each_child(eb->context, child)
-		if (!--context_number)
-			return child;
-
-	GEM_BUG_ON("Context not found");
-
-	return NULL;
-}
-
-static struct sync_file *
-eb_requests_create(struct i915_execbuffer *eb, struct dma_fence *in_fence,
-		   int out_fence_fd)
-{
-	struct sync_file *out_fence = NULL;
-	unsigned int i;
-	int err;
-
-	for_each_batch_create_order(eb, i) {
-		bool first_request_to_add = i + 1 == eb->num_batches;
-
-		/* Allocate a request for this batch buffer nice and early. */
-		eb->requests[i] = i915_request_create(eb_find_context(eb, i));
-		if (IS_ERR(eb->requests[i])) {
-			eb->requests[i] = NULL;
-			return ERR_PTR(PTR_ERR(eb->requests[i]));
-		}
-
-		if (unlikely(eb->gem_context->syncobj &&
-			     first_request_to_add)) {
-			struct dma_fence *fence;
-
-			fence = drm_syncobj_fence_get(eb->gem_context->syncobj);
-			err = i915_request_await_dma_fence(eb->requests[i], fence);
-			dma_fence_put(fence);
-			if (err)
-				return ERR_PTR(err);
-		}
-
-		if (in_fence && first_request_to_add) {
-			if (eb->args->flags & I915_EXEC_FENCE_SUBMIT)
-				err = i915_request_await_execution(eb->requests[i],
-								   in_fence);
-			else
-				err = i915_request_await_dma_fence(eb->requests[i],
-								   in_fence);
-			if (err < 0)
-				return ERR_PTR(err);
-		}
-
-		if (eb->fences && first_request_to_add) {
-			err = await_fence_array(eb, eb->requests[i]);
-			if (err)
-				return ERR_PTR(err);
-		}
-
-		if (first_request_to_add) {
-			if (intel_context_is_parallel(eb->context)) {
-				out_fence = eb_composite_fence_create(eb, out_fence_fd);
-				if (IS_ERR(out_fence))
-					return ERR_PTR(-ENOMEM);
-			} else if (out_fence_fd != -1) {
-				out_fence = sync_file_create(&eb->requests[i]->fence);
-				if (!out_fence)
-					return ERR_PTR(-ENOMEM);
-			}
-		}
-
-		/*
-		 * Whilst this request exists, batch_obj will be on the
-		 * active_list, and so will hold the active reference. Only when
-		 * this request is retired will the the batch_obj be moved onto
-		 * the inactive_list and lose its active reference. Hence we do
-		 * not need to explicitly hold another reference here.
-		 */
-		eb->requests[i]->batch = eb->batches[i]->vma;
-		if (eb->batch_pool) {
-			GEM_BUG_ON(intel_context_is_parallel(eb->context));
-			intel_gt_buffer_pool_mark_active(eb->batch_pool,
-							 eb->requests[i]);
-		}
-	}
-
-	return out_fence;
-}
-
 static int
 i915_gem_do_execbuffer(struct drm_device *dev,
 		       struct drm_file *file,
@@ -3103,6 +2788,7 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 	struct i915_execbuffer eb;
 	struct dma_fence *in_fence = NULL;
 	struct sync_file *out_fence = NULL;
+	struct i915_vma *batch;
 	int out_fence_fd = -1;
 	int err;
 
@@ -3126,15 +2812,12 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 
 	eb.buffer_count = args->buffer_count;
 	eb.batch_start_offset = args->batch_start_offset;
+	eb.batch_len = args->batch_len;
 	eb.trampoline = NULL;
 
 	eb.fences = NULL;
 	eb.num_fences = 0;
 
-	memset(eb.requests, 0, sizeof(struct i915_request *) *
-	       ARRAY_SIZE(eb.requests));
-	eb.composite_fence = NULL;
-
 	eb.batch_flags = 0;
 	if (args->flags & I915_EXEC_SECURE) {
 		if (GRAPHICS_VER(i915) >= 11)
@@ -3218,25 +2901,70 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 
 	ww_acquire_done(&eb.ww.ctx);
 
-	out_fence = eb_requests_create(&eb, in_fence, out_fence_fd);
-	if (IS_ERR(out_fence)) {
-		err = PTR_ERR(out_fence);
-		if (eb.requests[0])
-			goto err_request;
+	batch = eb.batch->vma;
+
+	/* Allocate a request for this batch buffer nice and early. */
+	eb.request = i915_request_create(eb.context);
+	if (IS_ERR(eb.request)) {
+		err = PTR_ERR(eb.request);
+		goto err_vma;
+	}
+
+	if (unlikely(eb.gem_context->syncobj)) {
+		struct dma_fence *fence;
+
+		fence = drm_syncobj_fence_get(eb.gem_context->syncobj);
+		err = i915_request_await_dma_fence(eb.request, fence);
+		dma_fence_put(fence);
+		if (err)
+			goto err_ext;
+	}
+
+	if (in_fence) {
+		if (args->flags & I915_EXEC_FENCE_SUBMIT)
+			err = i915_request_await_execution(eb.request,
+							   in_fence);
 		else
-			goto err_vma;
+			err = i915_request_await_dma_fence(eb.request,
+							   in_fence);
+		if (err < 0)
+			goto err_request;
+	}
+
+	if (eb.fences) {
+		err = await_fence_array(&eb);
+		if (err)
+			goto err_request;
+	}
+
+	if (out_fence_fd != -1) {
+		out_fence = sync_file_create(&eb.request->fence);
+		if (!out_fence) {
+			err = -ENOMEM;
+			goto err_request;
+		}
 	}
 
-	err = eb_submit(&eb);
+	/*
+	 * Whilst this request exists, batch_obj will be on the
+	 * active_list, and so will hold the active reference. Only when this
+	 * request is retired will the the batch_obj be moved onto the
+	 * inactive_list and lose its active reference. Hence we do not need
+	 * to explicitly hold another reference here.
+	 */
+	eb.request->batch = batch;
+	if (eb.batch_pool)
+		intel_gt_buffer_pool_mark_active(eb.batch_pool, eb.request);
+
+	trace_i915_request_queue(eb.request, eb.batch_flags);
+	err = eb_submit(&eb, batch);
 
 err_request:
-	eb_requests_get(&eb);
-	err = eb_requests_add(&eb, err);
+	i915_request_get(eb.request);
+	err = eb_request_add(&eb, err);
 
 	if (eb.fences)
-		signal_fence_array(&eb, eb.composite_fence ?
-				   eb.composite_fence :
-				   &eb.requests[0]->fence);
+		signal_fence_array(&eb);
 
 	if (out_fence) {
 		if (err == 0) {
@@ -3251,15 +2979,10 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 
 	if (unlikely(eb.gem_context->syncobj)) {
 		drm_syncobj_replace_fence(eb.gem_context->syncobj,
-					  eb.composite_fence ?
-					  eb.composite_fence :
-					  &eb.requests[0]->fence);
+					  &eb.request->fence);
 	}
 
-	if (!out_fence && eb.composite_fence)
-		dma_fence_put(eb.composite_fence);
-
-	eb_requests_put(&eb);
+	i915_request_put(eb.request);
 
 err_vma:
 	eb_release_vmas(&eb, true);
diff --git a/drivers/gpu/drm/i915/gt/intel_context.h b/drivers/gpu/drm/i915/gt/intel_context.h
index d229ba66ee17..70c03e19f0b3 100644
--- a/drivers/gpu/drm/i915/gt/intel_context.h
+++ b/drivers/gpu/drm/i915/gt/intel_context.h
@@ -237,13 +237,7 @@ intel_context_timeline_lock(struct intel_context *ce)
 	struct intel_timeline *tl = ce->timeline;
 	int err;
 
-	if (intel_context_is_parent(ce))
-		err = mutex_lock_interruptible_nested(&tl->mutex, 0);
-	else if (intel_context_is_child(ce))
-		err = mutex_lock_interruptible_nested(&tl->mutex,
-						      ce->guc_child_index + 1);
-	else
-		err = mutex_lock_interruptible(&tl->mutex);
+	err = mutex_lock_interruptible(&tl->mutex);
 	if (err)
 		return ERR_PTR(err);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_context_types.h b/drivers/gpu/drm/i915/gt/intel_context_types.h
index 094fcfb5cbe1..727f91e7f7c2 100644
--- a/drivers/gpu/drm/i915/gt/intel_context_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_context_types.h
@@ -261,18 +261,6 @@ struct intel_context {
 		 * context.
 		 */
 		struct i915_request *last_rq;
-
-		/**
-		 * @fence_context: fence context composite fence when doing
-		 * parallel submission
-		 */
-		u64 fence_context;
-
-		/**
-		 * @seqno: seqno for composite fence when doing parallel
-		 * submission
-		 */
-		u32 seqno;
 	};
 
 #ifdef CONFIG_DRM_I915_SELFTEST
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index 2d1f443450b9..bee38a0d21c6 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -3037,8 +3037,6 @@ guc_create_parallel(struct intel_engine_cs **engines,
 		}
 	}
 
-	parent->fence_context = dma_fence_context_alloc(1);
-
 	parent->engine->emit_bb_start =
 		emit_bb_start_parent_no_preempt_mid_batch;
 	parent->engine->emit_fini_breadcrumb =
diff --git a/drivers/gpu/drm/i915/i915_request.h b/drivers/gpu/drm/i915/i915_request.h
index 602cc246ba85..8f0073e19079 100644
--- a/drivers/gpu/drm/i915/i915_request.h
+++ b/drivers/gpu/drm/i915/i915_request.h
@@ -147,15 +147,6 @@ enum {
 	 * tail.
 	 */
 	I915_FENCE_FLAG_SUBMIT_PARALLEL,
-
-	/*
-	 * I915_FENCE_FLAG_SKIP_PARALLEL - request with a context in a
-	 * parent-child relationship (parallel submission, multi-lrc) that
-	 * hit an error while generating requests in the execbuf IOCTL.
-	 * Indicates this request should be skipped as another request in
-	 * submission / relationship encoutered an error.
-	 */
-	I915_FENCE_FLAG_SKIP_PARALLEL,
 };
 
 /**
diff --git a/drivers/gpu/drm/i915/i915_vma.c b/drivers/gpu/drm/i915/i915_vma.c
index 90546fa58fc1..4b7fc4647e46 100644
--- a/drivers/gpu/drm/i915/i915_vma.c
+++ b/drivers/gpu/drm/i915/i915_vma.c
@@ -1234,10 +1234,9 @@ int __i915_vma_move_to_active(struct i915_vma *vma, struct i915_request *rq)
 	return i915_active_add_request(&vma->active, rq);
 }
 
-int _i915_vma_move_to_active(struct i915_vma *vma,
-			     struct i915_request *rq,
-			     struct dma_fence *fence,
-			     unsigned int flags)
+int i915_vma_move_to_active(struct i915_vma *vma,
+			    struct i915_request *rq,
+			    unsigned int flags)
 {
 	struct drm_i915_gem_object *obj = vma->obj;
 	int err;
@@ -1258,11 +1257,9 @@ int _i915_vma_move_to_active(struct i915_vma *vma,
 			intel_frontbuffer_put(front);
 		}
 
-		if (fence) {
-			dma_resv_add_excl_fence(vma->resv, fence);
-			obj->write_domain = I915_GEM_DOMAIN_RENDER;
-			obj->read_domains = 0;
-		}
+		dma_resv_add_excl_fence(vma->resv, &rq->fence);
+		obj->write_domain = I915_GEM_DOMAIN_RENDER;
+		obj->read_domains = 0;
 	} else {
 		if (!(flags & __EXEC_OBJECT_NO_RESERVE)) {
 			err = dma_resv_reserve_shared(vma->resv, 1);
@@ -1270,10 +1267,8 @@ int _i915_vma_move_to_active(struct i915_vma *vma,
 				return err;
 		}
 
-		if (fence) {
-			dma_resv_add_shared_fence(vma->resv, fence);
-			obj->write_domain = 0;
-		}
+		dma_resv_add_shared_fence(vma->resv, &rq->fence);
+		obj->write_domain = 0;
 	}
 
 	if (flags & EXEC_OBJECT_NEEDS_FENCE && vma->fence)
diff --git a/drivers/gpu/drm/i915/i915_vma.h b/drivers/gpu/drm/i915/i915_vma.h
index 648dbe744c96..ed69f66c7ab0 100644
--- a/drivers/gpu/drm/i915/i915_vma.h
+++ b/drivers/gpu/drm/i915/i915_vma.h
@@ -57,16 +57,9 @@ static inline bool i915_vma_is_active(const struct i915_vma *vma)
 
 int __must_check __i915_vma_move_to_active(struct i915_vma *vma,
 					   struct i915_request *rq);
-int __must_check _i915_vma_move_to_active(struct i915_vma *vma,
-					  struct i915_request *rq,
-					  struct dma_fence *fence,
-					  unsigned int flags);
-static inline int __must_check
-i915_vma_move_to_active(struct i915_vma *vma, struct i915_request *rq,
-			unsigned int flags)
-{
-	return _i915_vma_move_to_active(vma, rq, &rq->fence, flags);
-}
+int __must_check i915_vma_move_to_active(struct i915_vma *vma,
+					 struct i915_request *rq,
+					 unsigned int flags);
 
 #define __i915_vma_flags(v) ((unsigned long *)&(v)->flags.counter)
 
-- 
2.25.1

