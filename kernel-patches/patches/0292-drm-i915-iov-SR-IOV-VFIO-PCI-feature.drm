From 717f0fc937d809f41a4ee14ee35b0c85eeaa69f8 Mon Sep 17 00:00:00 2001
From: "Zawawi, Muhammad Zul Husni" <muhammad.zul.husni.zawawi@intel.com>
Date: Mon, 20 Nov 2023 08:38:18 +0800
Subject: [PATCH 292/319] drm/i915/iov: SR-IOV VFIO/PCI feature

Signed-off-by: Zawawi, Muhammad Zul Husni <muhammad.zul.husni.zawawi@intel.com>
---
 drivers/gpu/drm/i915/README.sriov.vfio        |   3 +
 drivers/gpu/drm/i915/gt/intel_ggtt.c          | 102 ++++
 drivers/gpu/drm/i915/gt/intel_gtt.h           |  11 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h   |  10 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.c | 267 +++++++++-
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.h |   6 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c |  45 ++
 drivers/gpu/drm/i915/gt/iov/intel_iov_types.h |   4 +
 .../drm/i915/gt/uc/abi/guc_actions_pf_abi.h   |  55 ++
 drivers/gpu/drm/i915/i915_pci.c               |  22 +
 drivers/gpu/drm/i915/i915_pci.h               |  10 +
 drivers/gpu/drm/i915/i915_sriov.c             | 502 ++++++++++++++++++
 drivers/vfio/pci/Kconfig                      |   2 +
 drivers/vfio/pci/Makefile                     |   2 +
 drivers/vfio/pci/i915/Kconfig                 |  25 +
 drivers/vfio/pci/i915/Makefile                |   4 +
 drivers/vfio/pci/i915/data.c                  | 409 ++++++++++++++
 drivers/vfio/pci/i915/i915_vfio_pci.h         | 114 ++++
 drivers/vfio/pci/i915/main.c                  | 399 ++++++++++++++
 drivers/vfio/pci/i915/test/data_test.c        | 308 +++++++++++
 include/drm/i915_sriov.h                      |  33 ++
 21 files changed, 2332 insertions(+), 1 deletion(-)
 create mode 100644 drivers/gpu/drm/i915/README.sriov.vfio
 create mode 100644 drivers/vfio/pci/i915/Kconfig
 create mode 100644 drivers/vfio/pci/i915/Makefile
 create mode 100644 drivers/vfio/pci/i915/data.c
 create mode 100644 drivers/vfio/pci/i915/i915_vfio_pci.h
 create mode 100644 drivers/vfio/pci/i915/main.c
 create mode 100644 drivers/vfio/pci/i915/test/data_test.c
 create mode 100644 include/drm/i915_sriov.h

diff --git a/drivers/gpu/drm/i915/README.sriov.vfio b/drivers/gpu/drm/i915/README.sriov.vfio
new file mode 100644
index 000000000000..73737e0f3042
--- /dev/null
+++ b/drivers/gpu/drm/i915/README.sriov.vfio
@@ -0,0 +1,3 @@
+SR-IOV - VFIO/PCI (done)
+D:	Michał Winiarski <michal.winiarski@intel.com>
+M:	Michał Wajdeczko <michal.wajdeczko@intel.com>
diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index 60a6b01372da..1e06745676d7 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -1857,3 +1857,105 @@ void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
 
 	ggtt->invalidate(ggtt);
 }
+
+static inline unsigned int __ggtt_size_to_ptes_size(u64 ggtt_size)
+{
+	GEM_BUG_ON(!IS_ALIGNED(ggtt_size, I915_GTT_MIN_ALIGNMENT));
+
+	return (ggtt_size >> PAGE_SHIFT) * sizeof(gen8_pte_t);
+}
+
+static void ggtt_pte_clear_vfid(void *buf, u64 size)
+{
+	while (size) {
+		*(gen8_pte_t *)buf &= ~TGL_GGTT_PTE_VFID_MASK;
+
+		buf += sizeof(gen8_pte_t);
+		size -= sizeof(gen8_pte_t);
+	}
+}
+
+/**
+ * i915_ggtt_save_ptes - copy GGTT PTEs to preallocated buffer
+ * @ggtt: the &struct i915_ggtt
+ * @node: the &struct drm_mm_node - the @node->start is used as the start offset for save
+ * @buf: preallocated buffer in which PTEs will be saved
+ * @size: size of preallocated buffer (in bytes)
+ *        - must be sizeof(gen8_pte_t) aligned
+ * @flags: function flags:
+ *         - #I915_GGTT_SAVE_PTES_NO_VFID BIT - save PTEs without VFID
+ *
+ * Returns: size of the buffer used (or needed if both @buf and @size are (0)) to store all PTEs
+ *          for a given node, -EINVAL if one of @buf or @size is 0.
+ */
+int i915_ggtt_save_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, void *buf,
+			unsigned int size, unsigned int flags)
+{
+	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
+
+	if (!buf && !size)
+		return __ggtt_size_to_ptes_size(node->size);
+
+	if (!buf || !size)
+		return -EINVAL;
+
+	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
+	GEM_WARN_ON(size > __ggtt_size_to_ptes_size(SZ_4G));
+
+	if (size < __ggtt_size_to_ptes_size(node->size))
+		return -ENOSPC;
+	size = __ggtt_size_to_ptes_size(node->size);
+
+	gtt_entries += node->start >> PAGE_SHIFT;
+
+	memcpy_fromio(buf, gtt_entries, size);
+
+	if (flags & I915_GGTT_SAVE_PTES_NO_VFID)
+		ggtt_pte_clear_vfid(buf, size);
+
+	return size;
+}
+
+/**
+ * i915_ggtt_restore_ptes() -  restore GGTT PTEs from buffer
+ * @ggtt: the &struct i915_ggtt
+ * @node: the &struct drm_mm_node - the @node->start is used as the start offset for restore
+ * @buf: buffer from which PTEs will be restored
+ * @size: size of preallocated buffer (in bytes)
+ *        - must be sizeof(gen8_pte_t) aligned
+ * @flags: function flags:
+ *         - #I915_GGTT_RESTORE_PTES_VFID_MASK - VFID for restored PTEs
+ *         - #I915_GGTT_RESTORE_PTES_NEW_VFID - restore PTEs with new VFID
+ *           (from #I915_GGTT_RESTORE_PTES_VFID_MASK)
+ *
+ * Returns: 0 on success, -ENOSPC if @node->size is less than size.
+ */
+int i915_ggtt_restore_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, const void *buf,
+			   unsigned int size, unsigned int flags)
+{
+	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
+	u32 vfid = FIELD_GET(I915_GGTT_RESTORE_PTES_VFID_MASK, flags);
+	gen8_pte_t pte;
+
+	GEM_BUG_ON(!size);
+	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
+
+	if (size > __ggtt_size_to_ptes_size(node->size))
+		return -ENOSPC;
+
+	gtt_entries += node->start >> PAGE_SHIFT;
+
+	while (size) {
+		pte = *(gen8_pte_t *)buf;
+		if (flags & I915_GGTT_RESTORE_PTES_NEW_VFID)
+			pte |= tgl_prepare_vf_pte_vfid(vfid);
+		gen8_set_pte(gtt_entries++, pte);
+
+		buf += sizeof(gen8_pte_t);
+		size -= sizeof(gen8_pte_t);
+	}
+
+	ggtt->invalidate(ggtt);
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.h b/drivers/gpu/drm/i915/gt/intel_gtt.h
index addc3c6cecdc..eae3fdb4f9a5 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.h
@@ -628,6 +628,17 @@ gen8_pte_t i915_ggtt_prepare_vf_pte(u16 vfid);
 void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
 			       const struct drm_mm_node *node);
 
+#define I915_GGTT_SAVE_PTES_NO_VFID BIT(31)
+
+int i915_ggtt_save_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, void *buf,
+			unsigned int size, unsigned int flags);
+
+#define I915_GGTT_RESTORE_PTES_NEW_VFID  BIT(31)
+#define I915_GGTT_RESTORE_PTES_VFID_MASK GENMASK(19, 0)
+
+int i915_ggtt_restore_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, const void *buf,
+			   unsigned int size, unsigned int flags);
+
 int i915_ppgtt_init_hw(struct intel_gt *gt);
 
 struct i915_ppgtt *i915_ppgtt_create(struct intel_gt *gt,
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h
index 7ec0b60f0a46..ef09b32de892 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_reg.h
@@ -13,4 +13,14 @@
 /* IMR */
 #define I915_VF_IRQ_ENABLE 0x440
 
+/*
+ * VF registers, at offset 0x190000, are all accessible from PF after applying
+ * stride of 0x1000 or 0x400 (depending on the platform)
+ */
+#define GEN12_VF_REGISTERS_STRIDE	0x1000
+#define XEHPSDV_VF_REGISTERS_STRIDE	0x400
+
+#define GEN12_VF_GFX_MSTR_IRQ(__vfid)	_MMIO(0x190010 + (__vfid) * GEN12_VF_REGISTERS_STRIDE)
+#define XEHPSDV_VF_GFX_MSTR_IRQ(__vfid)	_MMIO(0x190010 + (__vfid) * XEHPSDV_VF_REGISTERS_STRIDE)
+
 #endif /* __INTEL_IOV_REG_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
index a4d557f52bad..6afb3166d31b 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
@@ -65,6 +65,7 @@ void intel_iov_state_release(struct intel_iov *iov)
 static void pf_reset_vf_state(struct intel_iov *iov, u32 vfid)
 {
 	iov->pf.state.data[vfid].state = 0;
+	iov->pf.state.data[vfid].paused = false;
 }
 
 /**
@@ -155,11 +156,22 @@ static void pf_clear_vf_ggtt_entries(struct intel_iov *iov, u32 vfid)
 	i915_ggtt_set_space_owner(gt->ggtt, vfid, &config->ggtt_region);
 }
 
+static void pf_reset_vf_guc_migration_state(struct intel_iov *iov, u32 vfid)
+{
+	struct intel_iov_data *data = &iov->pf.state.data[vfid];
+	void *guc_state = fetch_and_zero(&data->guc_state);
+
+	lockdep_assert_held(pf_provisioning_mutex(iov));
+
+	kfree(guc_state);
+}
+
 static int pf_process_vf_flr_finish(struct intel_iov *iov, u32 vfid)
 {
 	intel_iov_event_reset(iov, vfid);
 
 	mutex_lock(pf_provisioning_mutex(iov));
+	pf_reset_vf_guc_migration_state(iov, vfid);
 	pf_clear_vf_ggtt_entries(iov, vfid);
 	mutex_unlock(pf_provisioning_mutex(iov));
 
@@ -353,6 +365,7 @@ static void pf_handle_vf_flr(struct intel_iov *iov, u32 vfid)
 		return;
 	}
 
+	iov->pf.state.data[vfid].paused = false;
 	dev_info(dev, "VF%u FLR\n", vfid);
 
 	for_each_gt(gt, iov_to_i915(iov), gtid)
@@ -371,6 +384,7 @@ static void pf_handle_vf_pause_done(struct intel_iov *iov, u32 vfid)
 {
 	struct device *dev = iov_to_dev(iov);
 
+	iov->pf.state.data[vfid].paused = true;
 	dev_info(dev, "VF%u %s\n", vfid, "paused");
 }
 
@@ -504,7 +518,13 @@ int intel_iov_state_pause_vf(struct intel_iov *iov, u32 vfid)
  */
 int intel_iov_state_resume_vf(struct intel_iov *iov, u32 vfid)
 {
-	return pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_RESUME);
+	int err = pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_RESUME);
+
+	if (err < 0)
+		return err;
+
+	iov->pf.state.data[vfid].paused = false;
+	return 0;
 }
 
 /**
@@ -520,3 +540,248 @@ int intel_iov_state_stop_vf(struct intel_iov *iov, u32 vfid)
 {
 	return pf_control_vf(iov, vfid, GUC_PF_TRIGGER_VF_STOP);
 }
+
+/**
+ * intel_iov_state_save_ggtt - Save VF GGTT.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ * @buf: buffer to save VF GGTT
+ * @size: size of buffer to save VF GGTT
+ *
+ * This function is for PF only.
+ *
+ * Return: Size of data written on success or a negative error code on failure.
+ */
+ssize_t intel_iov_state_save_ggtt(struct intel_iov *iov, u32 vfid, void *buf, size_t size)
+{
+	struct drm_mm_node *node = &iov->pf.provisioning.configs[vfid].ggtt_region;
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	intel_wakeref_t wakeref;
+	ssize_t ret;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	if (!drm_mm_node_allocated(node)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	with_intel_runtime_pm(rpm, wakeref)
+		ret = i915_ggtt_save_ptes(ggtt, node, buf, size, I915_GGTT_SAVE_PTES_NO_VFID);
+
+out:
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return ret;
+}
+
+/**
+ * intel_iov_state_restore_ggtt - Restore VF GGTT.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ * @buf: buffer with VF GGTT to restore
+ * @size: size of buffer with VF GGTT
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_restore_ggtt(struct intel_iov *iov, u32 vfid, const void *buf, size_t size)
+{
+	struct drm_mm_node *node = &iov->pf.provisioning.configs[vfid].ggtt_region;
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	intel_wakeref_t wakeref;
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	with_intel_runtime_pm(rpm, wakeref)
+		ret = i915_ggtt_restore_ptes(ggtt, node, buf, size,
+					     FIELD_PREP(I915_GGTT_RESTORE_PTES_VFID_MASK, vfid) |
+					     I915_GGTT_RESTORE_PTES_NEW_VFID);
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return ret;
+}
+
+static int guc_action_save_restore_vf(struct intel_guc *guc, u32 vfid, u32 opcode, u64 offset)
+{
+	u32 request[PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_PF2GUC_SAVE_RESTORE_VF) |
+		FIELD_PREP(PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_0_OPCODE, opcode),
+		FIELD_PREP(PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_1_VFID, vfid),
+		FIELD_PREP(PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_2_BUFF_LO, lower_32_bits(offset)),
+		FIELD_PREP(PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_3_BUFF_HI, upper_32_bits(offset)),
+	};
+	int ret;
+
+	ret = intel_guc_send(guc, request, ARRAY_SIZE(request));
+
+	return ret > PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE ? -EPROTO : ret;
+}
+
+static int pf_save_vf(struct intel_iov *iov, u32 vfid, void *buf)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	void *blob;
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+	GEM_BUG_ON(!vfid);
+
+	ret = intel_guc_allocate_and_map_vma(guc, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE,
+					     &vma, (void **)&blob);
+	if (unlikely(ret))
+		goto failed;
+
+	ret = guc_action_save_restore_vf(guc, vfid, GUC_PF_OPCODE_VF_SAVE,
+					 intel_guc_ggtt_offset(guc, vma));
+
+	if (likely(ret > 0)) {
+		memcpy(buf, blob, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE);
+
+		if (IS_ENABLED(CONFIG_DRM_I915_SELFTEST) &&
+		    memchr_inv(buf + ret, 0, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE - ret)) {
+			pr_err("non-zero state found beyond offset %d!\n", ret);
+		}
+	}
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+
+	if (unlikely(ret < 0))
+		goto failed;
+
+	IOV_DEBUG(iov, "VF%u: state saved (%d bytes) %*ph ..\n",
+		  vfid, ret, min_t(int, 16, ret), buf);
+	return 0;
+
+failed:
+	IOV_ERROR(iov, "Failed to save VF%u state (%pe)\n", vfid, ERR_PTR(ret));
+	return ret;
+}
+
+/**
+ * intel_iov_state_save_vf - Save VF state.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ * @buf: buffer to save VF state (must be at least 4K)
+ * @size: size of the buffer
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_save_vf(struct intel_iov *iov, u32 vfid, void *buf, size_t size)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	if (size < PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE)
+		return -EINVAL;
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_save_vf(iov, vfid, buf);
+
+	return err;
+}
+
+static int pf_restore_vf(struct intel_iov *iov, u32 vfid, const void *buf)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	struct i915_vma *vma;
+	void *blob;
+	int ret;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(vfid > pf_get_totalvfs(iov));
+	GEM_BUG_ON(!vfid);
+
+	ret = intel_guc_allocate_and_map_vma(guc, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE,
+					     &vma, (void **)&blob);
+	if (unlikely(ret < 0))
+		goto failed;
+
+	memcpy(blob, buf, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE);
+
+	ret = guc_action_save_restore_vf(guc, vfid, GUC_PF_OPCODE_VF_RESTORE,
+					 intel_guc_ggtt_offset(guc, vma));
+
+	i915_vma_unpin_and_release(&vma, I915_VMA_RELEASE_MAP);
+
+	if (unlikely(ret < 0))
+		goto failed;
+
+	IOV_DEBUG(iov, "VF%u: state restored (%u bytes) %*ph\n",
+		  vfid, ret, min_t(int, 16, ret), buf);
+	return 0;
+
+failed:
+	IOV_ERROR(iov, "Failed to restore VF%u state (%pe) %*ph\n",
+		  vfid, ERR_PTR(ret), 16, buf);
+	return ret;
+}
+
+/**
+ * intel_iov_state_restore_vf - Restore VF state.
+ * @iov: the IOV struct
+ * @vfid: VF identifier
+ * @buf: buffer with VF state to restore (must be 4K)
+ * @size: size of the buffer
+ *
+ * This function is for PF only.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int intel_iov_state_restore_vf(struct intel_iov *iov, u32 vfid, const void *buf, size_t size)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	if (size != PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE)
+		return -EINVAL;
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_restore_vf(iov, vfid, buf);
+
+	if (err == 0)
+		iov->pf.state.data[vfid].paused = false;
+
+	return err;
+}
+
+int intel_iov_state_store_guc_migration_state(struct intel_iov *iov, u32 vfid,
+					      const void *buf, size_t size)
+{
+	struct intel_iov_data *data = &iov->pf.state.data[vfid];
+	void *guc_state;
+
+	if (size != PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE)
+		return -EINVAL;
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	guc_state = kzalloc(size, GFP_KERNEL);
+	if (!guc_state) {
+		mutex_unlock(pf_provisioning_mutex(iov));
+		return -ENOMEM;
+	}
+
+	memcpy(guc_state, buf, size);
+
+	data->guc_state = guc_state;
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
index 1b0db54e4baf..822a685fe3cb 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
@@ -20,6 +20,12 @@ bool intel_iov_state_no_flr(struct intel_iov *iov, u32 vfid);
 int intel_iov_state_pause_vf(struct intel_iov *iov, u32 vfid);
 int intel_iov_state_resume_vf(struct intel_iov *iov, u32 vfid);
 int intel_iov_state_stop_vf(struct intel_iov *iov, u32 vfid);
+ssize_t intel_iov_state_save_ggtt(struct intel_iov *iov, u32 vfid, void *buf, size_t size);
+int intel_iov_state_restore_ggtt(struct intel_iov *iov, u32 vfid, const void *buf, size_t size);
+int intel_iov_state_save_vf(struct intel_iov *iov, u32 vfid, void *buf, size_t size);
+int intel_iov_state_restore_vf(struct intel_iov *iov, u32 vfid, const void *buf, size_t size);
+int intel_iov_state_store_guc_migration_state(struct intel_iov *iov, u32 vfid,
+					      const void *buf, size_t size);
 
 int intel_iov_state_process_guc2pf(struct intel_iov *iov,
 				   const u32 *msg, u32 len);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c
index 6f060e4d52e4..2be9b4a8d57a 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.c
@@ -4,6 +4,7 @@
  */
 
 #include "intel_iov_provisioning.h"
+#include "intel_iov_state.h"
 #include "intel_iov_sysfs.h"
 #include "intel_iov_types.h"
 #include "intel_iov_utils.h"
@@ -485,9 +486,53 @@ static umode_t vf_attr_is_visible(struct kobject *kobj,
 	return attr->mode;
 }
 
+static ssize_t bin_attr_state_read(struct file *filp, struct kobject *kobj,
+				   struct bin_attribute *bin_attr, char *buf,
+				   loff_t off, size_t count)
+{
+	struct intel_iov *iov = kobj_to_iov(kobj);
+	unsigned int id = kobj_to_id(kobj);
+	int err;
+
+	if (off > 0)
+		return -EINVAL;
+
+	err = intel_iov_state_save_vf(iov, id, buf, count);
+	if (unlikely(err))
+		return err;
+
+	return SZ_4K;
+}
+
+static ssize_t bin_attr_state_write(struct file *filp, struct kobject *kobj,
+				    struct bin_attribute *bin_attr, char *buf,
+				    loff_t off, size_t count)
+{
+	struct intel_iov *iov = kobj_to_iov(kobj);
+	unsigned int id = kobj_to_id(kobj);
+	int err;
+
+	if (off > 0)
+		return -EINVAL;
+
+	err = intel_iov_state_restore_vf(iov, id, buf, count);
+	if (unlikely(err))
+		return err;
+
+	return count;
+}
+
+static BIN_ATTR(state, 0600, bin_attr_state_read, bin_attr_state_write, SZ_4K);
+
+static struct bin_attribute *vf_bin_attrs[] = {
+	&bin_attr_state,
+	NULL
+};
+
 static const struct attribute_group vf_attr_group = {
 	.attrs = vf_attrs,
 	.is_visible = vf_attr_is_visible,
+	.bin_attrs = vf_bin_attrs,
 };
 
 static const struct attribute_group vf_threshold_attr_group = {
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
index 09b828badf6f..68f243fdeab6 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
@@ -123,7 +123,9 @@ struct intel_iov_provisioning {
 /**
  * struct intel_iov_data - Data related to one VF.
  * @state: VF state bits
+ * @paused: FIXME missing doc
  * @adverse_events: FIXME missing doc
+ * @guc_state: pointer to VF state from GuC
  */
 struct intel_iov_data {
 	unsigned long state;
@@ -133,7 +135,9 @@ struct intel_iov_data {
 #define IOV_VF_NEEDS_FLR_FINISH		3
 #define IOV_VF_NEEDS_FLR_DONE_SYNC	4
 #define IOV_VF_FLR_FAILED		(BITS_PER_LONG - 1)
+	bool paused;
 	unsigned int adverse_events[IOV_THRESHOLD_MAX];
+	void *guc_state;
 };
 
 /**
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
index 9703ea9bbc66..158852a600b8 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_pf_abi.h
@@ -448,4 +448,59 @@
 #define   GUC_PF_TRIGGER_VF_FLR_START			4
 #define   GUC_PF_TRIGGER_VF_FLR_FINISH			5
 
+/**
+ * DOC: PF2GUC_SAVE_RESTORE_VF
+ *
+ * This message is used by the PF to migrate VF info state maintained by the GuC.
+ *
+ * This message must be sent as `CTB HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = **OPCODE** - operation to take:                      |
+ *  |   |       |                                                              |
+ *  |   |       |   - _`GUC_PF_OPCODE_VF_SAVE` = 0                             |
+ *  |   |       |   - _`GUC_PF_OPCODE_VF_RESTORE` = 1                          |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_PF2GUC_SAVE_RESTORE_VF` = 0x550B       |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 1 |  31:0 | DATA1 = **VFID** - VF identifier                             |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 2 |  31:0 | DATA2 = **BUFF_LO** - lower 32-bits of GGTT offset to the 4K |
+ *  |   |       | buffer where the VF info will be save to or restored from.   |
+ *  +---+-------+--------------------------------------------------------------+
+ *  | 3 |  31:0 | DATA3 = **BUFF_HI** - upper 32-bits of GGTT offset to the 4K |
+ *  |   |       | buffer where the VF info will be save to or restored from.   |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = **USED** - size of buffer used (in bytes)            |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_PF2GUC_SAVE_RESTORE_VF		0x550B
+
+#define PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_LEN		(GUC_HXG_EVENT_MSG_MIN_LEN + 3u)
+#define PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_0_OPCODE	GUC_HXG_EVENT_MSG_0_DATA0
+#define   GUC_PF_OPCODE_VF_SAVE				0
+#define   GUC_PF_OPCODE_VF_RESTORE			1
+#define PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_1_VFID	GUC_HXG_EVENT_MSG_n_DATAn
+#define PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_2_BUFF_LO	GUC_HXG_EVENT_MSG_n_DATAn
+#define PF2GUC_SAVE_RESTORE_VF_REQUEST_MSG_3_BUFF_HI	GUC_HXG_EVENT_MSG_n_DATAn
+
+#define PF2GUC_SAVE_RESTORE_VF_RESPONSE_MSG_LEN		GUC_HXG_RESPONSE_MSG_MIN_LEN
+#define PF2GUC_SAVE_RESTORE_VF_RESPONSE_MSG_0_USED	GUC_HXG_RESPONSE_MSG_0_DATA0
+
+#define PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE		SZ_4K
+
 #endif /* __GUC_ACTIONS_PF_ABI_H__ */
diff --git a/drivers/gpu/drm/i915/i915_pci.c b/drivers/gpu/drm/i915/i915_pci.c
index e26ba209004d..fbd6bc6dc2ff 100644
--- a/drivers/gpu/drm/i915/i915_pci.c
+++ b/drivers/gpu/drm/i915/i915_pci.c
@@ -1138,6 +1138,28 @@ static struct pci_driver i915_pci_driver = {
 	.sriov_configure = i915_pci_sriov_configure,
 };
 
+#ifdef CONFIG_PCI_IOV
+/* our Gen12 SR-IOV platforms are simple */
+#define GEN12_VF_OFFSET 1
+#define GEN12_VF_STRIDE 1
+#define GEN12_VF_ROUTING_OFFSET(id) (GEN12_VF_OFFSET + ((id) - 1) * GEN12_VF_STRIDE)
+
+struct pci_dev *i915_pci_pf_get_vf_dev(struct pci_dev *pdev, unsigned int id)
+{
+	u16 vf_devid = pci_dev_id(pdev) + GEN12_VF_ROUTING_OFFSET(id);
+
+	GEM_BUG_ON(!dev_is_pf(&pdev->dev));
+	GEM_BUG_ON(!id);
+	GEM_BUG_ON(id > pci_num_vf(pdev));
+
+	/* caller must use pci_dev_put() */
+	return pci_get_domain_bus_and_slot(pci_domain_nr(pdev->bus),
+					   PCI_BUS_NUM(vf_devid),
+					   PCI_DEVFN(PCI_SLOT(vf_devid),
+					   PCI_FUNC(vf_devid)));
+}
+#endif
+
 int i915_pci_register_driver(void)
 {
 	return pci_register_driver(&i915_pci_driver);
diff --git a/drivers/gpu/drm/i915/i915_pci.h b/drivers/gpu/drm/i915/i915_pci.h
index 8dfe19f9a775..1ec94df9c915 100644
--- a/drivers/gpu/drm/i915/i915_pci.h
+++ b/drivers/gpu/drm/i915/i915_pci.h
@@ -6,10 +6,20 @@
 #ifndef __I915_PCI_H__
 #define __I915_PCI_H__
 
+#include <linux/err.h>
 #include <linux/types.h>
 
 struct pci_dev;
 
+#ifdef CONFIG_PCI_IOV
+struct pci_dev *i915_pci_pf_get_vf_dev(struct pci_dev *pdev, unsigned int id);
+#else
+static inline struct pci_dev *i915_pci_pf_get_vf_dev(struct pci_dev *pdev, unsigned int id)
+{
+	return ERR_PTR(-ENODEV);
+}
+#endif
+
 int i915_pci_register_driver(void);
 void i915_pci_unregister_driver(void);
 
diff --git a/drivers/gpu/drm/i915/i915_sriov.c b/drivers/gpu/drm/i915/i915_sriov.c
index 43a142b8b870..f84190745c8b 100644
--- a/drivers/gpu/drm/i915/i915_sriov.c
+++ b/drivers/gpu/drm/i915/i915_sriov.c
@@ -3,10 +3,13 @@
  * Copyright © 2023 Intel Corporation
  */
 
+#include <drm/i915_sriov.h>
+
 #include "i915_sriov.h"
 #include "i915_sriov_sysfs.h"
 #include "i915_drv.h"
 #include "i915_pci.h"
+#include "i915_utils.h"
 #include "i915_reg.h"
 #include "intel_pci_config.h"
 
@@ -14,6 +17,7 @@
 #include "gt/intel_gt_pm.h"
 #include "gt/iov/intel_iov_provisioning.h"
 #include "gt/iov/intel_iov_service.h"
+#include "gt/iov/intel_iov_reg.h"
 #include "gt/iov/intel_iov_state.h"
 #include "gt/iov/intel_iov_utils.h"
 
@@ -670,6 +674,240 @@ int i915_sriov_pf_disable_vfs(struct drm_i915_private *i915)
 	return 0;
 }
 
+static bool needs_save_restore(struct drm_i915_private *i915, unsigned int vfid)
+{
+	struct pci_dev *pdev = to_pci_dev(i915->drm.dev);
+	struct pci_dev *vfpdev = i915_pci_pf_get_vf_dev(pdev, vfid);
+	bool ret;
+
+	if (!vfpdev)
+		return false;
+
+	/*
+	 * If VF has the same driver as PF loaded (from host perspective), we don't need
+	 * to save/restore its state, because the VF driver will receive the same PM
+	 * handling as all the host drivers. There is also no need to save/restore state
+	 * when no driver is loaded on VF.
+	 */
+	ret = (vfpdev->driver && strcmp(vfpdev->driver->name, pdev->driver->name) != 0);
+
+	pci_dev_put(vfpdev);
+	return ret;
+}
+
+static void pf_restore_vfs_pci_state(struct drm_i915_private *i915, unsigned int num_vfs)
+{
+	struct pci_dev *pdev = to_pci_dev(i915->drm.dev);
+	unsigned int vfid;
+
+	GEM_BUG_ON(num_vfs > pci_num_vf(pdev));
+
+	for (vfid = 1; vfid <= num_vfs; vfid++) {
+		struct pci_dev *vfpdev = i915_pci_pf_get_vf_dev(pdev, vfid);
+
+		if (!vfpdev)
+			continue;
+		if (!needs_save_restore(i915, vfid))
+			continue;
+
+		/*
+		 * XXX: Waiting for other drivers to do their job.
+		 * We can ignore the potential error in this function -
+		 * in case of an error, we still want to try to reinitialize
+		 * the MSI and set the PCI master.
+		 */
+		device_pm_wait_for_dev(&pdev->dev, &vfpdev->dev);
+
+		pci_restore_msi_state(vfpdev);
+		pci_set_master(vfpdev);
+
+		pci_dev_put(vfpdev);
+	}
+}
+
+#define I915_VF_PAUSE_TIMEOUT_MS 500
+#define I915_VF_REPROVISION_TIMEOUT_MS 1000
+
+static int pf_gt_save_vf_guc_state(struct intel_gt *gt, unsigned int vfid)
+{
+	struct pci_dev *pdev = to_pci_dev(gt->i915->drm.dev);
+	struct intel_iov *iov = &gt->iov;
+	unsigned long timeout_ms = I915_VF_PAUSE_TIMEOUT_MS;
+	void **guc_state = &iov->pf.state.data[vfid].guc_state;
+	int err;
+
+	GEM_BUG_ON(!vfid);
+	GEM_BUG_ON(vfid > pci_num_vf(pdev));
+
+	err = intel_iov_state_pause_vf(iov, vfid);
+	if (err) {
+		IOV_ERROR(iov, "Failed to pause VF%u: (%pe)", vfid, ERR_PTR(err));
+		return err;
+	}
+
+	/* FIXME: How long we should wait? */
+	if (wait_for(iov->pf.state.data[vfid].paused, timeout_ms)) {
+		IOV_ERROR(iov, "VF%u pause didn't complete within %lu ms\n", vfid, timeout_ms);
+		return -ETIMEDOUT;
+	}
+
+	if (*guc_state)
+		memset(*guc_state, 0, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE);
+	else
+		*guc_state = kzalloc(PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE, GFP_KERNEL);
+
+	if (!*guc_state) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	err = intel_iov_state_save_vf(iov, vfid, *guc_state, PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE);
+error:
+	if (err)
+		IOV_ERROR(iov, "Failed to save VF%u GuC state: (%pe)", vfid, ERR_PTR(err));
+
+	return err;
+}
+
+static void pf_save_vfs_guc_state(struct drm_i915_private *i915, unsigned int num_vfs)
+{
+	unsigned int saved = 0;
+	struct intel_gt *gt;
+	unsigned int gt_id;
+	unsigned int vfid;
+
+	for (vfid = 1; vfid <= num_vfs; vfid++) {
+		if (!needs_save_restore(i915, vfid)) {
+			drm_dbg(&i915->drm, "Save of VF%u GuC state has been skipped\n", vfid);
+			continue;
+		}
+
+		for_each_gt(gt, i915, gt_id) {
+			int err = pf_gt_save_vf_guc_state(gt, vfid);
+
+			if (err < 0)
+				goto skip_vf;
+		}
+		saved++;
+		continue;
+skip_vf:
+		break;
+	}
+
+	drm_dbg(&i915->drm, "%u of %u VFs GuC state successfully saved", saved, num_vfs);
+}
+
+static int pf_gt_restore_vf_guc_state(struct intel_gt *gt, unsigned int vfid)
+{
+	struct pci_dev *pdev = to_pci_dev(gt->i915->drm.dev);
+	struct intel_iov *iov = &gt->iov;
+	unsigned long timeout_ms = I915_VF_REPROVISION_TIMEOUT_MS;
+	int err;
+
+	GEM_BUG_ON(!vfid);
+	GEM_BUG_ON(vfid > pci_num_vf(pdev));
+
+	if (!iov->pf.state.data[vfid].guc_state)
+		return -EINVAL;
+
+	if (wait_for(iov->pf.provisioning.num_pushed >= vfid, timeout_ms)) {
+		IOV_ERROR(iov,
+			  "Failed to restore VF%u GuC state. Provisioning didn't complete within %lu ms\n",
+			  vfid, timeout_ms);
+		return -ETIMEDOUT;
+	}
+
+	err = intel_iov_state_restore_vf(iov, vfid, iov->pf.state.data[vfid].guc_state,
+					 PF2GUC_SAVE_RESTORE_VF_BUFF_SIZE);
+	if (err < 0) {
+		IOV_ERROR(iov, "Failed to restore VF%u GuC state: (%pe)", vfid, ERR_PTR(err));
+		return err;
+	}
+
+	kfree(iov->pf.state.data[vfid].guc_state);
+	iov->pf.state.data[vfid].guc_state = NULL;
+
+	return 0;
+}
+
+static void pf_restore_vfs_guc_state(struct drm_i915_private *i915, unsigned int num_vfs)
+{
+	unsigned int restored = 0;
+	struct intel_gt *gt;
+	unsigned int gt_id;
+	unsigned int vfid;
+
+	for (vfid = 1; vfid <= num_vfs; vfid++) {
+		if (!needs_save_restore(i915, vfid)) {
+			drm_dbg(&i915->drm, "Restoration of VF%u GuC state has been skipped\n",
+				vfid);
+			continue;
+		}
+
+		for_each_gt(gt, i915, gt_id) {
+			int err = pf_gt_restore_vf_guc_state(gt, vfid);
+
+			if (err < 0)
+				goto skip_vf;
+		}
+		restored++;
+		continue;
+skip_vf:
+		break;
+	}
+
+	drm_dbg(&i915->drm, "%u of %u VFs GuC state restored successfully", restored, num_vfs);
+}
+
+static i915_reg_t vf_master_irq(struct drm_i915_private *i915, unsigned int vfid)
+{
+	return (GRAPHICS_VER_FULL(i915) < IP_VER(12, 50)) ?
+		GEN12_VF_GFX_MSTR_IRQ(vfid) :
+		XEHPSDV_VF_GFX_MSTR_IRQ(vfid);
+}
+
+static void pf_restore_vfs_irqs(struct drm_i915_private *i915, unsigned int num_vfs)
+{
+	struct intel_gt *gt;
+	unsigned int gt_id;
+
+	for_each_gt(gt, i915, gt_id) {
+		unsigned int vfid;
+
+		for (vfid = 1; vfid <= num_vfs; vfid++)
+			raw_reg_write(gt->uncore->regs, vf_master_irq(i915, vfid),
+				      GEN11_MASTER_IRQ);
+	}
+}
+
+static void pf_suspend_active_vfs(struct drm_i915_private *i915)
+{
+	struct pci_dev *pdev = to_pci_dev(i915->drm.dev);
+	unsigned int num_vfs = pci_num_vf(pdev);
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	if (num_vfs == 0)
+		return;
+
+	pf_save_vfs_guc_state(i915, num_vfs);
+}
+
+static void pf_resume_active_vfs(struct drm_i915_private *i915)
+{
+	struct pci_dev *pdev = to_pci_dev(i915->drm.dev);
+	unsigned int num_vfs = pci_num_vf(pdev);
+
+	GEM_BUG_ON(!IS_SRIOV_PF(i915));
+
+	if (num_vfs == 0)
+		return;
+
+	pf_restore_vfs_pci_state(i915, num_vfs);
+	pf_restore_vfs_guc_state(i915, num_vfs);
+	pf_restore_vfs_irqs(i915, num_vfs);
+}
+
 /**
  * i915_sriov_pf_stop_vf - Stop VF.
  * @i915: the i915 struct
@@ -763,6 +1001,267 @@ int i915_sriov_pf_resume_vf(struct drm_i915_private *i915, unsigned int vfid)
 	return result;
 }
 
+/**
+ * i915_sriov_pause_vf - Pause VF.
+ * @pdev: the i915 struct
+ * @vfid: VF identifier
+ *
+ * This function will pause VF on all tiles.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_pause_vf(struct pci_dev *pdev, unsigned int vfid)
+{
+	struct drm_i915_private *i915 = pci_get_drvdata(pdev);
+
+	if (!IS_SRIOV_PF(i915))
+		return -ENODEV;
+
+	return i915_sriov_pf_pause_vf(i915, vfid);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_pause_vf, I915);
+
+/**
+ * i915_sriov_resume_vf - Resume VF.
+ * @pdev: the i915 struct
+ * @vfid: VF identifier
+ *
+ * This function will resume VF on all tiles.
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_resume_vf(struct pci_dev *pdev, unsigned int vfid)
+{
+	struct drm_i915_private *i915 = pci_get_drvdata(pdev);
+
+	if (!IS_SRIOV_PF(i915))
+		return -ENODEV;
+
+	return i915_sriov_pf_resume_vf(i915, vfid);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_resume_vf, I915);
+
+/**
+ * i915_sriov_wait_vf_flr_done - Wait for VF FLR completion.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ *
+ * This function will wait until VF FLR is processed by PF on all tiles (or
+ * until timeout occurs).
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int i915_sriov_wait_vf_flr_done(struct pci_dev *pdev, unsigned int vfid)
+{
+	struct drm_i915_private *i915 = pci_get_drvdata(pdev);
+	struct intel_gt *gt;
+	unsigned int id;
+	int ret;
+
+	if (!IS_SRIOV_PF(i915))
+		return -ENODEV;
+
+	for_each_gt(gt, i915, id) {
+		ret = wait_for(intel_iov_state_no_flr(&gt->iov, vfid), I915_VF_FLR_TIMEOUT_MS);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_wait_vf_flr_done, I915);
+
+static struct intel_gt *
+sriov_to_gt(struct pci_dev *pdev, unsigned int tile)
+{
+	struct drm_i915_private *i915 = pci_get_drvdata(pdev);
+	struct intel_gt *gt;
+
+	if (!i915 || !IS_SRIOV_PF(i915))
+		return NULL;
+
+	if (!HAS_EXTRA_GT_LIST(i915) && tile > 0)
+		return NULL;
+
+	gt = NULL;
+	if (tile < ARRAY_SIZE(i915->gt))
+		gt = i915->gt[tile];
+
+	return gt;
+}
+
+/**
+ * i915_sriov_ggtt_size - Get size needed to store VF GGTT.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ * @tile: tile identifier
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: Size in bytes.
+ */
+size_t
+i915_sriov_ggtt_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
+{
+	struct intel_gt *gt;
+	ssize_t size;
+
+	gt = sriov_to_gt(pdev, tile);
+	if (!gt)
+		return 0;
+
+	if (gt->type == GT_MEDIA)
+		return 0;
+
+	size = intel_iov_state_save_ggtt(&gt->iov, vfid, NULL, 0);
+	WARN_ON(size < 0);
+
+	return size;
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_ggtt_size, I915);
+
+/**
+ * i915_sriov_ggtt_save - Save VF GGTT.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ * @tile: tile identifier
+ * @buf: buffer to save VF GGTT
+ * @size: size of buffer to save VF GGTT
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: Size of data written on success or a negative error code on failure.
+ */
+ssize_t i915_sriov_ggtt_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			     void *buf, size_t size)
+{
+	struct intel_gt *gt;
+
+	gt = sriov_to_gt(pdev, tile);
+	if (!gt)
+		return -ENODEV;
+
+	if (gt->type == GT_MEDIA)
+		return -ENODEV;
+
+	WARN_ON(buf == NULL && size == 0);
+
+	return intel_iov_state_save_ggtt(&gt->iov, vfid, buf, size);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_ggtt_save, I915);
+
+/**
+ * i915_sriov_ggtt_load - Load VF GGTT.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ * @tile: tile identifier
+ * @buf: buffer with VF GGTT
+ * @size: size of buffer with VF GGTT
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int
+i915_sriov_ggtt_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+		     const void *buf, size_t size)
+{
+	struct intel_gt *gt;
+
+	gt = sriov_to_gt(pdev, tile);
+	if (!gt)
+		return -ENODEV;
+
+	if (gt->type == GT_MEDIA)
+		return -ENODEV;
+
+	return intel_iov_state_restore_ggtt(&gt->iov, vfid, buf, size);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_ggtt_load, I915);
+
+/**
+ * i915_sriov_fw_state_size - Get size needed to store GuC FW state.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ * @tile: tile identifier
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: Size in bytes.
+ */
+size_t
+i915_sriov_fw_state_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
+{
+	struct intel_gt *gt;
+
+	gt = sriov_to_gt(pdev, tile);
+	if (!gt)
+		return 0;
+
+	return SZ_4K;
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_fw_state_size, I915);
+
+/**
+ * i915_sriov_fw_state_save - Save GuC FW state.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ * @tile: tile identifier
+ * @buf: buffer to save GuC FW state
+ * @size: size of buffer to save GuC FW state
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: Size of data written on success or a negative error code on failure.
+ */
+ssize_t
+i915_sriov_fw_state_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			 void *buf, size_t size)
+{
+	struct intel_gt *gt;
+	int ret;
+
+	gt = sriov_to_gt(pdev, tile);
+	if (!gt)
+		return -ENODEV;
+
+	ret = intel_iov_state_save_vf(&gt->iov, vfid, buf, size);
+	if (ret)
+		return ret;
+
+	return SZ_4K;
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_fw_state_save, I915);
+
+/**
+ * i915_sriov_fw_state_load - Load GuC FW state.
+ * @pdev: PF pci device
+ * @vfid: VF identifier
+ * @tile: tile identifier
+ * @buf: buffer with GuC FW state to load
+ * @size: size of buffer with GuC FW state
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: 0 on success or a negative error code on failure.
+ */
+int
+i915_sriov_fw_state_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			 const void *buf, size_t size)
+{
+	struct intel_gt *gt;
+
+	gt = sriov_to_gt(pdev, tile);
+	if (!gt)
+		return -ENODEV;
+
+	return intel_iov_state_store_guc_migration_state(&gt->iov, vfid, buf, size);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_fw_state_load, I915);
+
 /**
  * i915_sriov_pf_clear_vf - Unprovision VF.
  * @i915: the i915 struct
@@ -809,6 +1308,7 @@ int i915_sriov_suspend_prepare(struct drm_i915_private *i915)
 	unsigned int id;
 
 	if (IS_SRIOV_PF(i915)) {
+		pf_resume_active_vfs(i915);
 		/*
 		 * When we're enabling the VFs in i915_sriov_pf_enable_vfs(), we also get
 		 * a GT PM wakeref which we hold for the whole VFs life cycle.
@@ -819,6 +1319,8 @@ int i915_sriov_suspend_prepare(struct drm_i915_private *i915)
 			for_each_gt(gt, i915, id)
 				intel_gt_pm_put(gt);
 		}
+
+		pf_suspend_active_vfs(i915);
 	}
 
 	return 0;
diff --git a/drivers/vfio/pci/Kconfig b/drivers/vfio/pci/Kconfig
index 86bb7835cf3c..f666d4d3937e 100644
--- a/drivers/vfio/pci/Kconfig
+++ b/drivers/vfio/pci/Kconfig
@@ -63,4 +63,6 @@ source "drivers/vfio/pci/mlx5/Kconfig"
 
 source "drivers/vfio/pci/hisilicon/Kconfig"
 
+source "drivers/vfio/pci/i915/Kconfig"
+
 endmenu
diff --git a/drivers/vfio/pci/Makefile b/drivers/vfio/pci/Makefile
index 24c524224da5..f8a2ca9ee702 100644
--- a/drivers/vfio/pci/Makefile
+++ b/drivers/vfio/pci/Makefile
@@ -11,3 +11,5 @@ obj-$(CONFIG_VFIO_PCI) += vfio-pci.o
 obj-$(CONFIG_MLX5_VFIO_PCI)           += mlx5/
 
 obj-$(CONFIG_HISI_ACC_VFIO_PCI) += hisilicon/
+
+obj-$(CONFIG_I915_VFIO_PCI) += i915/
diff --git a/drivers/vfio/pci/i915/Kconfig b/drivers/vfio/pci/i915/Kconfig
new file mode 100644
index 000000000000..e2983fbaef19
--- /dev/null
+++ b/drivers/vfio/pci/i915/Kconfig
@@ -0,0 +1,25 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+config I915_VFIO_PCI
+	tristate "VFIO vendor-specific PCI driver for Intel Graphics"
+	depends on DRM_I915
+	help
+	  This option enables vendor-specific VFIO driver for Intel Graphics.
+	  In addition to generic VFIO PCI functionality, it implements VFIO
+	  migration uAPI (v1) allowing userspace to enable migration for
+	  Intel Graphics SR-IOV Virtual Functions.
+
+config I915_VFIO_PCI_TEST
+	tristate "KUnit tests for Intel Graphics VFIO" if !KUNIT_ALL_TESTS
+	depends on I915_VFIO_PCI && KUNIT
+	default KUNIT_ALL_TESTS
+	help
+	  This option enables unit tests for i915-vfio-pci.
+	  It is not useful for distributions or general kernels,
+	  but only for kernel developers working on the driver.
+
+	  For more information on KUnit and unit tests in general,
+	  please refer to the KUnit documentation in
+	  Documentation/dev-tools/kunit/.
+
+	  If in doubt, say "N".
diff --git a/drivers/vfio/pci/i915/Makefile b/drivers/vfio/pci/i915/Makefile
new file mode 100644
index 000000000000..7454a17ef5b3
--- /dev/null
+++ b/drivers/vfio/pci/i915/Makefile
@@ -0,0 +1,4 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+obj-$(CONFIG_I915_VFIO_PCI) += i915-vfio-pci.o
+i915-vfio-pci-y := main.o data.o
diff --git a/drivers/vfio/pci/i915/data.c b/drivers/vfio/pci/i915/data.c
new file mode 100644
index 000000000000..07d10a322341
--- /dev/null
+++ b/drivers/vfio/pci/i915/data.c
@@ -0,0 +1,409 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#include <linux/list.h>
+
+#include "i915_vfio_pci.h"
+
+#define BITSTREAM_MAGIC 0x4f49465635313949ULL
+#define BITSTREAM_VERSION 0x1
+
+struct i915_vfio_data_device_desc {
+	/** @magic: constant, driver specific value */
+	u64 magic;
+	/** @version: device data version */
+	u64 version;
+	u16 vendor;
+	u16 device;
+	u32 rsvd;
+	/** @flags: optional flags */
+	u64 flags;
+} __packed;
+
+enum i915_vfio_pci_migration_data_type {
+	I915_VFIO_DATA_DESC = 0,
+	I915_VFIO_DATA_GGTT,
+	I915_VFIO_DATA_GUC,
+	I915_VFIO_DATA_DONE,
+};
+
+static const char *i915_vfio_data_type_str(enum i915_vfio_pci_migration_data_type type)
+{
+	switch (type) {
+	case I915_VFIO_DATA_DESC: return "DESC";
+	case I915_VFIO_DATA_GGTT: return "GGTT";
+	case I915_VFIO_DATA_GUC: return "GUC";
+	case I915_VFIO_DATA_DONE: return "DONE";
+	default: return "";
+	}
+}
+
+static int
+__i915_vfio_produce(struct i915_vfio_pci_migration_file *migf, unsigned int tile, u32 type)
+{
+	struct i915_vfio_pci_core_device *i915_vdev = migf->i915_vdev;
+	struct device *dev = i915_vdev_to_dev(i915_vdev);
+	const struct i915_vfio_pci_resource_ops *ops;
+	struct i915_vfio_pci_migration_data *data;
+	size_t size;
+	void *buf;
+	int ret;
+
+	switch (type) {
+	case I915_VFIO_DATA_GGTT:
+		ops = &i915_vdev->pf_ops->ggtt;
+		break;
+	case I915_VFIO_DATA_GUC:
+		ops = &i915_vdev->pf_ops->fw;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	size = ops->size(i915_vdev->pf, i915_vdev->vfid, tile);
+	if (!size) {
+		dev_dbg(dev, "Skipping %s for tile%u, ret=%ld\n",
+			i915_vfio_data_type_str(type), tile, size);
+
+		return 0;
+	}
+	buf = kvmalloc(size, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data) {
+		ret = -ENOMEM;
+		goto out_free_buf;
+	}
+
+	ret = ops->save(i915_vdev->pf, i915_vdev->vfid, tile, buf, size);
+	if (ret < 0)
+		goto out_free_data;
+
+	data->hdr.type = type;
+	data->hdr.tile = tile;
+	data->hdr.offset = 0;
+	data->hdr.size = size;
+	data->hdr.flags = 0;
+
+	data->pos = 0;
+	data->buf = buf;
+
+	dev_dbg(dev, "Producing %s for tile%u, size=%ld\n",
+		i915_vfio_data_type_str(type), tile, size);
+
+	list_add(&data->link, &migf->save_data);
+
+	return 0;
+
+out_free_data:
+	kfree(data);
+out_free_buf:
+	kvfree(buf);
+
+	return ret;
+}
+
+static int __i915_vfio_consume(struct i915_vfio_pci_migration_file *migf, unsigned int tile,
+			       u32 type, const char __user *ubuf, size_t len)
+{
+	struct i915_vfio_pci_core_device *i915_vdev = migf->i915_vdev;
+	struct i915_vfio_pci_migration_data *data = &migf->resume_data;
+	const struct i915_vfio_pci_resource_ops *ops;
+	int ret;
+
+	switch (type) {
+	case I915_VFIO_DATA_GGTT:
+		ops = &i915_vdev->pf_ops->ggtt;
+		break;
+	case I915_VFIO_DATA_GUC:
+		ops = &i915_vdev->pf_ops->fw;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	if (data->pos + len > data->hdr.size)
+		return -EINVAL;
+
+	if (!data->buf) {
+		data->buf = kvmalloc(data->hdr.size, GFP_KERNEL);
+		if (!data->buf)
+			return -ENOMEM;
+	}
+
+	if (migf->copy_from(data->buf + data->pos, ubuf, len)) {
+		ret = -EFAULT;
+		goto out_free;
+	}
+
+	if (data->pos + len == data->hdr.size) {
+		ret = ops->load(i915_vdev->pf, i915_vdev->vfid, tile, data->buf, data->hdr.size);
+		if (ret)
+			goto out_free;
+	}
+
+	return 0;
+
+out_free:
+	kvfree(data->buf);
+
+	return ret;
+}
+
+#define __resource(x, type) \
+static int \
+i915_vfio_produce_##x(struct i915_vfio_pci_migration_file *migf, unsigned int tile) \
+{ \
+	return __i915_vfio_produce(migf, tile, type); \
+} \
+static int \
+i915_vfio_consume_##x(struct i915_vfio_pci_migration_file *migf, \
+		      unsigned int tile, const char __user *ubuf, size_t len) \
+{ \
+	return __i915_vfio_consume(migf, tile, type, ubuf, len); \
+}
+
+__resource(ggtt, I915_VFIO_DATA_GGTT);
+__resource(fw, I915_VFIO_DATA_GUC);
+
+static int i915_vfio_produce_desc(struct i915_vfio_pci_migration_file *migf)
+{
+	struct i915_vfio_pci_migration_data *data;
+	struct i915_vfio_data_device_desc desc;
+	void *buf;
+
+	desc.magic = BITSTREAM_MAGIC;
+	desc.version = BITSTREAM_VERSION;
+	desc.vendor = i915_vdev_to_pdev(migf->i915_vdev)->vendor;
+	desc.device = i915_vdev_to_pdev(migf->i915_vdev)->device;
+	desc.flags = 0x0;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	buf = kvmalloc(sizeof(desc), GFP_KERNEL);
+	if (!buf) {
+		kfree(data);
+		return -ENOMEM;
+	}
+
+	data->hdr.type = I915_VFIO_DATA_DESC;
+	data->hdr.tile = 0;
+	data->hdr.offset = 0;
+	data->hdr.size = sizeof(desc);
+	data->hdr.flags = 0;
+	data->pos = 0;
+	data->buf = buf;
+
+	memcpy(data->buf, &desc, sizeof(desc));
+
+	list_add(&data->link, &migf->save_data);
+
+	return 0;
+}
+
+static int i915_vfio_consume_desc(struct i915_vfio_pci_migration_file *migf,
+				  const char __user *ubuf, size_t len)
+{
+	struct i915_vfio_pci_migration_header *hdr = &migf->resume_data.hdr;
+	struct i915_vfio_data_device_desc desc;
+
+	if (hdr->size != sizeof(desc))
+		return -EINVAL;
+
+	if (sizeof(desc) != len)
+		return -EINVAL;
+
+	if (migf->copy_from(&desc, ubuf, len))
+		return -EFAULT;
+
+	if (desc.magic != BITSTREAM_MAGIC)
+		return -EINVAL;
+
+	if (desc.version != BITSTREAM_VERSION)
+		return -EINVAL;
+
+	if (desc.vendor != i915_vdev_to_pdev(migf->i915_vdev)->vendor)
+		return -EINVAL;
+
+	if (desc.device != i915_vdev_to_pdev(migf->i915_vdev)->device)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int
+i915_vfio_pci_produce_data(struct i915_vfio_pci_migration_file *migf,
+			   enum i915_vfio_pci_migration_data_type type, unsigned int tile)
+{
+	switch (type) {
+	case I915_VFIO_DATA_DESC:
+		if (tile)
+			return 0;
+		return i915_vfio_produce_desc(migf);
+	case I915_VFIO_DATA_GGTT:
+		return i915_vfio_produce_ggtt(migf, tile);
+	case I915_VFIO_DATA_GUC:
+		return i915_vfio_produce_fw(migf, tile);
+	default:
+		return -EINVAL;
+	}
+}
+
+static ssize_t
+i915_vfio_consume_data(struct i915_vfio_pci_migration_file *migf, const char __user *ubuf,
+		       size_t len)
+{
+	struct i915_vfio_pci_migration_header *hdr = &migf->resume_data.hdr;
+
+	switch (hdr->type) {
+	case I915_VFIO_DATA_DESC:
+		return i915_vfio_consume_desc(migf, ubuf, len);
+	case I915_VFIO_DATA_GGTT:
+		return i915_vfio_consume_ggtt(migf, hdr->tile, ubuf, len);
+	case I915_VFIO_DATA_GUC:
+		return i915_vfio_consume_fw(migf, hdr->tile, ubuf, len);
+	default:
+		return -EINVAL;
+	}
+}
+
+static void i915_vfio_save_data_free(struct i915_vfio_pci_migration_data *data)
+{
+	list_del_init(&data->link);
+	kvfree(data->buf);
+	kfree(data);
+}
+
+void i915_vfio_save_data_release(struct i915_vfio_pci_migration_file *migf)
+{
+	struct i915_vfio_pci_migration_data *data, *next;
+
+	if (!migf)
+		return;
+
+	list_for_each_entry_safe(data, next, &migf->save_data, link)
+		i915_vfio_save_data_free(data);
+}
+
+static void i915_vfio_resume_data_free(struct i915_vfio_pci_migration_data *data)
+{
+	data->hdr_processed = false;
+	data->pos = 0;
+
+	kvfree(data->buf);
+	data->buf = NULL;
+}
+
+int i915_vfio_pci_produce_save_data(struct i915_vfio_pci_migration_file *migf)
+{
+	enum i915_vfio_pci_migration_data_type type;
+	unsigned int tile;
+	int ret;
+
+	for (tile = 0; tile < I915_VFIO_MAX_TILE; tile++) {
+		for (type = I915_VFIO_DATA_DESC; type < I915_VFIO_DATA_DONE; type++) {
+			ret = i915_vfio_pci_produce_data(migf, type, tile);
+			if (ret)
+				goto out;
+		}
+	}
+
+	return 0;
+
+out:
+	i915_vfio_save_data_release(migf);
+	return ret;
+}
+
+ssize_t i915_vfio_data_read(struct i915_vfio_pci_migration_file *migf, char __user *ubuf,
+			    size_t len)
+{
+	struct i915_vfio_pci_migration_data *data;
+	size_t len_remain, len_hdr;
+	int ret;
+
+	data = list_first_entry_or_null(&migf->save_data, typeof(*data), link);
+	if (!data)
+		return 0;
+
+	if (!data->hdr_processed) {
+		if (len < sizeof(data->hdr))
+			return -EINVAL;
+
+		ret = migf->copy_to(ubuf, &data->hdr, sizeof(data->hdr));
+		if (ret)
+			return -EFAULT;
+
+		len_hdr = sizeof(data->hdr);
+		ubuf += sizeof(data->hdr);
+		data->hdr_processed = true;
+	} else {
+		len_hdr = 0;
+	}
+
+	len_remain = len_hdr + data->hdr.size - data->pos;
+	len = min(len, len_remain);
+
+	if (migf->copy_to(ubuf, data->buf + data->pos, len - len_hdr))
+		return -EFAULT;
+
+	if (len < len_remain)
+		data->pos += len - len_hdr;
+	else
+		i915_vfio_save_data_free(data);
+
+	return len;
+}
+
+ssize_t i915_vfio_data_write(struct i915_vfio_pci_migration_file *migf, const char __user *ubuf,
+			     size_t len)
+{
+	struct i915_vfio_pci_migration_data *data = &migf->resume_data;
+	size_t len_remain, len_hdr;
+	int ret;
+
+	if (!data->hdr_processed) {
+		if (len < sizeof(data->hdr))
+			return -EINVAL;
+
+		if (migf->copy_from(&data->hdr, ubuf, sizeof(data->hdr)))
+			return -EFAULT;
+
+		len_hdr = sizeof(data->hdr);
+		ubuf += sizeof(data->hdr);
+		data->hdr_processed = true;
+
+		dev_dbg(i915_vdev_to_dev(migf->i915_vdev),
+			"Consuming %s for tile%lld, size=%lld\n",
+			i915_vfio_data_type_str(data->hdr.type), data->hdr.tile, data->hdr.size);
+	} else {
+		len_hdr = 0;
+	}
+
+	len_remain = len_hdr + data->hdr.size - data->pos;
+	len = min(len, len_remain);
+
+	ret = i915_vfio_consume_data(migf, ubuf, len - len_hdr);
+	if (ret) {
+		i915_vfio_resume_data_free(data);
+
+		return ret;
+	}
+
+	if (len < len_remain)
+		data->pos += len - len_hdr;
+	else
+		i915_vfio_resume_data_free(data);
+
+	return len;
+}
+
+#if IS_ENABLED(CONFIG_I915_VFIO_PCI_TEST)
+#include "test/data_test.c"
+#endif
diff --git a/drivers/vfio/pci/i915/i915_vfio_pci.h b/drivers/vfio/pci/i915/i915_vfio_pci.h
new file mode 100644
index 000000000000..9cdb0ff2bcac
--- /dev/null
+++ b/drivers/vfio/pci/i915/i915_vfio_pci.h
@@ -0,0 +1,114 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#include <linux/vfio_pci_core.h>
+#include <linux/sizes.h>
+
+#include <drm/i915_sriov.h>
+
+#define I915_VFIO_MAX_DATA_SIZE SZ_32M
+#define I915_VFIO_MAX_TILE 2
+
+/**
+ * struct i915_vfio_pci_migration_header - Migration header
+ *
+ * Header describing each individual iteration of device data.
+ */
+struct i915_vfio_pci_migration_header {
+	/** @type: type of device state */
+	u64 type;
+	/** @tile: tile from which the device state comes from */
+	u64 tile;
+	/** @offset: foo */
+	u64 offset;
+	/** @size: size of device data that follows */
+	u64 size;
+	/** @flags: optional flags */
+	u64 flags;
+} __packed;
+
+struct i915_vfio_pci_migration_data {
+	struct i915_vfio_pci_migration_header hdr;
+	bool hdr_processed;
+	struct list_head link;
+	void *buf;
+	loff_t pos;
+};
+
+/**
+ * struct i915_vfio_pci_migration_file
+ */
+struct i915_vfio_pci_migration_file {
+	struct file *filp;
+	/* Protects save_data / resume_data */
+	struct mutex lock;
+	struct list_head save_data;
+	struct i915_vfio_pci_migration_data resume_data;
+	struct i915_vfio_pci_core_device *i915_vdev;
+	unsigned long (*copy_from)(void *to, const void __user *from, unsigned long n);
+	unsigned long (*copy_to)(void __user *to, const void *from, unsigned long n);
+};
+
+struct i915_vfio_pci_mappable_resource {
+	void *vaddr;
+	ssize_t size;
+};
+
+/**
+ * struct i915_vfio_pci_core_device - i915-specific vfio_pci_core_device
+ *
+ * Top level structure of i915_vfio_pci.
+ */
+struct i915_vfio_pci_core_device {
+	/** @core_device: vendor-agnostic VFIO device */
+	struct vfio_pci_core_device core_device;
+
+	enum vfio_device_mig_state mig_state;
+
+	/** @vfid: VF number used by PF, i915 uses 1-based indexing for vfid */
+	unsigned int vfid;
+
+	/** @pf: pointer to driver_private of physical function */
+	struct pci_dev *pf;
+	const struct i915_vfio_pci_migration_pf_ops *pf_ops;
+
+	struct i915_vfio_pci_mappable_resource lmem[I915_VFIO_MAX_TILE];
+
+	struct i915_vfio_pci_migration_file *fd;
+};
+
+struct i915_vfio_pci_mappable_resource_ops {
+	size_t (*size)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
+	void * (*map)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
+	void (*unmap)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
+};
+
+struct i915_vfio_pci_resource_ops {
+	size_t (*size)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
+	ssize_t (*save)(struct pci_dev *pf, unsigned int vfid, unsigned int tile,
+			void *buf, size_t size);
+	int (*load)(struct pci_dev *pf, unsigned int vfid, unsigned int tile,
+		    const void *buf, size_t size);
+};
+
+struct i915_vfio_pci_migration_pf_ops {
+	int (*pause)(struct pci_dev *pf, unsigned int vfid);
+	int (*resume)(struct pci_dev *pf, unsigned int vfid);
+	int (*wait_flr_done)(struct pci_dev *pf, unsigned int vfid);
+	struct i915_vfio_pci_resource_ops ggtt, fw;
+	struct i915_vfio_pci_mappable_resource_ops lmem;
+};
+
+#define i915_vdev_to_dev(i915_vdev) (&(i915_vdev)->core_device.pdev->dev)
+#define i915_vdev_to_pdev(i915_vdev) ((i915_vdev)->core_device.pdev)
+
+void i915_vfio_pci_reset(struct i915_vfio_pci_core_device *i915_vdev);
+ssize_t i915_vfio_data_read(struct i915_vfio_pci_migration_file *migf, char __user *buf,
+			    size_t len);
+ssize_t i915_vfio_data_write(struct i915_vfio_pci_migration_file *migf, const char __user *buf,
+			     size_t len);
+void i915_vfio_save_data_release(struct i915_vfio_pci_migration_file *migf);
+
+int i915_vfio_pci_produce_save_data(struct i915_vfio_pci_migration_file *migf);
diff --git a/drivers/vfio/pci/i915/main.c b/drivers/vfio/pci/i915/main.c
new file mode 100644
index 000000000000..880dc57175cf
--- /dev/null
+++ b/drivers/vfio/pci/i915/main.c
@@ -0,0 +1,399 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#include <linux/anon_inodes.h>
+#include <linux/delay.h>
+#include <linux/file.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/types.h>
+#include <linux/vfio.h>
+
+#include "i915_vfio_pci.h"
+
+static void i915_vfio_pci_reset_done(struct pci_dev *pdev)
+{
+	struct i915_vfio_pci_core_device *i915_vdev = pci_get_drvdata(pdev);
+	int ret;
+
+	ret = i915_vdev->pf_ops->wait_flr_done(i915_vdev->pf, i915_vdev->vfid);
+	if (ret)
+		dev_err(&pdev->dev, "Failed to wait for FLR: %d\n", ret);
+
+	i915_vfio_pci_reset(i915_vdev);
+}
+
+static const struct pci_error_handlers i915_vfio_pci_err_handlers = {
+	.reset_done = &i915_vfio_pci_reset_done,
+};
+
+static int i915_vfio_pci_open_device(struct vfio_device *core_vdev)
+{
+	struct i915_vfio_pci_core_device *i915_vdev =
+		container_of(core_vdev, struct i915_vfio_pci_core_device, core_device.vdev);
+	struct vfio_pci_core_device *vdev = &i915_vdev->core_device;
+	int ret;
+
+	ret = vfio_pci_core_enable(vdev);
+	if (ret)
+		return ret;
+
+	vfio_pci_core_finish_enable(vdev);
+
+	return 0;
+}
+
+static void i915_vfio_pci_disable_file(struct i915_vfio_pci_migration_file *migf)
+{
+	struct i915_vfio_pci_core_device *i915_vdev = migf->i915_vdev;
+
+	mutex_lock(&migf->lock);
+	i915_vfio_save_data_release(i915_vdev->fd);
+	i915_vdev->fd = NULL;
+	mutex_unlock(&migf->lock);
+}
+
+static int i915_vfio_pci_release_file(struct inode *inode, struct file *filp)
+{
+	struct i915_vfio_pci_migration_file *migf = filp->private_data;
+
+	i915_vfio_pci_disable_file(migf);
+	mutex_destroy(&migf->lock);
+	kfree(migf);
+
+	return 0;
+}
+
+static ssize_t i915_vfio_pci_save_read(struct file *filp, char __user *buf, size_t len, loff_t *pos)
+{
+	struct i915_vfio_pci_migration_file *migf = filp->private_data;
+	ssize_t ret;
+
+	if (pos)
+		return -ESPIPE;
+
+	mutex_lock(&migf->lock);
+	ret = i915_vfio_data_read(migf, buf, len);
+	mutex_unlock(&migf->lock);
+
+	return ret;
+}
+
+static const struct file_operations i915_vfio_pci_save_fops = {
+	.owner = THIS_MODULE,
+	.read = i915_vfio_pci_save_read,
+	.release = i915_vfio_pci_release_file,
+	.llseek = no_llseek,
+};
+
+static ssize_t i915_vfio_pci_resume_write(struct file *filp, const char __user *buf,
+					  size_t len, loff_t *pos)
+{
+	struct i915_vfio_pci_migration_file *migf = filp->private_data;
+	ssize_t ret;
+
+	if (pos)
+		return -ESPIPE;
+
+	mutex_lock(&migf->lock);
+	ret = i915_vfio_data_write(migf, buf, len);
+	mutex_unlock(&migf->lock);
+
+	return ret;
+}
+
+static const struct file_operations i915_vfio_pci_resume_fops = {
+	.owner = THIS_MODULE,
+	.write = i915_vfio_pci_resume_write,
+	.release = i915_vfio_pci_release_file,
+	.llseek = no_llseek,
+};
+
+void i915_vfio_pci_reset(struct i915_vfio_pci_core_device *i915_vdev)
+{
+	if (i915_vdev->fd)
+		i915_vfio_pci_disable_file(i915_vdev->fd);
+
+	i915_vdev->mig_state = VFIO_DEVICE_STATE_RUNNING;
+}
+
+static const char *i915_vfio_dev_state_str(u32 state)
+{
+	switch (state) {
+	case VFIO_DEVICE_STATE_RUNNING: return "running";
+	case VFIO_DEVICE_STATE_STOP_COPY: return "stopcopy";
+	case VFIO_DEVICE_STATE_STOP: return "stop";
+	case VFIO_DEVICE_STATE_RESUMING: return "resuming";
+	case VFIO_DEVICE_STATE_ERROR: return "error";
+	default: return "";
+	}
+}
+
+enum i915_vfio_pci_file_type {
+	I915_VFIO_FILE_SAVE = 0,
+	I915_VFIO_FILE_RESUME,
+};
+
+static struct i915_vfio_pci_migration_file *
+i915_vfio_pci_alloc_file(struct i915_vfio_pci_core_device *i915_vdev,
+			 enum i915_vfio_pci_file_type type)
+{
+	struct i915_vfio_pci_migration_file *migf;
+	const struct file_operations *fops;
+	int flags;
+
+	migf = kzalloc(sizeof(*migf), GFP_KERNEL);
+	if (!migf)
+		return ERR_PTR(-ENOMEM);
+
+	fops = type == I915_VFIO_FILE_SAVE ? &i915_vfio_pci_save_fops : &i915_vfio_pci_resume_fops;
+	flags = type == I915_VFIO_FILE_SAVE ? O_RDONLY : O_WRONLY;
+	migf->filp = anon_inode_getfile("i915_vfio_mig", fops, migf, flags);
+	if (IS_ERR(migf->filp)) {
+		kfree(migf);
+		return ERR_CAST(migf->filp);
+	}
+
+	INIT_LIST_HEAD(&migf->save_data);
+	mutex_init(&migf->lock);
+	migf->i915_vdev = i915_vdev;
+	migf->copy_from = copy_from_user;
+	migf->copy_to = copy_to_user;
+	i915_vdev->fd = migf;
+
+	stream_open(migf->filp->f_inode, migf->filp);
+
+	return migf;
+}
+
+static struct file *
+i915_vfio_set_state(struct i915_vfio_pci_core_device *i915_vdev, u32 new)
+{
+	const struct i915_vfio_pci_migration_pf_ops *ops = i915_vdev->pf_ops;
+	u32 cur = i915_vdev->mig_state;
+	int ret;
+
+	dev_dbg(i915_vdev_to_dev(i915_vdev),
+		"state: %s->%s\n", i915_vfio_dev_state_str(cur), i915_vfio_dev_state_str(new));
+
+	if (cur == VFIO_DEVICE_STATE_RUNNING && new == VFIO_DEVICE_STATE_STOP) {
+		ret = ops->pause(i915_vdev->pf, i915_vdev->vfid);
+		if (ret) {
+			dev_dbg(i915_vdev_to_dev(i915_vdev),
+				"Failed to transition state: %s->%s err=%d\n",
+				i915_vfio_dev_state_str(cur), i915_vfio_dev_state_str(new), ret);
+			return ERR_PTR(ret);
+		}
+		return NULL;
+	}
+
+	if (cur == VFIO_DEVICE_STATE_STOP && new == VFIO_DEVICE_STATE_RUNNING) {
+		ret = ops->resume(i915_vdev->pf, i915_vdev->vfid);
+		if (ret) {
+			dev_dbg(i915_vdev_to_dev(i915_vdev),
+				"Failed to transition state: %s->%s err=%d\n",
+				i915_vfio_dev_state_str(cur), i915_vfio_dev_state_str(new), ret);
+			return ERR_PTR(ret);
+		}
+		return NULL;
+	}
+
+	if (cur == VFIO_DEVICE_STATE_STOP && new == VFIO_DEVICE_STATE_STOP_COPY) {
+		struct i915_vfio_pci_migration_file *migf;
+		int ret;
+
+		migf = i915_vfio_pci_alloc_file(i915_vdev, I915_VFIO_FILE_SAVE);
+		if (IS_ERR(migf))
+			return ERR_CAST(migf);
+
+		ret = i915_vfio_pci_produce_save_data(migf);
+		if (ret) {
+			fput(migf->filp);
+			return ERR_PTR(ret);
+		}
+
+		return migf->filp;
+	}
+
+	if ((cur == VFIO_DEVICE_STATE_STOP_COPY && new == VFIO_DEVICE_STATE_STOP)) {
+		if (i915_vdev->fd)
+			i915_vfio_pci_disable_file(i915_vdev->fd);
+
+		return NULL;
+	}
+
+	if (cur == VFIO_DEVICE_STATE_STOP && new == VFIO_DEVICE_STATE_RESUMING) {
+		struct i915_vfio_pci_migration_file *migf;
+
+		migf = i915_vfio_pci_alloc_file(i915_vdev, I915_VFIO_FILE_RESUME);
+		if (IS_ERR(migf))
+			return ERR_CAST(migf);
+
+		return migf->filp;
+	}
+
+	if (cur == VFIO_DEVICE_STATE_RESUMING && new == VFIO_DEVICE_STATE_STOP) {
+		if (i915_vdev->fd)
+			i915_vfio_pci_disable_file(i915_vdev->fd);
+
+		return NULL;
+	}
+
+	WARN_ON(true);
+	return ERR_PTR(-EINVAL);
+}
+
+static struct file *
+i915_vfio_pci_set_device_state(struct vfio_device *core_vdev,
+			       enum vfio_device_mig_state new_state)
+{
+	struct i915_vfio_pci_core_device *i915_vdev =
+		container_of(core_vdev, struct i915_vfio_pci_core_device, core_device.vdev);
+	enum vfio_device_mig_state next_state;
+	struct file *f = NULL;
+	int ret;
+
+	while (new_state != i915_vdev->mig_state) {
+		ret = vfio_mig_get_next_state(core_vdev, i915_vdev->mig_state,
+					      new_state, &next_state);
+		if (ret) {
+			f = ERR_PTR(ret);
+			break;
+		}
+		f = i915_vfio_set_state(i915_vdev, next_state);
+		if (IS_ERR(f))
+			break;
+
+		i915_vdev->mig_state = next_state;
+
+		/* Multiple state transitions with non-NULL file in the middle */
+		if (f && new_state != i915_vdev->mig_state) {
+			fput(f);
+			f = ERR_PTR(-EINVAL);
+			break;
+		}
+	}
+
+	return f;
+}
+
+static int i915_vfio_pci_get_device_state(struct vfio_device *core_vdev,
+					  enum vfio_device_mig_state *curr_state)
+{
+	struct i915_vfio_pci_core_device *i915_vdev =
+		container_of(core_vdev, struct i915_vfio_pci_core_device, core_device.vdev);
+
+	*curr_state = i915_vdev->mig_state;
+
+	return 0;
+}
+
+static int i915_vfio_pci_get_data_size(struct vfio_device *vdev,
+				       unsigned long *stop_copy_length)
+{
+	return 0;
+}
+
+static const struct vfio_migration_ops i915_vfio_pci_migration_ops = {
+	.migration_set_state = i915_vfio_pci_set_device_state,
+	.migration_get_state = i915_vfio_pci_get_device_state,
+	.migration_get_data_size = i915_vfio_pci_get_data_size,
+};
+
+static const struct i915_vfio_pci_migration_pf_ops pf_ops = {
+	.pause = i915_sriov_pause_vf,
+	.resume = i915_sriov_resume_vf,
+	.wait_flr_done = i915_sriov_wait_vf_flr_done,
+	.ggtt.size = i915_sriov_ggtt_size,
+	.ggtt.save = i915_sriov_ggtt_save,
+	.ggtt.load = i915_sriov_ggtt_load,
+	.fw.size = i915_sriov_fw_state_size,
+	.fw.save = i915_sriov_fw_state_save,
+	.fw.load = i915_sriov_fw_state_load,
+};
+
+static int i915_vfio_pci_init_dev(struct vfio_device *core_vdev)
+{
+	struct i915_vfio_pci_core_device *i915_vdev =
+		container_of(core_vdev, struct i915_vfio_pci_core_device, core_device.vdev);
+	struct pci_dev *pdev = to_pci_dev(core_vdev->dev);
+
+	/* vfid starts from 1 for i915 */
+	i915_vdev->vfid = pci_iov_vf_id(pdev) + 1;
+	i915_vdev->pf = pdev->physfn;
+	i915_vdev->pf_ops = &pf_ops;
+
+	core_vdev->migration_flags = VFIO_MIGRATION_STOP_COPY;
+	core_vdev->mig_ops = &i915_vfio_pci_migration_ops;
+
+	return vfio_pci_core_init_dev(core_vdev);
+}
+
+static const struct vfio_device_ops i915_vfio_pci_ops = {
+	.name		= "i915-vfio-pci",
+	.init		= i915_vfio_pci_init_dev,
+	.release	= vfio_pci_core_release_dev,
+	.open_device	= i915_vfio_pci_open_device,
+	.close_device	= vfio_pci_core_close_device,
+	.ioctl		= vfio_pci_core_ioctl,
+	.device_feature = vfio_pci_core_ioctl_feature,
+	.read		= vfio_pci_core_read,
+	.write		= vfio_pci_core_write,
+	.mmap		= vfio_pci_core_mmap,
+	.request	= vfio_pci_core_request,
+	.match		= vfio_pci_core_match,
+	.bind_iommufd	= vfio_iommufd_physical_bind,
+	.unbind_iommufd = vfio_iommufd_physical_unbind,
+	.attach_ioas	= vfio_iommufd_physical_attach_ioas,
+};
+
+static int i915_vfio_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+	struct i915_vfio_pci_core_device *i915_vdev;
+	int ret;
+
+	if (!pdev->is_virtfn)
+		return -EINVAL;
+
+	if (strcmp(pdev->physfn->dev.driver->name, "i915"))
+		return -EINVAL;
+
+	i915_vdev = vfio_alloc_device(i915_vfio_pci_core_device, core_device.vdev, &pdev->dev,
+				      &i915_vfio_pci_ops);
+	if (IS_ERR(i915_vdev))
+		return PTR_ERR(i915_vdev);
+
+	dev_set_drvdata(&pdev->dev, &i915_vdev->core_device);
+
+	ret = vfio_pci_core_register_device(&i915_vdev->core_device);
+	if (ret) {
+		vfio_put_device(&i915_vdev->core_device.vdev);
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct pci_device_id i915_vfio_pci_table[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_ANY_ID),
+	  .class = PCI_BASE_CLASS_DISPLAY << 8, .class_mask = 0xff << 16,
+	  .override_only = PCI_ID_F_VFIO_DRIVER_OVERRIDE },
+	{}
+};
+MODULE_DEVICE_TABLE(pci, i915_vfio_pci_table);
+
+static struct pci_driver i915_vfio_pci_driver = {
+	.name = "i915-vfio-pci",
+	.id_table = i915_vfio_pci_table,
+	.probe = i915_vfio_pci_probe,
+	.err_handler = &i915_vfio_pci_err_handlers,
+	.driver_managed_dma = true,
+};
+module_pci_driver(i915_vfio_pci_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("VFIO PCI driver with migration support for Intel Graphics");
+MODULE_IMPORT_NS(I915);
diff --git a/drivers/vfio/pci/i915/test/data_test.c b/drivers/vfio/pci/i915/test/data_test.c
new file mode 100644
index 000000000000..d013a8a38518
--- /dev/null
+++ b/drivers/vfio/pci/i915/test/data_test.c
@@ -0,0 +1,308 @@
+#include <kunit/test.h>
+
+#include <linux/mman.h>
+#include <linux/random.h>
+
+#include <linux/sched/mm.h>
+
+unsigned long i915_vfio_test_copy(void *from, const void *to, unsigned long n)
+{
+	memcpy(from, to, n);
+
+	return 0;
+}
+
+struct i915_vfio_pci_data_test {
+	struct {
+		void *vaddr;
+		size_t size;
+	} resource;
+	struct i915_vfio_pci_core_device *i915_vdev;
+};
+
+size_t
+i915_vfio_test_res_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
+{
+	struct i915_vfio_pci_data_test *priv = pci_get_drvdata(pdev);
+
+	return priv->resource.size;
+}
+
+ssize_t i915_vfio_test_res_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+				void *buf, size_t size)
+{
+	struct i915_vfio_pci_data_test *priv = pci_get_drvdata(pdev);
+
+	memcpy(buf, priv->resource.vaddr, size);
+
+	return size;
+}
+
+int
+i915_vfio_test_res_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			const void *buf, size_t size)
+{
+	struct i915_vfio_pci_data_test *priv = pci_get_drvdata(pdev);
+
+	memcpy(priv->resource.vaddr, buf, size);
+
+	return 0;
+}
+
+static const struct i915_vfio_pci_migration_pf_ops pf_test_ops = {
+	.ggtt.size = i915_vfio_test_res_size,
+	.ggtt.save = i915_vfio_test_res_save,
+	.ggtt.load = i915_vfio_test_res_load,
+};
+
+#define I915_VFIO_TEST_RES_SIZE SZ_4K
+static int i915_vfio_pci_data_test_init(struct kunit *test)
+{
+	struct i915_vfio_pci_core_device *i915_vdev;
+	struct i915_vfio_pci_migration_file *fd;
+	struct i915_vfio_pci_data_test *priv;
+	struct pci_dev *pdev;
+	unsigned long *resource;
+	size_t resource_size = I915_VFIO_TEST_RES_SIZE;
+	int i;
+
+	priv = kunit_kzalloc(test, sizeof(*priv), GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, priv);
+
+	i915_vdev = kunit_kzalloc(test, sizeof(*i915_vdev), GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, i915_vdev);
+
+	resource = kunit_kzalloc(test, resource_size, GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, resource);
+
+	for (i = 0; i < resource_size / sizeof(*resource); i++)
+		resource[i] = get_random_long();
+
+	priv->i915_vdev = i915_vdev;
+	priv->resource.vaddr = resource;
+	priv->resource.size = resource_size;
+
+	i915_vdev->lmem[0].vaddr = resource;
+	i915_vdev->lmem[0].size = resource_size;
+
+	i915_vdev->pf_ops = &pf_test_ops;
+
+	fd = kunit_kzalloc(test, sizeof(*fd), GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, fd);
+	i915_vdev->fd = fd;
+	INIT_LIST_HEAD(&fd->save_data);
+	fd->i915_vdev = i915_vdev;
+	fd->copy_from = i915_vfio_test_copy;
+	fd->copy_to = i915_vfio_test_copy;
+
+	pdev = kunit_kzalloc(test, sizeof(*pdev), GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, pdev);
+	i915_vdev->core_device.pdev = pdev;
+	i915_vdev->pf = pdev;
+
+	pdev->vendor = 0x1234;
+	pdev->device = 0x4321;
+
+	pci_set_drvdata(pdev, priv);
+
+	test->priv = priv;
+
+	return 0;
+}
+
+static void i915_vfio_pci_data_test_exit(struct kunit *test)
+{
+	struct i915_vfio_pci_data_test *priv = test->priv;
+	struct i915_vfio_pci_core_device *i915_vdev = priv->i915_vdev;
+
+	i915_vfio_save_data_release(i915_vdev->fd);
+}
+
+static void test_produce_consume_desc(struct kunit *test)
+{
+	struct i915_vfio_pci_data_test *priv = test->priv;
+	struct i915_vfio_pci_core_device *i915_vdev = priv->i915_vdev;
+	struct i915_vfio_pci_migration_data *data;
+	size_t data_len;
+	ssize_t ret;
+	void *buf;
+
+	KUNIT_ASSERT_TRUE(test, list_empty(&i915_vdev->fd->save_data));
+	ret = i915_vfio_produce_desc(i915_vdev->fd);
+	KUNIT_ASSERT_EQ(test, ret, 0);
+	KUNIT_ASSERT_FALSE(test, list_empty(&i915_vdev->fd->save_data));
+
+	data = list_first_entry_or_null(&i915_vdev->fd->save_data, typeof(*data), link);
+	KUNIT_ASSERT_PTR_NE(test, data, NULL);
+	data_len = data->hdr.size + sizeof(data->hdr);
+
+	buf = kunit_kzalloc(test, data_len, GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, buf);
+
+	ret = i915_vfio_data_read(i915_vdev->fd, buf, data_len);
+	KUNIT_EXPECT_EQ(test, ret, data_len);
+
+	KUNIT_EXPECT_TRUE(test, list_empty(&i915_vdev->fd->save_data));
+
+	ret = i915_vfio_data_write(i915_vdev->fd, buf, data_len);
+	KUNIT_EXPECT_EQ(test, ret, data_len);
+}
+
+static void test_produce_res(struct kunit *test)
+{
+	struct i915_vfio_pci_data_test *priv = test->priv;
+	struct i915_vfio_pci_core_device *i915_vdev = priv->i915_vdev;
+	struct i915_vfio_pci_migration_data *data;
+	struct i915_vfio_pci_migration_header hdr;
+	size_t data_len;
+	ssize_t ret;
+	void *buf;
+
+	ret = i915_vfio_produce_ggtt(i915_vdev->fd, 0);
+	KUNIT_ASSERT_EQ(test, ret, 0);
+	KUNIT_ASSERT_FALSE(test, list_empty(&i915_vdev->fd->save_data));
+
+	data = list_first_entry_or_null(&i915_vdev->fd->save_data, typeof(*data), link);
+	KUNIT_ASSERT_PTR_NE(test, data, NULL);
+	KUNIT_EXPECT_EQ(test, data->hdr.size, priv->resource.size);
+
+	data_len = data->hdr.size + sizeof(data->hdr);
+	hdr = data->hdr;
+
+	buf = kunit_kzalloc(test, data_len, GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, buf);
+
+	ret = i915_vfio_data_read(i915_vdev->fd, buf, data_len);
+	KUNIT_ASSERT_EQ(test, ret, data_len);
+
+	KUNIT_EXPECT_TRUE(test, list_empty(&i915_vdev->fd->save_data));
+
+	KUNIT_EXPECT_EQ(test, memcmp(buf, &hdr, sizeof(hdr)), 0);
+
+	KUNIT_EXPECT_EQ(test,
+			memcmp(buf + sizeof(hdr), priv->resource.vaddr, priv->resource.size),
+			0);
+}
+
+static void test_consume_res(struct kunit *test)
+{
+	struct i915_vfio_pci_data_test *priv = test->priv;
+	struct i915_vfio_pci_core_device *i915_vdev = priv->i915_vdev;
+	struct i915_vfio_pci_migration_data *data = &i915_vdev->fd->resume_data;
+	struct i915_vfio_pci_migration_header hdr = {
+		.type = I915_VFIO_DATA_GGTT,
+		.tile = 0,
+		.offset = 0,
+		.size = priv->resource.size,
+		.flags = 0,
+	};
+	void *buf;
+	size_t data_len, data_chunk;
+	ssize_t ret;
+
+	data_len = hdr.size + sizeof(hdr);
+	data_chunk = sizeof(hdr) + 16;
+
+	buf = kunit_kzalloc(test, data_len, GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, buf);
+
+	memcpy(buf, &hdr, sizeof(hdr));
+
+	KUNIT_EXPECT_FALSE(test, data->hdr_processed);
+	ret = i915_vfio_data_write(i915_vdev->fd, buf, data_chunk);
+	KUNIT_ASSERT_EQ(test, ret, data_chunk);
+	KUNIT_EXPECT_TRUE(test, data->hdr_processed);
+
+	KUNIT_EXPECT_EQ(test, memcmp(&hdr, &data->hdr, sizeof(hdr)), 0);
+
+	ret = i915_vfio_data_write(i915_vdev->fd, buf + ret, data_len - data_chunk);
+	KUNIT_ASSERT_EQ(test, ret, data_len - data_chunk);
+
+	KUNIT_EXPECT_EQ(test,
+			memcmp(buf + sizeof(hdr), priv->resource.vaddr, priv->resource.size),
+			0);
+}
+
+struct invalid_device_desc_test {
+	char *name;
+	int expected_err;
+	struct i915_vfio_data_device_desc desc;
+};
+
+static void test_invalid_device_desc(struct kunit *test)
+{
+	const struct invalid_device_desc_test *param = test->param_value;
+	struct i915_vfio_pci_data_test *priv = test->priv;
+	struct i915_vfio_pci_core_device *i915_vdev = priv->i915_vdev;
+	struct i915_vfio_pci_migration_header *hdr = &i915_vdev->fd->resume_data.hdr;
+	void *buf;
+	int ret;
+
+	hdr->type = I915_VFIO_DATA_DESC;
+	hdr->tile = 0;
+	hdr->offset = 0;
+	hdr->size = sizeof(param->desc);
+	hdr->flags = 0;
+
+	buf = kunit_kzalloc(test, sizeof(*hdr) + sizeof(param->desc), GFP_KERNEL);
+	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, buf);
+
+	ret = i915_vfio_consume_data(i915_vdev->fd, buf, sizeof(param->desc));
+	KUNIT_EXPECT_EQ(test, ret, param->expected_err);
+}
+
+static const struct invalid_device_desc_test invalid_device_desc_tests[] = {
+	{
+		.name = "bad magic",
+		.expected_err = -EINVAL,
+		.desc = {
+			.magic = 0xbad,
+		},
+	},
+	{
+		.name = "bad version",
+		.expected_err = -EINVAL,
+		.desc = {
+			.version = 0xbad,
+		},
+	},
+	{
+		.name = "bad vendor",
+		.expected_err = -EINVAL,
+		.desc = {
+			.vendor = 0xbad,
+		},
+	},
+	{
+		.name = "bad device",
+		.expected_err = -EINVAL,
+		.desc = {
+			.device = 0xbad,
+		},
+	},
+};
+
+static void invalid_device_desc_test_name(const struct invalid_device_desc_test *t,
+					  char *name)
+{
+	snprintf(name, KUNIT_PARAM_DESC_SIZE, "%s", t->name);
+}
+
+KUNIT_ARRAY_PARAM(invalid_device_desc, invalid_device_desc_tests, invalid_device_desc_test_name);
+
+static struct kunit_case i915_vfio_pci_data_tests[] = {
+	KUNIT_CASE(test_produce_consume_desc),
+	KUNIT_CASE(test_produce_res),
+	KUNIT_CASE(test_consume_res),
+	KUNIT_CASE_PARAM(test_invalid_device_desc, invalid_device_desc_gen_params),
+	{},
+};
+
+static struct kunit_suite i915_vfio_pci_data_test_suite = {
+	.name = "i915_vfio_pci_data",
+	.init = i915_vfio_pci_data_test_init,
+	.exit = i915_vfio_pci_data_test_exit,
+	.test_cases = i915_vfio_pci_data_tests
+};
+
+kunit_test_suite(i915_vfio_pci_data_test_suite);
diff --git a/include/drm/i915_sriov.h b/include/drm/i915_sriov.h
new file mode 100644
index 000000000000..0522ec0b2786
--- /dev/null
+++ b/include/drm/i915_sriov.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: MIT */
+
+#include <linux/pci.h>
+#include <linux/types.h>
+
+int i915_sriov_pause_vf(struct pci_dev *pdev, unsigned int vfid);
+int i915_sriov_resume_vf(struct pci_dev *pdev, unsigned int vfid);
+
+int i915_sriov_wait_vf_flr_done(struct pci_dev *pdev, unsigned int vfid);
+
+size_t
+i915_sriov_ggtt_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile);
+ssize_t i915_sriov_ggtt_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			     void *buf, size_t size);
+int
+i915_sriov_ggtt_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+		     const void *buf, size_t size);
+
+size_t
+i915_sriov_fw_state_size(struct pci_dev *pdev, unsigned int vfid,
+			 unsigned int tile);
+ssize_t
+i915_sriov_fw_state_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			 void *buf, size_t size);
+int
+i915_sriov_fw_state_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			 const void *buf, size_t size);
+
+size_t
+i915_sriov_lmem_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile);
+void *i915_sriov_lmem_map(struct pci_dev *pdev, unsigned int vfid, unsigned int tile);
+void
+i915_sriov_lmem_unmap(struct pci_dev *pdev, unsigned int vfid, unsigned int tile);
-- 
2.25.1

