From 3d45d8d4ece183ec177dc05c4a7532fc17dfde05 Mon Sep 17 00:00:00 2001
From: Junxiao Chang <junxiao.chang@intel.com>
Date: Fri, 13 May 2022 14:40:39 +0800
Subject: [PATCH 05/29] Revert "drm/i915/guc: Update to GuC version 70.0.3"

This reverts commit 2ca838ce3d7522a42242e289a45271b667e274dc.
---
 drivers/gpu/drm/i915/gt/intel_context.c       |   2 +-
 .../drm/i915/gt/iov/intel_iov_provisioning.c  |   8 +-
 .../gpu/drm/i915/gt/uc/abi/guc_actions_abi.h  |   8 +-
 drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h |  15 -
 drivers/gpu/drm/i915/gt/uc/intel_guc.h        |   7 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c     |   2 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h   |  71 +--
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c | 515 ++++++++----------
 drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c      |  30 +-
 drivers/gpu/drm/i915/gt/uc/selftest_guc.c     |   2 +-
 10 files changed, 286 insertions(+), 374 deletions(-)

diff --git a/drivers/gpu/drm/i915/gt/intel_context.c b/drivers/gpu/drm/i915/gt/intel_context.c
index 4e82f56c2e90..5634d14052bc 100644
--- a/drivers/gpu/drm/i915/gt/intel_context.c
+++ b/drivers/gpu/drm/i915/gt/intel_context.c
@@ -399,7 +399,7 @@ intel_context_init(struct intel_context *ce, struct intel_engine_cs *engine)
 	INIT_LIST_HEAD(&ce->guc_state.fences);
 	INIT_LIST_HEAD(&ce->guc_state.requests);
 
-	ce->guc_id.id = GUC_INVALID_CONTEXT_ID;
+	ce->guc_id.id = GUC_INVALID_LRC_ID;
 	INIT_LIST_HEAD(&ce->guc_id.link);
 
 	INIT_LIST_HEAD(&ce->destroyed_link);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
index f05eecf009e6..be36d11f1b67 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
@@ -671,15 +671,15 @@ static int pf_push_config_ctxs(struct intel_iov *iov, unsigned int id, u16 begin
  * To facilitate the implementation of dynamic context provisioning, we introduced
  * the concept of granularity of contexts. For this purpose, we divided all contexts
  * into packages with size CTXS_GRANULARITY. The exception is the first package, whose
- * size is CTXS_MODULO, because GUC_MAX_CONTEXT_ID is an odd number.
+ * size is CTXS_MODULO, because GUC_MAX_LRC_DESCRIPTORS is an odd number.
  */
 #define CTXS_GRANULARITY 128
-#define CTXS_MODULO (GUC_MAX_CONTEXT_ID % CTXS_GRANULARITY)
+#define CTXS_MODULO (GUC_MAX_LRC_DESCRIPTORS % CTXS_GRANULARITY)
 #define CTXS_DELTA (CTXS_GRANULARITY - CTXS_MODULO)
 
 static u16 ctxs_bitmap_total_bits(void)
 {
-	return ALIGN(GUC_MAX_CONTEXT_ID, CTXS_GRANULARITY) / CTXS_GRANULARITY;
+	return ALIGN(GUC_MAX_LRC_DESCRIPTORS, CTXS_GRANULARITY) / CTXS_GRANULARITY;
 }
 
 static u16 __encode_ctxs_count(u16 num_ctxs, bool first)
@@ -1465,7 +1465,7 @@ static void pf_assign_ctxs_for_pf(struct intel_iov *iov)
 	pf_ctxs = decode_pf_ctxs_count(pf_ctxs_bits);
 
 	IOV_DEBUG(iov, "config: %s %u = %u pf + %u available\n",
-		  "contexts", GUC_MAX_CONTEXT_ID, pf_ctxs, GUC_MAX_CONTEXT_ID - pf_ctxs);
+		  "contexts", GUC_MAX_LRC_DESCRIPTORS, pf_ctxs, GUC_MAX_LRC_DESCRIPTORS - pf_ctxs);
 
 	provisioning->configs[0].begin_ctx = 0;
 	provisioning->configs[0].num_ctxs = pf_ctxs;
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
index e0606b60cde4..a9c17b1057a5 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
@@ -122,19 +122,23 @@ enum intel_guc_action {
 	INTEL_GUC_ACTION_SCHED_CONTEXT_MODE_DONE = 0x1002,
 	INTEL_GUC_ACTION_SCHED_ENGINE_MODE_SET = 0x1003,
 	INTEL_GUC_ACTION_SCHED_ENGINE_MODE_DONE = 0x1004,
+	INTEL_GUC_ACTION_SET_CONTEXT_PRIORITY = 0x1005,
+	INTEL_GUC_ACTION_SET_CONTEXT_EXECUTION_QUANTUM = 0x1006,
+	INTEL_GUC_ACTION_SET_CONTEXT_PREEMPTION_TIMEOUT = 0x1007,
 	INTEL_GUC_ACTION_CONTEXT_RESET_NOTIFICATION = 0x1008,
 	INTEL_GUC_ACTION_ENGINE_FAILURE_NOTIFICATION = 0x1009,
-	INTEL_GUC_ACTION_HOST2GUC_UPDATE_CONTEXT_POLICIES = 0x100B,
 	INTEL_GUC_ACTION_SETUP_PC_GUCRC = 0x3004,
 	INTEL_GUC_ACTION_AUTHENTICATE_HUC = 0x4000,
 	INTEL_GUC_ACTION_GET_HWCONFIG = 0x4100,
 	INTEL_GUC_ACTION_REGISTER_CONTEXT = 0x4502,
 	INTEL_GUC_ACTION_DEREGISTER_CONTEXT = 0x4503,
+	INTEL_GUC_ACTION_REGISTER_COMMAND_TRANSPORT_BUFFER = 0x4505,
+	INTEL_GUC_ACTION_DEREGISTER_COMMAND_TRANSPORT_BUFFER = 0x4506,
 	INTEL_GUC_ACTION_DEREGISTER_CONTEXT_DONE = 0x4600,
 	INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC = 0x4601,
 	INTEL_GUC_ACTION_CLIENT_SOFT_RESET = 0x5507,
 	INTEL_GUC_ACTION_SET_ENG_UTIL_BUFF = 0x550A,
-	INTEL_GUC_ACTION_NOTIFY_MEMORY_CAT_ERROR = 0x6000,
+	INTEL_GUC_ACTION_REPORT_MEMORY_CAT_ERROR_CONTEXT = 0x6000,
 	INTEL_GUC_ACTION_PAGE_FAULT_NOTIFICATION = 0x6001,
 	INTEL_GUC_ACTION_TLB_INVALIDATION = 0x7000,
 	INTEL_GUC_ACTION_TLB_INVALIDATION_DONE = 0x7001,
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
index 641a9fed0779..4c96c39b0631 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
@@ -6,8 +6,6 @@
 #ifndef _ABI_GUC_KLVS_ABI_H
 #define _ABI_GUC_KLVS_ABI_H
 
-#include <linux/types.h>
-
 /**
  * DOC: GuC KLV
  *
@@ -83,19 +81,6 @@
 #define GUC_KLV_SELF_CFG_G2H_CTB_SIZE_KEY		0x0907
 #define GUC_KLV_SELF_CFG_G2H_CTB_SIZE_LEN		1u
 
-/*
- * Per context scheduling policy update keys.
- */
-enum  {
-	GUC_CONTEXT_POLICIES_KLV_ID_EXECUTION_QUANTUM			= 0x2001,
-	GUC_CONTEXT_POLICIES_KLV_ID_PREEMPTION_TIMEOUT			= 0x2002,
-	GUC_CONTEXT_POLICIES_KLV_ID_SCHEDULING_PRIORITY			= 0x2003,
-	GUC_CONTEXT_POLICIES_KLV_ID_PREEMPT_TO_IDLE_ON_QUANTUM_EXPIRY	= 0x2004,
-	GUC_CONTEXT_POLICIES_KLV_ID_SLPM_GT_FREQUENCY			= 0x2005,
-
-	GUC_CONTEXT_POLICIES_KLV_NUM_IDS = 5,
-};
-
 /**
  * DOC: GuC VGT Policy KLVs
  *
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
index d25fc1877efc..1f0c7b57b242 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
@@ -142,8 +142,6 @@ struct intel_guc {
 	bool submission_supported;
 	/** @submission_selected: tracks whether the user enabled GuC submission */
 	bool submission_selected;
-	/** @submission_initialized: tracks whether GuC submission has been initialised */
-	bool submission_initialized;
 	/**
 	 * @rc_supported: tracks whether we support GuC rc on the current platform
 	 */
@@ -162,6 +160,11 @@ struct intel_guc {
 	/** @ads_engine_usage_size: size of engine usage in the ADS */
 	u32 ads_engine_usage_size;
 
+	/** @lrc_desc_pool: object allocated to hold the GuC LRC descriptor pool */
+	struct i915_vma *lrc_desc_pool;
+	/** @lrc_desc_pool_vaddr: contents of the GuC LRC descriptor pool */
+	void *lrc_desc_pool_vaddr;
+
 	/**
 	 * @context_lookup: used to resolve intel_context from guc_id, if a
 	 * context is present in this structure it is registered with the GuC
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
index df37b6c112a1..de72fed5b2f2 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
@@ -1074,7 +1074,7 @@ static int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 	case INTEL_GUC_ACTION_ENGINE_FAILURE_NOTIFICATION:
 		ret = intel_guc_engine_failure_process_msg(guc, payload, len);
 		break;
-	case INTEL_GUC_ACTION_NOTIFY_MEMORY_CAT_ERROR:
+	case INTEL_GUC_ACTION_REPORT_MEMORY_CAT_ERROR_CONTEXT:
 		ret = intel_pagefault_process_cat_error_msg(guc, payload, len);
 		break;
 	case INTEL_GUC_ACTION_PAGE_FAULT_NOTIFICATION:
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
index 4903362e8608..f63df3af4616 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
@@ -55,8 +55,8 @@ static inline const char *hxg_type_to_string(u32 type)
 #define GUC_CLIENT_PRIORITY_NORMAL	3
 #define GUC_CLIENT_PRIORITY_NUM		4
 
-#define GUC_MAX_CONTEXT_ID		65535
-#define	GUC_INVALID_CONTEXT_ID		GUC_MAX_CONTEXT_ID
+#define GUC_MAX_LRC_DESCRIPTORS		65535
+#define	GUC_INVALID_LRC_ID		GUC_MAX_LRC_DESCRIPTORS
 
 #define GUC_RENDER_ENGINE		0
 #define GUC_VIDEO_ENGINE		1
@@ -219,45 +219,54 @@ struct guc_wq_item {
 	u32 fence_id;
 } __packed;
 
-struct guc_sched_wq_desc {
+struct guc_process_desc {
+	u32 stage_id;
+	u64 db_base_addr;
 	u32 head;
 	u32 tail;
 	u32 error_offset;
+	u64 wq_base_addr;
+	u32 wq_size_bytes;
 	u32 wq_status;
-	u32 reserved[28];
+	u32 engine_presence;
+	u32 priority;
+	u32 reserved[36];
 } __packed;
 
-/* Helper for context registration H2G */
-struct guc_ctxt_registration_info {
-	u32 flags;
-	u32 context_idx;
-	u32 engine_class;
-	u32 engine_submit_mask;
-	u32 wq_desc_lo;
-	u32 wq_desc_hi;
-	u32 wq_base_lo;
-	u32 wq_base_hi;
-	u32 wq_size;
-	u32 hwlrca_lo;
-	u32 hwlrca_hi;
-};
 #define CONTEXT_REGISTRATION_FLAG_KMD	BIT(0)
 
-/* 32-bit KLV structure as used by policy updates and others */
-struct guc_klv_generic_dw_t {
-	u32 kl;
-	u32 value;
-} __packed;
+#define CONTEXT_POLICY_DEFAULT_EXECUTION_QUANTUM_US 1000000
+#define CONTEXT_POLICY_DEFAULT_PREEMPTION_TIME_US 500000
 
-/* Format of the UPDATE_CONTEXT_POLICIES H2G data packet */
-struct guc_update_context_policy_header {
-	u32 action;
-	u32 ctx_id;
-} __packed;
+/* Preempt to idle on quantum expiry */
+#define CONTEXT_POLICY_FLAG_PREEMPT_TO_IDLE	BIT(0)
 
-struct guc_update_context_policy {
-	struct guc_update_context_policy_header header;
-	struct guc_klv_generic_dw_t klv[GUC_CONTEXT_POLICIES_KLV_NUM_IDS];
+/*
+ * GuC Context registration descriptor.
+ * FIXME: This is only required to exist during context registration.
+ * The current 1:1 between guc_lrc_desc and LRCs for the lifetime of the LRC
+ * is not required.
+ */
+struct guc_lrc_desc {
+	u32 hw_context_desc;
+	u32 slpm_perf_mode_hint;	/* SPLC v1 only */
+	u32 slpm_freq_hint;
+	u32 engine_submit_mask;		/* In logical space */
+	u8 engine_class;
+	u8 reserved0[3];
+	u32 priority;
+	u32 process_desc;
+	u32 wq_addr;
+	u32 wq_size;
+	u32 context_flags;		/* CONTEXT_REGISTRATION_* */
+	/* Time for one workload to execute. (in micro seconds) */
+	u32 execution_quantum;
+	/* Time to wait for a preemption request to complete before issuing a
+	 * reset. (in micro seconds).
+	 */
+	u32 preemption_timeout;
+	u32 policy_flags;		/* CONTEXT_POLICY_* */
+	u32 reserved1[19];
 } __packed;
 
 #define GUC_POWER_UNSPECIFIED	0
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index 3e0dd2858be6..c8c96bd8fbda 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -172,8 +172,11 @@ static inline void init_sched_state(struct intel_context *ce)
 __maybe_unused
 static bool sched_state_is_init(struct intel_context *ce)
 {
-	/* Kernel contexts can have SCHED_STATE_REGISTERED after suspend. */
-	return !(ce->guc_state.sched_state &
+	/*
+	 * XXX: Kernel contexts can have SCHED_STATE_NO_LOCK_REGISTERED after
+	 * suspend.
+	 */
+	return !(ce->guc_state.sched_state &=
 		 ~(SCHED_STATE_BLOCKED_MASK | SCHED_STATE_REGISTERED));
 }
 
@@ -349,12 +352,12 @@ request_to_scheduling_context(struct i915_request *rq)
 
 static inline bool context_guc_id_invalid(struct intel_context *ce)
 {
-	return ce->guc_id.id == GUC_INVALID_CONTEXT_ID;
+	return ce->guc_id.id == GUC_INVALID_LRC_ID;
 }
 
 static inline void set_context_guc_id_invalid(struct intel_context *ce)
 {
-	ce->guc_id.id = GUC_INVALID_CONTEXT_ID;
+	ce->guc_id.id = GUC_INVALID_LRC_ID;
 }
 
 static inline struct intel_guc *ce_to_guc(struct intel_context *ce)
@@ -393,12 +396,12 @@ struct sync_semaphore {
 };
 
 struct parent_scratch {
-	struct guc_sched_wq_desc wq_desc;
+	struct guc_process_desc pdesc;
 
 	struct sync_semaphore go;
 	struct sync_semaphore join[MAX_ENGINE_INSTANCE + 1];
 
-	u8 unused[WQ_OFFSET - sizeof(struct guc_sched_wq_desc) -
+	u8 unused[WQ_OFFSET - sizeof(struct guc_process_desc) -
 		sizeof(struct sync_semaphore) * (MAX_ENGINE_INSTANCE + 2)];
 
 	u32 wq[WQ_SIZE / sizeof(u32)];
@@ -435,15 +438,15 @@ __get_parent_scratch(struct intel_context *ce)
 		   LRC_STATE_OFFSET) / sizeof(u32)));
 }
 
-static struct guc_sched_wq_desc *
-__get_wq_desc(struct intel_context *ce)
+static struct guc_process_desc *
+__get_process_desc(struct intel_context *ce)
 {
 	struct parent_scratch *ps = __get_parent_scratch(ce);
 
-	return &ps->wq_desc;
+	return &ps->pdesc;
 }
 
-static u32 *get_wq_pointer(struct guc_sched_wq_desc *wq_desc,
+static u32 *get_wq_pointer(struct guc_process_desc *desc,
 			   struct intel_context *ce,
 			   u32 wqi_size)
 {
@@ -455,7 +458,7 @@ static u32 *get_wq_pointer(struct guc_sched_wq_desc *wq_desc,
 #define AVAILABLE_SPACE	\
 	CIRC_SPACE(ce->parallel.guc.wqi_tail, ce->parallel.guc.wqi_head, WQ_SIZE)
 	if (wqi_size > AVAILABLE_SPACE) {
-		ce->parallel.guc.wqi_head = READ_ONCE(wq_desc->head);
+		ce->parallel.guc.wqi_head = READ_ONCE(desc->head);
 
 		if (wqi_size > AVAILABLE_SPACE)
 			return NULL;
@@ -465,27 +468,75 @@ static u32 *get_wq_pointer(struct guc_sched_wq_desc *wq_desc,
 	return &__get_parent_scratch(ce)->wq[ce->parallel.guc.wqi_tail / sizeof(u32)];
 }
 
+static struct guc_lrc_desc *__get_lrc_desc(struct intel_guc *guc, u32 index)
+{
+	struct guc_lrc_desc *base = guc->lrc_desc_pool_vaddr;
+
+	GEM_BUG_ON(index >= GUC_MAX_LRC_DESCRIPTORS);
+
+	return &base[index];
+}
+
 static inline struct intel_context *__get_context(struct intel_guc *guc, u32 id)
 {
 	struct intel_context *ce = xa_load(&guc->context_lookup, id);
 
-	GEM_BUG_ON(id >= GUC_MAX_CONTEXT_ID);
+	GEM_BUG_ON(id >= GUC_MAX_LRC_DESCRIPTORS);
 
 	return ce;
 }
 
+static int guc_lrc_desc_pool_create(struct intel_guc *guc)
+{
+	u32 size;
+	int ret;
+
+	size = PAGE_ALIGN(sizeof(struct guc_lrc_desc) *
+			  GUC_MAX_LRC_DESCRIPTORS);
+	ret = intel_guc_allocate_and_map_vma(guc, size, &guc->lrc_desc_pool,
+					     (void **)&guc->lrc_desc_pool_vaddr);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void guc_lrc_desc_pool_destroy(struct intel_guc *guc)
+{
+	guc->lrc_desc_pool_vaddr = NULL;
+	i915_vma_unpin_and_release(&guc->lrc_desc_pool, I915_VMA_RELEASE_MAP);
+}
+
 static inline bool guc_submission_initialized(struct intel_guc *guc)
 {
-	return guc->submission_initialized;
+	return !!guc->lrc_desc_pool_vaddr;
 }
 
-static inline bool ctx_id_mapped(struct intel_guc *guc, u32 id)
+static inline void reset_lrc_desc(struct intel_guc *guc, u32 id)
+{
+	if (likely(guc_submission_initialized(guc))) {
+		struct guc_lrc_desc *desc = __get_lrc_desc(guc, id);
+		unsigned long flags;
+
+		memset(desc, 0, sizeof(*desc));
+
+		/*
+		 * xarray API doesn't have xa_erase_irqsave wrapper, so calling
+		 * the lower level functions directly.
+		 */
+		xa_lock_irqsave(&guc->context_lookup, flags);
+		__xa_erase(&guc->context_lookup, id);
+		xa_unlock_irqrestore(&guc->context_lookup, flags);
+	}
+}
+
+static inline bool lrc_desc_registered(struct intel_guc *guc, u32 id)
 {
 	return __get_context(guc, id);
 }
 
-static inline void set_ctx_id_mapping(struct intel_guc *guc, u32 id,
-				      struct intel_context *ce)
+static inline void set_lrc_desc_registered(struct intel_guc *guc, u32 id,
+					   struct intel_context *ce)
 {
 	unsigned long flags;
 
@@ -498,22 +549,6 @@ static inline void set_ctx_id_mapping(struct intel_guc *guc, u32 id,
 	xa_unlock_irqrestore(&guc->context_lookup, flags);
 }
 
-static inline void clr_ctx_id_mapping(struct intel_guc *guc, u32 id)
-{
-	unsigned long flags;
-
-	if (unlikely(!guc_submission_initialized(guc)))
-		return;
-
-	/*
-	 * xarray API doesn't have xa_erase_irqsave wrapper, so calling
-	 * the lower level functions directly.
-	 */
-	xa_lock_irqsave(&guc->context_lookup, flags);
-	__xa_erase(&guc->context_lookup, id);
-	xa_unlock_irqrestore(&guc->context_lookup, flags);
-}
-
 static void decr_outstanding_submission_g2h(struct intel_guc *guc)
 {
 	if (atomic_dec_and_test(&guc->outstanding_submission_g2h))
@@ -590,7 +625,7 @@ int intel_guc_wait_for_idle(struct intel_guc *guc, long timeout)
 					      true, timeout);
 }
 
-static int try_context_registration(struct intel_context *ce, bool loop);
+static int guc_lrc_desc_pin(struct intel_context *ce, bool loop);
 
 static int __guc_add_request(struct intel_guc *guc, struct i915_request *rq)
 {
@@ -709,7 +744,7 @@ static u32 wq_space_until_wrap(struct intel_context *ce)
 	return (WQ_SIZE - ce->parallel.guc.wqi_tail);
 }
 
-static void write_wqi(struct guc_sched_wq_desc *wq_desc,
+static void write_wqi(struct guc_process_desc *desc,
 		      struct intel_context *ce,
 		      u32 wqi_size)
 {
@@ -722,13 +757,13 @@ static void write_wqi(struct guc_sched_wq_desc *wq_desc,
 
 	ce->parallel.guc.wqi_tail = (ce->parallel.guc.wqi_tail + wqi_size) &
 		(WQ_SIZE - 1);
-	WRITE_ONCE(wq_desc->tail, ce->parallel.guc.wqi_tail);
+	WRITE_ONCE(desc->tail, ce->parallel.guc.wqi_tail);
 }
 
 static int guc_wq_noop_append(struct intel_context *ce)
 {
-	struct guc_sched_wq_desc *wq_desc = __get_wq_desc(ce);
-	u32 *wqi = get_wq_pointer(wq_desc, ce, wq_space_until_wrap(ce));
+	struct guc_process_desc *desc = __get_process_desc(ce);
+	u32 *wqi = get_wq_pointer(desc, ce, wq_space_until_wrap(ce));
 	u32 len_dw = wq_space_until_wrap(ce) / sizeof(u32) - 1;
 
 	if (!wqi)
@@ -747,7 +782,7 @@ static int __guc_wq_item_append(struct i915_request *rq)
 {
 	struct intel_context *ce = request_to_scheduling_context(rq);
 	struct intel_context *child;
-	struct guc_sched_wq_desc *wq_desc = __get_wq_desc(ce);
+	struct guc_process_desc *desc = __get_process_desc(ce);
 	unsigned int wqi_size = (ce->parallel.number_children + 4) *
 		sizeof(u32);
 	u32 *wqi;
@@ -758,7 +793,7 @@ static int __guc_wq_item_append(struct i915_request *rq)
 	GEM_BUG_ON(!atomic_read(&ce->guc_id.ref));
 	GEM_BUG_ON(context_guc_id_invalid(ce));
 	GEM_BUG_ON(context_wait_for_deregister_to_register(ce));
-	GEM_BUG_ON(!ctx_id_mapped(ce_to_guc(ce), ce->guc_id.id));
+	GEM_BUG_ON(!lrc_desc_registered(ce_to_guc(ce), ce->guc_id.id));
 
 	/* Insert NOOP if this work queue item will wrap the tail pointer. */
 	if (wqi_size > wq_space_until_wrap(ce)) {
@@ -767,7 +802,7 @@ static int __guc_wq_item_append(struct i915_request *rq)
 			return ret;
 	}
 
-	wqi = get_wq_pointer(wq_desc, ce, wqi_size);
+	wqi = get_wq_pointer(desc, ce, wqi_size);
 	if (!wqi)
 		return -EBUSY;
 
@@ -782,7 +817,7 @@ static int __guc_wq_item_append(struct i915_request *rq)
 	for_each_child(ce, child)
 		*wqi++ = child->ring->tail / sizeof(u64);
 
-	write_wqi(wq_desc, ce, wqi_size);
+	write_wqi(desc, ce, wqi_size);
 
 	return 0;
 }
@@ -886,9 +921,9 @@ static int guc_dequeue_one_context(struct intel_guc *guc)
 	if (submit) {
 		struct intel_context *ce = request_to_scheduling_context(last);
 
-		if (unlikely(!ctx_id_mapped(guc, ce->guc_id.id) &&
+		if (unlikely(!lrc_desc_registered(guc, ce->guc_id.id) &&
 			     !intel_context_is_banned(ce))) {
-			ret = try_context_registration(ce, false);
+			ret = guc_lrc_desc_pin(ce, false);
 			if (unlikely(ret == -EPIPE)) {
 				goto deadlk;
 			} else if (ret == -EBUSY) {
@@ -1727,10 +1762,20 @@ static void destroyed_worker_func(struct work_struct *w);
 int intel_guc_submission_init(struct intel_guc *guc)
 {
 	struct intel_gt *gt = guc_to_gt(guc);
+	int ret;
 
-	if (guc->submission_initialized)
+	if (guc->lrc_desc_pool)
 		return 0;
 
+	ret = guc_lrc_desc_pool_create(guc);
+	if (ret)
+		return ret;
+	/*
+	 * Keep static analysers happy, let them know that we allocated the
+	 * vma after testing that it didn't exist earlier.
+	 */
+	GEM_BUG_ON(!guc->lrc_desc_pool);
+
 	xa_init_flags(&guc->context_lookup, XA_FLAGS_LOCK_IRQ);
 	xa_init_flags(&guc->tlb_lookup, XA_FLAGS_ALLOC);
 
@@ -1749,21 +1794,20 @@ int intel_guc_submission_init(struct intel_guc *guc)
 	spin_lock_init(&guc->timestamp.lock);
 	INIT_DELAYED_WORK(&guc->timestamp.work, guc_timestamp_ping);
 	guc->timestamp.ping_delay = (POLL_TIME_CLKS / gt->clock_frequency + 1) * HZ;
-	guc->submission_initialized = true;
 
 	return 0;
 }
 
 void intel_guc_submission_fini(struct intel_guc *guc)
 {
-	if (!guc->submission_initialized)
+	if (!guc->lrc_desc_pool)
 		return;
 
 	guc_flush_destroyed_contexts(guc);
+	guc_lrc_desc_pool_destroy(guc);
 	i915_sched_engine_put(guc->sched_engine);
 	bitmap_free(guc->submission_state.guc_ids_bitmap);
 	xa_destroy(&guc->tlb_lookup);
-	guc->submission_initialized = false;
 }
 
 static inline void queue_request(struct i915_sched_engine *sched_engine,
@@ -1810,7 +1854,7 @@ static bool need_tasklet(struct intel_guc *guc, struct i915_request *rq)
 
 	return submission_disabled(guc) || guc->stalled_request ||
 		!i915_sched_engine_is_empty(sched_engine) ||
-		!ctx_id_mapped(guc, ce->guc_id.id);
+		!lrc_desc_registered(guc, ce->guc_id.id);
 }
 
 static void guc_submit_request(struct i915_request *rq)
@@ -1832,7 +1876,7 @@ static void guc_submit_request(struct i915_request *rq)
 
 int intel_guc_submission_limit_ids(struct intel_guc *guc, u32 limit)
 {
-	if (limit > GUC_MAX_CONTEXT_ID)
+	if (limit > GUC_MAX_LRC_DESCRIPTORS)
 		return -E2BIG;
 
 	if (!ida_is_empty(&guc->submission_state.guc_ids))
@@ -1879,7 +1923,7 @@ static void __release_guc_id(struct intel_guc *guc, struct intel_context *ce)
 		else
 			ida_simple_remove(&guc->submission_state.guc_ids,
 					  ce->guc_id.id);
-		clr_ctx_id_mapping(guc, ce->guc_id.id);
+		reset_lrc_desc(guc, ce->guc_id.id);
 		set_context_guc_id_invalid(ce);
 	}
 	if (!list_empty(&ce->guc_id.link))
@@ -2032,94 +2076,65 @@ static void unpin_guc_id(struct intel_guc *guc, struct intel_context *ce)
 
 static int __guc_action_register_multi_lrc(struct intel_guc *guc,
 					   struct intel_context *ce,
-					   struct guc_ctxt_registration_info *info,
+					   u32 guc_id,
+					   u32 offset,
 					   bool loop)
 {
 	struct intel_context *child;
-	u32 action[11 + (MAX_ENGINE_INSTANCE * 2)];
+	u32 action[4 + MAX_ENGINE_INSTANCE];
 	int len = 0;
-	u32 next_id;
 
 	GEM_BUG_ON(ce->parallel.number_children > MAX_ENGINE_INSTANCE);
 
 	action[len++] = INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC;
-	action[len++] = info->flags;
-	action[len++] = info->context_idx;
-	action[len++] = info->engine_class;
-	action[len++] = info->engine_submit_mask;
-	action[len++] = info->wq_desc_lo;
-	action[len++] = info->wq_desc_hi;
-	action[len++] = info->wq_base_lo;
-	action[len++] = info->wq_base_hi;
-	action[len++] = info->wq_size;
+	action[len++] = guc_id;
 	action[len++] = ce->parallel.number_children + 1;
-	action[len++] = info->hwlrca_lo;
-	action[len++] = info->hwlrca_hi;
-
-	next_id = info->context_idx + 1;
+	action[len++] = offset;
 	for_each_child(ce, child) {
-		GEM_BUG_ON(next_id++ != child->guc_id.id);
-
-		/* Interface support 64 bit LRCA even though i915 only supports 32 bit currently */
-		action[len++] = lower_32_bits(child->lrc.lrca);
-		action[len++] = upper_32_bits(child->lrc.lrca);
+		offset += sizeof(struct guc_lrc_desc);
+		action[len++] = offset;
 	}
 
-	GEM_BUG_ON(len >= ARRAY_SIZE(action));
-
 	return guc_submission_send_busy_loop(guc, action, len, 0, loop);
 }
 
 static int __guc_action_register_context(struct intel_guc *guc,
-					 struct guc_ctxt_registration_info *info,
+					 u32 guc_id,
+					 u32 offset,
 					 bool loop)
 {
 	u32 action[] = {
 		INTEL_GUC_ACTION_REGISTER_CONTEXT,
-		info->flags,
-		info->context_idx,
-		info->engine_class,
-		info->engine_submit_mask,
-		info->wq_desc_lo,
-		info->wq_desc_hi,
-		info->wq_base_lo,
-		info->wq_base_hi,
-		info->wq_size,
-		info->hwlrca_lo,
-		info->hwlrca_hi,
+		guc_id,
+		offset,
 	};
 
 	return guc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),
 					     0, loop);
 }
 
-static void prepare_context_registration_info(struct intel_context *ce,
-					      struct guc_ctxt_registration_info *info);
-static int guc_context_policy_init(struct intel_context *ce, bool loop);
-
 static int register_context(struct intel_context *ce, bool loop)
 {
-	struct guc_ctxt_registration_info info;
 	struct intel_guc *guc = ce_to_guc(ce);
+	u32 offset = intel_guc_ggtt_offset(guc, guc->lrc_desc_pool) +
+		ce->guc_id.id * sizeof(struct guc_lrc_desc);
 	int ret;
 
 	GEM_BUG_ON(intel_context_is_child(ce));
 	trace_intel_context_register(ce);
 
-	prepare_context_registration_info(ce, &info);
-
 	if (intel_context_is_parent(ce))
-		ret = __guc_action_register_multi_lrc(guc, ce, &info, loop);
+		ret = __guc_action_register_multi_lrc(guc, ce, ce->guc_id.id,
+						      offset, loop);
 	else
-		ret = __guc_action_register_context(guc, &info, loop);
+		ret = __guc_action_register_context(guc, ce->guc_id.id, offset,
+						    loop);
 	if (likely(!ret)) {
 		unsigned long flags;
 
 		spin_lock_irqsave(&ce->guc_state.lock, flags);
 		set_context_registered(ce);
 		spin_unlock_irqrestore(&ce->guc_state.lock, flags);
-
-		guc_context_policy_init(ce, loop);
 	}
 
 	return ret;
@@ -2169,119 +2184,30 @@ static inline u32 get_children_join_value(struct intel_context *ce,
 	return __get_parent_scratch(ce)->join[child_index].semaphore;
 }
 
-struct context_policy {
-	u32 count;
-	struct guc_update_context_policy h2g;
-};
-
-static u32 __guc_context_policy_action_size(struct context_policy *policy)
-{
-	size_t bytes = sizeof(policy->h2g.header) +
-		       (sizeof(policy->h2g.klv[0]) * policy->count);
-
-	return bytes / sizeof(u32);
-}
-
-static void __guc_context_policy_start_klv(struct context_policy *policy, u16 guc_id)
-{
-	policy->h2g.header.action = INTEL_GUC_ACTION_HOST2GUC_UPDATE_CONTEXT_POLICIES;
-	policy->h2g.header.ctx_id = guc_id;
-	policy->count = 0;
-}
-
-#define MAKE_CONTEXT_POLICY_ADD(func, id) \
-static void __guc_context_policy_add_##func(struct context_policy *policy, u32 data) \
-{ \
-	GEM_BUG_ON(policy->count >= GUC_CONTEXT_POLICIES_KLV_NUM_IDS); \
- \
-	policy->h2g.klv[policy->count].kl = FIELD_PREP(GUC_KLV_0_KEY, GUC_CONTEXT_POLICIES_KLV_ID_##id) | \
-					       FIELD_PREP(GUC_KLV_0_LEN, 1); \
-	policy->h2g.klv[policy->count].value = data; \
-	policy->count++; \
-}
-
-MAKE_CONTEXT_POLICY_ADD(execution_quantum, EXECUTION_QUANTUM)
-MAKE_CONTEXT_POLICY_ADD(preemption_timeout, PREEMPTION_TIMEOUT)
-MAKE_CONTEXT_POLICY_ADD(priority, SCHEDULING_PRIORITY)
-MAKE_CONTEXT_POLICY_ADD(preempt_to_idle, PREEMPT_TO_IDLE_ON_QUANTUM_EXPIRY)
-
-#undef MAKE_CONTEXT_POLICY_ADD
-
-static int __guc_context_set_context_policies(struct intel_guc *guc,
-					      struct context_policy *policy,
-					      bool loop)
-{
-	return guc_submission_send_busy_loop(guc, (u32 *) &policy->h2g,
-					__guc_context_policy_action_size(policy),
-					0, loop);
-}
-
-static int guc_context_policy_init(struct intel_context *ce, bool loop)
+static void guc_context_policy_init(struct intel_engine_cs *engine,
+				    struct guc_lrc_desc *desc)
 {
-	struct intel_engine_cs *engine = ce->engine;
-	struct intel_guc *guc = &engine->gt->uc.guc;
-	struct context_policy policy;
-	u32 execution_quantum;
-	u32 preemption_timeout;
-	int ret;
-
-	/* NB: For both of these, zero means disabled. */
-	execution_quantum = engine->props.timeslice_duration_ms * 1000;
-	preemption_timeout = engine->props.preempt_timeout_ms * 1000;
-
-	__guc_context_policy_start_klv(&policy, ce->guc_id.id);
-
-	__guc_context_policy_add_priority(&policy, ce->guc_state.prio);
-	__guc_context_policy_add_execution_quantum(&policy, execution_quantum);
-	__guc_context_policy_add_preemption_timeout(&policy, preemption_timeout);
+	desc->policy_flags = 0;
 
 	if (engine->flags & I915_ENGINE_WANT_FORCED_PREEMPTION)
-		__guc_context_policy_add_preempt_to_idle(&policy, 1);
-
-	ret = __guc_context_set_context_policies(guc, &policy, loop);
-	if (ret) {
-		/*
-		 * FIXME: Need to unwind and retry later. Unfortunately, the
-		 * rewind almost certainly can't be done because that means
-		 * sending a context deregister H2G and presumably the reason
-		 * the policy update failed is because the H2G CTB is blocked.
-		 * So just flag an error for now!
-		 */
-		drm_err(&engine->i915->drm, "Failed to send policy update for 0x%04X: %d!\n", ce->guc_id.id, ret);
-	}
-
-	if (intel_context_is_parent(ce)) {
-		struct intel_context *child;
-
-		for_each_child(ce, child) {
-			__guc_context_policy_start_klv(&policy, child->guc_id.id);
-
-			if (engine->flags & I915_ENGINE_WANT_FORCED_PREEMPTION)
-				__guc_context_policy_add_preempt_to_idle(&policy, 1);
+		desc->policy_flags |= CONTEXT_POLICY_FLAG_PREEMPT_TO_IDLE;
 
-			child->guc_state.prio = ce->guc_state.prio;
-			__guc_context_policy_add_priority(&policy, ce->guc_state.prio);
-			__guc_context_policy_add_execution_quantum(&policy, execution_quantum);
-			__guc_context_policy_add_preemption_timeout(&policy, preemption_timeout);
-
-			ret = __guc_context_set_context_policies(guc, &policy, loop);
-			if (ret) {
-				/* FIXME: As above. */
-				drm_err(&engine->i915->drm, "Failed to send policy update for 0x%04X of 0x%04X: %d!\n",
-					child->guc_id.id, ce->guc_id.id, ret);
-			}
-		}
-	}
-
-	return ret;
+	/* NB: For both of these, zero means disabled. */
+	desc->execution_quantum = engine->props.timeslice_duration_ms * 1000;
+	desc->preemption_timeout = engine->props.preempt_timeout_ms * 1000;
 }
 
-static void prepare_context_registration_info(struct intel_context *ce,
-					      struct guc_ctxt_registration_info *info)
+static int guc_lrc_desc_pin(struct intel_context *ce, bool loop)
 {
 	struct intel_engine_cs *engine = ce->engine;
+	struct intel_runtime_pm *runtime_pm = engine->uncore->rpm;
 	struct intel_guc *guc = &engine->gt->uc.guc;
-	u32 ctx_id = ce->guc_id.id;
+	u32 desc_idx = ce->guc_id.id;
+	struct guc_lrc_desc *desc;
+	bool context_registered;
+	intel_wakeref_t wakeref;
+	struct intel_context *child;
+	int ret = 0;
 
 	GEM_BUG_ON(!engine->mask);
 	GEM_BUG_ON(!sched_state_is_init(ce));
@@ -2293,66 +2219,57 @@ static void prepare_context_registration_info(struct intel_context *ce,
 	GEM_BUG_ON(i915_gem_object_is_lmem(guc->ct.vma->obj) !=
 		   i915_gem_object_is_lmem(ce->ring->vma->obj));
 
-	clr_ctx_id_mapping(guc, ctx_id);
-	set_ctx_id_mapping(guc, ctx_id, ce);
+	context_registered = lrc_desc_registered(guc, desc_idx);
+
+	reset_lrc_desc(guc, desc_idx);
+	set_lrc_desc_registered(guc, desc_idx, ce);
 
-	memset(info, 0, sizeof(*info));
-	info->context_idx = ctx_id;
-	info->engine_class = engine_class_to_guc_class(engine->class);
-	info->engine_submit_mask = engine->logical_mask;
-	/* Interface support 64 bit LRCA even though i915 only supports 32 bit currently */
-	info->hwlrca_lo = lower_32_bits(ce->lrc.lrca);
-	info->hwlrca_hi = upper_32_bits(ce->lrc.lrca);
+	desc = __get_lrc_desc(guc, desc_idx);
+	desc->engine_class = engine_class_to_guc_class(engine->class);
+	desc->engine_submit_mask = engine->logical_mask;
+	desc->hw_context_desc = ce->lrc.lrca;
+	desc->priority = ce->guc_state.prio;
 	if (engine->flags & I915_ENGINE_HAS_EU_PRIORITY)
-		info->hwlrca_lo |= lrc_desc_priority(ce->guc_state.prio);
-	info->flags = CONTEXT_REGISTRATION_FLAG_KMD;
+		desc->hw_context_desc |= lrc_desc_priority(ce->guc_state.prio);
+	desc->context_flags = CONTEXT_REGISTRATION_FLAG_KMD;
+	guc_context_policy_init(engine, desc);
 
 	/*
 	 * If context is a parent, we need to register a process descriptor
 	 * describing a work queue and register all child contexts.
 	 */
 	if (intel_context_is_parent(ce)) {
-		struct guc_sched_wq_desc *wq_desc;
-		u64 wq_desc_offset, wq_base_offset;
+		struct guc_process_desc *pdesc;
 
 		ce->parallel.guc.wqi_tail = 0;
 		ce->parallel.guc.wqi_head = 0;
 
-		wq_desc_offset = i915_ggtt_offset(ce->state) +
-				 __get_parent_scratch_offset(ce);
-		wq_base_offset = i915_ggtt_offset(ce->state) +
-				 __get_wq_offset(ce);
-		info->wq_desc_lo = lower_32_bits(wq_desc_offset);
-		info->wq_desc_hi = upper_32_bits(wq_desc_offset);
-		info->wq_base_lo = lower_32_bits(wq_base_offset);
-		info->wq_base_hi = upper_32_bits(wq_base_offset);
-		info->wq_size = WQ_SIZE;
+		desc->process_desc = i915_ggtt_offset(ce->state) +
+			__get_parent_scratch_offset(ce);
+		desc->wq_addr = i915_ggtt_offset(ce->state) +
+			__get_wq_offset(ce);
+		desc->wq_size = WQ_SIZE;
 
-		wq_desc = __get_wq_desc(ce);
-		memset(wq_desc, 0, sizeof(*wq_desc));
-		wq_desc->wq_status = WQ_STATUS_ACTIVE;
+		pdesc = __get_process_desc(ce);
+		memset(pdesc, 0, sizeof(*(pdesc)));
+		pdesc->stage_id = ce->guc_id.id;
+		pdesc->wq_base_addr = desc->wq_addr;
+		pdesc->wq_size_bytes = desc->wq_size;
+		pdesc->wq_status = WQ_STATUS_ACTIVE;
+
+		for_each_child(ce, child) {
+			desc = __get_lrc_desc(guc, child->guc_id.id);
+
+			desc->engine_class =
+				engine_class_to_guc_class(engine->class);
+			desc->hw_context_desc = child->lrc.lrca;
+			desc->priority = ce->guc_state.prio;
+			desc->context_flags = CONTEXT_REGISTRATION_FLAG_KMD;
+			guc_context_policy_init(engine, desc);
+		}
 
 		clear_children_join_go_memory(ce);
 	}
-}
-
-static int try_context_registration(struct intel_context *ce, bool loop)
-{
-	struct intel_engine_cs *engine = ce->engine;
-	struct intel_runtime_pm *runtime_pm = engine->uncore->rpm;
-	struct intel_guc *guc = &engine->gt->uc.guc;
-	intel_wakeref_t wakeref;
-	u32 ctx_id = ce->guc_id.id;
-	bool context_registered;
-	int ret = 0;
-
-	GEM_BUG_ON(!sched_state_is_init(ce));
-
-	context_registered = ctx_id_mapped(guc, ctx_id);
-
-	if (context_registered)
-		clr_ctx_id_mapping(guc, ctx_id);
-	set_ctx_id_mapping(guc, ctx_id, ce);
 
 	/*
 	 * The context_lookup xarray is used to determine if the hardware
@@ -2378,7 +2295,7 @@ static int try_context_registration(struct intel_context *ce, bool loop)
 		}
 		spin_unlock_irqrestore(&ce->guc_state.lock, flags);
 		if (unlikely(disabled)) {
-			clr_ctx_id_mapping(guc, ctx_id);
+			reset_lrc_desc(guc, desc_idx);
 			return 0;	/* Will get registered later */
 		}
 
@@ -2394,9 +2311,9 @@ static int try_context_registration(struct intel_context *ce, bool loop)
 		with_intel_runtime_pm(runtime_pm, wakeref)
 			ret = register_context(ce, loop);
 		if (unlikely(ret == -EBUSY)) {
-			clr_ctx_id_mapping(guc, ctx_id);
+			reset_lrc_desc(guc, desc_idx);
 		} else if (unlikely(ret == -ENODEV)) {
-			clr_ctx_id_mapping(guc, ctx_id);
+			reset_lrc_desc(guc, desc_idx);
 			ret = 0;	/* Will get registered later */
 		}
 	}
@@ -2486,7 +2403,7 @@ static void __guc_context_sched_disable(struct intel_guc *guc,
 		GUC_CONTEXT_DISABLE
 	};
 
-	GEM_BUG_ON(guc_id == GUC_INVALID_CONTEXT_ID);
+	GEM_BUG_ON(guc_id == GUC_INVALID_LRC_ID);
 
 	GEM_BUG_ON(intel_context_is_child(ce));
 	trace_intel_context_sched_disable(ce);
@@ -2583,7 +2500,7 @@ static bool context_cant_unblock(struct intel_context *ce)
 
 	return (ce->guc_state.sched_state & SCHED_STATE_NO_UNBLOCK) ||
 		context_guc_id_invalid(ce) ||
-		!ctx_id_mapped(ce_to_guc(ce), ce->guc_id.id) ||
+		!lrc_desc_registered(ce_to_guc(ce), ce->guc_id.id) ||
 		!intel_context_is_pinned(ce);
 }
 
@@ -2653,11 +2570,13 @@ static void __guc_context_set_preemption_timeout(struct intel_guc *guc,
 						 u16 guc_id,
 						 u32 preemption_timeout)
 {
-	struct context_policy policy;
+	u32 action[] = {
+		INTEL_GUC_ACTION_SET_CONTEXT_PREEMPTION_TIMEOUT,
+		guc_id,
+		preemption_timeout
+	};
 
-	__guc_context_policy_start_klv(&policy, guc_id);
-	__guc_context_policy_add_preemption_timeout(&policy, preemption_timeout);
-	__guc_context_set_context_policies(guc, &policy, true);
+	intel_guc_send_busy_loop(guc, action, ARRAY_SIZE(action), 0, true);
 }
 
 static void guc_context_ban(struct intel_context *ce, struct i915_request *rq)
@@ -2757,7 +2676,7 @@ static inline void guc_lrc_desc_unpin(struct intel_context *ce)
 	bool disabled;
 
 	GEM_BUG_ON(!intel_gt_pm_is_awake(gt));
-	GEM_BUG_ON(!ctx_id_mapped(guc, ce->guc_id.id));
+	GEM_BUG_ON(!lrc_desc_registered(guc, ce->guc_id.id));
 	GEM_BUG_ON(ce != __get_context(guc, ce->guc_id.id));
 	GEM_BUG_ON(context_enabled(ce));
 
@@ -2874,7 +2793,7 @@ static void guc_context_destroy(struct kref *kref)
 	 */
 	spin_lock_irqsave(&guc->submission_state.lock, flags);
 	destroy = submission_disabled(guc) || context_guc_id_invalid(ce) ||
-		!ctx_id_mapped(guc, ce->guc_id.id);
+		!lrc_desc_registered(guc, ce->guc_id.id);
 	if (likely(!destroy)) {
 		if (!list_empty(&ce->guc_id.link))
 			list_del_init(&ce->guc_id.link);
@@ -2902,20 +2821,16 @@ static int guc_context_alloc(struct intel_context *ce)
 	return lrc_alloc(ce, ce->engine);
 }
 
-static void __guc_context_set_prio(struct intel_guc *guc,
-				   struct intel_context *ce)
-{
-	struct context_policy policy;
-
-	__guc_context_policy_start_klv(&policy, ce->guc_id.id);
-	__guc_context_policy_add_priority(&policy, ce->guc_state.prio);
-	__guc_context_set_context_policies(guc, &policy, true);
-}
-
 static void guc_context_set_prio(struct intel_guc *guc,
 				 struct intel_context *ce,
 				 u8 prio)
 {
+	u32 action[] = {
+		INTEL_GUC_ACTION_SET_CONTEXT_PRIORITY,
+		ce->guc_id.id,
+		prio,
+	};
+
 	GEM_BUG_ON(prio < GUC_CLIENT_PRIORITY_KMD_HIGH ||
 		   prio > GUC_CLIENT_PRIORITY_NORMAL);
 	lockdep_assert_held(&ce->guc_state.lock);
@@ -2926,9 +2841,9 @@ static void guc_context_set_prio(struct intel_guc *guc,
 		return;
 	}
 
-	ce->guc_state.prio = prio;
-	__guc_context_set_prio(guc, ce);
+	guc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action), 0, true);
 
+	ce->guc_state.prio = prio;
 	trace_intel_context_set_prio(ce);
 }
 
@@ -3121,7 +3036,7 @@ static void guc_signal_context_fence(struct intel_context *ce)
 static bool context_needs_register(struct intel_context *ce, bool new_guc_id)
 {
 	return (new_guc_id || test_bit(CONTEXT_LRCA_DIRTY, &ce->flags) ||
-		!ctx_id_mapped(ce_to_guc(ce), ce->guc_id.id)) &&
+		!lrc_desc_registered(ce_to_guc(ce), ce->guc_id.id)) &&
 		!submission_disabled(ce_to_guc(ce));
 }
 
@@ -3198,7 +3113,7 @@ static int guc_request_alloc(struct i915_request *rq)
 	if (unlikely(ret < 0))
 		return ret;
 	if (context_needs_register(ce, !!ret)) {
-		ret = try_context_registration(ce, true);
+		ret = guc_lrc_desc_pin(ce, true);
 		if (unlikely(ret)) {	/* unwind */
 			if (ret == -EPIPE) {
 				disable_submission(guc);
@@ -3707,17 +3622,9 @@ static void guc_set_default_submission(struct intel_engine_cs *engine)
 static inline void guc_kernel_context_pin(struct intel_guc *guc,
 					  struct intel_context *ce)
 {
-	/*
-	 * Note: we purposefully do not check the returns below because
-	 * the registration can only fail if a reset is just starting.
-	 * This is called at the end of reset so presumably another reset
-	 * isn't happening and even it did this code would be run again.
-	 */
-
 	if (context_guc_id_invalid(ce))
 		pin_guc_id(guc, ce);
-
-	try_context_registration(ce, true);
+	guc_lrc_desc_pin(ce, true);
 }
 
 static inline void guc_init_lrc_mapping(struct intel_guc *guc)
@@ -3735,7 +3642,13 @@ static inline void guc_init_lrc_mapping(struct intel_guc *guc)
 	 * Also, after a reset the of the GuC we want to make sure that the
 	 * information shared with GuC is properly reset. The kernel LRCs are
 	 * not attached to the gem_context, so they need to be added separately.
+	 *
+	 * Note: we purposefully do not check the return of guc_lrc_desc_pin,
+	 * because that function can only fail if a reset is just starting. This
+	 * is at the end of reset so presumably another reset isn't happening
+	 * and even it did this code would be run again.
 	 */
+
 	for_each_engine(engine, gt, id) {
 		struct intel_context *ce;
 
@@ -3926,32 +3839,32 @@ static bool __guc_submission_selected(struct intel_guc *guc)
 
 void intel_guc_submission_init_early(struct intel_guc *guc)
 {
-	guc->submission_state.num_guc_ids = GUC_MAX_CONTEXT_ID;
+	guc->submission_state.num_guc_ids = GUC_MAX_LRC_DESCRIPTORS;
 	guc->submission_supported = __guc_submission_supported(guc);
 	guc->submission_selected = __guc_submission_selected(guc);
 }
 
 static inline struct intel_context *
-g2h_context_lookup(struct intel_guc *guc, u32 ctx_id)
+g2h_context_lookup(struct intel_guc *guc, u32 desc_idx)
 {
 	struct intel_context *ce;
 
-	if (unlikely(ctx_id >= GUC_MAX_CONTEXT_ID)) {
+	if (unlikely(desc_idx >= GUC_MAX_LRC_DESCRIPTORS)) {
 		drm_err(&guc_to_gt(guc)->i915->drm,
-			"Invalid ctx_id %u\n", ctx_id);
+			"Invalid desc_idx %u", desc_idx);
 		return NULL;
 	}
 
-	ce = __get_context(guc, ctx_id);
+	ce = __get_context(guc, desc_idx);
 	if (unlikely(!ce)) {
 		drm_err(&guc_to_gt(guc)->i915->drm,
-			"Context is NULL, ctx_id %u\n", ctx_id);
+			"Context is NULL, desc_idx %u", desc_idx);
 		return NULL;
 	}
 
 	if (unlikely(intel_context_is_child(ce))) {
 		drm_err(&guc_to_gt(guc)->i915->drm,
-			"Context is child, ctx_id %u\n", ctx_id);
+			"Context is child, desc_idx %u", desc_idx);
 		return NULL;
 	}
 
@@ -3989,15 +3902,14 @@ int intel_guc_deregister_done_process_msg(struct intel_guc *guc,
 					  u32 len)
 {
 	struct intel_context *ce;
-	u32 ctx_id;
+	u32 desc_idx = msg[0];
 
 	if (unlikely(len < 1)) {
-		drm_err(&guc_to_gt(guc)->i915->drm, "Invalid length %u\n", len);
+		drm_err(&guc_to_gt(guc)->i915->drm, "Invalid length %u", len);
 		return -EPROTO;
 	}
-	ctx_id = msg[0];
 
-	ce = g2h_context_lookup(guc, ctx_id);
+	ce = g2h_context_lookup(guc, desc_idx);
 	if (unlikely(!ce))
 		return -EPROTO;
 
@@ -4041,15 +3953,14 @@ int intel_guc_sched_done_process_msg(struct intel_guc *guc,
 {
 	struct intel_context *ce;
 	unsigned long flags;
-	u32 ctx_id;
+	u32 desc_idx = msg[0];
 
 	if (unlikely(len < 2)) {
-		drm_err(&guc_to_gt(guc)->i915->drm, "Invalid length %u\n", len);
+		drm_err(&guc_to_gt(guc)->i915->drm, "Invalid length %u", len);
 		return -EPROTO;
 	}
-	ctx_id = msg[0];
 
-	ce = g2h_context_lookup(guc, ctx_id);
+	ce = g2h_context_lookup(guc, desc_idx);
 	if (unlikely(!ce))
 		return -EPROTO;
 
@@ -4057,8 +3968,8 @@ int intel_guc_sched_done_process_msg(struct intel_guc *guc,
 		     (!context_pending_enable(ce) &&
 		     !context_pending_disable(ce)))) {
 		drm_err(&guc_to_gt(guc)->i915->drm,
-			"Bad context sched_state 0x%x, ctx_id %u\n",
-			ce->guc_state.sched_state, ctx_id);
+			"Bad context sched_state 0x%x, desc_idx %u",
+			ce->guc_state.sched_state, desc_idx);
 		return -EPROTO;
 	}
 
@@ -4156,15 +4067,15 @@ int intel_guc_context_reset_process_msg(struct intel_guc *guc,
 					const u32 *msg, u32 len)
 {
 	struct intel_context *ce;
-	int ctx_id;
+	int desc_idx;
 
 	if (unlikely(len != 1)) {
 		drm_err(&guc_to_gt(guc)->i915->drm, "Invalid length %u", len);
 		return -EPROTO;
 	}
 
-	ctx_id = msg[0];
-	ce = g2h_context_lookup(guc, ctx_id);
+	desc_idx = msg[0];
+	ce = g2h_context_lookup(guc, desc_idx);
 	if (unlikely(!ce))
 		return -EPROTO;
 
@@ -4405,17 +4316,17 @@ void intel_guc_submission_print_context_info(struct intel_guc *guc,
 		guc_log_context_priority(p, ce);
 
 		if (intel_context_is_parent(ce)) {
-			struct guc_sched_wq_desc *wq_desc = __get_wq_desc(ce);
+			struct guc_process_desc *desc = __get_process_desc(ce);
 			struct intel_context *child;
 
 			drm_printf(p, "\t\tNumber children: %u\n",
 				   ce->parallel.number_children);
 			drm_printf(p, "\t\tWQI Head: %u\n",
-				   READ_ONCE(wq_desc->head));
+				   READ_ONCE(desc->head));
 			drm_printf(p, "\t\tWQI Tail: %u\n",
-				   READ_ONCE(wq_desc->tail));
+				   READ_ONCE(desc->tail));
 			drm_printf(p, "\t\tWQI Status: %u\n\n",
-				   READ_ONCE(wq_desc->wq_status));
+				   READ_ONCE(desc->wq_status));
 
 			if (ce->engine->emit_bb_start ==
 			    emit_bb_start_parent_no_preempt_mid_batch) {
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
index d2bc07f39139..ded6b3f6672d 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
@@ -50,21 +50,21 @@ void intel_uc_fw_change_status(struct intel_uc_fw *uc_fw,
  * firmware as TGL.
  */
 #define INTEL_UC_FIRMWARE_DEFS(fw_def, guc_def, huc_def) \
-	fw_def(ALDERLAKE_P, 0, guc_def(adlp, 70, 0, 3), huc_def(tgl, 7, 9, 3)) \
-	fw_def(ALDERLAKE_S, 0, guc_def(tgl, 70, 0, 3), huc_def(tgl,  7, 9, 3)) \
-	fw_def(DG1,         0, guc_def(dg1, 70, 0, 3), huc_def(dg1,  7, 9, 3)) \
-	fw_def(ROCKETLAKE,  0, guc_def(tgl, 70, 0, 3), huc_def(tgl,  7, 9, 3)) \
-	fw_def(TIGERLAKE,   0, guc_def(tgl, 70, 0, 3), huc_def(tgl,  7, 9, 3)) \
-	fw_def(JASPERLAKE,  0, guc_def(ehl, 70, 0, 3), huc_def(ehl,  9, 0, 0)) \
-	fw_def(ELKHARTLAKE, 0, guc_def(ehl, 70, 0, 3), huc_def(ehl,  9, 0, 0)) \
-	fw_def(ICELAKE,     0, guc_def(icl, 70, 0, 3), huc_def(icl,  9, 0, 0)) \
-	fw_def(COMETLAKE,   5, guc_def(cml, 70, 0, 3), huc_def(cml,  4, 0, 0)) \
-	fw_def(COMETLAKE,   0, guc_def(kbl, 70, 0, 3), huc_def(kbl,  4, 0, 0)) \
-	fw_def(COFFEELAKE,  0, guc_def(kbl, 70, 0, 3), huc_def(kbl,  4, 0, 0)) \
-	fw_def(GEMINILAKE,  0, guc_def(glk, 70, 0, 3), huc_def(glk,  4, 0, 0)) \
-	fw_def(KABYLAKE,    0, guc_def(kbl, 70, 0, 3), huc_def(kbl,  4, 0, 0)) \
-	fw_def(BROXTON,     0, guc_def(bxt, 70, 0, 3), huc_def(bxt,  2, 0, 0)) \
-	fw_def(SKYLAKE,     0, guc_def(skl, 70, 0, 3), huc_def(skl,  2, 0, 0))
+	fw_def(ALDERLAKE_P, 0, guc_def(adlp, 69, 0, 3), huc_def(tgl, 7, 9, 3)) \
+	fw_def(ALDERLAKE_S, 0, guc_def(tgl, 69, 0, 3), huc_def(tgl,  7, 9, 3)) \
+	fw_def(DG1,         0, guc_def(dg1, 69, 0, 3), huc_def(dg1,  7, 9, 3)) \
+	fw_def(ROCKETLAKE,  0, guc_def(tgl, 69, 0, 3), huc_def(tgl,  7, 9, 3)) \
+	fw_def(TIGERLAKE,   0, guc_def(tgl, 69, 0, 3), huc_def(tgl,  7, 9, 3)) \
+	fw_def(JASPERLAKE,  0, guc_def(ehl, 69, 0, 3), huc_def(ehl,  9, 0, 0)) \
+	fw_def(ELKHARTLAKE, 0, guc_def(ehl, 69, 0, 3), huc_def(ehl,  9, 0, 0)) \
+	fw_def(ICELAKE,     0, guc_def(icl, 69, 0, 3), huc_def(icl,  9, 0, 0)) \
+	fw_def(COMETLAKE,   5, guc_def(cml, 69, 0, 3), huc_def(cml,  4, 0, 0)) \
+	fw_def(COMETLAKE,   0, guc_def(kbl, 69, 0, 3), huc_def(kbl,  4, 0, 0)) \
+	fw_def(COFFEELAKE,  0, guc_def(kbl, 69, 0, 3), huc_def(kbl,  4, 0, 0)) \
+	fw_def(GEMINILAKE,  0, guc_def(glk, 69, 0, 3), huc_def(glk,  4, 0, 0)) \
+	fw_def(KABYLAKE,    0, guc_def(kbl, 69, 0, 3), huc_def(kbl,  4, 0, 0)) \
+	fw_def(BROXTON,     0, guc_def(bxt, 69, 0, 3), huc_def(bxt,  2, 0, 0)) \
+	fw_def(SKYLAKE,     0, guc_def(skl, 69, 0, 3), huc_def(skl,  2, 0, 0))
 
 #define __MAKE_UC_FW_PATH(prefix_, name_, major_, minor_, patch_) \
 	"i915/" \
diff --git a/drivers/gpu/drm/i915/gt/uc/selftest_guc.c b/drivers/gpu/drm/i915/gt/uc/selftest_guc.c
index e0484feae906..2ae414446112 100644
--- a/drivers/gpu/drm/i915/gt/uc/selftest_guc.c
+++ b/drivers/gpu/drm/i915/gt/uc/selftest_guc.c
@@ -148,7 +148,7 @@ static int intel_guc_steal_guc_ids(void *arg)
 	struct i915_request *spin_rq = NULL, *rq, *last = NULL;
 	int number_guc_id_stolen = guc->number_guc_id_stolen;
 
-	ce = kcalloc(GUC_MAX_CONTEXT_ID, sizeof(*ce), GFP_KERNEL);
+	ce = kzalloc(sizeof(*ce) * GUC_MAX_LRC_DESCRIPTORS, GFP_KERNEL);
 	if (!ce) {
 		pr_err("Context array allocation failed\n");
 		return -ENOMEM;
-- 
2.25.1

