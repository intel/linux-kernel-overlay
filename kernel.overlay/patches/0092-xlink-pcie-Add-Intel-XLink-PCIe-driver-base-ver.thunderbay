From ca7761cd625bf2e302ce22256e7e2cfb86278da7 Mon Sep 17 00:00:00 2001
From: Srikanth Thokala <srikanth.thokala@intel.com>
Date: Sun, 21 Feb 2021 16:51:38 +0530
Subject: [PATCH 092/170] xlink-pcie: Add Intel XLink PCIe driver base version

Base version of xlink-pcie used for upstreaming

Signed-off-by: Srikanth Thokala <srikanth.thokala@intel.com>
---
 drivers/misc/xlink-pcie/Kconfig               |   21 +
 drivers/misc/xlink-pcie/Makefile              |    2 +
 drivers/misc/xlink-pcie/common/mxlk.h         |  137 ++
 drivers/misc/xlink-pcie/common/mxlk_boot.h    |   65 +
 .../xlink-pcie/common/mxlk_capabilities.h     |  134 ++
 drivers/misc/xlink-pcie/common/mxlk_common.h  |  169 +++
 drivers/misc/xlink-pcie/common/mxlk_core.h    |   56 +
 drivers/misc/xlink-pcie/common/mxlk_util.c    |  426 ++++++
 drivers/misc/xlink-pcie/common/mxlk_util.h    |   69 +
 drivers/misc/xlink-pcie/local_host/Makefile   |    9 +
 .../misc/xlink-pcie/local_host/mxlk_core.c    |  797 +++++++++++
 drivers/misc/xlink-pcie/local_host/mxlk_dma.c |  556 ++++++++
 drivers/misc/xlink-pcie/local_host/mxlk_dma.h |   24 +
 drivers/misc/xlink-pcie/local_host/mxlk_epf.c |  497 +++++++
 drivers/misc/xlink-pcie/local_host/mxlk_epf.h |   31 +
 drivers/misc/xlink-pcie/local_host/mxlk_inf.c |  119 ++
 .../misc/xlink-pcie/local_host/mxlk_struct.h  |   77 ++
 drivers/misc/xlink-pcie/remote_host/Makefile  |    8 +
 .../misc/xlink-pcie/remote_host/mxlk_core.c   |  660 +++++++++
 .../misc/xlink-pcie/remote_host/mxlk_inf.c    |   93 ++
 .../misc/xlink-pcie/remote_host/mxlk_main.c   |   90 ++
 .../misc/xlink-pcie/remote_host/mxlk_pci.c    | 1191 +++++++++++++++++
 .../misc/xlink-pcie/remote_host/mxlk_pci.h    |   75 ++
 include/linux/xlink_drv_inf.h                 |   63 +
 24 files changed, 5369 insertions(+)
 create mode 100644 drivers/misc/xlink-pcie/Kconfig
 create mode 100644 drivers/misc/xlink-pcie/Makefile
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk.h
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk_boot.h
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk_capabilities.h
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk_common.h
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk_core.h
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk_util.c
 create mode 100644 drivers/misc/xlink-pcie/common/mxlk_util.h
 create mode 100644 drivers/misc/xlink-pcie/local_host/Makefile
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_core.c
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_dma.c
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_dma.h
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_epf.c
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_epf.h
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_inf.c
 create mode 100644 drivers/misc/xlink-pcie/local_host/mxlk_struct.h
 create mode 100644 drivers/misc/xlink-pcie/remote_host/Makefile
 create mode 100644 drivers/misc/xlink-pcie/remote_host/mxlk_core.c
 create mode 100644 drivers/misc/xlink-pcie/remote_host/mxlk_inf.c
 create mode 100644 drivers/misc/xlink-pcie/remote_host/mxlk_main.c
 create mode 100644 drivers/misc/xlink-pcie/remote_host/mxlk_pci.c
 create mode 100644 drivers/misc/xlink-pcie/remote_host/mxlk_pci.h
 create mode 100644 include/linux/xlink_drv_inf.h

diff --git a/drivers/misc/xlink-pcie/Kconfig b/drivers/misc/xlink-pcie/Kconfig
new file mode 100644
index 000000000000..5c5c958638c3
--- /dev/null
+++ b/drivers/misc/xlink-pcie/Kconfig
@@ -0,0 +1,21 @@
+config XLINK_PCIE_RH_DRIVER
+	tristate "XLink PCIe Remote Host driver"
+	depends on PCI && XLINK_PCIE_LH_DRIVER=n
+	help
+	  This option enables XLink PCIe Remote Host driver.
+
+	  Choose M here to compile this driver as a module, name is mxlk.
+	  This driver is used for XLink communication over PCIe,
+	  and is to be loaded on the IA host which is connected to
+	  the Intel Keem Bay.
+
+config XLINK_PCIE_LH_DRIVER
+	tristate "XLink PCIe Local Host driver"
+	depends on PCI_ENDPOINT
+	help
+	  This option enables XLink PCIe Local Host driver.
+
+	  Choose M here to compile this driver as a module, name is mxlk_ep.
+	  This driver is used for XLink communication over PCIe and is to be
+	  loaded on the Intel Keem Bay platform.
+
diff --git a/drivers/misc/xlink-pcie/Makefile b/drivers/misc/xlink-pcie/Makefile
new file mode 100644
index 000000000000..1dd984d8d88c
--- /dev/null
+++ b/drivers/misc/xlink-pcie/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_XLINK_PCIE_RH_DRIVER) += remote_host/
+obj-$(CONFIG_XLINK_PCIE_LH_DRIVER) += local_host/
diff --git a/drivers/misc/xlink-pcie/common/mxlk.h b/drivers/misc/xlink-pcie/common/mxlk.h
new file mode 100644
index 000000000000..71ded535f7d9
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk.h
@@ -0,0 +1,137 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_HEADER_
+#define MXLK_HEADER_
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/workqueue.h>
+#include <linux/stddef.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/slab.h>
+#include <linux/mutex.h>
+#include <linux/version.h>
+#include <linux/mempool.h>
+#include <linux/dma-mapping.h>
+#include <linux/cache.h>
+#include <linux/wait.h>
+
+#include "mxlk_common.h"
+#include "mxlk_boot.h"
+
+#ifdef XLINK_PCIE_REMOTE
+#define MXLK_DRIVER_NAME "mxlk"
+#define MXLK_DRIVER_DESC "Intel(R) Keem Bay XLink PCIe driver"
+#else
+#define MXLK_DRIVER_NAME "mxlk_pcie_epf"
+#define MXLK_DRIVER_DESC "Intel(R) xLink PCIe endpoint function driver"
+#endif
+
+struct mxlk_pipe {
+	u32 old;
+	u32 ndesc;
+	u32 *head;
+	u32 *tail;
+	struct mxlk_transfer_desc *tdr;
+};
+
+struct mxlk_buf_desc {
+	struct mxlk_buf_desc *next;
+	void *head;
+	dma_addr_t phys;
+	size_t true_len;
+	void *data;
+	size_t length;
+	int interface;
+	bool own_mem;
+};
+
+struct mxlk_stream {
+	size_t frag;
+	struct mxlk_pipe pipe;
+#ifdef XLINK_PCIE_REMOTE
+	struct mxlk_buf_desc **ddr;
+#endif
+};
+
+struct mxlk_list {
+	spinlock_t lock;
+	size_t bytes;
+	size_t buffers;
+	struct mxlk_buf_desc *head;
+	struct mxlk_buf_desc *tail;
+};
+
+struct mxlk_interface {
+	int id;
+	struct mxlk *mxlk;
+	struct mutex rlock;
+	struct mxlk_list read;
+	struct mxlk_buf_desc *partial_read;
+	bool data_available;
+	wait_queue_head_t rx_waitqueue;
+};
+
+struct mxlk_debug_stats {
+	struct {
+		size_t cnts;
+		size_t bytes;
+	} tx_krn, rx_krn, tx_usr, rx_usr;
+	size_t send_ints;
+	size_t interrupts;
+	size_t rx_event_runs;
+	size_t tx_event_runs;
+};
+
+struct mxlk {
+	u32 status;
+	bool legacy_a0;
+
+#ifdef XLINK_PCIE_REMOTE
+	void __iomem *bar0;
+	struct mxlk_bootio __iomem *io_comm; /* IO communication space */
+	struct mxlk_mmio __iomem *mmio; /* XLink memory space */
+	void __iomem *bar4;
+#else
+	struct mxlk_bootio *io_comm; /* IO communication space */
+	struct mxlk_mmio *mmio; /* XLink memory space */
+	void *bar4;
+#endif
+
+	struct workqueue_struct *rx_wq;
+	struct workqueue_struct *tx_wq;
+
+	struct mxlk_interface interfaces[MXLK_NUM_INTERFACES];
+
+	size_t fragment_size;
+	struct mxlk_cap_txrx *txrx;
+	struct mxlk_stream tx;
+	struct mxlk_stream rx;
+
+	struct mutex wlock;
+	struct mxlk_list write;
+	bool no_tx_buffer;
+	wait_queue_head_t tx_waitqueue;
+	bool tx_pending;
+	bool stop_flag;
+
+	struct mxlk_list rx_pool;
+	struct mxlk_list tx_pool;
+
+	struct delayed_work rx_event;
+	struct delayed_work tx_event;
+
+	struct device_attribute debug;
+	bool debug_enable;
+	struct mxlk_debug_stats stats;
+};
+
+#endif
diff --git a/drivers/misc/xlink-pcie/common/mxlk_boot.h b/drivers/misc/xlink-pcie/common/mxlk_boot.h
new file mode 100644
index 000000000000..5f396d7ea65a
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk_boot.h
@@ -0,0 +1,65 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_BOOT_HEADER_
+#define MXLK_BOOT_HEADER_
+
+#include <linux/types.h>
+
+#define MXLK_BOOT_MAGIC_ROM "VPUROM"
+#define MXLK_BOOT_MAGIC_EMMC "VPUEMMC"
+#define MXLK_BOOT_MAGIC_BL2 "VPUBL2"
+#define MXLK_BOOT_MAGIC_UBOOT "VPUUBOOT"
+#define MXLK_BOOT_MAGIC_RECOV "VPURECOV"
+#define MXLK_BOOT_MAGIC_YOCTO "VPUYOCTO"
+
+enum mxlk_stage {
+	STAGE_UNINIT,
+	STAGE_ROM,
+	STAGE_BL2,
+	STAGE_UBOOT,
+	STAGE_RECOV,
+	STAGE_OS
+};
+
+#define MXLK_BOOT_FIP_ID (0xFFFFFFFF)
+#define MXLK_BOOT_BOOT_ID (0xFFFFFF4F)
+#define MXLK_BOOT_SYSTEM_ID (0xFFFFFF46)
+#define MXLK_BOOT_RAW_ID (0xFFFFFF00)
+#define MXLK_BOOT_ERASE_ID (0xFFFFFF01)
+#define MXLK_BOOT_FLASH_ID (0xFFFFFF02)
+
+#define MXLK_BOOT_STATUS_START (0x55555555)
+#define MXLK_BOOT_STATUS_INVALID (0xDEADFFFF)
+#define MXLK_BOOT_STATUS_DOWNLOADED (0xDDDDDDDD)
+#define MXLK_BOOT_STATUS_ERROR (0xDEADAAAA)
+#define MXLK_BOOT_STATUS_DONE (0xBBBBBBBB)
+
+#define MXLK_INT_ENABLE (0x1)
+#define MXLK_INT_MASK (0x1)
+
+#define MXLK_BOOT_MAGIC_STRLEN (16)
+#define MXLK_BOOT_DEST_STRLEN (128)
+
+struct mxlk_bootio {
+	u8 magic[MXLK_BOOT_MAGIC_STRLEN];
+	u32 mf_ready;
+	u32 mf_len;
+	u64 reserved1;
+	u64 mf_start;
+	u32 int_enable;
+	u32 int_mask;
+	u32 int_identity;
+	u32 reserved2;
+	u64 mf_offset;
+	u8 mf_dest[MXLK_BOOT_DEST_STRLEN];
+	u64 dev_id;
+} __packed;
+
+#endif // MXLK_BOOT_HEADER_
diff --git a/drivers/misc/xlink-pcie/common/mxlk_capabilities.h b/drivers/misc/xlink-pcie/common/mxlk_capabilities.h
new file mode 100644
index 000000000000..74001efa47e2
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk_capabilities.h
@@ -0,0 +1,134 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_CAPABILITIES_HEADER_
+#define MXLK_CAPABILITIES_HEADER_
+
+#include "mxlk.h"
+#include "mxlk_common.h"
+
+#define MXLK_CAP_TTL (32)
+
+static inline
+void *mxlk_cap_find(struct mxlk *mxlk, u32 start, u16 id)
+{
+	int ttl = MXLK_CAP_TTL;
+	struct mxlk_cap_hdr *hdr;
+	struct mxlk_cap_hdr cur_hdr;
+
+	// If user didn't specify start, assume start of mmio
+	if (!start)
+		start = mxlk_ioread32(&mxlk->mmio->cap_offset);
+
+	// Read header info
+#ifdef XLINK_PCIE_REMOTE
+	hdr = (struct mxlk_cap_hdr *)((void __iomem *)mxlk->mmio + start);
+#else
+	hdr = (struct mxlk_cap_hdr *)((void *)mxlk->mmio + start);
+#endif
+	// Check if we still have time to live
+	while (ttl--) {
+#ifdef XLINK_PCIE_REMOTE
+		memcpy_fromio(&cur_hdr, hdr, sizeof(struct mxlk_cap_hdr));
+#else
+		cur_hdr = *hdr;
+#endif
+		// If cap matches, return header
+		if (cur_hdr.id == id)
+			return hdr;
+		// If cap is NULL, we are at the end of the list
+		else if (cur_hdr.id == MXLK_CAP_NULL)
+			return NULL;
+		// If no match and no end of list, traverse the linked list
+		else
+#ifdef XLINK_PCIE_REMOTE
+			hdr = (struct mxlk_cap_hdr *)
+				((void __iomem *)mxlk->mmio + cur_hdr.next);
+#else
+			hdr = (struct mxlk_cap_hdr *)
+				((void *)mxlk->mmio + cur_hdr.next);
+#endif
+	}
+
+	// If we reached here, the capability list is corrupted
+	return NULL;
+}
+
+static inline
+void mxlk_set_td_address(struct mxlk_transfer_desc *td, u64 address)
+{
+	mxlk_iowrite64(address, &td->address);
+}
+
+static inline
+u64 mxlk_get_td_address(struct mxlk_transfer_desc *td)
+{
+	return mxlk_ioread64(&td->address);
+}
+
+static inline
+void mxlk_set_td_length(struct mxlk_transfer_desc *td, u32 length)
+{
+	mxlk_iowrite32(length, &td->length);
+}
+
+static inline
+u32 mxlk_get_td_length(struct mxlk_transfer_desc *td)
+{
+	return mxlk_ioread32(&td->length);
+}
+
+static inline
+void mxlk_set_td_interface(struct mxlk_transfer_desc *td, u16 interface)
+{
+	mxlk_iowrite16(interface, &td->interface);
+}
+
+static inline
+u16 mxlk_get_td_interface(struct mxlk_transfer_desc *td)
+{
+	return mxlk_ioread16(&td->interface);
+}
+
+static inline
+void mxlk_set_td_status(struct mxlk_transfer_desc *td, u16 status)
+{
+	mxlk_iowrite16(status, &td->status);
+}
+
+static inline
+u16 mxlk_get_td_status(struct mxlk_transfer_desc *td)
+{
+	return mxlk_ioread16(&td->status);
+}
+
+static inline
+void mxlk_set_tdr_head(struct mxlk_pipe *p, u32 head)
+{
+	mxlk_iowrite32(head, p->head);
+}
+
+static inline
+u32 mxlk_get_tdr_head(struct mxlk_pipe *p)
+{
+	return mxlk_ioread32(p->head);
+}
+
+static inline
+void mxlk_set_tdr_tail(struct mxlk_pipe *p, u32 tail)
+{
+	mxlk_iowrite32(tail, p->tail);
+}
+
+static inline
+u32 mxlk_get_tdr_tail(struct mxlk_pipe *p)
+{
+	return mxlk_ioread32(p->tail);
+}
+#endif // MXLK_CAPABILITIES_HEADER_
diff --git a/drivers/misc/xlink-pcie/common/mxlk_common.h b/drivers/misc/xlink-pcie/common/mxlk_common.h
new file mode 100644
index 000000000000..eb068fe1a88a
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk_common.h
@@ -0,0 +1,169 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_COMMON_HEADER_
+#define MXLK_COMMON_HEADER_
+
+#include <linux/io.h>
+#include <linux/types.h>
+#include <linux/pci_ids.h>
+#include <linux/sizes.h>
+
+#ifndef PCI_DEVICE_ID_INTEL_KEEMBAY
+#define PCI_DEVICE_ID_INTEL_KEEMBAY 0x6240
+#endif
+
+/*
+ * Number of interfaces to statically allocate resources for
+ */
+#define MXLK_NUM_INTERFACES (1)
+
+#define MXLK_FRAGMENT_SIZE SZ_128K
+#define MXLK_NUM_TX_DESCS (64)
+#define MXLK_NUM_RX_DESCS (64)
+
+/*
+ * Status encoding of the transfer descriptors
+ */
+#define MXLK_DESC_STATUS_SUCCESS (0)
+#define MXLK_DESC_STATUS_ERROR (0xFFFF)
+
+/*
+ * Layout transfer descriptors used by device and host
+ */
+struct mxlk_transfer_desc {
+	uint64_t address;
+	uint32_t length;
+	uint16_t status;
+	uint16_t interface;
+} __packed;
+
+#define MXLK_IO_COMM_SIZE SZ_16K
+#define MXLK_MMIO_OFFSET SZ_4K
+
+#define MXLK_VERSION_MAJOR 0
+#define MXLK_VERSION_MINOR 5
+#define MXLK_VERSION_BUILD 0
+#define _TOSTR(X) #X
+#define _VERSION(A, B, C) _TOSTR(A) "." _TOSTR(B) "." _TOSTR(C)
+#define MXLK_DRIVER_VERSION \
+	_VERSION(MXLK_VERSION_MAJOR, MXLK_VERSION_MINOR, MXLK_VERSION_BUILD)
+
+struct mxlk_version {
+	uint8_t major;
+	uint8_t minor;
+	uint16_t build;
+} __packed;
+
+/*
+ * Status encoding of both device and host
+ */
+#define MXLK_STATUS_ERROR (0xFFFFFFFF)
+#define MXLK_STATUS_UNINIT (0)
+#define MXLK_STATUS_BOOT_FW (1)
+#define MXLK_STATUS_BOOT_OS (2)
+#define MXLK_STATUS_READY (3)
+#define MXLK_STATUS_RECOVERY (4)
+#define MXLK_STATUS_RUN (5)
+#define MXLK_STATUS_OFF (6)
+#define MXLK_STATUS_BOOT_PRE_OS (7)
+
+/*
+ * MMIO layout and offsets shared between device and host
+ */
+struct mxlk_mmio {
+	struct mxlk_version version;
+	uint32_t device_status;
+	uint32_t host_status;
+	uint8_t legacy_a0;
+	uint8_t htod_tx_doorbell;
+	uint8_t htod_rx_doorbell;
+	uint8_t htod_event_doorbell;
+	uint8_t dtoh_tx_doorbell;
+	uint8_t dtoh_rx_doorbell;
+	uint8_t dtoh_event_doorbell;
+	uint8_t reserved;
+	uint32_t cap_offset;
+} __packed;
+
+/*
+ * Defined capabilities located in mmio space
+ */
+#define MXLK_CAP_NULL (0)
+#define MXLK_CAP_TXRX (1)
+
+/*
+ * Header at the beginning of each capability to define and link to next
+ */
+struct mxlk_cap_hdr {
+	uint16_t id;
+	uint16_t next;
+} __packed;
+
+#define MXLK_CAP_HDR_ID (offsetof(struct mxlk_cap_hdr, id))
+#define MXLK_CAP_HDR_NEXT (offsetof(struct mxlk_cap_hdr, next))
+
+struct mxlk_cap_pipe {
+	uint32_t ring;
+	uint32_t ndesc;
+	uint32_t head;
+	uint32_t tail;
+} __packed;
+
+/*
+ * Transmit and Receive capability
+ */
+struct mxlk_cap_txrx {
+	struct mxlk_cap_hdr hdr;
+	u32 fragment_size;
+	struct mxlk_cap_pipe tx;
+	struct mxlk_cap_pipe rx;
+} __packed;
+
+static inline u64 _ioread64(void __iomem *addr)
+{
+	u64 low, high;
+
+	low = ioread32(addr);
+	high = ioread32(addr + sizeof(u32));
+
+	return low | (high << 32);
+}
+
+static inline void _iowrite64(u64 value, void __iomem *addr)
+{
+	iowrite32(value, addr);
+	iowrite32(value >> 32, addr + sizeof(u32));
+}
+
+#ifdef XLINK_PCIE_REMOTE
+
+#define mxlk_iowrite64 _iowrite64
+#define mxlk_iowrite32 iowrite32
+#define mxlk_iowrite16 iowrite16
+#define mxlk_iowrite8 iowrite8
+#define mxlk_ioread64 _ioread64
+#define mxlk_ioread32 ioread32
+#define mxlk_ioread16 ioread16
+#define mxlk_ioread8 ioread8
+
+#else
+
+#define mxlk_iowrite64(value, addr)	{ *(addr) = value; }
+#define mxlk_iowrite32(value, addr)	{ *(addr) = value; }
+#define mxlk_iowrite16(value, addr)	{ *(addr) = value; }
+#define mxlk_iowrite8(value, addr)	{ *(addr) = value; }
+#define mxlk_ioread64(addr) (*(addr))
+#define mxlk_ioread32(addr) (*(addr))
+#define mxlk_ioread16(addr) (*(addr))
+#define mxlk_ioread8(addr)  (*(addr))
+
+#endif // XLINK_PCIE_REMOTE
+
+#endif
diff --git a/drivers/misc/xlink-pcie/common/mxlk_core.h b/drivers/misc/xlink-pcie/common/mxlk_core.h
new file mode 100644
index 000000000000..0b911af7c5e6
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk_core.h
@@ -0,0 +1,56 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_CORE_HEADER_
+#define MXLK_CORE_HEADER_
+
+#include "mxlk.h"
+
+int mxlk_core_init(struct mxlk *mxlk);
+void mxlk_core_cleanup(struct mxlk *mxlk);
+
+/*
+ * @brief Read buffer from mxlk. Function will block when no data.
+ *
+ * @param[in] mxlk          - pointer to mxlk instance
+ * @param[in] buffer        - pointer to buffer
+ * @param[in] length        - max bytes to copy into buffer
+ * @param[in] timeout_ms    - timeout in ms for blocking when no data
+ *
+ * @return:
+ *      >=0 - number of bytes read
+ *      <0  - linux error code
+ *              -ETIME - timeout
+ *              -EINTR - interrupted
+ */
+int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
+		   uint32_t timeout_ms);
+
+/*
+ * @brief Writes buffer to mxlk. Function will block when no buffer.
+ *
+ * @param[in] mxlk          - pointer to mxlk instance
+ * @param[in] buffer        - pointer to buffer
+ * @param[in] length        - length of buffer to copy from
+ * @param[in] timeout_ms    - timeout in ms for blocking when no buffer
+ *
+ * @return:
+ *      >=0 - number of bytes write
+ *      <0  - linux error code
+ *              -ETIME - timeout
+ *              -EINTR - interrupted
+ */
+int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
+		    uint32_t timeout_ms);
+
+#ifdef XLINK_PCIE_LOCAL
+struct mxlk *mxlk_core_get_by_id(uint32_t sw_device_id);
+#endif
+
+#endif
diff --git a/drivers/misc/xlink-pcie/common/mxlk_util.c b/drivers/misc/xlink-pcie/common/mxlk_util.c
new file mode 100644
index 000000000000..e3f5c4c71587
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk_util.c
@@ -0,0 +1,426 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include "mxlk_util.h"
+
+void mxlk_set_device_status(struct mxlk *mxlk, u32 status)
+{
+	mxlk->status = status;
+	mxlk_iowrite32(status, &mxlk->mmio->device_status);
+}
+
+u32 mxlk_get_device_status(struct mxlk *mxlk)
+{
+	return mxlk_ioread32(&mxlk->mmio->device_status);
+}
+
+static u8 *mxlk_doorbell_offset(struct mxlk *mxlk,
+				enum mxlk_doorbell_direction dirt,
+				enum mxlk_doorbell_type type)
+{
+	if (dirt == TO_DEVICE && type == DATA_SENT)
+		return &mxlk->mmio->htod_tx_doorbell;
+	if (dirt == TO_DEVICE && type == DATA_RECEIVED)
+		return &mxlk->mmio->htod_rx_doorbell;
+	if (dirt == TO_DEVICE && type == DEV_EVENT)
+		return &mxlk->mmio->htod_event_doorbell;
+	if (dirt == FROM_DEVICE && type == DATA_SENT)
+		return &mxlk->mmio->dtoh_tx_doorbell;
+	if (dirt == FROM_DEVICE && type == DATA_RECEIVED)
+		return &mxlk->mmio->dtoh_rx_doorbell;
+	if (dirt == FROM_DEVICE && type == DEV_EVENT)
+		return &mxlk->mmio->dtoh_event_doorbell;
+
+	return NULL;
+}
+
+void mxlk_set_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
+		       enum mxlk_doorbell_type type, u8 value)
+{
+	mxlk_iowrite8(value, mxlk_doorbell_offset(mxlk, dirt, type));
+}
+
+u8 mxlk_get_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
+		       enum mxlk_doorbell_type type)
+{
+	return mxlk_ioread8(mxlk_doorbell_offset(mxlk, dirt, type));
+}
+
+u32 mxlk_get_host_status(struct mxlk *mxlk)
+{
+	return mxlk_ioread32(&mxlk->mmio->host_status);
+}
+
+void mxlk_set_host_status(struct mxlk *mxlk, u32 status)
+{
+	mxlk->status = status;
+	mxlk_iowrite32(status, &mxlk->mmio->host_status);
+}
+
+struct mxlk_buf_desc *mxlk_alloc_bd(size_t length)
+{
+	struct mxlk_buf_desc *bd;
+
+	bd = kzalloc(sizeof(*bd), GFP_KERNEL);
+	if (!bd)
+		return NULL;
+
+	bd->head = kzalloc(roundup(length, cache_line_size()), GFP_KERNEL);
+	if (!bd->head) {
+		kfree(bd);
+		return NULL;
+	}
+
+	bd->data = bd->head;
+	bd->length = bd->true_len = length;
+	bd->next = NULL;
+	bd->own_mem = true;
+
+	return bd;
+}
+
+struct mxlk_buf_desc *mxlk_alloc_bd_reuse(size_t length, void *virt,
+					  dma_addr_t phys)
+{
+	struct mxlk_buf_desc *bd;
+
+	bd = kzalloc(sizeof(*bd), GFP_KERNEL);
+	if (!bd)
+		return NULL;
+
+	bd->head = virt;
+	bd->phys = phys;
+	bd->data = bd->head;
+	bd->length = bd->true_len = length;
+	bd->next = NULL;
+	bd->own_mem = false;
+
+	return bd;
+}
+
+void mxlk_free_bd(struct mxlk_buf_desc *bd)
+{
+	if (bd) {
+		if (bd->own_mem)
+			kfree(bd->head);
+		kfree(bd);
+	}
+}
+
+int mxlk_list_init(struct mxlk_list *list)
+{
+	spin_lock_init(&list->lock);
+	list->bytes = 0;
+	list->buffers = 0;
+	list->head = NULL;
+	list->tail = NULL;
+
+	return 0;
+}
+
+void mxlk_list_cleanup(struct mxlk_list *list)
+{
+	struct mxlk_buf_desc *bd;
+
+	spin_lock(&list->lock);
+	while (list->head) {
+		bd = list->head;
+		list->head = bd->next;
+		mxlk_free_bd(bd);
+	}
+
+	list->head = list->tail = NULL;
+	spin_unlock(&list->lock);
+}
+
+int mxlk_list_put(struct mxlk_list *list, struct mxlk_buf_desc *bd)
+{
+	if (!bd)
+		return -EINVAL;
+
+	spin_lock(&list->lock);
+	if (list->head)
+		list->tail->next = bd;
+	else
+		list->head = bd;
+	while (bd) {
+		list->tail = bd;
+		list->bytes += bd->length;
+		list->buffers++;
+		bd = bd->next;
+	}
+	spin_unlock(&list->lock);
+
+	return 0;
+}
+
+int mxlk_list_put_head(struct mxlk_list *list, struct mxlk_buf_desc *bd)
+{
+	struct mxlk_buf_desc *old_head;
+
+	if (!bd)
+		return -EINVAL;
+
+	spin_lock(&list->lock);
+	old_head = list->head;
+	list->head = bd;
+	while (bd) {
+		list->bytes += bd->length;
+		list->buffers++;
+		if (!bd->next) {
+			list->tail = list->tail ? list->tail : bd;
+			bd->next = old_head;
+			break;
+		}
+		bd = bd->next;
+	}
+	spin_unlock(&list->lock);
+
+	return 0;
+}
+
+struct mxlk_buf_desc *mxlk_list_get(struct mxlk_list *list)
+{
+	struct mxlk_buf_desc *bd;
+
+	spin_lock(&list->lock);
+	bd = list->head;
+	if (list->head) {
+		list->head = list->head->next;
+		if (!list->head)
+			list->tail = NULL;
+		bd->next = NULL;
+		list->bytes -= bd->length;
+		list->buffers--;
+	}
+	spin_unlock(&list->lock);
+
+	return bd;
+}
+
+void mxlk_list_info(struct mxlk_list *list, size_t *bytes, size_t *buffers)
+{
+	spin_lock(&list->lock);
+	*bytes = list->bytes;
+	*buffers = list->buffers;
+	spin_unlock(&list->lock);
+}
+
+struct mxlk_buf_desc *mxlk_alloc_rx_bd(struct mxlk *mxlk)
+{
+	struct mxlk_buf_desc *bd;
+
+	bd = mxlk_list_get(&mxlk->rx_pool);
+	if (bd) {
+		bd->data = bd->head;
+		bd->length = bd->true_len;
+		bd->next = NULL;
+		bd->interface = 0;
+	}
+
+	return bd;
+}
+
+void mxlk_free_rx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd)
+{
+	if (bd)
+		mxlk_list_put(&mxlk->rx_pool, bd);
+}
+
+struct mxlk_buf_desc *mxlk_alloc_tx_bd(struct mxlk *mxlk)
+{
+	struct mxlk_buf_desc *bd;
+
+	bd = mxlk_list_get(&mxlk->tx_pool);
+	if (bd) {
+		bd->data = bd->head;
+		bd->length = bd->true_len;
+		bd->next = NULL;
+		bd->interface = 0;
+	} else {
+		mxlk->no_tx_buffer = true;
+	}
+
+	return bd;
+}
+
+void mxlk_free_tx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd)
+{
+	if (!bd)
+		return;
+
+	mxlk_list_put(&mxlk->tx_pool, bd);
+
+	mxlk->no_tx_buffer = false;
+	wake_up_interruptible(&mxlk->tx_waitqueue);
+}
+
+int mxlk_interface_init(struct mxlk *mxlk, int id)
+{
+	struct mxlk_interface *inf = mxlk->interfaces + id;
+
+	inf->id = id;
+	inf->mxlk = mxlk;
+
+	inf->partial_read = NULL;
+	mxlk_list_init(&inf->read);
+	mutex_init(&inf->rlock);
+	inf->data_available = false;
+	init_waitqueue_head(&inf->rx_waitqueue);
+
+	return 0;
+}
+
+void mxlk_interface_cleanup(struct mxlk_interface *inf)
+{
+	struct mxlk_buf_desc *bd;
+
+	mutex_destroy(&inf->rlock);
+
+	mxlk_free_rx_bd(inf->mxlk, inf->partial_read);
+	while ((bd = mxlk_list_get(&inf->read)))
+		mxlk_free_rx_bd(inf->mxlk, bd);
+}
+
+void mxlk_interfaces_cleanup(struct mxlk *mxlk)
+{
+	int index;
+
+	for (index = 0; index < MXLK_NUM_INTERFACES; index++)
+		mxlk_interface_cleanup(mxlk->interfaces + index);
+
+	mxlk_list_cleanup(&mxlk->write);
+	mutex_destroy(&mxlk->wlock);
+}
+
+int mxlk_interfaces_init(struct mxlk *mxlk)
+{
+	int index;
+
+	mutex_init(&mxlk->wlock);
+	mxlk_list_init(&mxlk->write);
+	init_waitqueue_head(&mxlk->tx_waitqueue);
+	mxlk->no_tx_buffer = false;
+
+	for (index = 0; index < MXLK_NUM_INTERFACES; index++)
+		mxlk_interface_init(mxlk, index);
+
+	return 0;
+}
+
+void mxlk_add_bd_to_interface(struct mxlk *mxlk, struct mxlk_buf_desc *bd)
+{
+	struct mxlk_interface *inf;
+
+	inf = mxlk->interfaces + bd->interface;
+
+	mxlk_list_put(&inf->read, bd);
+
+	mutex_lock(&inf->rlock);
+	inf->data_available = true;
+	mutex_unlock(&inf->rlock);
+	wake_up_interruptible(&inf->rx_waitqueue);
+}
+
+#ifdef XLINK_PCIE_REMOTE
+#include "../remote_host/mxlk_pci.h"
+#else
+#include "../local_host/mxlk_struct.h"
+#endif
+
+static ssize_t debug_show(struct device *dev, struct device_attribute *attr,
+			  char *buf)
+{
+#ifdef XLINK_PCIE_LOCAL
+	struct pci_epf *epf = container_of(dev, struct pci_epf, dev);
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct mxlk *mxlk = &mxlk_epf->mxlk;
+#else
+	struct pci_dev *pdev = container_of(dev, struct pci_dev, dev);
+	struct mxlk_pcie *xdev = pci_get_drvdata(pdev);
+	struct mxlk *mxlk = &xdev->mxlk;
+#endif
+	size_t bytes, tx_list_num, rx_list_num, tx_pool_num, rx_pool_num;
+
+	if (!mxlk->debug_enable)
+		return 0;
+
+	mxlk_list_info(&mxlk->write, &bytes, &tx_list_num);
+	mxlk_list_info(&mxlk->interfaces[0].read, &bytes, &rx_list_num);
+	mxlk_list_info(&mxlk->tx_pool, &bytes, &tx_pool_num);
+	mxlk_list_info(&mxlk->rx_pool, &bytes, &rx_pool_num);
+
+	snprintf(buf, 4096,
+		 "tx_krn, cnts %zu bytes %zu\n"
+		 "tx_usr, cnts %zu bytes %zu\n"
+		 "rx_krn, cnts %zu bytes %zu\n"
+		 "rx_usr, cnts %zu bytes %zu\n"
+		 "tx_list %zu tx_pool %zu\n"
+		 "rx_list %zu rx_pool %zu\n"
+		 "interrupts %zu, send_ints %zu\n"
+		 "rx runs %zu tx runs %zu\n",
+		 mxlk->stats.tx_krn.cnts, mxlk->stats.tx_krn.bytes,
+		 mxlk->stats.tx_usr.cnts, mxlk->stats.tx_usr.bytes,
+		 mxlk->stats.rx_krn.cnts, mxlk->stats.rx_krn.bytes,
+		 mxlk->stats.rx_usr.cnts, mxlk->stats.rx_usr.bytes,
+		 tx_list_num, tx_pool_num, rx_list_num, rx_pool_num,
+		 mxlk->stats.interrupts, mxlk->stats.send_ints,
+		 mxlk->stats.rx_event_runs, mxlk->stats.tx_event_runs
+	 );
+
+	return strlen(buf);
+}
+
+static ssize_t debug_store(struct device *dev, struct device_attribute *attr,
+			   const char *buf, size_t count)
+{
+	long value;
+	int rc;
+
+#ifdef XLINK_PCIE_LOCAL
+	struct pci_epf *epf = container_of(dev, struct pci_epf, dev);
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct mxlk *mxlk = &mxlk_epf->mxlk;
+#else
+	struct pci_dev *pdev = container_of(dev, struct pci_dev, dev);
+	struct mxlk_pcie *xdev = pci_get_drvdata(pdev);
+	struct mxlk *mxlk = &xdev->mxlk;
+#endif
+
+	rc = kstrtol(buf, 10, &value);
+	if (rc)
+		return rc;
+
+	mxlk->debug_enable = value ? true : false;
+
+	if (!mxlk->debug_enable)
+		memset(&mxlk->stats, 0, sizeof(struct mxlk_debug_stats));
+
+	return count;
+}
+
+void mxlk_init_debug(struct mxlk *mxlk, struct device *dev)
+{
+	DEVICE_ATTR_RW(debug);
+	memset(&mxlk->stats, 0, sizeof(struct mxlk_debug_stats));
+	mxlk->debug = dev_attr_debug;
+	device_create_file(dev, &mxlk->debug);
+}
+
+void mxlk_uninit_debug(struct mxlk *mxlk, struct device *dev)
+{
+	device_remove_file(dev, &mxlk->debug);
+	memset(&mxlk->stats, 0, sizeof(struct mxlk_debug_stats));
+}
+
+void mxlk_debug_incr(struct mxlk *mxlk, size_t *attr, size_t v)
+{
+	if (unlikely(mxlk->debug_enable))
+		*attr += v;
+}
diff --git a/drivers/misc/xlink-pcie/common/mxlk_util.h b/drivers/misc/xlink-pcie/common/mxlk_util.h
new file mode 100644
index 000000000000..3b3da4cbf247
--- /dev/null
+++ b/drivers/misc/xlink-pcie/common/mxlk_util.h
@@ -0,0 +1,69 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_UTIL_HEADER_
+#define MXLK_UTIL_HEADER_
+
+#include "mxlk.h"
+
+enum mxlk_doorbell_direction {
+	TO_DEVICE,
+	FROM_DEVICE
+};
+
+enum mxlk_doorbell_type {
+	DATA_SENT,
+	DATA_RECEIVED,
+	DEV_EVENT
+};
+
+enum mxlk_event_type {
+	NO_OP,
+	REQUEST_RESET,
+	DEV_SHUTDOWN
+};
+
+void mxlk_set_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
+		       enum mxlk_doorbell_type type, u8 value);
+u8 mxlk_get_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
+		     enum mxlk_doorbell_type type);
+
+void mxlk_set_device_status(struct mxlk *mxlk, u32 status);
+u32 mxlk_get_device_status(struct mxlk *mxlk);
+u32 mxlk_get_host_status(struct mxlk *mxlk);
+void mxlk_set_host_status(struct mxlk *mxlk, u32 status);
+
+struct mxlk_buf_desc *mxlk_alloc_bd(size_t length);
+struct mxlk_buf_desc *mxlk_alloc_bd_reuse(size_t length, void *virt,
+					  dma_addr_t phys);
+void mxlk_free_bd(struct mxlk_buf_desc *bd);
+
+int mxlk_list_init(struct mxlk_list *list);
+void mxlk_list_cleanup(struct mxlk_list *list);
+int mxlk_list_put(struct mxlk_list *list, struct mxlk_buf_desc *bd);
+int mxlk_list_put_head(struct mxlk_list *list, struct mxlk_buf_desc *bd);
+struct mxlk_buf_desc *mxlk_list_get(struct mxlk_list *list);
+void mxlk_list_info(struct mxlk_list *list, size_t *bytes, size_t *buffers);
+
+struct mxlk_buf_desc *mxlk_alloc_rx_bd(struct mxlk *mxlk);
+void mxlk_free_rx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd);
+struct mxlk_buf_desc *mxlk_alloc_tx_bd(struct mxlk *mxlk);
+void mxlk_free_tx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd);
+
+int mxlk_interface_init(struct mxlk *mxlk, int id);
+void mxlk_interface_cleanup(struct mxlk_interface *inf);
+void mxlk_interfaces_cleanup(struct mxlk *mxlk);
+int mxlk_interfaces_init(struct mxlk *mxlk);
+void mxlk_add_bd_to_interface(struct mxlk *mxlk, struct mxlk_buf_desc *bd);
+
+void mxlk_init_debug(struct mxlk *mxlk, struct device *dev);
+void mxlk_uninit_debug(struct mxlk *mxlk, struct device *dev);
+void mxlk_debug_incr(struct mxlk *mxlk, size_t *attr, size_t v);
+
+#endif // MXLK_UTIL_HEADER_
diff --git a/drivers/misc/xlink-pcie/local_host/Makefile b/drivers/misc/xlink-pcie/local_host/Makefile
new file mode 100644
index 000000000000..05fbf4e71000
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/Makefile
@@ -0,0 +1,9 @@
+ccflags-y += -Wall -Wno-unused-function -Werror -DXLINK_PCIE_LOCAL=1
+ccflags-y += -I$(srctree)/drivers/pci/controller/dwc
+
+obj-$(CONFIG_XLINK_PCIE_LH_DRIVER) += mxlk_ep.o
+mxlk_ep-objs := mxlk_epf.o
+mxlk_ep-objs += mxlk_core.o
+mxlk_ep-objs += mxlk_dma.o
+mxlk_ep-objs += mxlk_inf.o
+mxlk_ep-objs += ../common/mxlk_util.o
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_core.c b/drivers/misc/xlink-pcie/local_host/mxlk_core.c
new file mode 100644
index 000000000000..744636752fa4
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_core.c
@@ -0,0 +1,797 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include <linux/uaccess.h>
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+
+#include "../common/mxlk_core.h"
+#include "../common/mxlk_util.h"
+#include "../common/mxlk_capabilities.h"
+#include "mxlk_epf.h"
+#include "mxlk_struct.h"
+
+static struct mxlk *global_mxlk;
+
+#define MXLK_CIRCULAR_INC(val, max) (((val) + 1) & (max - 1))
+
+static int rx_pool_size = SZ_32M;
+module_param(rx_pool_size, int, 0664);
+MODULE_PARM_DESC(rx_pool_size, "receiving pool size (default 32 MiB)");
+
+static int tx_pool_size = SZ_32M;
+module_param(tx_pool_size, int, 0664);
+MODULE_PARM_DESC(tx_pool_size, "transmitting pool size (default 32 MiB)");
+
+static int fragment_size = MXLK_FRAGMENT_SIZE;
+module_param(fragment_size, int, 0664);
+MODULE_PARM_DESC(fragment_size, "transfer descriptor size (default 128 KiB)");
+
+static bool tx_pool_coherent = true;
+module_param(tx_pool_coherent, bool, 0664);
+MODULE_PARM_DESC(tx_pool_coherent, "transmitting pool using coherent memory (default true)");
+
+static bool rx_pool_coherent;
+module_param(rx_pool_coherent, bool, 0664);
+MODULE_PARM_DESC(rx_pool_coherent, "receiving pool using coherent memory (default false)");
+
+static int mxlk_map_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
+			int direction)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct pci_epf *epf = mxlk_epf->epf;
+	struct device *dma_dev = epf->epc->dev.parent;
+
+	bd->phys = dma_map_single(dma_dev, bd->data, bd->length, direction);
+
+	return dma_mapping_error(dma_dev, bd->phys);
+}
+
+static void mxlk_unmap_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
+			   int direction)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct pci_epf *epf = mxlk_epf->epf;
+	struct device *dma_dev = epf->epc->dev.parent;
+
+	dma_unmap_single(dma_dev, bd->phys, bd->length, direction);
+}
+
+static void mxlk_set_cap_txrx(struct mxlk *mxlk)
+{
+	struct mxlk_cap_txrx *cap;
+	struct mxlk_cap_hdr *hdr;
+	uint32_t start = sizeof(struct mxlk_mmio);
+	size_t hdr_len = sizeof(struct mxlk_cap_txrx);
+	size_t tx_len = sizeof(struct mxlk_transfer_desc) * MXLK_NUM_TX_DESCS;
+	size_t rx_len = sizeof(struct mxlk_transfer_desc) * MXLK_NUM_RX_DESCS;
+	uint16_t next = (uint16_t)(start + hdr_len + tx_len + rx_len);
+
+	mxlk->mmio->cap_offset = start;
+	cap = (void *)mxlk->mmio + start;
+	memset(cap, 0, sizeof(struct mxlk_cap_txrx));
+	cap->hdr.id = MXLK_CAP_TXRX;
+	cap->hdr.next = next;
+	cap->fragment_size = fragment_size;
+	cap->tx.ring = start + hdr_len;
+	cap->tx.ndesc = MXLK_NUM_TX_DESCS;
+	cap->rx.ring = start + hdr_len + tx_len;
+	cap->rx.ndesc = MXLK_NUM_RX_DESCS;
+
+	hdr = (struct mxlk_cap_hdr *)((void *)mxlk->mmio + next);
+	hdr->id = MXLK_CAP_NULL;
+}
+
+static int mxlk_set_version(struct mxlk *mxlk)
+{
+	struct mxlk_version version;
+
+	version.major = MXLK_VERSION_MAJOR;
+	version.minor = MXLK_VERSION_MINOR;
+	version.build = MXLK_VERSION_BUILD;
+
+	memcpy(&mxlk->mmio->version, &version, sizeof(version));
+
+	dev_info(mxlk_to_dev(mxlk), "ver: device %u.%u.%u\n",
+		 version.major, version.minor, version.build);
+
+	return 0;
+}
+
+static void mxlk_txrx_cleanup(struct mxlk *mxlk)
+{
+	int index;
+	struct mxlk_transfer_desc *td;
+	struct mxlk_stream *tx = &mxlk->tx;
+	struct mxlk_stream *rx = &mxlk->rx;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
+	struct mxlk_interface *inf = &mxlk->interfaces[0];
+
+	mxlk->stop_flag = true;
+	mxlk->no_tx_buffer = false;
+	inf->data_available = true;
+	wake_up_interruptible(&mxlk->tx_waitqueue);
+	wake_up_interruptible(&inf->rx_waitqueue);
+	mutex_lock(&mxlk->wlock);
+	mutex_lock(&inf->rlock);
+
+	for (index = 0; index < rx->pipe.ndesc; index++) {
+		td = rx->pipe.tdr + index;
+		mxlk_set_td_address(td, 0);
+		mxlk_set_td_length(td, 0);
+	}
+	for (index = 0; index < tx->pipe.ndesc; index++) {
+		td = tx->pipe.tdr + index;
+		mxlk_set_td_address(td, 0);
+		mxlk_set_td_length(td, 0);
+	}
+
+	mxlk_list_cleanup(&mxlk->tx_pool);
+	mxlk_list_cleanup(&mxlk->rx_pool);
+
+	if (rx_pool_coherent && mxlk_epf->rx_virt) {
+		dma_free_coherent(dma_dev, mxlk_epf->rx_size,
+				  mxlk_epf->rx_virt, mxlk_epf->rx_phys);
+	}
+
+	if (tx_pool_coherent && mxlk_epf->tx_virt) {
+		dma_free_coherent(dma_dev, mxlk_epf->tx_size,
+				  mxlk_epf->tx_virt, mxlk_epf->tx_phys);
+	}
+
+	mutex_unlock(&inf->rlock);
+	mutex_unlock(&mxlk->wlock);
+}
+
+/*
+ * The RX/TX are named for Remote Host, in Local Host RX/TX is reversed.
+ */
+static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
+{
+	int index;
+	int ndesc;
+	struct mxlk_buf_desc *bd;
+	struct mxlk_stream *tx = &mxlk->tx;
+	struct mxlk_stream *rx = &mxlk->rx;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
+
+	mxlk->txrx = cap;
+	mxlk->fragment_size = cap->fragment_size;
+	mxlk->stop_flag = false;
+
+	rx->pipe.ndesc = cap->tx.ndesc;
+	rx->pipe.head = &cap->tx.head;
+	rx->pipe.tail = &cap->tx.tail;
+	rx->pipe.tdr = (void *)mxlk->mmio + cap->tx.ring;
+
+	tx->pipe.ndesc = cap->rx.ndesc;
+	tx->pipe.head = &cap->rx.head;
+	tx->pipe.tail = &cap->rx.tail;
+	tx->pipe.tdr = (void *)mxlk->mmio + cap->rx.ring;
+
+	mxlk_list_init(&mxlk->rx_pool);
+	rx_pool_size = roundup(rx_pool_size, mxlk->fragment_size);
+	ndesc = rx_pool_size / mxlk->fragment_size;
+
+	if (rx_pool_coherent) {
+		mxlk_epf->rx_size = rx_pool_size;
+		mxlk_epf->rx_virt = dma_alloc_coherent(dma_dev,
+			mxlk_epf->rx_size, &mxlk_epf->rx_phys, GFP_KERNEL);
+		if (!mxlk_epf->rx_virt)
+			goto error;
+	}
+
+	for (index = 0; index < ndesc; index++) {
+		if (rx_pool_coherent) {
+			bd = mxlk_alloc_bd_reuse(mxlk->fragment_size,
+			mxlk_epf->rx_virt + index * mxlk->fragment_size,
+			mxlk_epf->rx_phys + index * mxlk->fragment_size);
+		} else {
+			bd = mxlk_alloc_bd(mxlk->fragment_size);
+		}
+		if (bd) {
+			mxlk_list_put(&mxlk->rx_pool, bd);
+		} else {
+			dev_err(mxlk_to_dev(mxlk),
+				"failed to alloc all rx pool descriptors\n");
+			goto error;
+		}
+	}
+
+	mxlk_list_init(&mxlk->tx_pool);
+	tx_pool_size = roundup(tx_pool_size, mxlk->fragment_size);
+	ndesc = tx_pool_size / mxlk->fragment_size;
+
+	if (tx_pool_coherent) {
+		mxlk_epf->tx_size = tx_pool_size;
+		mxlk_epf->tx_virt = dma_alloc_coherent(dma_dev,
+			mxlk_epf->tx_size, &mxlk_epf->tx_phys, GFP_KERNEL);
+		if (!mxlk_epf->tx_virt)
+			goto error;
+	}
+
+	for (index = 0; index < ndesc; index++) {
+		if (tx_pool_coherent) {
+			bd = mxlk_alloc_bd_reuse(mxlk->fragment_size,
+			mxlk_epf->tx_virt + index * mxlk->fragment_size,
+			mxlk_epf->tx_phys + index * mxlk->fragment_size);
+		} else {
+			bd = mxlk_alloc_bd(mxlk->fragment_size);
+		}
+		if (bd) {
+			mxlk_list_put(&mxlk->tx_pool, bd);
+		} else {
+			dev_err(mxlk_to_dev(mxlk),
+				"failed to alloc all tx pool descriptors\n");
+			goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	mxlk_txrx_cleanup(mxlk);
+
+	return -ENOMEM;
+}
+
+static int mxlk_discover_txrx(struct mxlk *mxlk)
+{
+	int error;
+	struct mxlk_cap_txrx *cap;
+
+	cap = mxlk_cap_find(mxlk, 0, MXLK_CAP_TXRX);
+	if (cap) {
+		error = mxlk_txrx_init(mxlk, cap);
+	} else {
+		dev_err(mxlk_to_dev(mxlk), "mxlk txrx info not found\n");
+		error = -EIO;
+	}
+
+	return error;
+}
+
+static void mxlk_start_tx(struct mxlk *mxlk, unsigned long delay)
+{
+	if (mxlk->legacy_a0)
+		queue_delayed_work(mxlk->rx_wq, &mxlk->tx_event, delay);
+	else
+		queue_delayed_work(mxlk->tx_wq, &mxlk->tx_event, delay);
+}
+
+static void mxlk_start_rx(struct mxlk *mxlk, unsigned long delay)
+{
+	queue_delayed_work(mxlk->rx_wq, &mxlk->rx_event, delay);
+}
+
+static void mxlk_rx_event_handler(struct work_struct *work)
+{
+	struct mxlk *mxlk = container_of(work, struct mxlk, rx_event.work);
+
+	int rc;
+	u16 interface;
+	u32 head, tail, ndesc, length;
+	u64 address;
+	u32 initial_head;
+	int descs_num = 0;
+	int chan = 0;
+	struct mxlk_stream *rx = &mxlk->rx;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct mxlk_dma_ll_desc *desc;
+	struct mxlk_buf_desc *bd_head, *bd_tail, *bd;
+	struct mxlk_transfer_desc *td;
+	unsigned long delay = msecs_to_jiffies(1);
+	bool reset_work = false;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.rx_event_runs, 1);
+
+	if (mxlk_get_host_status(mxlk) != MXLK_STATUS_RUN)
+		return;
+
+	bd_head = bd_tail = NULL;
+	ndesc = rx->pipe.ndesc;
+	tail = mxlk_get_tdr_tail(&rx->pipe);
+	initial_head = head = mxlk_get_tdr_head(&rx->pipe);
+
+	while (head != tail) {
+		td = rx->pipe.tdr + head;
+
+		bd = mxlk_alloc_rx_bd(mxlk);
+		if (!bd) {
+			reset_work = true;
+			if (descs_num == 0) {
+				delay = msecs_to_jiffies(10);
+				goto task_exit;
+			}
+			break;
+		}
+
+		interface = mxlk_get_td_interface(td);
+		length = mxlk_get_td_length(td);
+		address = mxlk_get_td_address(td);
+
+		bd->length = length;
+		bd->interface = interface;
+		if (!rx_pool_coherent) {
+			rc = mxlk_map_dma(mxlk, bd, DMA_FROM_DEVICE);
+			if (rc) {
+				dev_err(mxlk_to_dev(mxlk),
+					"failed to map rx bd (%d)\n", rc);
+				mxlk_free_rx_bd(mxlk, bd);
+				break;
+			}
+		}
+
+		desc = &mxlk_epf->rx_desc_buf[chan].virt[descs_num++];
+		desc->dma_transfer_size = length;
+		desc->dst_addr = bd->phys;
+		desc->src_addr = address;
+
+		if (bd_head)
+			bd_tail->next = bd;
+		else
+			bd_head = bd;
+		bd_tail = bd;
+
+		head = MXLK_CIRCULAR_INC(head, ndesc);
+	}
+
+	if (descs_num == 0)
+		goto task_exit;
+
+	rc = mxlk_copy_from_host_ll(mxlk, chan, descs_num);
+
+	bd = bd_head;
+	while (bd && !rx_pool_coherent) {
+		mxlk_unmap_dma(mxlk, bd, DMA_FROM_DEVICE);
+		bd = bd->next;
+	}
+
+	if (rc) {
+		dev_err(mxlk_to_dev(mxlk),
+			"failed to DMA from host (%d)\n", rc);
+		mxlk_free_rx_bd(mxlk, bd_head);
+		delay = msecs_to_jiffies(5);
+		reset_work = true;
+		goto task_exit;
+	}
+
+	head = initial_head;
+	bd = bd_head;
+	while (bd) {
+		td = rx->pipe.tdr + head;
+		bd_head = bd_head->next;
+		bd->next = NULL;
+
+		if (likely(bd->interface < MXLK_NUM_INTERFACES)) {
+			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.cnts, 1);
+			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.bytes,
+					bd->length);
+
+			mxlk_set_td_status(td, MXLK_DESC_STATUS_SUCCESS);
+			mxlk_add_bd_to_interface(mxlk, bd);
+		} else {
+			dev_err(mxlk_to_dev(mxlk),
+				"detected rx desc interface failure (%u)\n",
+				bd->interface);
+			mxlk_set_td_status(td, MXLK_DESC_STATUS_ERROR);
+			mxlk_free_rx_bd(mxlk, bd);
+		}
+
+		bd = bd_head;
+		head = MXLK_CIRCULAR_INC(head, ndesc);
+	}
+
+	if (head != initial_head) {
+		mxlk_set_tdr_head(&rx->pipe, head);
+		mxlk_raise_irq(mxlk, DATA_RECEIVED);
+	}
+
+task_exit:
+	if (reset_work)
+		mxlk_start_rx(mxlk, delay);
+}
+
+static void mxlk_tx_event_handler(struct work_struct *work)
+{
+	struct mxlk *mxlk = container_of(work, struct mxlk, tx_event.work);
+
+	int rc;
+	u32 head, tail, ndesc;
+	u64 address;
+	u32 initial_tail;
+	int descs_num = 0;
+	int chan = 0;
+	struct mxlk_stream *tx = &mxlk->tx;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct mxlk_dma_ll_desc *desc;
+	struct mxlk_buf_desc *bd_head, *bd_tail, *bd;
+	struct mxlk_transfer_desc *td;
+	size_t bytes = 0, buffers = 0;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.tx_event_runs, 1);
+
+	if (mxlk_get_host_status(mxlk) != MXLK_STATUS_RUN)
+		return;
+
+	bd_head = bd_tail = NULL;
+	ndesc = tx->pipe.ndesc;
+	initial_tail = tail = mxlk_get_tdr_tail(&tx->pipe);
+	head = mxlk_get_tdr_head(&tx->pipe);
+
+	// add new entries
+	while (MXLK_CIRCULAR_INC(tail, ndesc) != head) {
+		bd = mxlk_list_get(&mxlk->write);
+		if (!bd)
+			break;
+
+		if (!tx_pool_coherent) {
+			if (mxlk_map_dma(mxlk, bd, DMA_TO_DEVICE)) {
+				dev_err(mxlk_to_dev(mxlk),
+				"dma mapping error bd addr %p, size %zu\n",
+				bd->data, bd->length);
+				mxlk_list_put_head(&mxlk->write, bd);
+				break;
+			}
+		}
+
+		td = tx->pipe.tdr + tail;
+		address = mxlk_get_td_address(td);
+
+		desc = &mxlk_epf->tx_desc_buf[chan].virt[descs_num++];
+		desc->dma_transfer_size = bd->length;
+		desc->src_addr = bd->phys;
+		desc->dst_addr = address;
+
+		if (bd_head)
+			bd_tail->next = bd;
+		else
+			bd_head = bd;
+		bd_tail = bd;
+
+		tail = MXLK_CIRCULAR_INC(tail, ndesc);
+	}
+
+	if (descs_num == 0)
+		goto task_exit;
+
+	rc = mxlk_copy_to_host_ll(mxlk, chan, descs_num);
+
+	tail = initial_tail;
+	bd = bd_head;
+	while (bd) {
+		if (!tx_pool_coherent)
+			mxlk_unmap_dma(mxlk, bd, DMA_TO_DEVICE);
+
+		if (rc) {
+			bd = bd->next;
+			continue;
+		}
+
+		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.cnts, 1);
+		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.bytes, bd->length);
+
+		td = tx->pipe.tdr + tail;
+		mxlk_set_td_status(td, MXLK_DESC_STATUS_SUCCESS);
+		mxlk_set_td_length(td, bd->length);
+		mxlk_set_td_interface(td, bd->interface);
+
+		bd = bd->next;
+		tail = MXLK_CIRCULAR_INC(tail, ndesc);
+	}
+
+	if (rc) {
+		dev_err(mxlk_to_dev(mxlk), "failed to DMA to host (%d)\n", rc);
+		mxlk_list_put_head(&mxlk->write, bd_head);
+		return;
+	}
+
+	mxlk_free_tx_bd(mxlk, bd_head);
+
+	if (mxlk_get_tdr_tail(&tx->pipe) != tail) {
+		mxlk_set_tdr_tail(&tx->pipe, tail);
+		mxlk_raise_irq(mxlk, DATA_SENT);
+		mxlk_debug_incr(mxlk, &mxlk->stats.send_ints, 1);
+	}
+
+task_exit:
+	mxlk_list_info(&mxlk->write, &bytes, &buffers);
+	if (buffers) {
+		mxlk->tx_pending = true;
+		head = mxlk_get_tdr_head(&tx->pipe);
+		if (MXLK_CIRCULAR_INC(tail, ndesc) != head)
+			mxlk_start_tx(mxlk, 0);
+	} else {
+		mxlk->tx_pending = false;
+	}
+}
+
+static irqreturn_t mxlk_core_irq_cb(int irq, void *args)
+{
+	struct mxlk *mxlk = args;
+
+	if (mxlk_get_doorbell(mxlk, TO_DEVICE, DATA_SENT)) {
+		mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_SENT, 0);
+		mxlk_debug_incr(mxlk, &mxlk->stats.interrupts, 1);
+		mxlk_start_rx(mxlk, 0);
+	}
+	if (mxlk_get_doorbell(mxlk, TO_DEVICE, DATA_RECEIVED)) {
+		mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_RECEIVED, 0);
+		if (mxlk->tx_pending)
+			mxlk_start_tx(mxlk, 0);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int mxlk_events_init(struct mxlk *mxlk)
+{
+	mxlk->rx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
+	if (!mxlk->rx_wq) {
+		dev_err(mxlk_to_dev(mxlk), "failed to allocate workqueue\n");
+		return -ENOMEM;
+	}
+
+	if (!mxlk->legacy_a0) {
+		mxlk->tx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
+		if (!mxlk->tx_wq) {
+			dev_err(mxlk_to_dev(mxlk),
+				"failed to allocate workqueue\n");
+			destroy_workqueue(mxlk->rx_wq);
+			return -ENOMEM;
+		}
+	}
+
+	INIT_DELAYED_WORK(&mxlk->rx_event, mxlk_rx_event_handler);
+	INIT_DELAYED_WORK(&mxlk->tx_event, mxlk_tx_event_handler);
+
+	return 0;
+}
+
+static void mxlk_events_cleanup(struct mxlk *mxlk)
+{
+	cancel_delayed_work_sync(&mxlk->rx_event);
+	cancel_delayed_work_sync(&mxlk->tx_event);
+
+	destroy_workqueue(mxlk->rx_wq);
+	if (!mxlk->legacy_a0)
+		destroy_workqueue(mxlk->tx_wq);
+}
+
+int mxlk_core_init(struct mxlk *mxlk)
+{
+	int error;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+
+	mxlk_init_debug(mxlk, &mxlk_epf->epf->dev);
+
+	global_mxlk = mxlk;
+
+	mxlk_set_version(mxlk);
+	mxlk_set_cap_txrx(mxlk);
+
+	error = mxlk_events_init(mxlk);
+	if (error)
+		return error;
+
+	error = mxlk_discover_txrx(mxlk);
+	if (error)
+		goto error_txrx;
+
+	mxlk_interfaces_init(mxlk);
+
+	mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_SENT, 0);
+	mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_RECEIVED, 0);
+	mxlk_set_doorbell(mxlk, TO_DEVICE, DEV_EVENT, NO_OP);
+	mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_SENT, 0);
+	mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_RECEIVED, 0);
+	mxlk_set_doorbell(mxlk, FROM_DEVICE, DEV_EVENT, NO_OP);
+
+	mxlk_register_host_irq(mxlk, mxlk_core_irq_cb);
+
+	return 0;
+
+error_txrx:
+	mxlk_events_cleanup(mxlk);
+
+	return error;
+}
+
+void mxlk_core_cleanup(struct mxlk *mxlk)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+
+	if (mxlk->status == MXLK_STATUS_RUN) {
+		mxlk_events_cleanup(mxlk);
+		mxlk_interfaces_cleanup(mxlk);
+		mxlk_txrx_cleanup(mxlk);
+	}
+
+	mxlk_uninit_debug(mxlk, &mxlk_epf->epf->dev);
+}
+
+int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
+		   unsigned int timeout_ms)
+{
+	int ret = 0;
+	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	size_t len = *length;
+	size_t remaining = len;
+	struct mxlk_buf_desc *bd;
+	unsigned long jiffies_start = jiffies;
+	long jiffies_passed = 0;
+	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
+
+	*length = 0;
+	if (len == 0)
+		return -EINVAL;
+
+	if (mxlk->status != MXLK_STATUS_RUN)
+		return -ENODEV;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.cnts, 1);
+
+	ret = mutex_lock_interruptible(&inf->rlock);
+	if (ret < 0)
+		return -EINTR;
+
+	do {
+		while (!inf->data_available) {
+			mutex_unlock(&inf->rlock);
+			if (timeout_ms == 0) {
+				ret = wait_event_interruptible(
+					inf->rx_waitqueue, inf->data_available);
+			} else {
+				ret = wait_event_interruptible_timeout(
+					inf->rx_waitqueue, inf->data_available,
+					jiffies_timeout - jiffies_passed);
+				if (ret == 0)
+					return -ETIME;
+			}
+			if (ret < 0 || mxlk->stop_flag)
+				return -EINTR;
+
+			ret = mutex_lock_interruptible(&inf->rlock);
+			if (ret < 0)
+				return -EINTR;
+		}
+
+		bd = (inf->partial_read) ? inf->partial_read :
+					   mxlk_list_get(&inf->read);
+
+		while (remaining && bd) {
+			size_t bcopy;
+
+			bcopy = min(remaining, bd->length);
+			memcpy(buffer, bd->data, bcopy);
+
+			buffer += bcopy;
+			remaining -= bcopy;
+			bd->data += bcopy;
+			bd->length -= bcopy;
+
+			mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.bytes, bcopy);
+
+			if (bd->length == 0) {
+				mxlk_free_rx_bd(mxlk, bd);
+				bd = mxlk_list_get(&inf->read);
+			}
+		}
+
+		// save for next time
+		inf->partial_read = bd;
+
+		if (!bd)
+			inf->data_available = false;
+
+		*length = len - remaining;
+
+		jiffies_passed = (long)jiffies - (long)jiffies_start;
+	} while (remaining > 0 && (jiffies_passed < jiffies_timeout ||
+				   timeout_ms == 0));
+
+	mutex_unlock(&inf->rlock);
+
+	return 0;
+}
+
+int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
+		    unsigned int timeout_ms)
+{
+	int ret;
+	size_t len = *length;
+	size_t remaining = len;
+	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	struct mxlk_buf_desc *bd, *head;
+	unsigned long jiffies_start = jiffies;
+	long jiffies_passed = 0;
+	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
+
+	*length = 0;
+	if (len == 0)
+		return -EINVAL;
+
+	if (mxlk->status != MXLK_STATUS_RUN)
+		return -ENODEV;
+
+	if (mxlk_get_host_status(mxlk) != MXLK_STATUS_RUN)
+		return -ENODEV;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.cnts, 1);
+
+	ret = mutex_lock_interruptible(&mxlk->wlock);
+	if (ret < 0)
+		return -EINTR;
+
+	do {
+		bd = head = mxlk_alloc_tx_bd(mxlk);
+		while (!head) {
+			mutex_unlock(&mxlk->wlock);
+			if (timeout_ms == 0) {
+				ret = wait_event_interruptible(
+						mxlk->tx_waitqueue,
+						!mxlk->no_tx_buffer);
+			} else {
+				ret = wait_event_interruptible_timeout(
+					mxlk->tx_waitqueue, !mxlk->no_tx_buffer,
+					jiffies_timeout - jiffies_passed);
+				if (ret == 0)
+					return -ETIME;
+			}
+			if (ret < 0 || mxlk->stop_flag)
+				return -EINTR;
+
+			ret = mutex_lock_interruptible(&mxlk->wlock);
+			if (ret < 0)
+				return -EINTR;
+
+			bd = head = mxlk_alloc_tx_bd(mxlk);
+		}
+
+		while (remaining && bd) {
+			size_t bcopy;
+
+			bcopy = min(bd->length, remaining);
+			memcpy(bd->data, buffer, bcopy);
+
+			buffer += bcopy;
+			remaining -= bcopy;
+			bd->length = bcopy;
+			bd->interface = inf->id;
+
+			mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.bytes, bcopy);
+
+			if (remaining) {
+				bd->next = mxlk_alloc_tx_bd(mxlk);
+				bd = bd->next;
+			}
+		}
+
+		mxlk_list_put(&inf->mxlk->write, head);
+		mxlk_start_tx(mxlk, 0);
+
+		*length = len - remaining;
+
+		jiffies_passed = (long)jiffies - (long)jiffies_start;
+	} while (remaining > 0 && (jiffies_passed < jiffies_timeout ||
+				   timeout_ms == 0));
+
+	mutex_unlock(&mxlk->wlock);
+
+	return 0;
+}
+
+struct mxlk *mxlk_core_get_by_id(uint32_t sw_device_id)
+{
+	return (sw_device_id == xlink_sw_id) ? global_mxlk : NULL;
+}
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_dma.c b/drivers/misc/xlink-pcie/local_host/mxlk_dma.c
new file mode 100644
index 000000000000..fa6c4484568d
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_dma.c
@@ -0,0 +1,556 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include <linux/delay.h>
+
+#include "mxlk_dma.h"
+#include "mxlk_struct.h"
+#include "../common/mxlk.h"
+
+#define DMA_DBI_OFFSET (0x380000)
+
+/* PCIe DMA control 1 register definitions. */
+#define DMA_CH_CONTROL1_CB_SHIFT (0)
+#define DMA_CH_CONTROL1_TCB_SHIFT (1)
+#define DMA_CH_CONTROL1_LLP_SHIFT (2)
+#define DMA_CH_CONTROL1_LIE_SHIFT (3)
+#define DMA_CH_CONTROL1_CS_SHIFT (5)
+#define DMA_CH_CONTROL1_CCS_SHIFT (8)
+#define DMA_CH_CONTROL1_LLE_SHIFT (9)
+#define DMA_CH_CONTROL1_CB_MASK (BIT(DMA_CH_CONTROL1_CB_SHIFT))
+#define DMA_CH_CONTROL1_TCB_MASK (BIT(DMA_CH_CONTROL1_TCB_SHIFT))
+#define DMA_CH_CONTROL1_LLP_MASK (BIT(DMA_CH_CONTROL1_LLP_SHIFT))
+#define DMA_CH_CONTROL1_LIE_MASK (BIT(DMA_CH_CONTROL1_LIE_SHIFT))
+#define DMA_CH_CONTROL1_CS_MASK (0x3 << DMA_CH_CONTROL1_CS_SHIFT)
+#define DMA_CH_CONTROL1_CCS_MASK (BIT(DMA_CH_CONTROL1_CCS_SHIFT))
+#define DMA_CH_CONTROL1_LLE_MASK (BIT(DMA_CH_CONTROL1_LLE_SHIFT))
+
+/* DMA control 1 register Channel Status */
+#define DMA_CH_CONTROL1_CS_RUNNING (0x1 << DMA_CH_CONTROL1_CS_SHIFT)
+#define DMA_CH_CONTROL1_CS_HALTED  (0x2 << DMA_CH_CONTROL1_CS_SHIFT)
+#define DMA_CH_CONTROL1_CS_STOPPED (0x3 << DMA_CH_CONTROL1_CS_SHIFT)
+
+/* PCIe DMA Engine enable register definitions. */
+#define DMA_ENGINE_EN_SHIFT (0)
+#define DMA_ENGINE_EN_MASK (BIT(DMA_ENGINE_EN_SHIFT))
+
+/* PCIe DMA interrupt registers definitions. */
+#define DMA_ABORT_INTERRUPT_SHIFT (16)
+#define DMA_ABORT_INTERRUPT_MASK (0xFF << DMA_ABORT_INTERRUPT_SHIFT)
+#define DMA_ABORT_INTERRUPT_CH_MASK(_c) (BIT(_c) << DMA_ABORT_INTERRUPT_SHIFT)
+#define DMA_DONE_INTERRUPT_MASK (0xFF)
+#define DMA_DONE_INTERRUPT_CH_MASK(_c) (BIT(_c))
+#define DMA_DONE_ABORT_INTERRUPT_CH_MASK(_c)				\
+	(DMA_DONE_INTERRUPT_CH_MASK(_c) | DMA_ABORT_INTERRUPT_CH_MASK(_c))
+#define DMA_ALL_INTERRUPT_MASK                                            \
+	(DMA_ABORT_INTERRUPT_MASK | DMA_DONE_INTERRUPT_MASK)
+
+#define DMA_LL_ERROR_SHIFT (16)
+#define DMA_CPL_ABORT_SHIFT (8)
+#define DMA_CPL_TIMEOUT_SHIFT (16)
+#define DMA_DATA_POI_SHIFT (24)
+#define DMA_AR_ERROR_CH_MASK(_c) (BIT(_c))
+#define DMA_LL_ERROR_CH_MASK(_c) (BIT(_c) << DMA_LL_ERROR_SHIFT)
+#define DMA_UNREQ_ERROR_CH_MASK(_c) (BIT(_c))
+#define DMA_CPL_ABORT_ERROR_CH_MASK(_c) (BIT(_c) << DMA_CPL_ABORT_SHIFT)
+#define DMA_CPL_TIMEOUT_ERROR_CH_MASK(_c) (BIT(_c) << DMA_CPL_TIMEOUT_SHIFT)
+#define DMA_DATA_POI_ERROR_CH_MASK(_c) (BIT(_c) << DMA_DATA_POI_SHIFT)
+
+#define DMA_LLLAIE_SHIFT (16)
+#define DMA_LLLAIE_MASK (0xF << DMA_LLLAIE_SHIFT)
+
+#define DMA_CHAN_WRITE_MAX_WEIGHT (0x7)
+#define DMA_CHAN_READ_MAX_WEIGHT (0x3)
+#define DMA_CHAN0_WEIGHT_OFFSET (0)
+#define DMA_CHAN1_WEIGHT_OFFSET (5)
+#define DMA_CHAN2_WEIGHT_OFFSET (10)
+#define DMA_CHAN3_WEIGHT_OFFSET (15)
+#define DMA_CHAN_WRITE_ALL_MAX_WEIGHT					\
+	((DMA_CHAN_WRITE_MAX_WEIGHT << DMA_CHAN0_WEIGHT_OFFSET) |	\
+	 (DMA_CHAN_WRITE_MAX_WEIGHT << DMA_CHAN1_WEIGHT_OFFSET) |	\
+	 (DMA_CHAN_WRITE_MAX_WEIGHT << DMA_CHAN2_WEIGHT_OFFSET) |	\
+	 (DMA_CHAN_WRITE_MAX_WEIGHT << DMA_CHAN3_WEIGHT_OFFSET))
+#define DMA_CHAN_READ_ALL_MAX_WEIGHT					\
+	((DMA_CHAN_READ_MAX_WEIGHT << DMA_CHAN0_WEIGHT_OFFSET) |	\
+	 (DMA_CHAN_READ_MAX_WEIGHT << DMA_CHAN1_WEIGHT_OFFSET) |	\
+	 (DMA_CHAN_READ_MAX_WEIGHT << DMA_CHAN2_WEIGHT_OFFSET) |	\
+	 (DMA_CHAN_READ_MAX_WEIGHT << DMA_CHAN3_WEIGHT_OFFSET))
+
+#define PCIE_REGS_PCIE_APP_CNTRL 0x8
+#define APP_XFER_PENDING BIT(6)
+#define PCIE_REGS_PCIE_SII_PM_STATE_1 0xb4
+#define PM_LINKST_IN_L1 BIT(10)
+
+struct __packed pcie_dma_reg {
+	u32 dma_ctrl_data_arb_prior;
+	u32 reserved1;
+	u32 dma_ctrl;
+	u32 dma_write_engine_en;
+	u32 dma_write_doorbell;
+	u32 reserved2;
+	u32 dma_write_channel_arb_weight_low;
+	u32 dma_write_channel_arb_weight_high;
+	u32 reserved3[3];
+	u32 dma_read_engine_en;
+	u32 dma_read_doorbell;
+	u32 reserved4;
+	u32 dma_read_channel_arb_weight_low;
+	u32 dma_read_channel_arb_weight_high;
+	u32 reserved5[3];
+	u32 dma_write_int_status;
+	u32 reserved6;
+	u32 dma_write_int_mask;
+	u32 dma_write_int_clear;
+	u32 dma_write_err_status;
+	u32 dma_write_done_imwr_low;
+	u32 dma_write_done_imwr_high;
+	u32 dma_write_abort_imwr_low;
+	u32 dma_write_abort_imwr_high;
+	u16 dma_write_ch_imwr_data[8];
+	u32 reserved7[4];
+	u32 dma_write_linked_list_err_en;
+	u32 reserved8[3];
+	u32 dma_read_int_status;
+	u32 reserved9;
+	u32 dma_read_int_mask;
+	u32 dma_read_int_clear;
+	u32 reserved10;
+	u32 dma_read_err_status_low;
+	u32 dma_read_err_status_high;
+	u32 reserved11[2];
+	u32 dma_read_linked_list_err_en;
+	u32 reserved12;
+	u32 dma_read_done_imwr_low;
+	u32 dma_read_done_imwr_high;
+	u32 dma_read_abort_imwr_low;
+	u32 dma_read_abort_imwr_high;
+	u16 dma_read_ch_imwr_data[8];
+};
+
+struct __packed pcie_dma_chan {
+	u32 dma_ch_control1;
+	u32 reserved1;
+	u32 dma_transfer_size;
+	u32 dma_sar_low;
+	u32 dma_sar_high;
+	u32 dma_dar_low;
+	u32 dma_dar_high;
+	u32 dma_llp_low;
+	u32 dma_llp_high;
+};
+
+enum mxlk_ep_engine_type {
+	WRITE_ENGINE,
+	READ_ENGINE
+};
+
+#define DMA_CHAN_NUM (4)
+
+static u32 dma_chan_offset[2][DMA_CHAN_NUM] = {
+	{ 0x200, 0x400, 0x600, 0x800 },
+	{ 0x300, 0x500, 0x700, 0x900 }
+};
+
+static void __iomem *mxlk_ep_get_dma_base(struct pci_epf *epf)
+{
+	struct pci_epc *epc = epf->epc;
+	struct dw_pcie_ep *ep = epc_get_drvdata(epc);
+	struct dw_pcie *pci = to_dw_pcie_from_ep(ep);
+
+	return pci->dbi_base + DMA_DBI_OFFSET;
+}
+
+static int mxlk_ep_dma_disable(void __iomem *dma_base,
+			       enum mxlk_ep_engine_type rw)
+{
+	int i;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)(dma_base);
+	void __iomem *engine_en = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_engine_en :
+					&dma_reg->dma_read_engine_en;
+	void __iomem *int_mask = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_int_mask :
+					&dma_reg->dma_read_int_mask;
+	void __iomem *int_clear = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_int_clear :
+					&dma_reg->dma_read_int_clear;
+	void __iomem *ll_err = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_linked_list_err_en :
+					&dma_reg->dma_read_linked_list_err_en;
+
+	iowrite32(0x0, engine_en);
+
+	/* Mask all interrupts. */
+	iowrite32(DMA_ALL_INTERRUPT_MASK, int_mask);
+
+	/* Clear all interrupts. */
+	iowrite32(DMA_ALL_INTERRUPT_MASK, int_clear);
+
+	/* Disable LL abort interrupt (LLLAIE). */
+	iowrite32(0, ll_err);
+
+	/* Wait until the engine is disabled. */
+	for (i = 0; i < 1000; i++) {
+		if (!(ioread32(engine_en) & DMA_ENGINE_EN_MASK))
+			return 0;
+		msleep(20);
+	}
+
+	return -EBUSY;
+}
+
+static void mxlk_ep_dma_enable(void __iomem *dma_base,
+			       enum mxlk_ep_engine_type rw)
+{
+	int i;
+	u32 offset;
+	struct pcie_dma_chan *dma_chan;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)(dma_base);
+	void __iomem *engine_en = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_engine_en :
+					&dma_reg->dma_read_engine_en;
+	void __iomem *int_mask = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_int_mask :
+					&dma_reg->dma_read_int_mask;
+	void __iomem *int_clear = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_int_clear :
+					&dma_reg->dma_read_int_clear;
+	void __iomem *ll_err = (rw == WRITE_ENGINE) ?
+					&dma_reg->dma_write_linked_list_err_en :
+					&dma_reg->dma_read_linked_list_err_en;
+	void __iomem *arb_weight = (rw == WRITE_ENGINE) ?
+				&dma_reg->dma_write_channel_arb_weight_low :
+				&dma_reg->dma_read_channel_arb_weight_low;
+	u32 weight = (rw == WRITE_ENGINE) ? DMA_CHAN_WRITE_ALL_MAX_WEIGHT :
+					    DMA_CHAN_READ_ALL_MAX_WEIGHT;
+
+
+	iowrite32(DMA_ENGINE_EN_MASK, engine_en);
+
+	/* Unmask all interrupts, so that the interrupt line gets asserted. */
+	iowrite32(~(u32)DMA_ALL_INTERRUPT_MASK, int_mask);
+
+	/* Clear all interrupts. */
+	iowrite32(DMA_ALL_INTERRUPT_MASK, int_clear);
+
+	/* Set channel round robin weight. */
+	iowrite32(weight, arb_weight);
+
+	/* Enable LL abort interrupt (LLLAIE). */
+	iowrite32(DMA_LLLAIE_MASK, ll_err);
+
+	/* Enable linked list mode. */
+	for (i = 0; i < DMA_CHAN_NUM; i++) {
+		offset = dma_chan_offset[rw][i];
+		dma_chan = (struct pcie_dma_chan *)(dma_base + offset);
+		iowrite32(DMA_CH_CONTROL1_LLE_MASK, &dma_chan->dma_ch_control1);
+	}
+}
+
+/*
+ * Make sure EP is not in L1 state when DMA doorbell.
+ * The DMA controller may start the wrong channel if doorbell occurs at the
+ * same time as controller is transitioning to L1.
+ */
+static int mxlk_ep_dma_doorbell(struct mxlk_epf *mxlk_epf, int chan,
+				void __iomem *doorbell)
+{
+	int rc = 0;
+	int i = 20;
+	u32 val, pm_val;
+
+	val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
+	iowrite32(val | APP_XFER_PENDING,
+		  mxlk_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
+	pm_val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_SII_PM_STATE_1);
+	while (pm_val & PM_LINKST_IN_L1) {
+		if (i-- < 0) {
+			rc = -ETIME;
+			break;
+		}
+		udelay(5);
+		pm_val = ioread32(mxlk_epf->apb_base +
+				  PCIE_REGS_PCIE_SII_PM_STATE_1);
+	}
+
+	iowrite32((u32)chan, doorbell);
+
+	iowrite32(val & ~APP_XFER_PENDING,
+		  mxlk_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
+
+	return rc;
+}
+
+static int mxlk_ep_dma_err_status(void __iomem *err_status, int chan)
+{
+	if (ioread32(err_status) &
+	    (DMA_AR_ERROR_CH_MASK(chan) | DMA_LL_ERROR_CH_MASK(chan)))
+		return -EIO;
+
+	return 0;
+}
+
+static int mxlk_ep_dma_rd_err_status_high(void __iomem *err_status, int chan)
+{
+	if (ioread32(err_status) &
+	    (DMA_UNREQ_ERROR_CH_MASK(chan) |
+	     DMA_CPL_ABORT_ERROR_CH_MASK(chan) |
+	     DMA_CPL_TIMEOUT_ERROR_CH_MASK(chan) |
+	     DMA_DATA_POI_ERROR_CH_MASK(chan)))
+		return -EIO;
+
+	return 0;
+}
+
+static void mxlk_ep_dma_setup_ll_descs(struct pcie_dma_chan *dma_chan,
+				       struct mxlk_dma_ll_desc_buf *desc_buf,
+				       int descs_num)
+{
+	int i = 0;
+	struct mxlk_dma_ll_desc *descs = desc_buf->virt;
+
+	/* Setup linked list descriptors */
+	for (i = 0; i < descs_num; i++)
+		descs[i].dma_ch_control1 = DMA_CH_CONTROL1_CB_MASK;
+	descs[descs_num - 1].dma_ch_control1 |= DMA_CH_CONTROL1_LIE_MASK;
+	descs[descs_num].dma_ch_control1 = DMA_CH_CONTROL1_LLP_MASK |
+					   DMA_CH_CONTROL1_TCB_MASK;
+	descs[descs_num].src_addr = (phys_addr_t)desc_buf->phys;
+
+	/* Setup linked list settings */
+	iowrite32(DMA_CH_CONTROL1_LLE_MASK | DMA_CH_CONTROL1_CCS_MASK,
+		  &dma_chan->dma_ch_control1);
+	iowrite32((u32)desc_buf->phys, &dma_chan->dma_llp_low);
+	iowrite32((u64)desc_buf->phys >> 32, &dma_chan->dma_llp_high);
+
+}
+
+int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
+{
+	int i, rc = 0;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	void __iomem *dma_base = mxlk_epf->dma_base;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
+	struct pcie_dma_chan *dma_chan;
+	struct mxlk_dma_ll_desc_buf *desc_buf;
+
+	if (descs_num <= 0 || descs_num > MXLK_NUM_TX_DESCS)
+		return -EINVAL;
+
+	if (chan < 0 || chan >= DMA_CHAN_NUM)
+		return -EINVAL;
+
+	dma_chan = (struct pcie_dma_chan *)
+		(dma_base + dma_chan_offset[WRITE_ENGINE][chan]);
+
+	desc_buf = &mxlk_epf->tx_desc_buf[chan];
+
+	mxlk_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
+
+	/* Start DMA transfer. */
+	rc = mxlk_ep_dma_doorbell(mxlk_epf, chan, &dma_reg->dma_write_doorbell);
+	if (rc)
+		return rc;
+
+	/* Wait for DMA transfer to complete. */
+	for (i = 0; i < 1000000; i++) {
+		usleep_range(5, 10);
+		if (ioread32(&dma_reg->dma_write_int_status) &
+		    DMA_DONE_ABORT_INTERRUPT_CH_MASK(chan))
+			break;
+	}
+	if (i == 1000000) {
+		rc = -ETIME;
+		goto cleanup;
+	}
+
+	rc = mxlk_ep_dma_err_status(&dma_reg->dma_write_err_status, chan);
+
+cleanup:
+	/* Clear the done/abort interrupt. */
+	iowrite32(DMA_DONE_ABORT_INTERRUPT_CH_MASK(chan),
+		  &dma_reg->dma_write_int_clear);
+
+	if (rc) {
+		mxlk_ep_dma_disable(dma_base, WRITE_ENGINE);
+		mxlk_ep_dma_enable(dma_base, WRITE_ENGINE);
+	}
+
+	return rc;
+}
+
+int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
+{
+	int i, rc = 0;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	void __iomem *dma_base = mxlk_epf->dma_base;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
+	struct pcie_dma_chan *dma_chan;
+	struct mxlk_dma_ll_desc_buf *desc_buf;
+
+	if (descs_num <= 0 || descs_num > MXLK_NUM_RX_DESCS)
+		return -EINVAL;
+
+	if (chan < 0 || chan >= DMA_CHAN_NUM)
+		return -EINVAL;
+
+	dma_chan = (struct pcie_dma_chan *)
+		(dma_base + dma_chan_offset[READ_ENGINE][chan]);
+
+	desc_buf = &mxlk_epf->rx_desc_buf[chan];
+
+	mxlk_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
+
+	/* Start DMA transfer. */
+	rc = mxlk_ep_dma_doorbell(mxlk_epf, chan, &dma_reg->dma_read_doorbell);
+	if (rc)
+		return rc;
+
+	/* Wait for DMA transfer to complete. */
+	for (i = 0; i < 1000000; i++) {
+		usleep_range(5, 10);
+		if (ioread32(&dma_reg->dma_read_int_status) &
+		    DMA_DONE_ABORT_INTERRUPT_CH_MASK(chan))
+			break;
+	}
+	if (i == 1000000) {
+		rc = -ETIME;
+		goto cleanup;
+	}
+
+	rc = mxlk_ep_dma_err_status(&dma_reg->dma_read_err_status_low, chan);
+	if (!rc) {
+		rc = mxlk_ep_dma_rd_err_status_high(
+			&dma_reg->dma_read_err_status_high, chan);
+	}
+cleanup:
+	/* Clear the done/abort interrupt. */
+	iowrite32(DMA_DONE_ABORT_INTERRUPT_CH_MASK(chan),
+		  &dma_reg->dma_read_int_clear);
+
+	if (rc) {
+		mxlk_ep_dma_disable(dma_base, READ_ENGINE);
+		mxlk_ep_dma_enable(dma_base, READ_ENGINE);
+	}
+
+	return rc;
+}
+
+static void mxlk_ep_dma_free_ll_descs_mem(struct mxlk_epf *mxlk_epf)
+{
+	int i;
+	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
+
+	for (i = 0; i < DMA_CHAN_NUM; i++) {
+		if (mxlk_epf->tx_desc_buf[i].virt) {
+			dma_free_coherent(dma_dev,
+					  mxlk_epf->tx_desc_buf[i].size,
+					  mxlk_epf->tx_desc_buf[i].virt,
+					  mxlk_epf->tx_desc_buf[i].phys);
+		}
+		if (mxlk_epf->rx_desc_buf[i].virt) {
+			dma_free_coherent(dma_dev,
+					  mxlk_epf->rx_desc_buf[i].size,
+					  mxlk_epf->rx_desc_buf[i].virt,
+					  mxlk_epf->rx_desc_buf[i].phys);
+		}
+
+		memset(&mxlk_epf->tx_desc_buf[i], 0,
+		       sizeof(struct mxlk_dma_ll_desc_buf));
+		memset(&mxlk_epf->rx_desc_buf[i], 0,
+		       sizeof(struct mxlk_dma_ll_desc_buf));
+	}
+}
+
+static int mxlk_ep_dma_alloc_ll_descs_mem(struct mxlk_epf *mxlk_epf)
+{
+	int i;
+	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
+	int tx_num = MXLK_NUM_TX_DESCS + 1;
+	int rx_num = MXLK_NUM_RX_DESCS + 1;
+	size_t tx_size = tx_num * sizeof(struct mxlk_dma_ll_desc);
+	size_t rx_size = rx_num * sizeof(struct mxlk_dma_ll_desc);
+
+	for (i = 0; i < DMA_CHAN_NUM; i++) {
+		mxlk_epf->tx_desc_buf[i].virt =
+			dma_alloc_coherent(dma_dev, tx_size,
+					   &mxlk_epf->tx_desc_buf[i].phys,
+					   GFP_KERNEL);
+		mxlk_epf->rx_desc_buf[i].virt =
+			dma_alloc_coherent(dma_dev, rx_size,
+					   &mxlk_epf->rx_desc_buf[i].phys,
+					   GFP_KERNEL);
+
+		if (!mxlk_epf->tx_desc_buf[i].virt ||
+		    !mxlk_epf->rx_desc_buf[i].virt) {
+			mxlk_ep_dma_free_ll_descs_mem(mxlk_epf);
+			return -ENOMEM;
+		}
+
+		mxlk_epf->tx_desc_buf[i].size = tx_size;
+		mxlk_epf->rx_desc_buf[i].size = rx_size;
+	}
+	return 0;
+}
+
+bool mxlk_ep_dma_enabled(struct pci_epf *epf)
+{
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)
+					mxlk_epf->dma_base;
+	void __iomem *w_engine_en = &dma_reg->dma_write_engine_en;
+	void __iomem *r_engine_en = &dma_reg->dma_read_engine_en;
+
+	return (ioread32(w_engine_en) & DMA_ENGINE_EN_MASK) &&
+		(ioread32(r_engine_en) & DMA_ENGINE_EN_MASK);
+}
+
+int mxlk_ep_dma_reset(struct pci_epf *epf)
+{
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	/* Disable the DMA read/write engine. */
+	if (mxlk_ep_dma_disable(mxlk_epf->dma_base, WRITE_ENGINE) ||
+	    mxlk_ep_dma_disable(mxlk_epf->dma_base, READ_ENGINE))
+		return -EBUSY;
+
+	mxlk_ep_dma_enable(mxlk_epf->dma_base, WRITE_ENGINE);
+	mxlk_ep_dma_enable(mxlk_epf->dma_base, READ_ENGINE);
+
+	return 0;
+}
+
+int mxlk_ep_dma_uninit(struct pci_epf *epf)
+{
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	if (mxlk_ep_dma_disable(mxlk_epf->dma_base, WRITE_ENGINE) ||
+	    mxlk_ep_dma_disable(mxlk_epf->dma_base, READ_ENGINE))
+		return -EBUSY;
+
+	mxlk_ep_dma_free_ll_descs_mem(mxlk_epf);
+
+	return 0;
+}
+
+int mxlk_ep_dma_init(struct pci_epf *epf)
+{
+	int rc = 0;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	mxlk_epf->dma_base = mxlk_ep_get_dma_base(epf);
+
+	rc = mxlk_ep_dma_alloc_ll_descs_mem(mxlk_epf);
+	if (rc)
+		return rc;
+
+	return mxlk_ep_dma_reset(epf);
+}
+
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_dma.h b/drivers/misc/xlink-pcie/local_host/mxlk_dma.h
new file mode 100644
index 000000000000..cd75666d3f66
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_dma.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_DMA_HEADER_
+#define MXLK_DMA_HEADER_
+
+#include <linux/types.h>
+#include <linux/pci-epc.h>
+#include <linux/pci-epf.h>
+
+int mxlk_ep_dma_init(struct pci_epf *epf);
+int mxlk_ep_dma_uninit(struct pci_epf *epf);
+int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num);
+int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num);
+bool mxlk_ep_dma_enabled(struct pci_epf *epf);
+int mxlk_ep_dma_reset(struct pci_epf *epf);
+
+#endif // MXLK_DMA_HEADER_
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_epf.c b/drivers/misc/xlink-pcie/local_host/mxlk_epf.c
new file mode 100644
index 000000000000..0ea94c8b0fec
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_epf.c
@@ -0,0 +1,497 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/jiffies.h>
+#include <linux/compiler.h>
+#include <linux/pci_ids.h>
+#include <linux/reboot.h>
+#include <linux/xlink_drv_inf.h>
+#include "../common/mxlk.h"
+#include "../common/mxlk_core.h"
+#include "../common/mxlk_util.h"
+#include "../common/mxlk_boot.h"
+#include "mxlk_struct.h"
+#include "mxlk_dma.h"
+
+#define BAR2_MIN_SIZE SZ_16K
+#define BAR4_MIN_SIZE SZ_16K
+#define KMB_EP_OUTBOUND_MAP_MIN_SIZE SZ_16K
+
+#define PCIE_REGS_PCIE_SYS_CFG_CORE 0x7c
+#define PCIE_CFG_PBUS_NUM_OFFSET 8
+#define PCIE_CFG_PBUS_NUM_MASK 0xFF
+#define PCIE_CFG_PBUS_DEV_NUM_OFFSET 16
+#define PCIE_CFG_PBUS_DEV_NUM_MASK 0x1F
+
+#define PCIE_REGS_PCIE_INTR_ENABLE 0x18
+#define PCIE_REGS_PCIE_INTR_FLAGS 0x1c
+#define LBC_CII_EVENT_FLAG BIT(18)
+#define PCIE_REGS_MEM_ACCESS_IRQ_VECTOR	0x180
+#define PCIE_REGS_PCIE_ERR_INTR_FLAGS 0x24
+#define LINK_REQ_RST_FLG BIT(15)
+
+static struct pci_epf_header mxlk_pcie_header = {
+	.vendorid = PCI_VENDOR_ID_INTEL,
+	.deviceid = PCI_DEVICE_ID_INTEL_KEEMBAY,
+	.baseclass_code = PCI_BASE_CLASS_MULTIMEDIA,
+	.subclass_code = 0x0,
+	.subsys_vendor_id = 0x0,
+	.subsys_id = 0x0,
+};
+
+static const struct pci_epf_device_id mxlk_pcie_epf_ids[] = {
+	{
+		.name = "mxlk_pcie_epf",
+	},
+	{},
+};
+
+u32 xlink_sw_id;
+
+static irqreturn_t mxlk_err_interrupt(int irq, void *args)
+{
+	struct mxlk *mxlk = args;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	u32 val;
+
+	val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_ERR_INTR_FLAGS);
+	if (val & LINK_REQ_RST_FLG)
+		mxlk_ep_dma_reset(mxlk_epf->epf);
+
+	iowrite32(val, mxlk_epf->apb_base + PCIE_REGS_PCIE_ERR_INTR_FLAGS);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t mxlk_host_interrupt(int irq, void *args)
+{
+	struct mxlk *mxlk = args;
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	u32 val;
+	u8 event;
+
+	val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_INTR_FLAGS);
+	if (val & LBC_CII_EVENT_FLAG) {
+		iowrite32(LBC_CII_EVENT_FLAG,
+			  mxlk_epf->apb_base + PCIE_REGS_PCIE_INTR_FLAGS);
+
+		event = mxlk_get_doorbell(mxlk, TO_DEVICE, DEV_EVENT);
+		if (unlikely(event != NO_OP)) {
+			mxlk_set_doorbell(mxlk, TO_DEVICE, DEV_EVENT, NO_OP);
+			if (event == REQUEST_RESET)
+				orderly_reboot();
+			return IRQ_HANDLED;
+		}
+
+		if (likely(mxlk_epf->core_irq_callback))
+			mxlk_epf->core_irq_callback(irq, mxlk);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void mxlk_register_host_irq(struct mxlk *mxlk, irq_handler_t func)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+
+	mxlk_epf->core_irq_callback = func;
+}
+
+int mxlk_raise_irq(struct mxlk *mxlk, enum mxlk_doorbell_type type)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct pci_epf *epf = mxlk_epf->epf;
+
+	mxlk_set_doorbell(mxlk, FROM_DEVICE, type, 1);
+
+	return pci_epc_raise_irq(epf->epc, epf->func_no, PCI_EPC_IRQ_MSI, 1);
+}
+
+static void __iomem *mxlk_epc_alloc_addr(struct pci_epc *epc,
+					 phys_addr_t *phys_addr, size_t size)
+{
+	void __iomem *virt_addr;
+	unsigned long flags;
+
+	spin_lock_irqsave(&epc->lock, flags);
+	virt_addr = pci_epc_mem_alloc_addr(epc, phys_addr, size);
+	spin_unlock_irqrestore(&epc->lock, flags);
+
+	return virt_addr;
+}
+
+static void mxlk_epc_free_addr(struct pci_epc *epc, phys_addr_t phys_addr,
+			       void __iomem *virt_addr, size_t size)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&epc->lock, flags);
+	pci_epc_mem_free_addr(epc, phys_addr, virt_addr, size);
+	spin_unlock_irqrestore(&epc->lock, flags);
+}
+
+int mxlk_copy_from_host_ll(struct mxlk *mxlk, int chan, int descs_num)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct pci_epf *epf = mxlk_epf->epf;
+
+	return mxlk_ep_dma_read_ll(epf, chan, descs_num);
+}
+
+int mxlk_copy_to_host_ll(struct mxlk *mxlk, int chan, int descs_num)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct pci_epf *epf = mxlk_epf->epf;
+
+	return mxlk_ep_dma_write_ll(epf, chan, descs_num);
+}
+
+static int mxlk_check_bar(struct pci_epf *epf, struct pci_epf_bar *epf_bar,
+			  enum pci_barno barno, size_t size, u8 reserved_bar)
+{
+	if (reserved_bar & (1 << barno)) {
+		dev_err(&epf->dev, "BAR%d is already reserved\n", barno);
+		return -EFAULT;
+	}
+
+	if (epf_bar->size != 0 && epf_bar->size < size) {
+		dev_err(&epf->dev, "BAR%d fixed size is not enough\n", barno);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int mxlk_configure_bar(struct pci_epf *epf,
+			      const struct pci_epc_features *epc_features)
+{
+	struct pci_epf_bar *epf_bar;
+	bool bar_fixed_64bit;
+	int i;
+	int ret;
+
+	for (i = BAR_0; i <= BAR_5; i++) {
+		epf_bar = &epf->bar[i];
+		bar_fixed_64bit = !!(epc_features->bar_fixed_64bit & (1 << i));
+		if (bar_fixed_64bit)
+			epf_bar->flags |= PCI_BASE_ADDRESS_MEM_TYPE_64;
+		if (epc_features->bar_fixed_size[i])
+			epf_bar->size = epc_features->bar_fixed_size[i];
+
+		if (i == BAR_2) {
+			ret = mxlk_check_bar(epf, epf_bar, BAR_2,
+					     BAR2_MIN_SIZE,
+					     epc_features->reserved_bar);
+			if (ret)
+				return ret;
+		}
+
+		if (i == BAR_4) {
+			ret = mxlk_check_bar(epf, epf_bar, BAR_4,
+					     BAR4_MIN_SIZE,
+					     epc_features->reserved_bar);
+			if (ret)
+				return ret;
+		}
+	}
+
+	return 0;
+}
+
+static void mxlk_cleanup_bar(struct pci_epf *epf, enum pci_barno barno)
+{
+	struct pci_epc *epc = epf->epc;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	if (mxlk_epf->vaddr[barno]) {
+		pci_epc_clear_bar(epc, epf->func_no, &epf->bar[barno]);
+		pci_epf_free_space(epf, mxlk_epf->vaddr[barno], barno);
+	}
+
+	mxlk_epf->vaddr[barno] = NULL;
+}
+
+static void mxlk_cleanup_bars(struct pci_epf *epf)
+{
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	mxlk_cleanup_bar(epf, BAR_2);
+	mxlk_cleanup_bar(epf, BAR_4);
+	mxlk_epf->mxlk.io_comm = NULL;
+	mxlk_epf->mxlk.mmio = NULL;
+	mxlk_epf->mxlk.bar4 = NULL;
+}
+
+static int mxlk_setup_bar(struct pci_epf *epf, enum pci_barno barno,
+			  size_t min_size, size_t align)
+{
+	int ret;
+	void *vaddr = NULL;
+	struct pci_epc *epc = epf->epc;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct pci_epf_bar *bar = &epf->bar[barno];
+
+	bar->flags |= PCI_BASE_ADDRESS_MEM_TYPE_64;
+	if (!bar->size)
+		bar->size = min_size;
+
+	if (barno == BAR_4)
+		bar->flags |= PCI_BASE_ADDRESS_MEM_PREFETCH;
+
+	vaddr = pci_epf_alloc_space(epf, bar->size, barno, align);
+
+	if (!vaddr) {
+		dev_err(&epf->dev, "Failed to map BAR%d\n", barno);
+		return -ENOMEM;
+	}
+
+	ret = pci_epc_set_bar(epc, epf->func_no, bar);
+
+	if (ret) {
+		pci_epf_free_space(epf, vaddr, barno);
+		dev_err(&epf->dev, "Failed to set BAR%d\n", barno);
+		return ret;
+	}
+
+	mxlk_epf->vaddr[barno] = vaddr;
+
+	return 0;
+}
+
+static int mxlk_setup_bars(struct pci_epf *epf, size_t align)
+{
+	int ret;
+
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	ret = mxlk_setup_bar(epf, BAR_2, BAR2_MIN_SIZE, align);
+	if (ret)
+		return ret;
+
+	ret = mxlk_setup_bar(epf, BAR_4, BAR4_MIN_SIZE, align);
+	if (ret) {
+		mxlk_cleanup_bar(epf, BAR_2);
+		return ret;
+	}
+
+	mxlk_epf->comm_bar = BAR_2;
+	mxlk_epf->mxlk.io_comm = mxlk_epf->vaddr[BAR_2];
+	mxlk_epf->mxlk.mmio = (void *)mxlk_epf->mxlk.io_comm + MXLK_MMIO_OFFSET;
+
+	mxlk_epf->bar4 = BAR_4;
+	mxlk_epf->mxlk.bar4 = mxlk_epf->vaddr[BAR_4];
+
+	return 0;
+}
+
+static int epf_bind(struct pci_epf *epf)
+{
+	struct pci_epc *epc = epf->epc;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct dw_pcie_ep *ep = epc_get_drvdata(epc);
+	struct dw_pcie *pci = to_dw_pcie_from_ep(ep);
+	struct keembay_pcie *keembay = to_keembay_pcie(pci);
+	const struct pci_epc_features *features;
+	bool msi_capable = true;
+	size_t align = 0;
+	int ret;
+	u32 bus_num = 0;
+	u32 dev_num = 0;
+
+	if (WARN_ON_ONCE(!epc))
+		return -EINVAL;
+
+	features = pci_epc_get_features(epc, epf->func_no);
+	mxlk_epf->epc_features = features;
+	if (features) {
+		msi_capable = features->msi_capable;
+		align = features->align;
+		ret = mxlk_configure_bar(epf, features);
+		if (ret)
+			return ret;
+	}
+
+	ret = mxlk_setup_bars(epf, align);
+	if (ret) {
+		dev_err(&epf->dev, "BAR initialization failed\n");
+		return ret;
+	}
+
+	mxlk_epf->irq = keembay->ev_irq;
+	mxlk_epf->irq_dma = keembay->irq;
+	mxlk_epf->irq_err = keembay->err_irq;
+	mxlk_epf->apb_base = keembay->base;
+	if (!strcmp(keembay->stepping, "A0")) {
+		mxlk_epf->mxlk.legacy_a0 = true;
+		mxlk_epf->mxlk.mmio->legacy_a0 = 1;
+	} else {
+		mxlk_epf->mxlk.legacy_a0 = false;
+		mxlk_epf->mxlk.mmio->legacy_a0 = 0;
+	}
+
+	ret = mxlk_ep_dma_init(epf);
+	if (ret) {
+		dev_err(&epf->dev, "DMA initialization failed\n");
+		goto bind_error;
+	}
+
+	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_READY);
+
+	ret = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_SYS_CFG_CORE);
+	bus_num = (ret >> PCIE_CFG_PBUS_NUM_OFFSET) & PCIE_CFG_PBUS_NUM_MASK;
+	dev_num = (ret >> PCIE_CFG_PBUS_DEV_NUM_OFFSET) &
+			PCIE_CFG_PBUS_DEV_NUM_MASK;
+
+	xlink_sw_id = (XLINK_DEV_INF_PCIE << XLINK_DEV_INF_TYPE_SHIFT) |
+		   ((bus_num << 8 | dev_num) << XLINK_DEV_PHYS_ID_SHIFT) |
+		   (XLINK_DEV_TYPE_KMB << XLINK_DEV_TYPE_SHIFT) |
+		   (XLINK_DEV_SLICE_0 << XLINK_DEV_SLICE_ID_SHIFT) |
+		   (XLINK_DEV_FUNC_VPU << XLINK_DEV_FUNC_SHIFT);
+
+	ret = mxlk_core_init(&mxlk_epf->mxlk);
+	if (ret) {
+		dev_err(&epf->dev, "Core component configuration failed\n");
+		goto bind_error;
+	}
+
+	/* Enable interrupt */
+	writel(LBC_CII_EVENT_FLAG,
+	       mxlk_epf->apb_base + PCIE_REGS_PCIE_INTR_ENABLE);
+	ret = request_irq(mxlk_epf->irq, &mxlk_host_interrupt,
+			  0, MXLK_DRIVER_NAME, &mxlk_epf->mxlk);
+	if (ret) {
+		dev_err(&epf->dev, "failed to request irq\n");
+		goto bind_error;
+	}
+
+	ret = request_irq(mxlk_epf->irq_err, &mxlk_err_interrupt, 0,
+			  MXLK_DRIVER_NAME, &mxlk_epf->mxlk);
+	if (ret) {
+		dev_err(&epf->dev, "failed to request error irq\n");
+		free_irq(mxlk_epf->irq, &mxlk_epf->mxlk);
+		goto bind_error;
+	}
+
+	if (!mxlk_ep_dma_enabled(mxlk_epf->epf))
+		mxlk_ep_dma_reset(mxlk_epf->epf);
+
+	mxlk_epf->mxlk.mmio->host_status = MXLK_STATUS_UNINIT;
+	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_RUN);
+	mxlk_set_doorbell(&mxlk_epf->mxlk, FROM_DEVICE, DEV_EVENT, NO_OP);
+	strncpy(mxlk_epf->mxlk.io_comm->magic, MXLK_BOOT_MAGIC_YOCTO,
+		strlen(MXLK_BOOT_MAGIC_YOCTO));
+
+	return 0;
+
+bind_error:
+	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_ERROR);
+	strncpy(mxlk_epf->mxlk.io_comm->magic, MXLK_BOOT_MAGIC_YOCTO,
+		strlen(MXLK_BOOT_MAGIC_YOCTO));
+
+	return ret;
+}
+
+static void epf_unbind(struct pci_epf *epf)
+{
+	struct pci_epc *epc = epf->epc;
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	free_irq(mxlk_epf->irq, &mxlk_epf->mxlk);
+	free_irq(mxlk_epf->irq_err, &mxlk_epf->mxlk);
+
+	mxlk_core_cleanup(&mxlk_epf->mxlk);
+	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_READY);
+
+	mxlk_ep_dma_uninit(epf);
+
+	pci_epc_stop(epc);
+
+	mxlk_cleanup_bars(epf);
+}
+
+static void epf_linkup(struct pci_epf *epf)
+{
+}
+
+static int epf_probe(struct pci_epf *epf)
+{
+	struct mxlk_epf *mxlk_epf;
+	struct device *dev = &epf->dev;
+
+	mxlk_epf = devm_kzalloc(dev, sizeof(*mxlk_epf), GFP_KERNEL);
+	if (!mxlk_epf)
+		return -ENOMEM;
+
+	epf->header = &mxlk_pcie_header;
+	mxlk_epf->epf = epf;
+
+	epf_set_drvdata(epf, mxlk_epf);
+
+	return 0;
+}
+
+static void epf_shutdown(struct device *dev)
+{
+	struct pci_epf *epf = to_pci_epf(dev);
+	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+
+	/*
+	 * Notify host in case PCIe hot plug not supported
+	 */
+	if (mxlk_epf && mxlk_epf->mxlk.status == MXLK_STATUS_RUN) {
+		mxlk_set_doorbell(&mxlk_epf->mxlk, FROM_DEVICE, DEV_EVENT,
+				  DEV_SHUTDOWN);
+		pci_epc_raise_irq(epf->epc, epf->func_no, PCI_EPC_IRQ_MSI, 1);
+	}
+}
+
+static struct pci_epf_ops ops = {
+	.bind = epf_bind,
+	.unbind = epf_unbind,
+	.linkup = epf_linkup,
+};
+
+static struct pci_epf_driver mxlk_pcie_epf_driver = {
+	.driver.name = "mxlk_pcie_epf",
+	.driver.shutdown = epf_shutdown,
+	.probe = epf_probe,
+	.id_table = mxlk_pcie_epf_ids,
+	.ops = &ops,
+	.owner = THIS_MODULE,
+};
+
+static int __init mxlk_epf_init(void)
+{
+	int ret = -EBUSY;
+
+	ret = pci_epf_register_driver(&mxlk_pcie_epf_driver);
+	if (ret) {
+		pr_err("Failed to register xlink pcie epf driver: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+module_init(mxlk_epf_init);
+
+static void __exit mxlk_epf_exit(void)
+{
+	pci_epf_unregister_driver(&mxlk_pcie_epf_driver);
+}
+module_exit(mxlk_epf_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION(MXLK_DRIVER_DESC);
+MODULE_VERSION(MXLK_DRIVER_VERSION);
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_epf.h b/drivers/misc/xlink-pcie/local_host/mxlk_epf.h
new file mode 100644
index 000000000000..83bf38e83e6f
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_epf.h
@@ -0,0 +1,31 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_EPF_HEADER_
+#define MXLK_EPF_HEADER_
+
+#include "../common/mxlk.h"
+#include "../common/mxlk_util.h"
+
+extern u32 xlink_sw_id;
+
+extern void mxlk_register_host_irq(struct mxlk *mxlk, irq_handler_t func);
+
+extern int mxlk_raise_irq(struct mxlk *mxlk, enum mxlk_doorbell_type type);
+
+/*
+ * These two functions are for DMA linked list mode.
+ *
+ * Caller should set the dst/src addresses and length for DMA descriptors in
+ * mxlk_epf.dma_ll_tx_descs/dma_ll_rx_descs.
+ */
+extern int mxlk_copy_from_host_ll(struct mxlk *mxlk, int chan, int descs_num);
+extern int mxlk_copy_to_host_ll(struct mxlk *mxlk, int chan, int descs_num);
+
+#endif // MXLK_EPF_HEADER_
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_inf.c b/drivers/misc/xlink-pcie/local_host/mxlk_inf.c
new file mode 100644
index 000000000000..740d72603e8c
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_inf.c
@@ -0,0 +1,119 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/xlink_drv_inf.h>
+#include "../common/mxlk_core.h"
+#include "mxlk_epf.h"
+
+int xlink_pcie_get_device_list(uint32_t *sw_device_id_list,
+			       uint32_t *num_devices)
+{
+	if (xlink_sw_id != 0) {
+		*num_devices = 1;
+		*sw_device_id_list = xlink_sw_id;
+	} else {
+		*num_devices = 0;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_get_device_list);
+
+int xlink_pcie_get_device_name(uint32_t sw_device_id, char *device_name,
+			       size_t name_size)
+{
+	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+
+	if (!mxlk)
+		return -ENODEV;
+
+	memset(device_name, 0, name_size);
+	if (name_size > strlen(MXLK_DRIVER_NAME))
+		name_size = strlen(MXLK_DRIVER_NAME);
+	strncpy(device_name, MXLK_DRIVER_NAME, name_size);
+
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_get_device_name);
+
+int xlink_pcie_get_device_status(uint32_t sw_device_id, uint32_t *device_status)
+{
+	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+
+	if (!mxlk)
+		return -ENODEV;
+
+	switch (mxlk->status) {
+	case MXLK_STATUS_READY:
+	case MXLK_STATUS_RUN:
+		*device_status = _XLINK_DEV_READY;
+		break;
+	case MXLK_STATUS_ERROR:
+		*device_status = _XLINK_DEV_ERROR;
+		break;
+	default:
+		*device_status = _XLINK_DEV_BUSY;
+		break;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_get_device_status);
+
+int xlink_pcie_boot_device(uint32_t sw_device_id, const char *binary_name)
+{
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_boot_device);
+
+int xlink_pcie_connect(uint32_t sw_device_id)
+{
+	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+
+	if (!mxlk)
+		return -ENODEV;
+
+	if (mxlk->status != MXLK_STATUS_RUN)
+		return -EIO;
+
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_connect);
+
+int xlink_pcie_read(uint32_t sw_device_id, void *data, size_t *const size,
+		    uint32_t timeout)
+{
+	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+
+	if (!mxlk)
+		return -ENODEV;
+
+	return mxlk_core_read(mxlk, data, size, timeout);
+}
+EXPORT_SYMBOL(xlink_pcie_read);
+
+int xlink_pcie_write(uint32_t sw_device_id, void *data, size_t *const size,
+		     uint32_t timeout)
+{
+	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+
+	if (!mxlk)
+		return -ENODEV;
+
+	return mxlk_core_write(mxlk, data, size, timeout);
+}
+EXPORT_SYMBOL(xlink_pcie_write);
+
+int xlink_pcie_reset_device(uint32_t sw_device_id)
+{
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_reset_device);
diff --git a/drivers/misc/xlink-pcie/local_host/mxlk_struct.h b/drivers/misc/xlink-pcie/local_host/mxlk_struct.h
new file mode 100644
index 000000000000..392a83221b8f
--- /dev/null
+++ b/drivers/misc/xlink-pcie/local_host/mxlk_struct.h
@@ -0,0 +1,77 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_STRUCT_HEADER_
+#define MXLK_STRUCT_HEADER_
+
+#include <linux/pci-epc.h>
+#include <linux/pci-epf.h>
+#include <pcie-keembay.h>
+#include "../common/mxlk.h"
+
+extern bool dma_ll_mode;
+
+struct mxlk_dma_ll_desc {
+	u32 dma_ch_control1;
+	u32 dma_transfer_size;
+	union {
+		struct {
+			u32 dma_sar_low;
+			u32 dma_sar_high;
+		};
+		phys_addr_t src_addr;
+	};
+	union {
+		struct {
+			u32 dma_dar_low;
+			u32 dma_dar_high;
+		};
+		phys_addr_t dst_addr;
+	};
+} __packed;
+
+struct mxlk_dma_ll_desc_buf {
+	struct mxlk_dma_ll_desc *virt;
+	dma_addr_t phys;
+	size_t size;
+};
+
+struct mxlk_epf {
+	struct pci_epf			*epf;
+	void				*vaddr[BAR_5 + 1];
+	enum pci_barno			comm_bar;
+	enum pci_barno			bar4;
+	const struct pci_epc_features	*epc_features;
+	struct mxlk			mxlk;
+	int				irq;
+	int				irq_dma;
+	int				irq_err;
+	void __iomem			*apb_base;
+	void __iomem			*dma_base;
+
+	irq_handler_t			core_irq_callback;
+	dma_addr_t			tx_phys;
+	void				*tx_virt;
+	size_t				tx_size;
+	dma_addr_t			rx_phys;
+	void				*rx_virt;
+	size_t				rx_size;
+
+	struct mxlk_dma_ll_desc_buf	tx_desc_buf[4];
+	struct mxlk_dma_ll_desc_buf	rx_desc_buf[4];
+};
+
+static inline struct device *mxlk_to_dev(struct mxlk *mxlk)
+{
+	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+
+	return &mxlk_epf->epf->dev;
+}
+
+#endif // MXLK_STRUCT_HEADER_
diff --git a/drivers/misc/xlink-pcie/remote_host/Makefile b/drivers/misc/xlink-pcie/remote_host/Makefile
new file mode 100644
index 000000000000..16454fc23967
--- /dev/null
+++ b/drivers/misc/xlink-pcie/remote_host/Makefile
@@ -0,0 +1,8 @@
+ccflags-y += -Wall -Wno-unused-function -Werror -DXLINK_PCIE_REMOTE=1
+
+obj-$(CONFIG_XLINK_PCIE_RH_DRIVER) += mxlk.o
+mxlk-objs := mxlk_main.o
+mxlk-objs += mxlk_pci.o
+mxlk-objs += mxlk_core.o
+mxlk-objs += mxlk_inf.o
+mxlk-objs += ../common/mxlk_util.o
diff --git a/drivers/misc/xlink-pcie/remote_host/mxlk_core.c b/drivers/misc/xlink-pcie/remote_host/mxlk_core.c
new file mode 100644
index 000000000000..72fad494eb2c
--- /dev/null
+++ b/drivers/misc/xlink-pcie/remote_host/mxlk_core.c
@@ -0,0 +1,660 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include <linux/uaccess.h>
+#include <linux/delay.h>
+
+#include "mxlk_pci.h"
+#include "../common/mxlk_core.h"
+#include "../common/mxlk_util.h"
+#include "../common/mxlk_capabilities.h"
+
+#define MXLK_CIRCULAR_INC(val, max) (((val) + 1) & (max - 1))
+
+static int rx_pool_size = SZ_32M;
+module_param(rx_pool_size, int, 0664);
+MODULE_PARM_DESC(rx_pool_size, "receive pool size (default 32 MiB)");
+
+static int tx_pool_size = SZ_32M;
+module_param(tx_pool_size, int, 0664);
+MODULE_PARM_DESC(tx_pool_size, "transmit pool size (default 32 MiB)");
+
+static int mxlk_version_check(struct mxlk *mxlk)
+{
+	struct mxlk_version version;
+
+	memcpy_fromio(&version, &mxlk->mmio->version, sizeof(version));
+
+	dev_info(mxlk_to_dev(mxlk), "ver: device %u.%u.%u, host %u.%u.%u\n",
+		 version.major, version.minor, version.build,
+		 MXLK_VERSION_MAJOR, MXLK_VERSION_MINOR, MXLK_VERSION_BUILD);
+
+	if (ioread8(&mxlk->mmio->legacy_a0))
+		mxlk->legacy_a0 = true;
+
+	return 0;
+}
+
+static int mxlk_map_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
+			int direction)
+{
+	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct device *dev = &xdev->pci->dev;
+
+	bd->phys = dma_map_single(dev, bd->data, bd->length, direction);
+
+	return dma_mapping_error(dev, bd->phys);
+}
+
+static void mxlk_unmap_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
+			   int direction)
+{
+	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct device *dev = &xdev->pci->dev;
+
+	dma_unmap_single(dev, bd->phys, bd->length, direction);
+}
+
+static void mxlk_txrx_cleanup(struct mxlk *mxlk)
+{
+	int index;
+	struct mxlk_buf_desc *bd;
+	struct mxlk_stream *tx = &mxlk->tx;
+	struct mxlk_stream *rx = &mxlk->rx;
+	struct mxlk_interface *inf = &mxlk->interfaces[0];
+
+	mxlk->stop_flag = true;
+	mxlk->no_tx_buffer = false;
+	inf->data_available = true;
+	wake_up_interruptible(&mxlk->tx_waitqueue);
+	wake_up_interruptible(&inf->rx_waitqueue);
+	mutex_lock(&mxlk->wlock);
+	mutex_lock(&inf->rlock);
+
+	if (tx->ddr) {
+		for (index = 0; index < tx->pipe.ndesc; index++) {
+			struct mxlk_transfer_desc *td = tx->pipe.tdr + index;
+
+			bd = tx->ddr[index];
+			if (bd) {
+				mxlk_unmap_dma(mxlk, bd, DMA_TO_DEVICE);
+				mxlk_free_tx_bd(mxlk, bd);
+				mxlk_set_td_address(td, 0);
+				mxlk_set_td_length(td, 0);
+			}
+		}
+		kfree(tx->ddr);
+	}
+
+	if (rx->ddr) {
+		for (index = 0; index < rx->pipe.ndesc; index++) {
+			struct mxlk_transfer_desc *td = rx->pipe.tdr + index;
+
+			bd = rx->ddr[index];
+			if (bd) {
+				mxlk_unmap_dma(mxlk, bd, DMA_FROM_DEVICE);
+				mxlk_free_rx_bd(mxlk, bd);
+				mxlk_set_td_address(td, 0);
+				mxlk_set_td_length(td, 0);
+			}
+		}
+		kfree(rx->ddr);
+	}
+
+	mxlk_list_cleanup(&mxlk->tx_pool);
+	mxlk_list_cleanup(&mxlk->rx_pool);
+
+	mutex_unlock(&inf->rlock);
+	mutex_unlock(&mxlk->wlock);
+}
+
+static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
+{
+	int rc;
+	int index;
+	int ndesc;
+	struct mxlk_buf_desc *bd;
+	struct mxlk_stream *tx = &mxlk->tx;
+	struct mxlk_stream *rx = &mxlk->rx;
+
+	mxlk->txrx = cap;
+	mxlk->fragment_size = ioread32(&cap->fragment_size);
+	mxlk->stop_flag = false;
+
+	tx->pipe.ndesc = ioread32(&cap->tx.ndesc);
+	tx->pipe.head = &cap->tx.head;
+	tx->pipe.tail = &cap->tx.tail;
+	tx->pipe.old = ioread32(&cap->tx.tail);
+	tx->pipe.tdr = (void __iomem *)mxlk->mmio + ioread32(&cap->tx.ring);
+
+	tx->ddr = kcalloc(tx->pipe.ndesc, sizeof(struct mxlk_buf_desc *),
+			  GFP_KERNEL);
+	if (!tx->ddr) {
+		rc = -ENOMEM;
+		goto error;
+	}
+
+	rx->pipe.ndesc = ioread32(&cap->rx.ndesc);
+	rx->pipe.head = &cap->rx.head;
+	rx->pipe.tail = &cap->rx.tail;
+	rx->pipe.old = ioread32(&cap->rx.head);
+	rx->pipe.tdr = (void __iomem *)mxlk->mmio + ioread32(&cap->rx.ring);
+
+	rx->ddr = kcalloc(rx->pipe.ndesc, sizeof(struct mxlk_buf_desc *),
+			  GFP_KERNEL);
+	if (!rx->ddr) {
+		rc = -ENOMEM;
+		goto error;
+	}
+
+	mxlk_list_init(&mxlk->rx_pool);
+	rx_pool_size = roundup(rx_pool_size, mxlk->fragment_size);
+	ndesc = rx_pool_size / mxlk->fragment_size;
+
+	for (index = 0; index < ndesc; index++) {
+		bd = mxlk_alloc_bd(mxlk->fragment_size);
+		if (bd) {
+			mxlk_list_put(&mxlk->rx_pool, bd);
+		} else {
+			rc = -ENOMEM;
+			goto error;
+		}
+	}
+
+	mxlk_list_init(&mxlk->tx_pool);
+	tx_pool_size = roundup(tx_pool_size, mxlk->fragment_size);
+	ndesc = tx_pool_size / mxlk->fragment_size;
+
+	for (index = 0; index < ndesc; index++) {
+		bd = mxlk_alloc_bd(mxlk->fragment_size);
+		if (bd) {
+			mxlk_list_put(&mxlk->tx_pool, bd);
+		} else {
+			rc = -ENOMEM;
+			goto error;
+		}
+	}
+
+	for (index = 0; index < rx->pipe.ndesc; index++) {
+		struct mxlk_transfer_desc *td = rx->pipe.tdr + index;
+
+		bd = mxlk_alloc_rx_bd(mxlk);
+		if (!bd) {
+			rc = -ENOMEM;
+			goto error;
+		}
+
+		if (mxlk_map_dma(mxlk, bd, DMA_FROM_DEVICE)) {
+			dev_err(mxlk_to_dev(mxlk), "failed to map rx bd\n");
+			rc = -ENOMEM;
+			goto error;
+		}
+
+		rx->ddr[index] = bd;
+		mxlk_set_td_address(td, bd->phys);
+		mxlk_set_td_length(td, bd->length);
+	}
+
+	return 0;
+
+error:
+	mxlk_txrx_cleanup(mxlk);
+
+	return rc;
+}
+
+static int mxlk_discover_txrx(struct mxlk *mxlk)
+{
+	int error;
+	struct mxlk_cap_txrx *cap;
+
+	cap = mxlk_cap_find(mxlk, 0, MXLK_CAP_TXRX);
+	if (cap)
+		error = mxlk_txrx_init(mxlk, cap);
+	else
+		error = -EIO;
+
+	return error;
+}
+
+static void mxlk_start_tx(struct mxlk *mxlk, unsigned long delay)
+{
+	queue_delayed_work(mxlk->tx_wq, &mxlk->tx_event, delay);
+}
+
+static void mxlk_start_rx(struct mxlk *mxlk, unsigned long delay)
+{
+	queue_delayed_work(mxlk->rx_wq, &mxlk->rx_event, delay);
+}
+
+static void mxlk_rx_event_handler(struct work_struct *work)
+{
+	struct mxlk *mxlk = container_of(work, struct mxlk, rx_event.work);
+
+	int rc;
+	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	u16 status, interface;
+	u32 head, tail, ndesc, length;
+	struct mxlk_stream *rx = &mxlk->rx;
+	struct mxlk_buf_desc *bd, *replacement = NULL;
+	struct mxlk_transfer_desc *td;
+	unsigned long delay = msecs_to_jiffies(1);
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.rx_event_runs, 1);
+
+	if (mxlk_get_device_status(mxlk) != MXLK_STATUS_RUN)
+		return;
+
+	ndesc = rx->pipe.ndesc;
+	tail = mxlk_get_tdr_tail(&rx->pipe);
+	head = mxlk_get_tdr_head(&rx->pipe);
+
+	while (head != tail) {
+		td = rx->pipe.tdr + head;
+		bd = rx->ddr[head];
+
+		replacement = mxlk_alloc_rx_bd(mxlk);
+		if (!replacement) {
+			delay = msecs_to_jiffies(20);
+			break;
+		}
+
+		rc = mxlk_map_dma(mxlk, replacement, DMA_FROM_DEVICE);
+		if (rc) {
+			dev_err(mxlk_to_dev(mxlk),
+				"failed to map rx bd (%d)\n", rc);
+			mxlk_free_rx_bd(mxlk, replacement);
+			break;
+		}
+
+		status = mxlk_get_td_status(td);
+		interface = mxlk_get_td_interface(td);
+		length = mxlk_get_td_length(td);
+		mxlk_unmap_dma(mxlk, bd, DMA_FROM_DEVICE);
+
+		if (unlikely(status != MXLK_DESC_STATUS_SUCCESS) ||
+		    unlikely(interface >= MXLK_NUM_INTERFACES)) {
+			dev_err(mxlk_to_dev(mxlk),
+			"detected rx desc failure, status(%u), interface(%u)\n",
+			status, interface);
+			mxlk_free_rx_bd(mxlk, bd);
+		} else {
+			bd->interface = interface;
+			bd->length = length;
+			bd->next = NULL;
+
+			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.cnts, 1);
+			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.bytes,
+					bd->length);
+
+			mxlk_add_bd_to_interface(mxlk, bd);
+		}
+
+		rx->ddr[head] = replacement;
+		mxlk_set_td_address(td, replacement->phys);
+		mxlk_set_td_length(td, replacement->length);
+		head = MXLK_CIRCULAR_INC(head, ndesc);
+	}
+
+	if (mxlk_get_tdr_head(&rx->pipe) != head) {
+		mxlk_set_tdr_head(&rx->pipe, head);
+		mxlk_pci_raise_irq(xdev, DATA_RECEIVED, 1);
+	}
+
+	if (!replacement)
+		mxlk_start_rx(mxlk, delay);
+}
+
+static void mxlk_tx_event_handler(struct work_struct *work)
+{
+	struct mxlk *mxlk = container_of(work, struct mxlk, tx_event.work);
+
+	u16 status;
+	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	u32 head, tail, old, ndesc;
+	struct mxlk_stream *tx = &mxlk->tx;
+	struct mxlk_buf_desc *bd;
+	struct mxlk_transfer_desc *td;
+	size_t bytes, buffers;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.tx_event_runs, 1);
+
+	if (mxlk_get_device_status(mxlk) != MXLK_STATUS_RUN)
+		return;
+
+	ndesc = tx->pipe.ndesc;
+	old = tx->pipe.old;
+	tail = mxlk_get_tdr_tail(&tx->pipe);
+	head = mxlk_get_tdr_head(&tx->pipe);
+
+	// clean old entries first
+	while (old != head) {
+		bd = tx->ddr[old];
+		td = tx->pipe.tdr + old;
+		status = mxlk_get_td_status(td);
+		if (status != MXLK_DESC_STATUS_SUCCESS)
+			dev_err(mxlk_to_dev(mxlk),
+				"detected tx desc failure (%u)\n", status);
+
+		mxlk_unmap_dma(mxlk, bd, DMA_TO_DEVICE);
+		mxlk_free_tx_bd(mxlk, bd);
+		tx->ddr[old] = NULL;
+		old = MXLK_CIRCULAR_INC(old, ndesc);
+	}
+	tx->pipe.old = old;
+
+	// add new entries
+	while (MXLK_CIRCULAR_INC(tail, ndesc) != head) {
+		bd = mxlk_list_get(&mxlk->write);
+		if (!bd)
+			break;
+
+		td = tx->pipe.tdr + tail;
+
+		if (mxlk_map_dma(mxlk, bd, DMA_TO_DEVICE)) {
+			dev_err(mxlk_to_dev(mxlk),
+				"dma mapping error bd addr %p, size %zu\n",
+				bd->data, bd->length);
+			break;
+		}
+
+		tx->ddr[tail] = bd;
+		mxlk_set_td_address(td, bd->phys);
+		mxlk_set_td_length(td, bd->length);
+		mxlk_set_td_interface(td, bd->interface);
+		mxlk_set_td_status(td, MXLK_DESC_STATUS_ERROR);
+
+		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.cnts, 1);
+		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.bytes, bd->length);
+
+		tail = MXLK_CIRCULAR_INC(tail, ndesc);
+	}
+
+	if (mxlk_get_tdr_tail(&tx->pipe) != tail) {
+		mxlk_set_tdr_tail(&tx->pipe, tail);
+		mxlk_pci_raise_irq(xdev, DATA_SENT, 1);
+		mxlk_debug_incr(mxlk, &mxlk->stats.send_ints, 1);
+	}
+
+	mxlk_list_info(&mxlk->write, &bytes, &buffers);
+	if (buffers)
+		mxlk->tx_pending = true;
+	else
+		mxlk->tx_pending = false;
+}
+
+static irqreturn_t mxlk_interrupt(int irq, void *args)
+{
+	struct mxlk_pcie *xdev = args;
+	struct mxlk *mxlk = &xdev->mxlk;
+
+	if (mxlk_get_doorbell(mxlk, FROM_DEVICE, DATA_SENT)) {
+		mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_SENT, 0);
+		mxlk_start_rx(mxlk, 0);
+
+		mxlk_debug_incr(mxlk, &xdev->mxlk.stats.interrupts, 1);
+	}
+	if (mxlk_get_doorbell(mxlk, FROM_DEVICE, DATA_RECEIVED)) {
+		mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_RECEIVED, 0);
+		if (mxlk->tx_pending)
+			mxlk_start_tx(mxlk, 0);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int mxlk_events_init(struct mxlk *mxlk)
+{
+	mxlk->rx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
+	if (!mxlk->rx_wq) {
+		dev_err(mxlk_to_dev(mxlk), "failed to allocate workqueue\n");
+		return -ENOMEM;
+	}
+
+	mxlk->tx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
+	if (!mxlk->tx_wq) {
+		dev_err(mxlk_to_dev(mxlk), "failed to allocate workqueue\n");
+		destroy_workqueue(mxlk->rx_wq);
+		return -ENOMEM;
+	}
+
+	INIT_DELAYED_WORK(&mxlk->rx_event, mxlk_rx_event_handler);
+	INIT_DELAYED_WORK(&mxlk->tx_event, mxlk_tx_event_handler);
+
+	return 0;
+}
+
+static void mxlk_events_cleanup(struct mxlk *mxlk)
+{
+	cancel_delayed_work_sync(&mxlk->rx_event);
+	cancel_delayed_work_sync(&mxlk->tx_event);
+
+	destroy_workqueue(mxlk->rx_wq);
+	destroy_workqueue(mxlk->tx_wq);
+}
+
+int mxlk_core_init(struct mxlk *mxlk)
+{
+	int rc;
+	int status;
+	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+
+	status = mxlk_get_device_status(mxlk);
+	if (status != MXLK_STATUS_RUN) {
+		dev_err(&xdev->pci->dev,
+			"device status not RUNNING (%d)\n", status);
+		rc = -EBUSY;
+		return rc;
+	}
+
+	mxlk_version_check(mxlk);
+
+	rc = mxlk_events_init(mxlk);
+	if (rc)
+		return rc;
+
+	rc = mxlk_discover_txrx(mxlk);
+	if (rc)
+		goto error_txrx;
+
+	mxlk_interfaces_init(mxlk);
+
+	rc = mxlk_pci_register_irq(xdev, &mxlk_interrupt);
+	if (rc)
+		goto error_txrx;
+
+	mxlk_set_host_status(mxlk, MXLK_STATUS_RUN);
+
+	return 0;
+
+error_txrx:
+	mxlk_events_cleanup(mxlk);
+	mxlk_set_host_status(mxlk, MXLK_STATUS_ERROR);
+
+	return rc;
+}
+
+void mxlk_core_cleanup(struct mxlk *mxlk)
+{
+	if (mxlk->status == MXLK_STATUS_RUN) {
+		mxlk_set_host_status(mxlk, MXLK_STATUS_UNINIT);
+		mxlk_events_cleanup(mxlk);
+		mxlk_interfaces_cleanup(mxlk);
+		mxlk_txrx_cleanup(mxlk);
+	}
+}
+
+int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
+		   uint32_t timeout_ms)
+{
+	int ret = 0;
+	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	size_t len = *length;
+	size_t remaining = len;
+	struct mxlk_buf_desc *bd;
+	unsigned long jiffies_start = jiffies;
+	long jiffies_passed = 0;
+	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
+
+	*length = 0;
+	if (len == 0)
+		return -EINVAL;
+
+	if (mxlk->status != MXLK_STATUS_RUN)
+		return -ENODEV;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.cnts, 1);
+
+	ret = mutex_lock_interruptible(&inf->rlock);
+	if (ret < 0)
+		return -EINTR;
+
+	do {
+		while (!inf->data_available) {
+			mutex_unlock(&inf->rlock);
+			if (timeout_ms == 0) {
+				ret = wait_event_interruptible(
+					inf->rx_waitqueue, inf->data_available);
+			} else {
+				ret = wait_event_interruptible_timeout(
+					inf->rx_waitqueue, inf->data_available,
+					jiffies_timeout - jiffies_passed);
+				if (ret == 0)
+					return -ETIME;
+			}
+			if (ret < 0 || mxlk->stop_flag)
+				return -EINTR;
+
+			ret = mutex_lock_interruptible(&inf->rlock);
+			if (ret < 0)
+				return -EINTR;
+		}
+
+		bd = (inf->partial_read) ? inf->partial_read :
+					   mxlk_list_get(&inf->read);
+		while (remaining && bd) {
+			size_t bcopy;
+
+			bcopy = min(remaining, bd->length);
+			memcpy(buffer, bd->data, bcopy);
+
+			buffer += bcopy;
+			remaining -= bcopy;
+			bd->data += bcopy;
+			bd->length -= bcopy;
+
+			mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.bytes, bcopy);
+
+			if (bd->length == 0) {
+				mxlk_free_rx_bd(mxlk, bd);
+				bd = mxlk_list_get(&inf->read);
+			}
+		}
+
+		// save for next time
+		inf->partial_read = bd;
+
+		if (!bd)
+			inf->data_available = false;
+
+		*length = len - remaining;
+
+		jiffies_passed = (long)jiffies - (long)jiffies_start;
+	} while (remaining > 0 && (jiffies_passed < jiffies_timeout ||
+				   timeout_ms == 0));
+
+	mutex_unlock(&inf->rlock);
+
+	return 0;
+}
+
+int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
+		    uint32_t timeout_ms)
+{
+	int ret;
+	size_t len = *length;
+	size_t remaining = len;
+	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	struct mxlk_buf_desc *bd, *head;
+	unsigned long jiffies_start = jiffies;
+	long jiffies_passed = 0;
+	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
+
+	*length = 0;
+	if (len == 0)
+		return -EINVAL;
+
+	if (mxlk->status != MXLK_STATUS_RUN)
+		return -ENODEV;
+
+	mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.cnts, 1);
+
+	ret = mutex_lock_interruptible(&mxlk->wlock);
+	if (ret < 0)
+		return -EINTR;
+
+	do {
+		bd = head = mxlk_alloc_tx_bd(mxlk);
+		while (!head) {
+			mutex_unlock(&mxlk->wlock);
+			if (timeout_ms == 0) {
+				ret = wait_event_interruptible(
+					mxlk->tx_waitqueue,
+					!mxlk->no_tx_buffer);
+			} else {
+				ret = wait_event_interruptible_timeout(
+					mxlk->tx_waitqueue, !mxlk->no_tx_buffer,
+					jiffies_timeout - jiffies_passed);
+				if (ret == 0)
+					return -ETIME;
+			}
+			if (ret < 0 || mxlk->stop_flag)
+				return -EINTR;
+
+			ret = mutex_lock_interruptible(&mxlk->wlock);
+			if (ret < 0)
+				return -EINTR;
+
+			bd = head = mxlk_alloc_tx_bd(mxlk);
+		}
+
+		while (remaining && bd) {
+			size_t bcopy;
+
+			bcopy = min(bd->length, remaining);
+			memcpy(bd->data, buffer, bcopy);
+
+			buffer += bcopy;
+			remaining -= bcopy;
+			bd->length = bcopy;
+			bd->interface = inf->id;
+
+			mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.bytes, bcopy);
+
+			if (remaining) {
+				bd->next = mxlk_alloc_tx_bd(mxlk);
+				bd = bd->next;
+			}
+		}
+
+		mxlk_list_put(&mxlk->write, head);
+		mxlk_start_tx(mxlk, 0);
+
+		*length = len - remaining;
+
+		jiffies_passed = (long)jiffies - (long)jiffies_start;
+	} while (remaining > 0 && (jiffies_passed < jiffies_timeout ||
+				   timeout_ms == 0));
+
+	mutex_unlock(&mxlk->wlock);
+
+	return 0;
+}
diff --git a/drivers/misc/xlink-pcie/remote_host/mxlk_inf.c b/drivers/misc/xlink-pcie/remote_host/mxlk_inf.c
new file mode 100644
index 000000000000..d79a013a987d
--- /dev/null
+++ b/drivers/misc/xlink-pcie/remote_host/mxlk_inf.c
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/xlink_drv_inf.h>
+#include "mxlk_pci.h"
+
+int xlink_pcie_get_device_list(uint32_t *sw_device_id_list,
+			       uint32_t *num_devices)
+{
+	*num_devices = mxlk_get_device_num(sw_device_id_list);
+
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_get_device_list);
+
+int xlink_pcie_get_device_name(uint32_t sw_device_id, char *device_name,
+			       size_t name_size)
+{
+	return mxlk_get_device_name_by_id(sw_device_id, device_name, name_size);
+}
+EXPORT_SYMBOL(xlink_pcie_get_device_name);
+
+int xlink_pcie_get_device_status(uint32_t sw_device_id, uint32_t *device_status)
+{
+	int rc;
+	u32 status;
+
+	rc = mxlk_get_device_status_by_id(sw_device_id, &status);
+	if (rc)
+		return rc;
+
+	switch (status) {
+	case MXLK_STATUS_READY:
+	case MXLK_STATUS_RUN:
+		*device_status = _XLINK_DEV_READY;
+		break;
+	case MXLK_STATUS_ERROR:
+		*device_status = _XLINK_DEV_ERROR;
+		break;
+	case MXLK_STATUS_RECOVERY:
+		*device_status = _XLINK_DEV_RECOVERY;
+		break;
+	case MXLK_STATUS_OFF:
+		*device_status = _XLINK_DEV_OFF;
+		break;
+	default:
+		*device_status = _XLINK_DEV_BUSY;
+		break;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(xlink_pcie_get_device_status);
+
+int xlink_pcie_boot_device(uint32_t sw_device_id, const char *binary_name)
+{
+	return mxlk_pci_boot_device(sw_device_id, binary_name);
+}
+EXPORT_SYMBOL(xlink_pcie_boot_device);
+
+int xlink_pcie_connect(uint32_t sw_device_id)
+{
+	return mxlk_pci_connect_device(sw_device_id);
+}
+EXPORT_SYMBOL(xlink_pcie_connect);
+
+int xlink_pcie_read(uint32_t sw_device_id, void *data, size_t *const size,
+		    uint32_t timeout)
+{
+	return mxlk_pci_read(sw_device_id, data, size, timeout);
+}
+EXPORT_SYMBOL(xlink_pcie_read);
+
+int xlink_pcie_write(uint32_t sw_device_id, void *data, size_t *const size,
+		     uint32_t timeout)
+{
+	return mxlk_pci_write(sw_device_id, data, size, timeout);
+}
+EXPORT_SYMBOL(xlink_pcie_write);
+
+int xlink_pcie_reset_device(uint32_t sw_device_id)
+{
+	return mxlk_pci_reset_device(sw_device_id);
+}
+EXPORT_SYMBOL(xlink_pcie_reset_device);
diff --git a/drivers/misc/xlink-pcie/remote_host/mxlk_main.c b/drivers/misc/xlink-pcie/remote_host/mxlk_main.c
new file mode 100644
index 000000000000..5cf19847a027
--- /dev/null
+++ b/drivers/misc/xlink-pcie/remote_host/mxlk_main.c
@@ -0,0 +1,90 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include "mxlk_pci.h"
+
+static const struct pci_device_id mxlk_pci_table[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_KEEMBAY), 0 },
+	{ 0 }
+};
+
+static bool driver_unload;
+
+static int mxlk_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	int ret = 0;
+	u32 sw_devid = 0;
+	u32 hw_id = 0;
+	bool new_device = false;
+	struct mxlk_pcie *xdev;
+
+	hw_id = ((u16)pdev->bus->number << 8) | PCI_SLOT(pdev->devfn);
+
+	sw_devid = (XLINK_DEV_INF_PCIE << XLINK_DEV_INF_TYPE_SHIFT) |
+		   (hw_id << XLINK_DEV_PHYS_ID_SHIFT) |
+		   (XLINK_DEV_TYPE_KMB << XLINK_DEV_TYPE_SHIFT) |
+		   (XLINK_DEV_SLICE_0 << XLINK_DEV_SLICE_ID_SHIFT) |
+		   (XLINK_DEV_FUNC_VPU << XLINK_DEV_FUNC_SHIFT);
+
+	xdev = mxlk_get_device_by_id(sw_devid);
+	if (!xdev) {
+		xdev = mxlk_create_device(sw_devid, pdev);
+		if (!xdev)
+			return -ENOMEM;
+
+		new_device = true;
+	}
+
+	ret = mxlk_pci_init(xdev, pdev);
+	if (ret) {
+		mxlk_remove_device(xdev);
+		return ret;
+	}
+
+	if (new_device)
+		mxlk_list_add_device(xdev);
+
+	return ret;
+}
+
+static void mxlk_remove(struct pci_dev *pdev)
+{
+	struct mxlk_pcie *xdev = pci_get_drvdata(pdev);
+
+	if (xdev) {
+		mxlk_pci_cleanup(xdev);
+		if (driver_unload)
+			mxlk_remove_device(xdev);
+	}
+}
+
+static struct pci_driver mxlk_driver = {
+	.name = MXLK_DRIVER_NAME,
+	.id_table = mxlk_pci_table,
+	.probe = mxlk_probe,
+	.remove = mxlk_remove
+};
+
+static int __init mxlk_init_module(void)
+{
+	return pci_register_driver(&mxlk_driver);
+}
+
+static void __exit mxlk_exit_module(void)
+{
+	driver_unload = true;
+	pci_unregister_driver(&mxlk_driver);
+}
+
+module_init(mxlk_init_module);
+module_exit(mxlk_exit_module);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION(MXLK_DRIVER_DESC);
+MODULE_VERSION(MXLK_DRIVER_VERSION);
diff --git a/drivers/misc/xlink-pcie/remote_host/mxlk_pci.c b/drivers/misc/xlink-pcie/remote_host/mxlk_pci.c
new file mode 100644
index 000000000000..33f432f21645
--- /dev/null
+++ b/drivers/misc/xlink-pcie/remote_host/mxlk_pci.c
@@ -0,0 +1,1191 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#include <linux/delay.h>
+#include <linux/firmware.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/workqueue.h>
+#include <linux/wait.h>
+#include <linux/types.h>
+#include <linux/mutex.h>
+#include "mxlk_pci.h"
+#include "../common/mxlk_core.h"
+#include "../common/mxlk_util.h"
+
+#define BOOT_TIMEOUT 60000
+#define STATUS_TIMEOUT 5000
+#define ERASE_TIMEOUT 30000
+
+#define SECTION_SIZE (1 * 1024 * 1024)
+
+static int aspm_enable;
+module_param(aspm_enable, int, 0664);
+MODULE_PARM_DESC(aspm_enable, "enable ASPM");
+
+static int mxlk_pci_setup_recovery_sysfs(struct mxlk_pcie *xdev);
+static void mxlk_pci_cleanup_recovery_sysfs(struct mxlk_pcie *xdev);
+
+static LIST_HEAD(dev_list);
+static DEFINE_MUTEX(dev_list_mutex);
+
+struct mxlk_pcie *mxlk_get_device_by_id(u32 id)
+{
+	struct mxlk_pcie *xdev;
+
+	mutex_lock(&dev_list_mutex);
+
+	if (list_empty(&dev_list)) {
+		mutex_unlock(&dev_list_mutex);
+		return NULL;
+	}
+
+	list_for_each_entry(xdev, &dev_list, list) {
+		if (xdev->devid == id) {
+			mutex_unlock(&dev_list_mutex);
+			return xdev;
+		}
+	}
+
+	mutex_unlock(&dev_list_mutex);
+
+	return NULL;
+}
+
+struct mxlk_pcie *mxlk_create_device(u32 sw_device_id, struct pci_dev *pdev)
+{
+	struct mxlk_pcie *xdev = kzalloc(sizeof(struct mxlk_pcie), GFP_KERNEL);
+
+	if (!xdev)
+		return NULL;
+
+	xdev->devid = sw_device_id;
+	snprintf(xdev->name, MXLK_MAX_NAME_LEN, "%02x:%02x.%x",
+		 pdev->bus->number,
+		 PCI_SLOT(pdev->devfn),
+		 PCI_FUNC(pdev->devfn));
+
+	mutex_init(&xdev->lock);
+
+	return xdev;
+}
+
+void mxlk_remove_device(struct mxlk_pcie *xdev)
+{
+	mutex_destroy(&xdev->lock);
+	kfree(xdev);
+}
+
+void mxlk_list_add_device(struct mxlk_pcie *xdev)
+{
+	mutex_lock(&dev_list_mutex);
+
+	list_add_tail(&xdev->list, &dev_list);
+
+	mutex_unlock(&dev_list_mutex);
+}
+
+void mxlk_list_del_device(struct mxlk_pcie *xdev)
+{
+	mutex_lock(&dev_list_mutex);
+
+	list_del(&xdev->list);
+
+	mutex_unlock(&dev_list_mutex);
+}
+
+static void mxlk_pci_set_aspm(struct mxlk_pcie *xdev, int aspm)
+{
+	u8 cap_exp;
+	u16 link_control;
+
+	cap_exp = pci_find_capability(xdev->pci, PCI_CAP_ID_EXP);
+	if (!cap_exp) {
+		dev_err(&xdev->pci->dev, "failed to find pcie capability\n");
+		return;
+	}
+
+	pci_read_config_word(xdev->pci, cap_exp + PCI_EXP_LNKCTL,
+			     &link_control);
+	link_control &= ~(PCI_EXP_LNKCTL_ASPMC);
+	link_control |= (aspm & PCI_EXP_LNKCTL_ASPMC);
+	pci_write_config_word(xdev->pci, cap_exp + PCI_EXP_LNKCTL,
+			      link_control);
+}
+
+static void mxlk_pci_unmap_bar(struct mxlk_pcie *xdev)
+{
+	if (xdev->mxlk.bar0) {
+		iounmap(xdev->mxlk.bar0);
+		xdev->mxlk.bar0 = NULL;
+	}
+
+	if (xdev->mxlk.io_comm) {
+		iounmap(xdev->mxlk.io_comm);
+		xdev->mxlk.io_comm = NULL;
+		xdev->mxlk.mmio = NULL;
+	}
+
+	if (xdev->mxlk.bar4) {
+		iounmap(xdev->mxlk.bar4);
+		xdev->mxlk.bar4 = NULL;
+	}
+}
+
+static int mxlk_pci_map_bar(struct mxlk_pcie *xdev)
+{
+	if (pci_resource_len(xdev->pci, 2) < MXLK_IO_COMM_SIZE) {
+		dev_err(&xdev->pci->dev, "device BAR region is too small\n");
+		return -EIO;
+	}
+
+	xdev->mxlk.bar0 = pci_ioremap_bar(xdev->pci, 0);
+	if (!xdev->mxlk.bar0) {
+		dev_err(&xdev->pci->dev, "failed to ioremap BAR0\n");
+		goto bar_error;
+	}
+
+	xdev->mxlk.io_comm = pci_ioremap_bar(xdev->pci, 2);
+	if (!xdev->mxlk.io_comm) {
+		dev_err(&xdev->pci->dev, "failed to ioremap BAR2\n");
+		goto bar_error;
+	}
+	xdev->mxlk.mmio = (void __iomem *)xdev->mxlk.io_comm + MXLK_MMIO_OFFSET;
+
+	xdev->mxlk.bar4 = pci_ioremap_wc_bar(xdev->pci, 4);
+	if (!xdev->mxlk.bar4) {
+		dev_err(&xdev->pci->dev, "failed to ioremap BAR4\n");
+		goto bar_error;
+	}
+
+	return 0;
+
+bar_error:
+	mxlk_pci_unmap_bar(xdev);
+	return -EIO;
+}
+
+#define STR_EQUAL(a, b) !strncmp(a, b, strlen(b))
+
+static enum mxlk_stage mxlk_check_magic(struct mxlk_pcie *xdev)
+{
+	char magic[MXLK_BOOT_MAGIC_STRLEN];
+
+	memcpy_fromio(magic, xdev->mxlk.io_comm->magic,
+		      MXLK_BOOT_MAGIC_STRLEN);
+
+	if (strlen(magic) == 0)
+		return STAGE_UNINIT;
+
+	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_ROM))
+		return STAGE_ROM;
+
+	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_EMMC))
+		return STAGE_ROM;
+
+	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_BL2))
+		return STAGE_BL2;
+
+	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_UBOOT))
+		return STAGE_UBOOT;
+
+	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_RECOV))
+		return STAGE_RECOV;
+
+	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_YOCTO))
+		return STAGE_OS;
+
+	return STAGE_UNINIT;
+}
+
+static irqreturn_t mxlk_interrupt(int irq, void *args)
+{
+	struct mxlk_pcie *xdev = args;
+	enum mxlk_stage stage;
+	u8 event;
+
+	event = mxlk_get_doorbell(&xdev->mxlk, FROM_DEVICE, DEV_EVENT);
+	if (event == DEV_SHUTDOWN || event == 0xFF) {
+		schedule_delayed_work(&xdev->shutdown_event, 0);
+		return IRQ_HANDLED;
+	}
+
+	if (likely(xdev->core_irq_callback))
+		return xdev->core_irq_callback(irq, args);
+
+	stage = mxlk_check_magic(xdev);
+	if (stage == STAGE_ROM) {
+		xdev->mxlk.status = MXLK_STATUS_BOOT_FW;
+		wake_up_interruptible(&xdev->waitqueue);
+	} else if (stage == STAGE_UBOOT) {
+		xdev->mxlk.status = MXLK_STATUS_BOOT_OS;
+		wake_up_interruptible(&xdev->waitqueue);
+	} else if (stage == STAGE_RECOV) {
+		xdev->mxlk.status = MXLK_STATUS_RECOVERY;
+		wake_up_interruptible(&xdev->waitqueue);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static void mxlk_pci_irq_cleanup(struct mxlk_pcie *xdev)
+{
+#if KERNEL_VERSION(4, 8, 0) <= LINUX_VERSION_CODE
+	int irq = pci_irq_vector(xdev->pci, 0);
+
+	if (irq < 0)
+		return;
+
+	synchronize_irq(irq);
+	free_irq(irq, xdev);
+	pci_free_irq_vectors(xdev->pci);
+#else
+	if (!pci_msi_enabled())
+		return;
+
+	synchronize_irq(xdev->pci->irq);
+	free_irq(xdev->pci->irq, xdev);
+	pci_disable_msi(xdev->pci);
+#endif
+}
+
+#if KERNEL_VERSION(4, 8, 0) <= LINUX_VERSION_CODE
+static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
+{
+	int irq;
+	int rc;
+
+	rc = pci_alloc_irq_vectors(xdev->pci, 1, 1, PCI_IRQ_MSI);
+	if (rc < 0) {
+		dev_err(&xdev->pci->dev,
+			"failed to allocate %d MSI vectors\n", 1);
+		return rc;
+	}
+
+	irq = pci_irq_vector(xdev->pci, 0);
+	if (irq < 0) {
+		dev_err(&xdev->pci->dev, "failed to get irq\n");
+		rc = irq;
+		goto error_irq;
+	}
+	rc = request_irq(irq, &mxlk_interrupt, 0, MXLK_DRIVER_NAME, xdev);
+	if (rc) {
+		dev_err(&xdev->pci->dev, "failed to request irqs\n");
+		goto error_irq;
+	}
+
+	return 0;
+
+error_irq:
+	pci_free_irq_vectors(xdev->pci);
+	return rc;
+}
+#else
+static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
+{
+	int rc;
+
+	rc = pci_enable_msi(xdev->pci);
+	if (rc < 1) {
+		dev_err(&xdev->pci->dev, "failed to allocate MSI vectors\n");
+		return rc;
+	}
+
+	rc = request_irq(xdev->pci->irq, &mxlk_interrupt, 0,
+			    MXLK_DRIVER_NAME, xdev);
+	if (rc) {
+		dev_err(&xdev->pci->dev, "failed to request irqs\n");
+		goto error_irq;
+	}
+
+	return 0;
+
+error_irq:
+	pci_disable_msi(xdev->pci);
+	return rc;
+}
+#endif
+
+static int mxlk_device_wait_status(struct mxlk_pcie *xdev, u32 image_id,
+				   u32 timeout_ms)
+{
+	u32 status = MXLK_BOOT_STATUS_START;
+	int count = 0;
+
+	if (timeout_ms == 0)
+		timeout_ms = STATUS_TIMEOUT;
+
+	iowrite32(image_id, &xdev->mxlk.io_comm->mf_ready);
+
+	while (status != MXLK_BOOT_STATUS_DOWNLOADED) {
+		mdelay(1);
+		if (++count > timeout_ms) {
+			dev_err(&xdev->pci->dev, "operation takes too long.\n");
+			return -ETIME;
+		}
+
+		status = ioread32(&xdev->mxlk.io_comm->mf_ready);
+
+		switch (status) {
+		case MXLK_BOOT_STATUS_INVALID:
+			dev_err(&xdev->pci->dev,
+				"the firmware image data is invalid.\n");
+			return -EINVAL;
+		case MXLK_BOOT_STATUS_ERROR:
+			dev_err(&xdev->pci->dev,
+				"failed to download firmware image.\n");
+			return -EINVAL;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static int mxlk_device_transfer(struct mxlk_pcie *xdev, u32 image_id,
+				dma_addr_t addr, size_t size)
+{
+	int rc;
+
+	_iowrite64(addr, &xdev->mxlk.io_comm->mf_start);
+	iowrite32(size, &xdev->mxlk.io_comm->mf_len);
+
+	if (image_id == MXLK_BOOT_RAW_ID)
+		_iowrite64(xdev->partition_offset,
+			   &xdev->mxlk.io_comm->mf_offset);
+
+	rc = mxlk_device_wait_status(xdev, image_id, 0);
+	if (!rc)
+		xdev->partition_offset += size;
+
+	return rc;
+}
+
+static int mxlk_device_download_common(struct mxlk_pcie *xdev, u32 image_id,
+				       const void *buf, size_t buf_size,
+				       bool no_copy)
+{
+	int rc = 0;
+	size_t size;
+	size_t size_left = buf_size;
+	dma_addr_t phys_addr;
+	struct device *dev = &xdev->pci->dev;
+
+	while (size_left) {
+		size = (size_left > SECTION_SIZE) ? SECTION_SIZE : size_left;
+
+		if (!no_copy)
+			memcpy(xdev->dma_buf, buf, size);
+
+		phys_addr = dma_map_single(dev, xdev->dma_buf, size,
+					   DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, phys_addr))
+			return -ENOMEM;
+
+		rc = mxlk_device_transfer(xdev, image_id, phys_addr, size);
+
+		dma_unmap_single(dev, phys_addr, size, DMA_TO_DEVICE);
+
+		if (rc)
+			return rc;
+
+		size_left -= size;
+		buf += size;
+	}
+
+	return rc;
+}
+
+static int mxlk_device_download_firmware(struct mxlk_pcie *xdev, u32 image_id,
+					 const char *fw_image)
+{
+	const struct firmware *firmware;
+	struct device *dev = &xdev->pci->dev;
+	int rc = 0;
+
+	switch (image_id) {
+	case MXLK_BOOT_FIP_ID:
+	case MXLK_BOOT_BOOT_ID:
+	case MXLK_BOOT_SYSTEM_ID:
+		break;
+	default:
+		dev_err(dev, "unknown firmware id\n");
+		return -EINVAL;
+	}
+
+	if (request_firmware(&firmware, fw_image, dev) < 0)
+		return -EIO;
+
+	if (firmware->size == 0) {
+		rc = -EIO;
+		goto firmware_cleanup;
+	}
+
+	xdev->dma_buf = kmalloc(SECTION_SIZE, GFP_KERNEL);
+	if (!xdev->dma_buf) {
+		rc = -ENOMEM;
+		goto firmware_cleanup;
+	}
+
+	rc = mxlk_device_download_common(xdev, image_id, firmware->data,
+					 firmware->size, false);
+
+	kfree(xdev->dma_buf);
+	xdev->dma_buf = NULL;
+
+firmware_cleanup:
+	release_firmware(firmware);
+
+	return rc;
+}
+
+static int mxlk_device_flashless_boot(struct mxlk_pcie *xdev)
+{
+	if (mxlk_device_download_firmware(xdev, MXLK_BOOT_BOOT_ID,
+					  xdev->fw_name)) {
+		dev_err(&xdev->pci->dev, "failed to download boot image\n");
+		return -EIO;
+	}
+
+	iowrite32(MXLK_BOOT_STATUS_DONE, &xdev->mxlk.io_comm->mf_ready);
+
+	return 0;
+}
+
+static int mxlk_device_fip(struct mxlk_pcie *xdev)
+{
+	if (mxlk_device_download_firmware(xdev, MXLK_BOOT_FIP_ID,
+					  xdev->fw_name)) {
+		dev_err(&xdev->pci->dev, "failed to download FIP image\n");
+		return -EIO;
+	}
+
+	iowrite32(MXLK_BOOT_STATUS_DONE, &xdev->mxlk.io_comm->mf_ready);
+
+	return 0;
+}
+
+static void mxlk_device_enable_irq(struct mxlk_pcie *xdev)
+{
+	iowrite32(MXLK_INT_ENABLE, &xdev->mxlk.io_comm->int_enable);
+	iowrite32(~MXLK_INT_MASK, &xdev->mxlk.io_comm->int_mask);
+}
+
+static void mxlk_device_poll(struct work_struct *work)
+{
+	struct mxlk_pcie *xdev = container_of(work, struct mxlk_pcie,
+					      wait_event.work);
+	enum mxlk_stage stage = mxlk_check_magic(xdev);
+
+	if (stage == STAGE_RECOV) {
+		xdev->mxlk.status = MXLK_STATUS_RECOVERY;
+		wake_up_interruptible(&xdev->waitqueue);
+	} else if (stage == STAGE_OS) {
+		xdev->mxlk.status = MXLK_STATUS_READY;
+		wake_up_interruptible(&xdev->waitqueue);
+		return;
+	}
+
+	schedule_delayed_work(&xdev->wait_event, msecs_to_jiffies(100));
+}
+
+static int mxlk_pci_prepare_dev_reset(struct mxlk_pcie *xdev, bool notify);
+
+static void mxlk_device_shutdown(struct work_struct *work)
+{
+	struct mxlk_pcie *xdev = container_of(work, struct mxlk_pcie,
+					      shutdown_event.work);
+
+	mxlk_pci_prepare_dev_reset(xdev, false);
+}
+
+static int mxlk_device_init(struct mxlk_pcie *xdev)
+{
+	int rc;
+
+	INIT_DELAYED_WORK(&xdev->wait_event, mxlk_device_poll);
+	INIT_DELAYED_WORK(&xdev->shutdown_event, mxlk_device_shutdown);
+
+	rc = mxlk_pci_irq_init(xdev);
+	if (rc)
+		return rc;
+
+	pci_set_master(xdev->pci);
+
+	xdev->mxlk.status = MXLK_STATUS_UNINIT;
+
+	init_waitqueue_head(&xdev->waitqueue);
+	schedule_delayed_work(&xdev->wait_event, 0);
+
+	return rc;
+}
+
+int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev)
+{
+	int rc;
+
+	if (mutex_lock_interruptible(&xdev->lock))
+		return -EINTR;
+
+	xdev->pci = pdev;
+	pci_set_drvdata(pdev, xdev);
+
+	rc = pci_enable_device_mem(xdev->pci);
+	if (rc) {
+		dev_err(&pdev->dev, "failed to enable pci device\n");
+		goto error_exit;
+	}
+
+	rc = pci_request_regions(xdev->pci, MXLK_DRIVER_NAME);
+	if (rc) {
+		dev_err(&pdev->dev, "failed to request mmio regions\n");
+		goto error_req_mem;
+	}
+
+	rc = mxlk_pci_map_bar(xdev);
+	if (rc)
+		goto error_map;
+
+	rc = dma_set_mask_and_coherent(&xdev->pci->dev, DMA_BIT_MASK(64));
+	if (rc) {
+		dev_err(&pdev->dev, "failed to set dma mask\n");
+		goto error_dma_mask;
+	}
+
+	mxlk_pci_set_aspm(xdev, aspm_enable);
+
+	rc = mxlk_pci_setup_recovery_sysfs(xdev);
+	if (rc) {
+		dev_err(&pdev->dev,
+			"failed to setup recovery sysfs facilities\n");
+		goto error_dma_mask;
+	}
+
+	mxlk_init_debug(&xdev->mxlk, &xdev->pci->dev);
+
+	rc = mxlk_device_init(xdev);
+	if (!rc)
+		goto init_exit;
+
+error_dma_mask:
+	mxlk_pci_unmap_bar(xdev);
+
+error_map:
+	pci_release_regions(xdev->pci);
+
+error_req_mem:
+	pci_disable_device(xdev->pci);
+
+error_exit:
+	xdev->mxlk.status = MXLK_STATUS_ERROR;
+
+init_exit:
+	mutex_unlock(&xdev->lock);
+	if (rc)
+		mutex_destroy(&xdev->lock);
+	return rc;
+}
+
+int mxlk_pci_cleanup(struct mxlk_pcie *xdev)
+{
+	if (mutex_lock_interruptible(&xdev->lock))
+		return -EINTR;
+
+	mxlk_pci_cleanup_recovery_sysfs(xdev);
+	mxlk_uninit_debug(&xdev->mxlk, &xdev->pci->dev);
+
+	cancel_delayed_work_sync(&xdev->wait_event);
+	cancel_delayed_work_sync(&xdev->shutdown_event);
+	xdev->core_irq_callback = NULL;
+	mxlk_pci_irq_cleanup(xdev);
+
+	kfree(xdev->dma_buf);
+	xdev->dma_buf = NULL;
+	xdev->dma_buf_offset = 0;
+
+	mxlk_core_cleanup(&xdev->mxlk);
+
+	mxlk_pci_unmap_bar(xdev);
+	pci_release_regions(xdev->pci);
+	pci_disable_device(xdev->pci);
+	pci_set_drvdata(xdev->pci, NULL);
+	xdev->mxlk.status = MXLK_STATUS_OFF;
+	xdev->irq_enabled = false;
+
+	mutex_unlock(&xdev->lock);
+
+	return 0;
+}
+
+int mxlk_pci_register_irq(struct mxlk_pcie *xdev, irq_handler_t irq_handler)
+{
+	if (xdev->mxlk.status != MXLK_STATUS_READY)
+		return -EINVAL;
+
+	xdev->core_irq_callback = irq_handler;
+
+	return 0;
+}
+
+int mxlk_pci_raise_irq(struct mxlk_pcie *xdev, enum mxlk_doorbell_type type,
+		       u8 value)
+{
+	u16 pci_status;
+
+	mxlk_set_doorbell(&xdev->mxlk, TO_DEVICE, type, value);
+	pci_read_config_word(xdev->pci, PCI_STATUS, &pci_status);
+
+	return 0;
+}
+
+u32 mxlk_get_device_num(u32 *id_list)
+{
+	u32 num = 0;
+	struct mxlk_pcie *p;
+
+	mutex_lock(&dev_list_mutex);
+
+	if (list_empty(&dev_list)) {
+		mutex_unlock(&dev_list_mutex);
+		return 0;
+	}
+
+	list_for_each_entry(p, &dev_list, list) {
+		*id_list++ = p->devid;
+		num++;
+	}
+	mutex_unlock(&dev_list_mutex);
+
+	return num;
+}
+
+int mxlk_get_device_name_by_id(u32 id, char *device_name, size_t name_size)
+{
+	struct mxlk_pcie *xdev;
+	size_t size;
+
+	xdev = mxlk_get_device_by_id(id);
+	if (!xdev)
+		return -ENODEV;
+
+	mutex_lock(&xdev->lock);
+
+	size = (name_size > MXLK_MAX_NAME_LEN) ? MXLK_MAX_NAME_LEN : name_size;
+	strncpy(device_name, xdev->name, size);
+
+	mutex_unlock(&xdev->lock);
+
+	return 0;
+}
+
+int mxlk_get_device_status_by_id(u32 id, u32 *status)
+{
+	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+
+	if (!xdev)
+		return -ENODEV;
+
+	mutex_lock(&xdev->lock);
+	*status = xdev->mxlk.status;
+	mutex_unlock(&xdev->lock);
+
+	return 0;
+}
+
+#define mxlk_wait_event(cond)						\
+({									\
+	int rc = 0;							\
+	int error = wait_event_interruptible_timeout(xdev->waitqueue,	\
+			cond, msecs_to_jiffies(BOOT_TIMEOUT));		\
+	if (error == 0)							\
+		rc = -ETIME;						\
+	if (error < 0)							\
+		rc = -EAGAIN;						\
+	rc;								\
+})
+
+int mxlk_pci_boot_device(u32 id, const char *binary_name)
+{
+	int rc = 0;
+	u32 expected = MXLK_STATUS_ERROR;
+	struct mxlk_pcie *xdev;
+
+	xdev = mxlk_get_device_by_id(id);
+	if (!xdev)
+		return -ENODEV;
+
+	if (mutex_lock_interruptible(&xdev->lock))
+		return -EINTR;
+
+	if (xdev->mxlk.status == MXLK_STATUS_OFF) {
+		rc = -ENODEV;
+		goto boot_cleanup;
+	}
+
+	strncpy(xdev->fw_name, binary_name, MXLK_MAX_NAME_LEN - 1);
+
+	if (!xdev->irq_enabled) {
+		mxlk_device_enable_irq(xdev);
+		xdev->irq_enabled = true;
+
+		rc = mxlk_wait_event(xdev->mxlk.status != MXLK_STATUS_UNINIT);
+		if (rc)
+			goto boot_cleanup;
+	}
+
+	switch (xdev->mxlk.status) {
+	case MXLK_STATUS_BOOT_FW:
+		xdev->mxlk.status = MXLK_STATUS_BOOT_PRE_OS;
+		rc = mxlk_device_fip(xdev);
+		goto boot_cleanup;
+	case MXLK_STATUS_BOOT_PRE_OS:
+		/*
+		 * This fake stage is to avoid boot timeout after flashing FIP
+		 * if the function only returns after entering BOOT_OS stage.
+		 * It's because if host doesn't support PCIe hot plug the
+		 * reset after flashing FIP cannot be detected so driver won't
+		 * know the stage change at all.
+		 * So let boot call on FIP return early and check stage in next
+		 * boot call for OS.
+		 */
+		rc = mxlk_wait_event(xdev->mxlk.status == MXLK_STATUS_BOOT_OS);
+		if (rc)
+			goto boot_cleanup;
+		rc = mxlk_device_flashless_boot(xdev);
+		if (rc)
+			goto boot_cleanup;
+		expected = MXLK_STATUS_READY;
+		break;
+	case MXLK_STATUS_BOOT_OS:
+		rc = mxlk_device_flashless_boot(xdev);
+		if (rc)
+			goto boot_cleanup;
+		expected = MXLK_STATUS_READY;
+		break;
+	case MXLK_STATUS_READY:
+	case MXLK_STATUS_RUN:
+		rc = 0;
+		goto boot_cleanup;
+	case MXLK_STATUS_RECOVERY:
+	case MXLK_STATUS_ERROR:
+	default:
+		rc = -EIO;
+		goto boot_cleanup;
+	}
+
+	rc = mxlk_wait_event((xdev->mxlk.status == expected) ||
+			     (xdev->mxlk.status == MXLK_STATUS_RECOVERY));
+
+	if (xdev->mxlk.status == MXLK_STATUS_RECOVERY)
+		rc = -EIO;
+
+boot_cleanup:
+	mutex_unlock(&xdev->lock);
+
+	return rc;
+}
+
+int mxlk_pci_connect_device(u32 id)
+{
+	int rc = 0;
+	struct mxlk_pcie *xdev;
+
+	xdev = mxlk_get_device_by_id(id);
+	if (!xdev)
+		return -ENODEV;
+
+	if (mutex_lock_interruptible(&xdev->lock))
+		return -EINTR;
+
+	if (xdev->mxlk.status == MXLK_STATUS_RUN)
+		goto connect_cleanup;
+
+	if (xdev->mxlk.status == MXLK_STATUS_OFF) {
+		rc = -ENODEV;
+		goto connect_cleanup;
+	}
+
+	if (xdev->mxlk.status != MXLK_STATUS_READY) {
+		rc = -EBUSY;
+		goto connect_cleanup;
+	}
+
+	rc = mxlk_core_init(&xdev->mxlk);
+	if (rc < 0) {
+		dev_err(&xdev->pci->dev, "failed to sync with device\n");
+		goto connect_cleanup;
+	}
+
+connect_cleanup:
+	mutex_unlock(&xdev->lock);
+	return rc;
+}
+
+int mxlk_pci_read(u32 id, void *data, size_t *size, u32 timeout)
+{
+	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+
+	if (!xdev)
+		return -ENODEV;
+
+	return mxlk_core_read(&xdev->mxlk, data, size, timeout);
+}
+
+int mxlk_pci_write(u32 id, void *data, size_t *size, u32 timeout)
+{
+	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+
+	if (!xdev)
+		return -ENODEV;
+
+	return mxlk_core_write(&xdev->mxlk, data, size, timeout);
+}
+
+static int mxlk_pci_prepare_dev_reset(struct mxlk_pcie *xdev, bool notify)
+{
+	if (mutex_lock_interruptible(&xdev->lock))
+		return -EINTR;
+
+	if (xdev->core_irq_callback) {
+		xdev->core_irq_callback = NULL;
+		mxlk_core_cleanup(&xdev->mxlk);
+	}
+	xdev->mxlk.status = MXLK_STATUS_OFF;
+	if (notify)
+		mxlk_pci_raise_irq(xdev, DEV_EVENT, REQUEST_RESET);
+
+	mutex_unlock(&xdev->lock);
+
+	return 0;
+}
+
+int mxlk_pci_reset_device(u32 id)
+{
+	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+
+	if (!xdev)
+		return -ENOMEM;
+
+	return mxlk_pci_prepare_dev_reset(xdev, true);
+}
+
+u64 mxlk_pci_hw_dev_id(struct mxlk_pcie *xdev)
+{
+	return _ioread64(&xdev->mxlk.io_comm->dev_id);
+}
+
+static int mxlk_boot_access_enter(struct mxlk_pcie *xdev)
+{
+	if (mutex_lock_interruptible(&xdev->lock))
+		return -EINTR;
+	if (xdev->mxlk.status != MXLK_STATUS_RECOVERY) {
+		mutex_unlock(&xdev->lock);
+		return -EROFS;
+	}
+	return 0;
+}
+
+static void mxlk_boot_access_exit(struct mxlk_pcie *xdev)
+{
+	mutex_unlock(&xdev->lock);
+}
+
+static int mxlk_recovery_send_left(struct mxlk_pcie *xdev, bool reset_offset)
+{
+	int rc;
+
+	rc = mxlk_device_download_common(xdev, MXLK_BOOT_RAW_ID, NULL,
+			xdev->dma_buf_offset, true);
+	xdev->dma_buf_offset = 0;
+	if (reset_offset)
+		xdev->partition_offset = 0;
+
+	return rc;
+}
+
+static int mxlk_pci_flash_gpt_table(struct mxlk_pcie *xdev)
+{
+	int rc = 0;
+
+	rc = mxlk_boot_access_enter(xdev);
+	if (rc)
+		return rc;
+
+	if (xdev->dma_buf_offset) {
+		rc = mxlk_recovery_send_left(xdev, true);
+		if (rc)
+			goto create_error;
+	}
+
+	rc = mxlk_device_wait_status(xdev, MXLK_BOOT_FLASH_ID, 0);
+
+create_error:
+	mxlk_boot_access_exit(xdev);
+	return rc;
+}
+
+static int mxlk_pci_erase_partition(struct mxlk_pcie *xdev,
+				    const char *partition, size_t len)
+{
+	int rc = 0;
+
+	rc = mxlk_boot_access_enter(xdev);
+	if (rc)
+		return rc;
+
+	if (xdev->dma_buf_offset) {
+		rc = mxlk_recovery_send_left(xdev, true);
+		if (rc)
+			goto erase_error;
+	}
+
+	memcpy_toio(xdev->mxlk.io_comm->mf_dest, partition, len);
+
+	rc = mxlk_device_wait_status(xdev, MXLK_BOOT_ERASE_ID, ERASE_TIMEOUT);
+
+erase_error:
+	mxlk_boot_access_exit(xdev);
+	return rc;
+}
+
+static int mxlk_pci_flash_partition_start(struct mxlk_pcie *xdev,
+					  const char *partition,
+					  size_t name_len)
+{
+	int rc = 0;
+
+	rc = mxlk_boot_access_enter(xdev);
+	if (rc)
+		return rc;
+
+	if (xdev->dma_buf_offset) {
+		rc = mxlk_recovery_send_left(xdev, true);
+		if (rc)
+			goto start_error;
+	}
+
+	memset(xdev->partition_name, 0, MXLK_BOOT_DEST_STRLEN);
+	memcpy(xdev->partition_name, partition,
+		(name_len >= MXLK_BOOT_DEST_STRLEN) ?
+			(MXLK_BOOT_DEST_STRLEN - 1) : name_len);
+	xdev->partition_offset = 0;
+
+	memcpy_toio(xdev->mxlk.io_comm->mf_dest, xdev->partition_name,
+		    MXLK_BOOT_DEST_STRLEN);
+
+start_error:
+	mxlk_boot_access_exit(xdev);
+	return rc;
+}
+
+static int mxlk_pci_flash_partition_send(struct mxlk_pcie *xdev,
+					 const void *data, size_t size)
+{
+	int rc = 0;
+	int size_left = size;
+
+	rc = mxlk_boot_access_enter(xdev);
+	if (rc)
+		return rc;
+
+	if (!xdev->dma_buf) {
+		xdev->dma_buf_offset = 0;
+		xdev->dma_buf = kmalloc(SECTION_SIZE, GFP_KERNEL);
+		if (!xdev->dma_buf) {
+			rc = -ENOMEM;
+			goto send_error;
+		}
+	}
+
+	while (size_left) {
+		size = (size > (SECTION_SIZE - xdev->dma_buf_offset)) ?
+			(SECTION_SIZE - xdev->dma_buf_offset) : size;
+		memcpy(xdev->dma_buf + xdev->dma_buf_offset,
+			data, size);
+		xdev->dma_buf_offset += size;
+		size_left -= size;
+		data += size;
+
+		if (xdev->dma_buf_offset == SECTION_SIZE)
+			rc = mxlk_recovery_send_left(xdev, false);
+	}
+
+send_error:
+	mxlk_boot_access_exit(xdev);
+	return rc;
+}
+
+static int mxlk_pci_flash_done(struct mxlk_pcie *xdev)
+{
+	int rc = 0;
+
+	rc = mxlk_boot_access_enter(xdev);
+	if (rc)
+		return rc;
+
+	if (xdev->dma_buf_offset) {
+		rc = mxlk_recovery_send_left(xdev, true);
+		if (rc)
+			goto done_error;
+	}
+
+	iowrite32(MXLK_BOOT_STATUS_DONE, &xdev->mxlk.io_comm->mf_ready);
+
+	kfree(xdev->dma_buf);
+	xdev->dma_buf = NULL;
+	xdev->dma_buf_offset = 0;
+
+done_error:
+	mxlk_boot_access_exit(xdev);
+	return rc;
+}
+
+static ssize_t partition_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	int rc;
+
+	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+
+	rc = mxlk_pci_flash_partition_start(xdev, buf, count);
+	if (rc) {
+		dev_err(dev, "failed to flash partition\n");
+		return rc;
+	}
+
+	return count;
+}
+static DEVICE_ATTR_WO(partition);
+
+static ssize_t create_partitions_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	int rc;
+	long value;
+	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+
+	rc = kstrtol(buf, 10, &value);
+	if (rc)
+		return rc;
+
+	if (value) {
+		rc = mxlk_pci_flash_gpt_table(xdev);
+		if (rc) {
+			dev_err(dev, "failed to flash gpt table\n");
+			return rc;
+		}
+	}
+
+	return count;
+}
+static DEVICE_ATTR_WO(create_partitions);
+
+static ssize_t erase_partition_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int rc;
+	long value;
+	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+
+	rc = kstrtol(buf, 10, &value);
+	if (rc)
+		return rc;
+
+	if (value) {
+		rc = mxlk_pci_erase_partition(xdev, xdev->partition_name,
+					      MXLK_BOOT_DEST_STRLEN);
+		if (rc) {
+			dev_err(dev, "failed to erase partition\n");
+			return rc;
+		}
+	}
+
+	return count;
+}
+static DEVICE_ATTR_WO(erase_partition);
+
+static ssize_t recovery_done_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+	int rc;
+	long value;
+	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+
+	rc = kstrtol(buf, 10, &value);
+	if (rc)
+		return rc;
+
+	if (value) {
+		rc = mxlk_pci_flash_done(xdev);
+		if (rc) {
+			dev_err(dev, "failed to recover\n");
+			return rc;
+		}
+	}
+
+	return count;
+}
+static DEVICE_ATTR_WO(recovery_done);
+
+static ssize_t recov_write_data(struct file *filp, struct kobject *kobj,
+				struct bin_attribute *attr,
+				char *buf, loff_t offset, size_t count)
+{
+	int rc;
+	struct device *dev = kobj_to_dev(kobj);
+	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+
+	rc = mxlk_pci_flash_partition_send(xdev, buf, count);
+	if (rc) {
+		dev_err(dev, "failed to flash partition\n");
+		return rc;
+	}
+
+	return count;
+}
+static BIN_ATTR(image_data, 0644, NULL, recov_write_data, 0);
+
+static struct attribute *recovery_attrs[] = {
+	&dev_attr_partition.attr,
+	&dev_attr_create_partitions.attr,
+	&dev_attr_erase_partition.attr,
+	&dev_attr_recovery_done.attr,
+	NULL,
+};
+
+static struct bin_attribute *recovery_bin_attrs[] = {
+	&bin_attr_image_data,
+	NULL,
+};
+
+static const struct attribute_group recovery_group = {
+	.attrs = recovery_attrs,
+	.bin_attrs = recovery_bin_attrs,
+};
+
+static const struct attribute_group *recovery_groups[] = {
+	&recovery_group,
+	NULL,
+};
+
+static int mxlk_pci_setup_recovery_sysfs(struct mxlk_pcie *xdev)
+{
+	return sysfs_create_groups(&xdev->pci->dev.kobj, recovery_groups);
+}
+
+static void mxlk_pci_cleanup_recovery_sysfs(struct mxlk_pcie *xdev)
+{
+	sysfs_remove_groups(&xdev->pci->dev.kobj, recovery_groups);
+}
diff --git a/drivers/misc/xlink-pcie/remote_host/mxlk_pci.h b/drivers/misc/xlink-pcie/remote_host/mxlk_pci.h
new file mode 100644
index 000000000000..835d2214991e
--- /dev/null
+++ b/drivers/misc/xlink-pcie/remote_host/mxlk_pci.h
@@ -0,0 +1,75 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef MXLK_PCI_HEADER_
+#define MXLK_PCI_HEADER_
+
+#include <linux/list.h>
+#include <linux/interrupt.h>
+#include <linux/xlink_drv_inf.h>
+#include "../common/mxlk.h"
+#include "../common/mxlk_boot.h"
+#include "../common/mxlk_util.h"
+
+#define MXLK_MAX_NAME_LEN (32)
+
+struct mxlk_pcie {
+	struct list_head list;
+	struct mutex lock;
+
+	struct pci_dev *pci;
+	char name[MXLK_MAX_NAME_LEN];
+	u32 devid;
+	char fw_name[MXLK_MAX_NAME_LEN];
+
+	struct delayed_work wait_event;
+	struct delayed_work shutdown_event;
+	wait_queue_head_t waitqueue;
+	bool irq_enabled;
+	irq_handler_t core_irq_callback;
+
+	char partition_name[MXLK_BOOT_DEST_STRLEN];
+	unsigned long partition_offset;
+	void *dma_buf;
+	size_t dma_buf_offset;
+
+	struct mxlk mxlk;
+};
+
+static inline struct device *mxlk_to_dev(struct mxlk *mxlk)
+{
+	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+
+	return &xdev->pci->dev;
+}
+
+int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev);
+int mxlk_pci_cleanup(struct mxlk_pcie *xdev);
+int mxlk_pci_register_irq(struct mxlk_pcie *xdev, irq_handler_t irq_handler);
+int mxlk_pci_raise_irq(struct mxlk_pcie *xdev, enum mxlk_doorbell_type type,
+		       u8 value);
+
+struct mxlk_pcie *mxlk_create_device(u32 sw_device_id, struct pci_dev *pdev);
+void mxlk_remove_device(struct mxlk_pcie *xdev);
+void mxlk_list_add_device(struct mxlk_pcie *xdev);
+void mxlk_list_del_device(struct mxlk_pcie *xdev);
+u32 mxlk_get_device_num(u32 *id_list);
+struct mxlk_pcie *mxlk_get_device_by_id(u32 id);
+int mxlk_get_device_name_by_id(u32 id, char *device_name, size_t name_size);
+int mxlk_get_device_status_by_id(u32 id, u32 *status);
+
+int mxlk_pci_boot_device(u32 id, const char *binary_name);
+int mxlk_pci_connect_device(u32 id);
+int mxlk_pci_read(u32 id, void *data, size_t *size, u32 timeout);
+int mxlk_pci_write(u32 id, void *data, size_t *size, u32 timeout);
+int mxlk_pci_reset_device(u32 id);
+
+u64 mxlk_pci_hw_dev_id(struct mxlk_pcie *xdev);
+
+#endif
diff --git a/include/linux/xlink_drv_inf.h b/include/linux/xlink_drv_inf.h
new file mode 100644
index 000000000000..f33a8cd3c77d
--- /dev/null
+++ b/include/linux/xlink_drv_inf.h
@@ -0,0 +1,63 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*****************************************************************************
+ *
+ * Intel Keem Bay XLink PCIe Driver
+ *
+ * Copyright (C) 2020 Intel Corporation
+ *
+ ****************************************************************************/
+
+#ifndef _XLINK_DRV_INF_H_
+#define _XLINK_DRV_INF_H_
+
+#include <linux/types.h>
+
+#define XLINK_DEV_INF_TYPE_SHIFT (24)
+#define XLINK_DEV_INT_TYPE_MASK (0x7)
+#define XLINK_DEV_PHYS_ID_SHIFT (8)
+#define XLINK_DEV_PHYS_ID_MASK (0xFFFF)
+#define XLINK_DEV_TYPE_SHIFT (4)
+#define XLINK_DEV_TYPE_MASK (0xF)
+#define XLINK_DEV_SLICE_ID_SHIFT (1)
+#define XLINK_DEV_SLICE_ID_MASK (0x7)
+#define XLINK_DEV_FUNC_SHIFT (0)
+#define XLINK_DEV_FUNC_MASK (0x1)
+
+enum xlink_device_inf_type {
+	XLINK_DEV_INF_PCIE = 1,
+};
+
+enum xlink_device_type {
+	XLINK_DEV_TYPE_KMB = 0,
+};
+
+enum xlink_device_slice {
+	XLINK_DEV_SLICE_0 = 0,
+};
+
+enum xlink_device_func {
+	XLINK_DEV_FUNC_VPU = 0,
+};
+
+enum _xlink_device_status {
+	_XLINK_DEV_OFF,
+	_XLINK_DEV_ERROR,
+	_XLINK_DEV_BUSY,
+	_XLINK_DEV_RECOVERY,
+	_XLINK_DEV_READY
+};
+
+int xlink_pcie_get_device_list(uint32_t *sw_device_id_list,
+			       uint32_t *num_devices);
+int xlink_pcie_get_device_name(uint32_t sw_device_id, char *device_name,
+			       size_t name_size);
+int xlink_pcie_get_device_status(uint32_t sw_device_id,
+				 uint32_t *device_status);
+int xlink_pcie_boot_device(uint32_t sw_device_id, const char *binary_name);
+int xlink_pcie_connect(uint32_t sw_device_id);
+int xlink_pcie_read(uint32_t sw_device_id, void *data, size_t *const size,
+		    uint32_t timeout);
+int xlink_pcie_write(uint32_t sw_device_id, void *data, size_t *const size,
+		     uint32_t timeout);
+int xlink_pcie_reset_device(uint32_t sw_device_id);
+#endif
-- 
2.27.0

