From d89070e9b156e7d4bdb160d15100a3132e6557d1 Mon Sep 17 00:00:00 2001
From: Len Brown <len.brown@intel.com>
Date: Thu, 10 Jun 2021 15:30:47 -0700
Subject: [PATCH 7/7] sched/fair: Check asym_packing priority on the wakeup
 path

Place wakeup tasks on idle high-priority CPUs. Otherwise, they may often be
placed on idle low-priority CPUs, even when idle high-priority CPUs are
available.

This is important for bursty workloads, where the load balancer may not
notice those tasks and may not migrate them to available idle high-priority
CPUs.

Signed-off-by: Len Brown <len.brown@intel.com>
Acked-by: Tim Chen <tim.c.chen@linux.intel.com>
Acked-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Reviewed-by: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
---
Changes since v3:
  * Introduced this patch

Changes since v2:
  * N/A

Changes since v1:
  * N/A

---
Internal change log:

Changes since v3:
 * If no idle CPUs available, choose highest priority SCHED_IDLE CPU.
   [Aubrey]
 * Break out of CPU search early when possible* [Srinivas, Nikunj]

Changes since v2:
 * Move asym_packing idle cpu search closer to EAS cpu search. The result
   is that both of them skip searching for FORK and EXEC, and both of them
   set want_affine the same way. This update does not impact speedometer
   performance.

Changes since v1:
  * Cleanup after proof of concept.
---
 kernel/sched/fair.c | 65 ++++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 64 insertions(+), 1 deletion(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 2d700bba66ad..2af300c52e72 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6819,6 +6819,64 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 	return -1;
 }
 
+/*
+ * find_asym_packing_idle_cpu: find highest priority idle CPU (or sched_idle CPU)
+ *
+ * Return the highest priority idle CPU.
+ * Return prev_cpu if a tie.
+ * Return -1 if no idle CPU found
+ *
+ * If no idle CPUs, return as above, but for sched_idle CPUs.
+ */
+static int find_asym_packing_idle_cpu(struct task_struct *p, int prev_cpu)
+{
+	struct sched_domain *sd;
+	int new_idle_cpu = -1;
+	int new_sched_idle_cpu = -1;
+	int i;
+
+	rcu_read_lock();
+	sd = rcu_dereference(per_cpu(sd_asym_packing, prev_cpu));
+	if (!sd) {
+		rcu_read_unlock();
+		return -1;
+	}
+
+	if (available_idle_cpu(prev_cpu))
+		new_idle_cpu = prev_cpu;
+	else if (sched_idle_cpu(prev_cpu))
+		new_sched_idle_cpu = prev_cpu;
+
+	for_each_cpu(i, sched_domain_span(sd)) {
+		if (!cpumask_test_cpu(i, p->cpus_ptr))
+			continue;
+		if (!available_idle_cpu(i)) {
+			if (sched_idle_cpu(i)) {
+				if (new_sched_idle_cpu == -1)
+					new_sched_idle_cpu = i;
+				else if (sched_asym_prefer(i, new_sched_idle_cpu))
+					new_sched_idle_cpu = i;
+			}
+			continue;
+		}
+		if (new_idle_cpu == -1)
+			new_idle_cpu = i;
+		else if (sched_asym_prefer(i, new_idle_cpu)) {
+			new_idle_cpu = i;
+			/*
+			 * Assume that finding a preference between two idle cpus
+			 * (including idle prev_cpu) is sufficent searching.
+			 */
+			break;
+		}
+	}
+	rcu_read_unlock();
+
+	if (new_idle_cpu == -1)
+		return new_sched_idle_cpu;
+	return new_idle_cpu;
+}
+
 /*
  * select_task_rq_fair: Select target runqueue for the waking task in domains
  * that have the relevant SD flag set. In practice, this is SD_BALANCE_WAKE,
@@ -6849,9 +6907,14 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 			new_cpu = find_energy_efficient_cpu(p, prev_cpu);
 			if (new_cpu >= 0)
 				return new_cpu;
-			new_cpu = prev_cpu;
 		}
 
+		new_cpu = find_asym_packing_idle_cpu(p, prev_cpu);
+		if (new_cpu >= 0)
+			return new_cpu;
+
+		new_cpu = prev_cpu;
+
 		want_affine = !wake_wide(p) && cpumask_test_cpu(cpu, p->cpus_ptr);
 	}
 
-- 
2.27.0

