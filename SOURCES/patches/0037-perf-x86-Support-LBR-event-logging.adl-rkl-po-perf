From 714832210f16c0410207dc829ed5ecc021bc0352 Mon Sep 17 00:00:00 2001
From: Kan Liang <kan.liang@linux.intel.com>
Date: Wed, 3 Mar 2021 13:29:43 -0800
Subject: [PATCH 37/88] perf/x86: Support LBR event logging

The LBR event logging introduces a per-counter indication of precise
event occurrences in LBRs. It can provides a means to attribute exposed
retirement latency to combinations of events across a block of
instructions. It also provides a means of attributing Timed LBR
latencies to events.

Add a new event/group modifier, branch_events, to indicate the event for
LBR event logging.

The feature is only supported on the first 4 GP counters. Force the
event constraint to the first 4 GP counters.

Extend the struct perf_branch_entry to save the event logging.
For a group with branch_events, the four saturate values will be
returned in events order.
For a event with branch_events, only return the event's saturate value.

(TODO: Need to make it more generic.)

Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
---
 arch/x86/events/intel/core.c      | 48 +++++++++++++++++++++++++++++--
 arch/x86/events/intel/lbr.c       | 48 +++++++++++++++++++++++++++++++
 arch/x86/events/perf_event.h      |  1 +
 arch/x86/include/asm/msr-index.h  |  2 ++
 arch/x86/include/asm/perf_event.h |  4 +++
 include/uapi/linux/perf_event.h   |  7 +++--
 6 files changed, 105 insertions(+), 5 deletions(-)

diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index 98f5b0b35163..9fa20ba4b67c 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -2679,15 +2679,22 @@ static void intel_pmu_enable_fixed(struct perf_event *event)
 static void intel_pmu_enable_event(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
+	u64 enable_mask = ARCH_PERFMON_EVENTSEL_ENABLE;
 	int idx = hwc->idx;
 
 	if (unlikely(event->attr.precise_ip))
 		intel_pmu_pebs_enable(event);
 
 	switch (idx) {
-	case 0 ... INTEL_PMC_IDX_FIXED - 1:
+	case 0 ... 3:
+		if (event->attr.branch_events &&
+		    (x86_pmu.lbr_events & (1 < idx)) &&
+		    this_cpu_ptr(&cpu_hw_events)->lbr_users)
+			enable_mask = ARCH_PERFMON_EVENTSEL_LBR_LOG;
+		fallthrough;
+	case 4 ... INTEL_PMC_IDX_FIXED - 1:
 		intel_set_masks(event, idx);
-		__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+		__x86_pmu_enable_event(hwc, enable_mask);
 		break;
 	case INTEL_PMC_IDX_FIXED ... INTEL_PMC_IDX_FIXED_BTS - 1:
 	case INTEL_PMC_IDX_METRIC_BASE ... INTEL_PMC_IDX_METRIC_END:
@@ -3444,6 +3451,14 @@ intel_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
 		c2 = c1;
 	}
 
+	if (event->attr.branch_events) {
+		c1 = &cpuc->constraint_list[idx];
+		*c1 = *c2;
+		c1->idxmsk64 &= (1ULL << PERF_MAX_BRANCH_EVENTS) - 1;
+		c1->weight = hweight64(c1->idxmsk64);
+		return c2;
+	}
+
 	if (cpuc->excl_cntrs)
 		return intel_get_excl_constraints(cpuc, event, idx, c2);
 
@@ -3738,6 +3753,33 @@ static int intel_pmu_hw_config(struct perf_event *event)
 			event->attr.sample_type |= __PERF_SAMPLE_CALLCHAIN_EARLY;
 	}
 
+	if (event->attr.branch_events) {
+		struct perf_event *leader, *sibling;
+		int num_branch_events;
+
+		if (!static_cpu_has(X86_FEATURE_ARCH_LBR) ||
+		    !event->attr.precise_ip ||
+		    !x86_pmu.lbr_events ||
+		    (event->attr.config & ~INTEL_ARCH_EVENT_MASK))
+			return -EINVAL;
+
+		/*
+		 * The number of group members with branch_events attribute
+		 * cannot exceed the number of the supported counters.
+		 */
+		leader = event->group_leader;
+		if (leader->nr_siblings >= PERF_MAX_BRANCH_EVENTS) {
+			if (leader->attr.branch_events)
+				num_branch_events++;
+			for_each_sibling_event(sibling, leader) {
+				if (sibling->attr.branch_events)
+					num_branch_events++;
+			}
+			if (num_branch_events > PERF_MAX_BRANCH_EVENTS)
+				return -EINVAL;
+		}
+	}
+
 	if (needs_branch_stack(event)) {
 		ret = intel_pmu_setup_lbr_filter(event);
 		if (ret)
@@ -4242,7 +4284,7 @@ int intel_cpuc_prepare(struct cpu_hw_events *cpuc, int cpu)
 			goto err;
 	}
 
-	if (x86_pmu.flags & (PMU_FL_EXCL_CNTRS | PMU_FL_TFA)) {
+	if (x86_pmu.flags & (PMU_FL_EXCL_CNTRS | PMU_FL_TFA) || x86_pmu.lbr_events) {
 		size_t sz = X86_PMC_IDX_MAX * sizeof(struct event_constraint);
 
 		cpuc->constraint_list = kzalloc_node(sz, GFP_KERNEL, cpu_to_node(cpu));
diff --git a/arch/x86/events/intel/lbr.c b/arch/x86/events/intel/lbr.c
index bb4486c4155a..5b642c02830d 100644
--- a/arch/x86/events/intel/lbr.c
+++ b/arch/x86/events/intel/lbr.c
@@ -929,6 +929,52 @@ static __always_inline u16 get_lbr_cycles(u64 info)
 	return info & LBR_INFO_CYCLES;
 }
 
+static inline u8 update_lbr_event(struct perf_event *event, u64 info,
+				  int *pos, unsigned long *mask)
+{
+	int idx = event->hw.idx;
+	u8 lbr_event;
+
+	if (!event->attr.branch_events)
+		return 0;
+
+	lbr_event = (info & LBR_INFO_EVENTS) >> (LBR_INFO_EVENTS_OFFSET + idx * 2) & 0x3;
+	lbr_event <<= *pos;
+	*pos++;
+	clear_bit(idx, mask);
+	return lbr_event;
+}
+
+static u8 get_lbr_events(struct cpu_hw_events *cpuc, u64 info)
+{
+	unsigned long mask = (1ULL << PERF_MAX_BRANCH_EVENTS) - 1;
+	struct perf_event *event, *leader;
+	u8 lbr_events = 0;
+	int idx, pos = 0;
+
+	if (!static_cpu_has(X86_FEATURE_ARCH_LBR) ||
+	    !x86_pmu.lbr_events ||
+	    !(info & LBR_INFO_EVENTS))
+		return 0;
+
+	mask &= *cpuc->active_mask;
+	if (!mask)
+		return 0;
+
+more:
+	idx = find_first_bit(&mask, PERF_MAX_BRANCH_EVENTS);
+	event = cpuc->events[idx];
+	leader = event->group_leader;
+	lbr_events |= update_lbr_event(leader, info, &pos, &mask);
+	for_each_sibling_event(event, leader)
+		lbr_events |= update_lbr_event(event, info, &pos, &mask);
+
+	if (mask)
+		goto more;
+
+	return lbr_events;
+}
+
 static void intel_pmu_store_lbr(struct cpu_hw_events *cpuc,
 				struct lbr_entry *entries)
 {
@@ -959,6 +1005,7 @@ static void intel_pmu_store_lbr(struct cpu_hw_events *cpuc,
 		e->abort	= !!(info & LBR_INFO_ABORT);
 		e->cycles	= get_lbr_cycles(info);
 		e->type		= get_lbr_br_type(info);
+		e->events	= get_lbr_events(cpuc, info);
 		e->reserved	= 0;
 	}
 
@@ -1756,6 +1803,7 @@ void __init intel_pmu_arch_lbr_init(void)
 	x86_pmu.lbr_mispred = ecx.split.lbr_mispred;
 	x86_pmu.lbr_timed_lbr = ecx.split.lbr_timed_lbr;
 	x86_pmu.lbr_br_type = ecx.split.lbr_br_type;
+	x86_pmu.lbr_events = ecx.split.lbr_events;
 	x86_pmu.lbr_nr = lbr_nr;
 
 
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 25d561c3db1b..083d9354b76b 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -820,6 +820,7 @@ struct x86_pmu {
 	unsigned int	lbr_mispred:1;
 	unsigned int	lbr_timed_lbr:1;
 	unsigned int	lbr_br_type:1;
+	unsigned int	lbr_events:4;
 
 	void		(*lbr_reset)(void);
 	void		(*lbr_read)(struct cpu_hw_events *cpuc);
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 163f5d26c6a9..0dd3bfc15f71 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -167,6 +167,8 @@
 #define LBR_INFO_CYCLES			0xffff
 #define LBR_INFO_BR_TYPE_OFFSET		56
 #define LBR_INFO_BR_TYPE		(0xfull << LBR_INFO_BR_TYPE_OFFSET)
+#define LBR_INFO_EVENTS_OFFSET		32
+#define LBR_INFO_EVENTS			(0xffull << LBR_INFO_EVENTS_OFFSET)
 
 #define MSR_ARCH_LBR_CTL		0x000014ce
 #define ARCH_LBR_CTL_LBREN		BIT(0)
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 544f41a179fb..ebf4ebd63a07 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -29,6 +29,7 @@
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
+#define ARCH_PERFMON_EVENTSEL_LBR_LOG			(1ULL << 35)
 
 #define HSW_IN_TX					(1ULL << 32)
 #define HSW_IN_TX_CHECKPOINTED				(1ULL << 33)
@@ -180,6 +181,9 @@ union cpuid28_ecx {
 		unsigned int    lbr_timed_lbr:1;
 		/* Branch Type Field Supported */
 		unsigned int    lbr_br_type:1;
+		unsigned int	reserved:13;
+		/* Event Logging Supported */
+		unsigned int	lbr_events:4;
 	} split;
 	unsigned int            full;
 };
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index c0a511eea498..6001f6a3d736 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -415,7 +415,8 @@ struct perf_event_attr {
 				cgroup         :  1, /* include cgroup events */
 				text_poke      :  1, /* include text poke events */
 				build_id       :  1, /* use build id in mmap2 events */
-				__reserved_1   : 29;
+				branch_events  :  1, /* include branch events */
+				__reserved_1   : 28;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -1305,6 +1306,7 @@ union perf_mem_data_src {
 #define PERF_MEM_S(a, s) \
 	(((__u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
 
+#define PERF_MAX_BRANCH_EVENTS	4
 /*
  * single taken branch record layout:
  *
@@ -1330,7 +1332,8 @@ struct perf_branch_entry {
 		abort:1,    /* transaction abort */
 		cycles:16,  /* cycle count to last branch */
 		type:4,     /* branch type */
-		reserved:40;
+		events:8,   /* TODO: */
+		reserved:32;
 };
 
 union perf_sample_weight {
-- 
2.27.0

