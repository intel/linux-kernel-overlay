From 707ccf341b6c8ad8ad00f3536c0bdd4b2f3945db Mon Sep 17 00:00:00 2001
From: "Alessandrelli, Daniele" <daniele.alessandrelli@intel.com>
Date: Wed, 3 Oct 2018 09:43:04 +0000
Subject: [PATCH 063/223] keembay-ipc: Add Keem Bay IPC module

Added Keem Bay IPC module

Signed-off-by: Alessandrelli, Daniele <daniele.alessandrelli@intel.com>
Signed-off-by: Murphy, Paul J <paul.j.murphy@intel.com>
Signed-off-by: Jean-Hugues Decker <jean-hugues.decker@intel.com>

(cherry picked from commit f2854501c4bc72c93fbc729dc52de4b220d882d2)
---
 drivers/misc/Kconfig                          |    1 +
 drivers/misc/Makefile                         |    1 +
 drivers/misc/keembay-ipc/Kconfig              |    5 +
 drivers/misc/keembay-ipc/Makefile             |    4 +
 drivers/misc/keembay-ipc/keembay-ipc-common.h |   49 +
 drivers/misc/keembay-ipc/keembay-ipc.c        | 1516 +++++++++++++++++
 include/linux/keembay-ipc.h                   |   33 +
 7 files changed, 1609 insertions(+)
 create mode 100644 drivers/misc/keembay-ipc/Kconfig
 create mode 100644 drivers/misc/keembay-ipc/Makefile
 create mode 100644 drivers/misc/keembay-ipc/keembay-ipc-common.h
 create mode 100644 drivers/misc/keembay-ipc/keembay-ipc.c
 create mode 100644 include/linux/keembay-ipc.h

diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index e68f0398f047..91cb49b06f93 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -477,4 +477,5 @@ source "drivers/misc/intel_tsens/Kconfig"
 source "drivers/misc/xlink-smbus/Kconfig"
 source "drivers/misc/hddl_device/Kconfig"
 source "drivers/misc/emc2103/Kconfig"
+source "drivers/misc/keembay-ipc/Kconfig"
 endmenu
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index 3da7fe251b03..4b4c9df05a6d 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -62,3 +62,4 @@ obj-y                           += intel_tsens/
 obj-$(CONFIG_XLINK_SMBUS)	+= xlink-smbus/
 obj-y				+= hddl_device/
 obj-$(CONFIG_HOST_KMB_EMC)	+= emc2103/
+obj-$(CONFIG_KEEMBAY_IPC)	+= keembay-ipc/
diff --git a/drivers/misc/keembay-ipc/Kconfig b/drivers/misc/keembay-ipc/Kconfig
new file mode 100644
index 000000000000..35f8ce80bf4d
--- /dev/null
+++ b/drivers/misc/keembay-ipc/Kconfig
@@ -0,0 +1,5 @@
+config KEEMBAY_IPC
+	tristate "Support for KeemBay IPC"
+	help
+	  KeemBay IPC enables communication between KeemBay CPU Sub-System
+	  (CSS) and KeemBay Media Sub-System (MSS).
diff --git a/drivers/misc/keembay-ipc/Makefile b/drivers/misc/keembay-ipc/Makefile
new file mode 100644
index 000000000000..4486e4231bd4
--- /dev/null
+++ b/drivers/misc/keembay-ipc/Makefile
@@ -0,0 +1,4 @@
+#
+# Makefile for KeemBay IPC Linux driver
+#
+obj-$(CONFIG_KEEMBAY_IPC) += keembay-ipc.o
diff --git a/drivers/misc/keembay-ipc/keembay-ipc-common.h b/drivers/misc/keembay-ipc/keembay-ipc-common.h
new file mode 100644
index 000000000000..ca132905fc3d
--- /dev/null
+++ b/drivers/misc/keembay-ipc/keembay-ipc-common.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * keembay-ipc-common.h - KeemBay IPC common definitions.
+ *
+ * Copyright (C) 2018-2019 Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; version 2.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __KEEMBAY_IPC_COMMON_H
+#define __KEEMBAY_IPC_COMMON_H
+
+/* The alignment to be used for IPC Buffers and IPC Data. */
+#define KMB_IPC_ALIGNMENT		64
+
+/* Maximum number of channels per link. */
+#define KMB_IPC_MAX_CHANNELS		1024
+
+/* The number of high-speed channels per link. */
+#define KMB_IPC_NUM_HIGH_SPEED_CHANNELS	10
+
+/* The possible states of an IPC buffer. */
+enum {
+	/*
+	 * KMB_IPC_BUF_FREE must be set to 0 to ensure that buffers can be
+	 * initialized with memset(&buf, 0, sizeof(buf)).
+	 */
+	KMB_IPC_BUF_FREE = 0,
+	KMB_IPC_BUF_ALLOCATED,
+};
+
+/* IPC buffer. */
+struct kmb_ipc_buf {
+	uint32_t data_paddr; /* Physical address where payload is located. */
+	uint32_t data_size;  /* Size of payload. */
+	uint16_t channel;    /* The channel used. */
+	uint8_t src_node;    /* The Node ID of the sender. */
+	uint8_t dst_node;    /* The Node ID of the intended receiver. */
+	uint8_t status;	     /* Either free or allocated. */
+} __packed __aligned(KMB_IPC_ALIGNMENT);
+
+#endif /* __KEEMBAY_IPC_COMMON_H */
diff --git a/drivers/misc/keembay-ipc/keembay-ipc.c b/drivers/misc/keembay-ipc/keembay-ipc.c
new file mode 100644
index 000000000000..acda52dccef7
--- /dev/null
+++ b/drivers/misc/keembay-ipc/keembay-ipc.c
@@ -0,0 +1,1516 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * keembay-ipc.c - KeemBay IPC Driver.
+ *
+ * Copyright (C) 2018-2019 Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; version 2.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/completion.h>
+#include <linux/circ_buf.h>
+#include <linux/dma-mapping.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/jiffies.h>
+#include <linux/keembay-ipc.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+
+#include "keembay-ipc-common.h"
+
+#define DRV_NAME		"kmb-ipc-driver"
+
+#define IPC_VERSION_MAJOR	0
+#define IPC_VERSION_MINOR	1
+
+/* The IPC Node ID of this node. */
+#define MY_NODE_ID		KMB_IPC_NODE_ARM_CSS
+/*
+ * This is used as index for retrieving reserved memory from the device
+ * tree.
+ */
+#define LOCAL_IPC_BUFFER_IDX	0
+#define REMOTE_IPC_BUFFER_IDX	1
+
+/*
+ * The length of the RX software FIFO.
+ *
+ * Note: This must be a power of 2 as required by the circular buffer API
+ * defined in "linux/circ_buf.h".
+ */
+#define RX_SW_FIFO_LEN		256
+
+/*
+ * The IPC FIFO registers (offsets to the base address defined in device tree).
+ */
+/* Read/Write a word from/to FIFO. */
+#define IPC_FIFO		0x00
+/* Read from FIFO using ATM mode (currently not used by this driver). */
+#define IPC_FIFO_ATM		0x04
+/* FIFO Status. */
+#define IPC_FIFO_STAT		0x0C
+/* IPC FIFO overflow status, IDs 63-0 */
+#define IPC_FIFO_OF_FLAG0	0x10
+#define IPC_FIFO_OF_FLAG1	0x14
+
+
+/* struct ipc_buf_mem - IPC Buffer Memory Region. */
+struct ipc_buf_mem {
+	struct device *dev;	/* Child device managing the memory region. */
+	void *vaddr;		/* The virtual address of the memory region. */
+	dma_addr_t dma_handle;  /* The physical address of the memory region. */
+	size_t size;		/* The size of the memory region. */
+};
+
+/* IPC buffer pool. */
+struct ipc_buf_pool {
+	struct kmb_ipc_buf *buffers; /* Pointer to the array of buffers. */
+	size_t buf_cnt;		     /* Pool size (i.e., number of buffers). */
+	size_t idx;		     /* Current index. */
+	spinlock_t lock;	     /* The lock protecting this pool. */
+};
+
+/* IPC channel. */
+struct ipc_chan {
+	/* The list of incoming messages. */
+	struct list_head rx_data_list;
+	/* The lock for modifying the rx_data_list. */
+	spinlock_t rx_lock;
+	/* The wait queue for RX Data (recv() waits on it). */
+	wait_queue_head_t rx_wait_queue;
+	/* Closing flag, set when the channel is being closed. */
+	bool closing;
+};
+
+/**
+ * struct tx_data - Element of a TX queue.
+ * @paddr:	The physical address of the data to be "transferred".
+ * @size:	The size of the data to be "transferred".
+ * @chan_id:	The channel to be used for the "transfer".
+ * @dst_node:	The destination node.
+ * @retv:	The result of the transfer.
+ * @list:	The list head used to concatenate TX data elements.
+ * @tx_done:	The completion struct used by the sender to wait for the
+ *		transfer to complete.
+ */
+struct tx_data {
+	u32 paddr;
+	u32 size;
+	u16 chan_id;
+	u8  dst_node;
+	int retv;
+	struct list_head list;
+	struct completion tx_done;
+};
+
+/**
+ * struct tx_queue - The TX queue structure.
+ * @tx_data_list: The list of pending TX data on this queue.
+ * @lock:	  The lock protecting the TX data list.
+ */
+struct tx_queue {
+	struct list_head tx_data_list;
+	spinlock_t lock;
+};
+
+/**
+ * struct ipc_link - An IPC link.
+ * @ipc_chan:	 The channels associated with this link (the pointers to the
+ *		 channels are RCU-protected).
+ * @chan_lock:	 The lock for modifying the channels array.
+ * @srcu_sp:	 The Sleepable RCU structs associated with the link's channels.
+ * @tx_queues:	 The TX queues for this link (1 queue for each high-speed
+ *		 channels + 1 shared among all the general-purpose channels).
+ * @tx_qidx:	 The index of the next tx_queue to be check for outgoing data.
+ * @tx_queued:	 The TX completion used to signal when new TX data is pending.
+ * @tx_thread:	 The TX thread.
+ * @tx_stopping: Flag signaling that the IPC Link is being closed.
+ */
+struct ipc_link {
+	struct ipc_chan __rcu *channels[KMB_IPC_MAX_CHANNELS];
+	spinlock_t chan_lock;
+	struct srcu_struct srcu_sp[KMB_IPC_MAX_CHANNELS];
+	struct tx_queue tx_queues[KMB_IPC_NUM_HIGH_SPEED_CHANNELS + 1];
+	int tx_qidx;
+	struct completion tx_queued;
+	struct task_struct *tx_thread;
+	bool tx_stopping;
+};
+
+/**
+ * struct keembay_ipc_dev - IPC private data.
+ *
+ * @plat_dev: Platform device for this driver.
+ * @local_ipc_mem:	    Local IPC Buffer memory region.
+ * @remote_ipc_mem:	    Remove IPC Buffer memory region.
+ * @ipc_buf_pool:	    The pool of IPC buffers.
+ * @leon_mss_link:	    The ARM_CSS-Leon_MSS link.
+ * @local_fifo_irq:	    The IRQ line used by the local hardware FIFO.
+ * @local_fifo_irq_enabled: Whether or not the local hardware FIFO IRQ is
+ *			    enabled.
+ * @local_fifo_irq_lock:    The spinlock to protect the enabled status of the
+ *			    local FIFO IRQ.
+ * @local_fifo_reg:	    The base address of the local HW FIFO.
+ * @remote_fifo_reg:	    The base address of the remote HW FIFO (i.e., the
+ *			    Leon MSS FIFO).
+ */
+struct keembay_ipc_dev {
+	struct platform_device *plat_dev;
+	struct ipc_buf_mem local_ipc_mem;
+	struct ipc_buf_mem remote_ipc_mem;
+	struct ipc_buf_pool ipc_buf_pool;
+	struct ipc_link leon_mss_link;
+	int local_fifo_irq;
+	int local_fifo_irq_enabled;
+	spinlock_t local_fifo_irq_lock;
+	void __iomem *local_fifo_reg;
+	void __iomem *remote_fifo_reg;
+};
+
+/*
+ * RX Data Descriptor.
+ *
+ * Instances of this struct are meant to be inserted in the RX Data queue
+ * (list) associated with each channel.
+ */
+struct rx_data {
+	/* The physical address of the received data. */
+	uint32_t data_paddr;
+	/* The size of the received data. */
+	uint32_t data_size;
+	/* List head for creating a list of rx_data elements. */
+	struct list_head list;
+};
+
+/*
+ * RX Circular Buffer struct.
+ *
+ * The producer inserts elements to the head and the consumers extracts
+ * elements from the tail. See Documentation/circular-buffers.txt for usage
+ * details.
+ */
+struct rx_circ_buf {
+	uint32_t buf[RX_SW_FIFO_LEN];
+	int head;
+	int tail;
+};
+
+/* Forward declaration of TX thread function. */
+static int tx_thread_fn(void *data);
+/* Forward declaration of ISR function. */
+static irqreturn_t local_fifo_irq_handler(int irq, void *dev_id);
+/* The RX tasklet. */
+static void rx_tasklet_func(unsigned long);
+static DECLARE_TASKLET(rx_tasklet, rx_tasklet_func, 0);
+
+/*
+ * The RX SW FIFO.
+ *
+ * The RX ISR writes to this FIFO and the RX tasklet reads from it.
+ */
+static struct rx_circ_buf rx_sw_fifo;
+
+/*
+ * Global variable pointing to our KeemBay IPC Device.
+ *
+ * This is meant to be used only when platform_get_drvdata() cannot be used
+ * because we lack a reference to our platform_device.
+ */
+static struct keembay_ipc_dev *kmb_ipc_dev;
+
+/*
+ * Functions related to reserved-memory sub-devices.
+ */
+
+/* Remove the IPC memory sub-devices. */
+static void ipc_reserved_memory_remove(struct keembay_ipc_dev *dev)
+{
+	device_unregister(dev->local_ipc_mem.dev);
+	device_unregister(dev->remote_ipc_mem.dev);
+}
+
+/* Release function for the reserved memory sub-devices. */
+static void ipc_reserved_mem_release(struct device *dev)
+{
+	of_reserved_mem_device_release(dev);
+}
+
+/* Get the size of the specified reserved memory region. */
+static resource_size_t get_ipc_reserved_mem_size(struct device *dev,
+						 unsigned int idx)
+{
+	struct resource mem;
+	struct device_node *np;
+	int rc;
+
+	np = of_parse_phandle(dev->of_node, "memory-region", idx);
+	if (!np) {
+		dev_err(dev, "Couldn't find memory-region %d\n", idx);
+		return 0;
+	}
+
+	rc = of_address_to_resource(np, 0, &mem);
+	if (rc) {
+		dev_err(dev, "Couldn't map address to resource %d\n", idx);
+		return 0;
+	}
+
+	return resource_size(&mem);
+}
+
+/* Init a reserved memory sub-devices. */
+static struct device *init_ipc_reserved_mem_dev(struct device *dev,
+						const char *name,
+						unsigned int idx)
+{
+	struct device *child;
+	int rc;
+
+	child = devm_kzalloc(dev, sizeof(struct device), GFP_KERNEL);
+	if (!child)
+		return NULL;
+
+	device_initialize(child);
+	dev_set_name(child, "%s:%s", dev_name(dev), name);
+	child->parent = dev;
+	child->coherent_dma_mask = dev->coherent_dma_mask;
+	child->dma_mask = dev->dma_mask;
+	child->release = ipc_reserved_mem_release;
+
+	rc = device_add(child);
+	if (rc)
+		goto err;
+
+	rc = of_reserved_mem_device_init_by_idx(child, dev->of_node, idx);
+	if (rc) {
+		dev_err(dev, "Couldn't get reserved memory with idx = %d, %d\n",
+			idx, rc);
+		device_del(child);
+		goto err;
+	}
+
+	return child;
+
+err:
+	put_device(child);
+	return NULL;
+}
+
+/* Init reserved memory for our diver. */
+static int ipc_reserved_memory_init(struct keembay_ipc_dev *ipc_dev)
+{
+	struct device *dev = &ipc_dev->plat_dev->dev;
+
+	ipc_dev->local_ipc_mem.dev = init_ipc_reserved_mem_dev(
+		dev, "ipc_local_reserved", LOCAL_IPC_BUFFER_IDX);
+	if (!ipc_dev->local_ipc_mem.dev)
+		return -ENOMEM;
+
+	ipc_dev->local_ipc_mem.size =
+		get_ipc_reserved_mem_size(dev, LOCAL_IPC_BUFFER_IDX);
+
+	ipc_dev->remote_ipc_mem.dev = init_ipc_reserved_mem_dev(
+		dev, "ipc_remote_reserved", REMOTE_IPC_BUFFER_IDX);
+	if (!ipc_dev->remote_ipc_mem.dev) {
+		device_unregister(ipc_dev->local_ipc_mem.dev);
+		return -ENOMEM;
+	}
+
+	ipc_dev->remote_ipc_mem.size =
+		get_ipc_reserved_mem_size(dev, REMOTE_IPC_BUFFER_IDX);
+
+	return 0;
+}
+
+/*
+ * Keem Bay IPC HW FIFO API.
+ *
+ * We use two HW FIFOs: the local one and the remote one (Leon MSS FIFO).
+ *
+ * The local FIFO holds incoming IPC FIFO entries and therefore is used by RX
+ * code.
+ *
+ * The remote FIFO is where we put outgoing FIFO entries and therefore is used
+ * by TX code.
+ */
+
+/* Get number of entries in the FIFO. */
+static int local_fifo_cnt(struct keembay_ipc_dev *ipc_dev)
+{
+	/*
+	 * TIM_IPC_FIFO_STAT
+	 *
+	 * Bits 31:24: Reserved
+	 * Bits 23:16: IPC FIFO fill level
+	 * Bits 15:8:  IPC FIFO write pointer
+	 * Bits  7:0:  IPC FIFO read pointer
+	 */
+	const uint32_t val = ioread32(ipc_dev->local_fifo_reg + IPC_FIFO_STAT);
+
+	return (val >> 16) & 0xFF;
+}
+
+/* Extract an entry from the FIFO. */
+static uint32_t local_fifo_get(struct keembay_ipc_dev *ipc_dev)
+{
+	/*
+	 * TIM_IPC_FIFO_ATM
+	 *
+	 * Read a value from IPC FIFO.
+	 *
+	 * If FIFO is empty return 0xFFFFFFFF, otherwise return value from FIFO
+	 * with 6 LSBs set to 0.
+	 * Increment FIFO read pointer and decrement fill level.
+	 */
+	return ioread32(ipc_dev->local_fifo_reg + IPC_FIFO_ATM);
+}
+
+/* Disable local FIFO IRQ. */
+static void local_fifo_irq_disable(struct keembay_ipc_dev *ipc_dev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&ipc_dev->local_fifo_irq_lock, flags);
+	if (ipc_dev->local_fifo_irq_enabled) {
+		disable_irq_nosync(ipc_dev->local_fifo_irq);
+		ipc_dev->local_fifo_irq_enabled = false;
+	}
+	spin_unlock_irqrestore(&ipc_dev->local_fifo_irq_lock, flags);
+}
+
+/* Enable local FIFO IRQ. */
+static void local_fifo_irq_enable(struct keembay_ipc_dev *ipc_dev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&ipc_dev->local_fifo_irq_lock, flags);
+	if (!ipc_dev->local_fifo_irq_enabled) {
+		enable_irq(ipc_dev->local_fifo_irq);
+		ipc_dev->local_fifo_irq_enabled = true;
+	}
+	spin_unlock_irqrestore(&ipc_dev->local_fifo_irq_lock, flags);
+}
+
+/* Add an entry to a remote FIFO. */
+static void remote_fifo_put(struct keembay_ipc_dev *ipc_dev, uint32_t entry)
+{
+	/*
+	 * TIM_IPC_FIFO.
+	 *
+	 * Write a value to IPC FIFO if not full.
+	 *
+	 * Increment FIFO write pointer and fill level.
+	 *
+	 * In normal usage the 6 LSBs bits are reserved for the writing
+	 * processor to include its processor ID, 0 <= x <= 62, so it can
+	 * determine if the word was written correctly by checking the
+	 * appropriate bit of register TIM_IPC_FIFO_OF_FLAGn.
+	 */
+	iowrite32(entry, ipc_dev->remote_fifo_reg + IPC_FIFO);
+}
+
+
+/*
+ * Check if we have overflowed the remote FIFO.
+ *
+ * Check the overflow register of the remote FIFO to see if we have overflowed
+ * it. If so clear our overflow.
+ *
+ * @return True if we have overflowed the FIFO, false otherwise.
+ *
+ */
+static bool remote_fifo_overflow(struct keembay_ipc_dev *ipc_dev)
+{
+	/*
+	 * TIM_IPC_FIFO_OF_FLAGn
+	 *
+	 * Read:
+	 *
+	 * A processor with ID = x can check that its writes to the IPC FIFO
+	 * were successful by reading 0 from bit x of TIM_IPC_FIFO_OF_FLAG0 or
+	 * TIM_IPC_FIFO_OF_FLAG1.
+	 *
+	 * Bit x, 0 <= x <= 31, of TIM_IPC_FIFO_OF_FLAG0 is set high if a write
+	 * to TIM_IPC_FIFO by processor ID x failed because the FIFO was full.
+	 *
+	 * Bit x, 0 <= x <= 30, of TIM_IPC_FIFO_OF_FLAG1 is set high if a write
+	 * to TIM_IPC_FIFO by processor ID x+32 failed because the FIFO was
+	 * full.
+	 *
+	 * Processors are identified by the 6 LSBs of words written to
+	 * TIM_IPC_FIFO, i.e. x = TIM_IPC_FIFO[5:0].  Processor ID = 0x3F is
+	 * reserved to indicate a read of an empty FIFO has occurred.
+	 *
+	 * Write:
+	 *
+	 * Writing 1 to bit position x of TIM_IPC_FIFO_OF_FLAG0 clears the
+	 * overflow flag corresponding to processor ID x.  Writing 1 to bit
+	 * position x of TIM_IPC_FIFO_OF_FLAGn clears the overflow flag
+	 * corresponding to processor ID x+32.
+	 *
+	 * Writing 0 to any bit position has not effect.
+	 */
+	const uint32_t offset = (MY_NODE_ID < 32) ? IPC_FIFO_OF_FLAG0
+						  : IPC_FIFO_OF_FLAG1;
+	const uint32_t mask = 1 << (MY_NODE_ID % 32);
+	const bool rc = ioread32(ipc_dev->remote_fifo_reg + offset) & mask;
+	/* Clear our overflow bit, if we overflowed. */
+	if (rc)
+		iowrite32(mask, ipc_dev->remote_fifo_reg + offset);
+	return rc;
+}
+
+/*
+ * IPC internal functions.
+ */
+
+/**
+ * channel_close() - Close a channel and return whether it was open or not.
+ * @link:	The link the channel belongs to.
+ * @chan_id:	The channel ID of the channel to close.
+ *
+ * Return:	0 if the channel was already closed, 1 otherwise.
+ */
+static int channel_close(struct ipc_link *link, u16 chan_id)
+{
+	struct ipc_chan *chan;
+	struct rx_data *pos, *nxt;
+	unsigned long flags;
+
+	/* Remove channel from channel array. */
+	spin_lock_irqsave(&link->chan_lock, flags);
+	chan = rcu_dereference(link->channels[chan_id]);
+	RCU_INIT_POINTER(link->channels[chan_id], NULL);
+	spin_unlock_irqrestore(&link->chan_lock, flags);
+
+	/* If channel was NULL, we are done. */
+	if (!chan)
+		return 0;
+	/* Set closing flag and wake up user threads waiting on recv(). */
+	chan->closing = true;
+	wake_up_all(&chan->rx_wait_queue);
+	/* Wait for channel users to drop the reference to the old channel. */
+	synchronize_srcu(&link->srcu_sp[chan_id]);
+	/* Free channel memory (rx_data queue and channel struct). */
+	/*
+	 * No need to get chan->rx_lock as we know that nobody is using the
+	 * channel at this point.
+	 */
+	list_for_each_entry_safe(pos, nxt, &chan->rx_data_list, list) {
+		list_del(&pos->list);
+		kfree(pos);
+	}
+	kfree(chan);
+
+	return 1;
+}
+
+/*
+ * Allocate an IPC buffer to be used for TX.
+ *
+ * @param pool The IPC buffer pool from which the IPC buffer will be allocated.
+ *
+ * @return The pointer to the allocated buffer, or NULL if allocation fails.
+ */
+static struct kmb_ipc_buf *ipc_buf_tx_alloc(struct ipc_buf_pool *pool)
+{
+	struct kmb_ipc_buf *buf;
+	size_t i = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pool->lock, flags);
+	/*
+	 * Look for a free buffer starting from the last index (pointing to the
+	 * next buffer after the last allocated one) and potentially going
+	 * through all the buffers in the pool.
+	 */
+	for (i = 0; i < pool->buf_cnt; i++) {
+		/*
+		 * Get reference to current buffer and increment index (for
+		 * next iteration or function call).
+		 */
+		buf = &pool->buffers[pool->idx++];
+		if (pool->idx == pool->buf_cnt)
+			pool->idx = 0;
+		/* Use current buffer if free. */
+		if (buf->status == KMB_IPC_BUF_FREE) {
+			buf->status = KMB_IPC_BUF_ALLOCATED;
+			spin_unlock_irqrestore(&pool->lock, flags);
+			return buf;
+		}
+	}
+	spin_unlock_irqrestore(&pool->lock, flags);
+	/* We went through all the buffers and found none free; return error. */
+	return NULL;
+}
+
+/*
+ * Init the IPC Buffer Pool.
+ *
+ * Set up the IPC Buffer Pool to be used for allocating TX buffers.
+ *
+ * The pool uses the local IPC Buffer memory previously allocated.
+ */
+static int init_ipc_buf_pool(struct keembay_ipc_dev *ipc_dev)
+{
+	struct ipc_buf_mem *mem = &ipc_dev->local_ipc_mem;
+	/* Initialize ipc_buf_poll global variable. */
+	/*
+	 * Start by setting everything to 0 to initialize the IPC Buffer array
+	 * (this works because the value of KMB_IPC_BUF_FREE is 0).
+	 */
+	memset(mem->vaddr, 0, mem->size);
+	ipc_dev->ipc_buf_pool.buffers = mem->vaddr;
+	ipc_dev->ipc_buf_pool.buf_cnt = mem->size / sizeof(struct kmb_ipc_buf);
+	ipc_dev->ipc_buf_pool.idx = 0;
+	dev_info(&ipc_dev->plat_dev->dev, "IPC Buffer Pool size: %zu\n",
+		 ipc_dev->ipc_buf_pool.buf_cnt);
+	spin_lock_init(&ipc_dev->ipc_buf_pool.lock);
+	return 0;
+}
+
+/*
+ * ipc_link_init() - Initialize an IPC link.
+ * @ipc_dev: The IPC device the link belongs to.
+ * @link:    The link to initialize.
+ *
+ * This function is expected to be called during probing.
+ *
+ * Return: 0 on success, negative error code otherwise.
+ */
+static int ipc_link_init(struct keembay_ipc_dev *ipc_dev, struct ipc_link *link)
+{
+	struct tx_queue *queue;
+	int i;
+
+	spin_lock_init(&link->chan_lock);
+	for (i = 0; i < ARRAY_SIZE(link->srcu_sp); i++)
+		init_srcu_struct(&link->srcu_sp[i]);
+	memset(link->channels, 0, sizeof(link->channels));
+	/* Init TX queues. */
+	for (i = 0; i < ARRAY_SIZE(link->tx_queues); i++) {
+		queue = &link->tx_queues[i];
+		INIT_LIST_HEAD(&queue->tx_data_list);
+		spin_lock_init(&queue->lock);
+	}
+	link->tx_qidx = 0;
+	link->tx_stopping = false;
+	init_completion(&link->tx_queued);
+	/* Start TX thread. */
+	link->tx_thread = kthread_run(tx_thread_fn, ipc_dev,
+				      "kmb_ipc_tx_thread");
+	if (IS_ERR(link->tx_thread))
+		return PTR_ERR(link->tx_thread);
+	return 0;
+}
+
+/**
+ * ipc_link_deinit() - De-initialize an IPC link.
+ * @ipc_dev:	The IPC device the link belongs to.
+ * @link:	The link to de-initialize.
+ */
+static void ipc_link_deinit(struct keembay_ipc_dev *ipc_dev,
+			    struct ipc_link *link)
+{
+	struct tx_queue *queue;
+	struct tx_data *pos, *nxt;
+	int i;
+
+	/* Close all channels. */
+	for (i = 0; i < ARRAY_SIZE(link->channels); i++)
+		channel_close(link, i);
+	/* Stop TX Thread. */
+	link->tx_stopping = true;
+	complete(&link->tx_queued);
+	kthread_stop(link->tx_thread);
+	/* Flush all TX queue. */
+	for (i = 0; i < ARRAY_SIZE(link->tx_queues); i++) {
+		queue = &link->tx_queues[i];
+		list_for_each_entry_safe(pos, nxt, &queue->tx_data_list, list) {
+			list_del(&pos->list);
+			pos->retv = -EPIPE;
+			complete(&pos->tx_done);
+		}
+	}
+}
+
+/**
+ * ipc_hw_init() - Init IPC-related hardware and memory.
+ */
+static int ipc_hw_init(struct platform_device *pdev,
+		       struct keembay_ipc_dev *ipc_dev)
+{
+	int rc, irq;
+	struct resource *res;
+	void __iomem *reg;
+
+	/* Get Local FIFO Register. */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "Couldn't find local FIFO registries\n");
+		return -EINVAL;
+	}
+	reg = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(reg)) {
+		dev_err(&pdev->dev, "Failed to iomap local FIFO registries\n");
+		return PTR_ERR(reg);
+	}
+	dev_info(&pdev->dev, "Local FIFO register base: 0x%p\n", reg);
+	ipc_dev->local_fifo_reg = reg;
+
+	/* Get Remote FIFO Register. */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	if (!res) {
+		dev_err(&pdev->dev, "Couldn't find remote FIFO registries\n");
+		return -EINVAL;
+	}
+	reg = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(reg)) {
+		dev_err(&pdev->dev, "Failed to iomap remote FIFO registries\n");
+		return PTR_ERR(reg);
+	}
+	dev_info(&pdev->dev, "Remote FIFO register base: 0x%p\n", reg);
+	ipc_dev->remote_fifo_reg = reg;
+
+	/*
+	 * Register interrupt handler and initialize IRQ-related fields in IPC
+	 * device struct.
+	 */
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		dev_err(&pdev->dev, "Failed getting IRQ for local fifo: %d\n",
+			irq);
+	}
+	ipc_dev->local_fifo_irq = irq;
+	ipc_dev->local_fifo_irq_enabled = true;
+	spin_lock_init(&ipc_dev->local_fifo_irq_lock);
+	dev_info(&pdev->dev, "Registering handler for IRQ %d\n", irq);
+	rc = devm_request_irq(&pdev->dev, irq, local_fifo_irq_handler, 0,
+			      "keembay-ipc", ipc_dev);
+	return 0;
+}
+
+/**
+ * ipc_hw_deinit() - De-initialize IPC-related hardware and memory.
+ */
+static void ipc_hw_deinit(struct keembay_ipc_dev *ipc_dev)
+{
+	/*
+	 * Just disable the tasklet as iomem and irq have been requested with
+	 * devm_* functions and, therefore, are freed automatically.
+	 */
+	tasklet_disable(&rx_tasklet);
+}
+
+/* Driver probing. */
+static int kmb_ipc_probe(struct platform_device *pdev)
+{
+	int rc;
+	struct keembay_ipc_dev *ipc_dev;
+
+	dev_info(&pdev->dev, "KeemBay IPC v%d.%d\n", IPC_VERSION_MAJOR,
+		 IPC_VERSION_MINOR);
+
+	ipc_dev = devm_kzalloc(&pdev->dev, sizeof(*ipc_dev), GFP_KERNEL);
+	if (!ipc_dev)
+		return -ENOMEM;
+
+	ipc_dev->plat_dev = pdev;
+
+	/* Grab reserved memory regions and assign to child devices */
+	rc = ipc_reserved_memory_init(ipc_dev);
+	if (rc < 0) {
+		dev_err(&pdev->dev,
+			"Failed to set up reserved memory regions.\n");
+		return rc;
+	}
+
+	/* Allocate memory from the reserved memory regions */
+	ipc_dev->local_ipc_mem.vaddr =
+		dmam_alloc_coherent(ipc_dev->local_ipc_mem.dev,
+				    ipc_dev->local_ipc_mem.size,
+				    &ipc_dev->local_ipc_mem.dma_handle,
+				    GFP_KERNEL);
+	if (!ipc_dev->local_ipc_mem.vaddr) {
+		dev_err(&pdev->dev,
+			"Failed to allocate from local reserved memory.\n");
+		ipc_reserved_memory_remove(ipc_dev);
+		return -ENOMEM;
+	}
+	ipc_dev->remote_ipc_mem.vaddr = dmam_alloc_coherent(
+		ipc_dev->remote_ipc_mem.dev, ipc_dev->remote_ipc_mem.size,
+		&ipc_dev->remote_ipc_mem.dma_handle, GFP_KERNEL);
+	if (!ipc_dev->remote_ipc_mem.vaddr) {
+		dev_err(&pdev->dev,
+			"Failed to allocate from remote reserved memory.\n");
+		ipc_reserved_memory_remove(ipc_dev);
+		return -ENOMEM;
+	}
+
+	dev_info(&pdev->dev, "Local vaddr 0x%p paddr 0x%pad size 0x%zX\n",
+		 ipc_dev->local_ipc_mem.vaddr,
+		 &ipc_dev->local_ipc_mem.dma_handle,
+		 ipc_dev->local_ipc_mem.size);
+	dev_info(&pdev->dev, "Remote vaddr 0x%p paddr 0x%pad size 0x%zX\n",
+		 ipc_dev->remote_ipc_mem.vaddr,
+		 &ipc_dev->remote_ipc_mem.dma_handle,
+		 ipc_dev->remote_ipc_mem.size);
+
+	/* Init the pool of IPC Buffer to be used to TX. */
+	init_ipc_buf_pool(ipc_dev);
+	/* Init the only link we have (ARM CSS -> LEON MSS). */
+	rc = ipc_link_init(ipc_dev, &ipc_dev->leon_mss_link);
+	if (rc)
+		return rc;
+
+	/* Init IPC HW FIFO. */
+	ipc_hw_init(pdev, ipc_dev);
+
+	platform_set_drvdata(pdev, ipc_dev);
+
+	/* Set the global reference to our device. */
+	kmb_ipc_dev = ipc_dev;
+
+	return 0;
+}
+
+/* Driver removal. */
+static int kmb_ipc_remove(struct platform_device *pdev)
+{
+	struct keembay_ipc_dev *ipc_dev = platform_get_drvdata(pdev);
+
+	ipc_hw_deinit(ipc_dev);
+
+	ipc_link_deinit(ipc_dev, &ipc_dev->leon_mss_link);
+
+	/*
+	 * No need to de-alloc IPC memory (local_ipc_mem and remote_ipc_mem)
+	 * since it was allocated with dmam_alloc.
+	 */
+
+	ipc_reserved_memory_remove(ipc_dev);
+
+	return 0;
+}
+
+/*
+ * Perform basic validity check on specified node ID and channel ID.
+ *
+ * Verify that the specified node ID and the channel ID are within the allowed
+ * ranges.
+ */
+static int validate_link_chan(struct device *dev, u8 node_id, u16 chan_id)
+{
+	if (node_id != KMB_IPC_NODE_LEON_MSS) {
+		dev_warn(dev, "Invalid Link ID\n");
+		return -EINVAL;
+	}
+	if (chan_id >= KMB_IPC_MAX_CHANNELS) {
+		dev_warn(dev, "Invalid Channel ID\n");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * ipc_phys_to_virt() - Convert IPC physical addresses to virtual addresses.
+ *
+ * @ipc_mem: The IPC memory region where the physical address is expected to be.
+ * @paddr:   The physical address to be converted to a virtual one.
+ *
+ * Return: The corresponding virtual address, or NULL if the physical address
+ *	   is not in the expected memory range.
+ */
+static void *ipc_phys_to_virt(const struct ipc_buf_mem *ipc_mem, uint32_t paddr)
+{
+	if (unlikely(paddr < ipc_mem->dma_handle) ||
+		     paddr >= (ipc_mem->dma_handle + ipc_mem->size))
+		return NULL;
+	return ipc_mem->vaddr + (paddr - ipc_mem->dma_handle);
+}
+
+/*
+ * ipc_virt_to_phys() - Convert IPC virtual addresses to physical addresses.
+ * @ipc_mem: [in]  The IPC memory region where the physical address is expected
+ *		   to be.
+ * @vaddr:   [in]  The virtual address to be converted to a physical one.
+ * @paddr:   [out] Where to store the computed physical address.
+ *
+ * Return: 0 on success, negative error code otherwise.
+ */
+static int ipc_virt_to_phys(struct ipc_buf_mem *ipc_mem, void *vaddr,
+			    uint32_t *paddr)
+{
+	if (unlikely((ipc_mem->dma_handle + ipc_mem->size) > 0xFFFFFFFF))
+		return -EINVAL;
+	if (unlikely(vaddr < ipc_mem->vaddr ||
+		     vaddr >= (ipc_mem->vaddr + ipc_mem->size)))
+		return -EINVAL;
+	*paddr = ipc_mem->dma_handle + (vaddr - ipc_mem->vaddr);
+
+	return 0;
+}
+
+/**
+ * process_rx_fifo_entry() - Process a FIFO entry.
+ * @entry	The FIFO entry to process.
+ * @ipc_dev:	The IPC device to use.
+ *
+ * This function performs the following tasks:
+ * - Check the source node id.
+ * - Process the IPC buffer (locate it, validate it, read data info, release
+ *   buffer).
+ * - Add an RX Data descriptor (data ptr and data size) to the channel RX queue.
+ */
+static void process_rx_fifo_entry(uint32_t entry,
+				  struct keembay_ipc_dev *ipc_dev)
+{
+	struct device *dev = &ipc_dev->plat_dev->dev;
+	struct ipc_link *link = &ipc_dev->leon_mss_link;
+	struct kmb_ipc_buf *ipc_buf;
+	struct rx_data *rx_data;
+	struct ipc_chan *chan;
+	int idx;
+	unsigned long flags;
+
+	dev_dbg(dev, "RX: Processing entry: %x\n", entry);
+	/* Get IPC buffer. */
+	ipc_buf = ipc_phys_to_virt(&ipc_dev->remote_ipc_mem, entry);
+	if (unlikely(!ipc_buf)) {
+		dev_warn(dev, "RX: Message out of expected memory range: %x\n",
+			 entry);
+		/* Return immediately (cannot mark the IPC buffer as free). */
+		return;
+	}
+	if (unlikely(ipc_buf->src_node != KMB_IPC_NODE_LEON_MSS)) {
+		dev_warn(dev, "RX: Message from unexpected source: %d\n",
+			 ipc_buf->src_node);
+		goto exit;
+	}
+	/* Check destination node. */
+	if (unlikely(ipc_buf->dst_node != MY_NODE_ID)) {
+		dev_warn(dev, "RX: Message is not for this node\n");
+		goto exit;
+	}
+	/* Preliminary channel check. */
+	if (unlikely(ipc_buf->channel >= KMB_IPC_MAX_CHANNELS)) {
+		dev_warn(dev, "RX: Message for invalid channel\n");
+		goto exit;
+	}
+
+	/* Access internal channel struct (this is protected by an SRCU). */
+	idx = srcu_read_lock(&link->srcu_sp[ipc_buf->channel]);
+	chan = rcu_dereference(link->channels[ipc_buf->channel]);
+	if (unlikely(!chan)) {
+		srcu_read_unlock(&link->srcu_sp[ipc_buf->channel], idx);
+		dev_warn(dev, "RX: Message for closed channel.\n");
+		goto exit;
+	}
+	rx_data = kmalloc(sizeof(*rx_data), GFP_ATOMIC);
+	if (unlikely(!rx_data)) {
+		/* If kmalloc fails, we are forced to discard the message. */
+		srcu_read_unlock(&link->srcu_sp[ipc_buf->channel], idx);
+		dev_err(dev, "RX: Message dropped: Cannot allocate RX Data.\n");
+		goto exit;
+	}
+	/* Read data info. */
+	rx_data->data_paddr = ipc_buf->data_paddr;
+	rx_data->data_size = ipc_buf->data_size;
+	/* Put data info in rx channel queue. */
+	spin_lock_irqsave(&chan->rx_lock, flags);
+	list_add_tail(&rx_data->list, &chan->rx_data_list);
+	spin_unlock_irqrestore(&chan->rx_lock, flags);
+	/* Wake up threads waiting on recv(). */
+	wake_up_interruptible(&chan->rx_wait_queue);
+	/* Exit SRCU region protecting chan struct. */
+	srcu_read_unlock(&link->srcu_sp[ipc_buf->channel], idx);
+exit:
+	barrier(); /* Ensure IPC buffer is fully processed before release. */
+	ipc_buf->status = KMB_IPC_BUF_FREE;
+}
+
+/*
+ * The function implementing the RX tasklet.
+ *
+ * Go through the RX SW FIFO (filled by the RX ISR) and process every entry.
+ */
+static void rx_tasklet_func(unsigned long unused)
+{
+	uint32_t entry;
+	/*
+	 * Memory barrier: make sure that we get buffer head before any other
+	 * operation on the circular buffer.
+	 */
+	const unsigned int head = smp_load_acquire(&rx_sw_fifo.head);
+	unsigned long tail = rx_sw_fifo.tail;
+	const size_t size = ARRAY_SIZE(rx_sw_fifo.buf);
+
+	while (CIRC_CNT(head, tail, size)) {
+		/* Extract one item from the buffer. */
+		entry = rx_sw_fifo.buf[tail];
+		/* Consume the item. */
+		process_rx_fifo_entry(entry, kmb_ipc_dev);
+		/* Update tail index (wrapping it if needed) */
+		tail = (tail + 1) & (size - 1);
+	}
+	/* Memory barrier ensuring tail is updated only at the end. */
+	smp_store_release(&rx_sw_fifo.tail, tail);
+	/* Enable Local FIFO interrupt. */
+	local_fifo_irq_enable(kmb_ipc_dev);
+}
+
+
+/*
+ * Local FIFO ISR.
+ *
+ * The HW FIFO produces interrupts when not full. This ISR is called to handle
+ * such interrupts.
+ *
+ * The ISR moves FIFO entries from the HW FIFO to an internal SW FIFO
+ * (implemented as a circular buffer) and then activates a tasklet to handle
+ * the entries in the SW FIFO.
+ *
+ * The ISR tries to empty the HW FIFO, thus making it stop generating
+ * interrupts. If it is not possible to empty the HW FIFO because the SW FIFO
+ * is full, the ISR disables the HW FIFO interrupts, thus preventing ISR
+ * re-activation and giving time to the tasklet to consume entries in the SW
+ * FIFO. When the tasklet it is done, it re-enables the interrupts.
+ */
+static irqreturn_t local_fifo_irq_handler(int irq, void *ptr)
+{
+	unsigned int head = rx_sw_fifo.head;
+	const unsigned int tail = READ_ONCE(rx_sw_fifo.tail);
+	const size_t size = ARRAY_SIZE(rx_sw_fifo.buf);
+	struct keembay_ipc_dev *ipc_dev = ptr;
+
+	/* Copy entries to internal SW FIFO. */
+	while (likely(local_fifo_cnt(ipc_dev))) {
+		/* If SW FIFO is full, disable HW FIFO interrupt. */
+		if (unlikely(!CIRC_SPACE(head, tail, size))) {
+			local_fifo_irq_disable(ipc_dev);
+			goto exit;
+		}
+		rx_sw_fifo.buf[head] = local_fifo_get(ipc_dev);
+		/* Update head index (wrapping it if needed). */
+		head = (head + 1) & (size - 1);
+	}
+exit:
+	/*
+	 * Memory barrier: make sure that updating the buffer head is the last
+	 * operation done on the circular buffer.
+	 */
+	smp_store_release(&rx_sw_fifo.head, head);
+	tasklet_schedule(&rx_tasklet);
+	return IRQ_HANDLED;
+}
+
+/**
+ * tx_data_send() - Send a TX data element.
+ * @ipc_dev:	The IPC device to use.
+ * @tx_data:	The TX data element to send.
+ */
+static int tx_data_send(struct keembay_ipc_dev *ipc_dev,
+			struct tx_data *tx_data)
+{
+	struct kmb_ipc_buf *ipc_buf = NULL;
+	struct device *dev = &ipc_dev->plat_dev->dev;
+	uint32_t entry;
+	int rc;
+
+	dev_dbg(dev, "%s(chan=%u)\n", __func__, tx_data->chan_id);
+	/* Allocate and set IPC buffer. */
+	ipc_buf = ipc_buf_tx_alloc(&kmb_ipc_dev->ipc_buf_pool);
+	if (unlikely(!ipc_buf)) {
+		rc = -ENOMEM;
+		goto exit;
+	}
+
+	/* Prepare IPC buffer. */
+	ipc_buf->channel = tx_data->chan_id;
+	ipc_buf->src_node = MY_NODE_ID;
+	ipc_buf->dst_node = tx_data->dst_node;
+	ipc_buf->data_paddr = tx_data->paddr;
+	ipc_buf->data_size = tx_data->size;
+	/* Ensure changes to IPC Buffer are performed before entry is sent. */
+	wmb();
+
+	/* Initialize entry to ipc_buf Physical address. */
+	rc = ipc_virt_to_phys(&kmb_ipc_dev->local_ipc_mem, ipc_buf, &entry);
+
+	/*
+	 * Check validity of IPC buffer physical address (these errors never
+	 * occur if IPC buffer region is defined properly in Device Tree).
+	 */
+	if (unlikely(rc)) {
+		dev_err(dev, "Cannot convert IPC buf vaddr to paddr: %p\n",
+			ipc_buf);
+		rc = -ENXIO;
+		goto exit;
+	}
+	if (unlikely(entry & 0x3F)) {
+		dev_err(dev, "Allocated IPC buf is not 64-bit aligned: %p\n",
+			ipc_buf);
+		rc = -EFAULT;
+		goto exit;
+	}
+
+	/* Set Processor ID in entry. */
+	entry |= (MY_NODE_ID & 0x3F);
+	dev_dbg(dev, "TX IPC FIFO entry: %x\n", entry);
+
+	/* Send entry and check for overflow. */
+	remote_fifo_put(kmb_ipc_dev, entry);
+	if (remote_fifo_overflow(kmb_ipc_dev)) {
+		rc = -EBUSY;
+		goto exit;
+	}
+
+exit:
+	/* If an error occurred and a buffer was allocated, free it. */
+	if (rc && ipc_buf)
+		ipc_buf->status = KMB_IPC_BUF_FREE;
+	return rc;
+}
+
+/**
+ * tx_data_dequeue() - Dequeue the next TX data waiting for transfer.
+ * @link:  The link from which we dequeue the TX Data.
+ *
+ * The de-queue policy is round robin between each high speed channel queue and
+ * the one queue for all low speed channels.
+ *
+ * Return: The next TX data waiting to be transferred, or NULL if no TX is
+ *	   pending.
+ */
+static struct tx_data *tx_data_dequeue(struct ipc_link *link)
+{
+	struct tx_queue *queue;
+	struct tx_data *tx_data;
+	int i;
+	unsigned long flags;
+
+	/*
+	 * TX queues are logically organized in a circular array.
+	 * We go through such an array until we find a non-empty queue.
+	 * We start from where we left since last function invocation.
+	 * If all queues are empty we return NULL.
+	 */
+	for (i = 0; i < ARRAY_SIZE(link->tx_queues); i++) {
+		queue = &link->tx_queues[link->tx_qidx];
+		link->tx_qidx++;
+		if (link->tx_qidx == ARRAY_SIZE(link->tx_queues))
+			link->tx_qidx = 0;
+		if (!list_empty(&queue->tx_data_list)) {
+			spin_lock_irqsave(&queue->lock, flags);
+			tx_data = list_first_entry_or_null(
+					&queue->tx_data_list,
+					struct tx_data, list);
+			/*
+			 * There is a remote chance that tx_data is NULL; this
+			 * can happen when tx_data_remove() is called between
+			 * our list_empty() check and our list_first_entry()
+			 * call. If tx_data is null, we simply continue.
+			 */
+			if (!tx_data) {
+				spin_unlock_irqrestore(&queue->lock, flags);
+				continue;
+			}
+			list_del(&tx_data->list);
+			spin_unlock_irqrestore(&queue->lock, flags);
+			return tx_data;
+		}
+	}
+
+	return NULL;
+}
+
+/**
+ * tx_data_enqueue() - Enqueue TX data for transfer into the specified link.
+ * @link:	The link the data is enqueued to.
+ * @tx_data:	The TX data to enqueue.
+ */
+static void tx_data_enqueue(struct ipc_link *link, struct tx_data *tx_data)
+{
+	struct tx_queue *queue;
+	int qid;
+	unsigned long flags;
+
+	/*
+	 * Find the right queue where to put TX data:
+	 * - Each high-speed channel has a dedicated queue, whose index is the
+	 *   same as the channel id (e.g., Channel 1 uses tx_queues[1]).
+	 * - All the general-purpose channels use the same TX queue, which is
+	 *   the last element in the tx_queues array.
+	 *
+	 * Note: tx_queues[] has KMB_IPC_NUM_HIGH_SPEED_CHANNELS+1 elements)
+	 */
+	qid = tx_data->chan_id < ARRAY_SIZE(link->tx_queues) ?
+	      tx_data->chan_id : (ARRAY_SIZE(link->tx_queues) - 1);
+
+	queue = &link->tx_queues[qid];
+
+	spin_lock_irqsave(&queue->lock, flags);
+	list_add_tail(&tx_data->list, &queue->tx_data_list);
+	spin_unlock_irqrestore(&queue->lock, flags);
+}
+
+/**
+ * tx_data_remove() - Remove TX data element from specified link.
+ * @link:	The link the data is currently enqueued to.
+ * @tx_data:	The TX data element to be removed.
+ *
+ * This function is called by the main send function, when the send is
+ * interrupted or has timed out.
+ */
+static void tx_data_remove(struct ipc_link *link, struct tx_data *tx_data)
+{
+	struct tx_queue *queue;
+	int qid;
+	unsigned long flags;
+
+	/*
+	 * Find the TX queue where TX data is currently located:
+	 * - Each high-speed channel has a dedicated queue, whose index is the
+	 *   same as the channel id (e.g., Channel 1 uses tx_queues[1]).
+	 * - All the general-purpose channels use the same TX queue, which is
+	 *   the last element in the tx_queues array.
+	 *
+	 * Note: tx_queues[] has KMB_IPC_NUM_HIGH_SPEED_CHANNELS+1 elements)
+	 */
+	qid = tx_data->chan_id < ARRAY_SIZE(link->tx_queues) ?
+	      tx_data->chan_id : (ARRAY_SIZE(link->tx_queues) - 1);
+
+	queue = &link->tx_queues[qid];
+
+	spin_lock_irqsave(&queue->lock, flags);
+	list_del(&tx_data->list);
+	spin_unlock_irqrestore(&queue->lock, flags);
+}
+
+/**
+ * tx_thread_fn() - The function run by the TX thread.
+ * @ptr: A pointer to the keembay_ipc_dev struct associated with the thread.
+ *
+ * This thread continuously dequeues and send TX data elements. The TX
+ * semaphore is used to pause the loop when all the pending TX data elements
+ * have been transmitted (the send function 'ups' the semaphore every time a
+ * new TX data element is enqueued).
+ */
+static int tx_thread_fn(void *ptr)
+{
+	DECLARE_WAIT_QUEUE_HEAD(wait_queue);
+	struct keembay_ipc_dev *ipc_dev = ptr;
+	struct ipc_link *link = &ipc_dev->leon_mss_link;
+	struct tx_data *tx_data;
+	int rc;
+
+	while (1) {
+		rc = wait_for_completion_interruptible(&link->tx_queued);
+		if (rc || link->tx_stopping)
+			break;
+		tx_data = tx_data_dequeue(link);
+		/*
+		 * We can get a null tx_data if tx_data_remove() has been
+		 * called. Just ignore it and continue.
+		 */
+		if (!tx_data)
+			continue;
+		tx_data->retv = tx_data_send(ipc_dev, tx_data);
+		complete(&tx_data->tx_done);
+	}
+	/* Wait until the process is stopped by removing the module. */
+	wait_event_interruptible(wait_queue, kthread_should_stop());
+
+	return rc;
+}
+
+/* Internal send. */
+static int __ipc_send(struct keembay_ipc_dev *ipc_dev, u8 dst_node,
+		      u16 chan_id, u32 paddr, size_t size)
+{
+	struct ipc_link *link = &ipc_dev->leon_mss_link;
+	struct tx_data *tx_data;
+	int rc;
+
+	/* Allocate and init TX data. */
+	tx_data = kmalloc(sizeof(*tx_data), GFP_KERNEL);
+	if (!tx_data)
+		return -ENOMEM;
+	tx_data->dst_node = dst_node;
+	tx_data->chan_id = chan_id;
+	tx_data->paddr = paddr;
+	tx_data->size = size;
+	tx_data->retv = 1;
+	INIT_LIST_HEAD(&tx_data->list);
+	init_completion(&tx_data->tx_done);
+	/* Add tx_data to tx queues. */
+	tx_data_enqueue(link, tx_data);
+	/* Signal that we have a new pending TX. */
+	complete(&link->tx_queued);
+	/* Wait until data is transmitted. */
+	rc = wait_for_completion_interruptible(&tx_data->tx_done);
+	if (unlikely(rc)) {
+		tx_data_remove(link, tx_data);
+		goto exit;
+	}
+	rc = tx_data->retv;
+exit:
+	kfree(tx_data);
+	return rc;
+}
+
+/*
+ * IPC Kernel API.
+ */
+
+/**
+ * intel_keembay_ipc_open_channel() - Open an IPC channel.
+ * @node_id:	The node ID of the remote node (used to identify the link the
+ *		channel must be added to)
+ * @chan_id:	The ID of the channel to be opened.
+ *
+ * Return:	0 on success, negative error code otherwise.
+ */
+int intel_keembay_ipc_open_channel(u8 node_id, u16 chan_id)
+{
+	int rc;
+	struct device *dev = &kmb_ipc_dev->plat_dev->dev;
+	struct ipc_link *link = &kmb_ipc_dev->leon_mss_link;
+	struct ipc_chan *chan;
+	unsigned long flags;
+
+	rc = validate_link_chan(dev, node_id, chan_id);
+	if (rc)
+		return rc;
+
+	/* Create channel before getting lock. */
+	chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+	if (!chan)
+		return -ENOMEM;
+	INIT_LIST_HEAD(&chan->rx_data_list);
+	spin_lock_init(&chan->rx_lock);
+	init_waitqueue_head(&chan->rx_wait_queue);
+
+	/* Add channel to the channel array (if not already present). */
+	spin_lock_irqsave(&link->chan_lock, flags);
+	if (link->channels[chan_id]) {
+		spin_unlock_irqrestore(&link->chan_lock, flags);
+		kfree(chan);
+		return -EEXIST;
+	}
+	rcu_assign_pointer(link->channels[chan_id], chan);
+	spin_unlock_irqrestore(&link->chan_lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(intel_keembay_ipc_open_channel);
+
+/**
+ * intel_keembay_ipc_close_channel() - Close an IPC channel.
+ * @node_id:	The node ID of the remote node (used to identify the link the
+ *		channel belongs to).
+ * @chan_id:	The ID of the channel to be closed.
+ *
+ * Return:	0 on success, negative error code otherwise.
+ */
+int intel_keembay_ipc_close_channel(u8 node_id, u16 chan_id)
+{
+	int rc;
+	struct device *dev = &kmb_ipc_dev->plat_dev->dev;
+	struct ipc_link *link = &kmb_ipc_dev->leon_mss_link;
+
+	rc = validate_link_chan(dev, node_id, chan_id);
+	if (rc)
+		return rc;
+
+	rc = channel_close(link, chan_id);
+	if (!rc)
+		dev_info(dev, "Channel was already closed\n");
+
+	return 0;
+}
+EXPORT_SYMBOL(intel_keembay_ipc_close_channel);
+
+/**
+ * intel_keembay_ipc_send() - Send data via IPC.
+ * @node_id:	The node ID of the remote node (i.e., the intended recipient of
+ *		the message).
+ * @chan_id:	The IPC channel to be used to send the message.
+ * @paddr:	The physical address of the data to be transferred.
+ * @size:	The size of the data to be transferred.
+ *
+ * Return:	0 on success, negative error code otherwise.
+ */
+int intel_keembay_ipc_send(u8 node_id, u16 chan_id, uint32_t paddr, size_t size)
+{
+	struct ipc_link *link = &kmb_ipc_dev->leon_mss_link;
+	struct device *dev = &kmb_ipc_dev->plat_dev->dev;
+	struct ipc_chan *chan;
+	int idx, rc;
+
+	rc = validate_link_chan(dev, node_id, chan_id);
+	if (rc)
+		return rc;
+	/*
+	 * Start Sleepable RCU critical section (this prevents close() from
+	 * destroying the channels struct while we are sending data)
+	 */
+	idx = srcu_read_lock(&link->srcu_sp[chan_id]);
+	/* Get channel. */
+	chan = rcu_dereference(link->channels[chan_id]);
+	if (unlikely(!chan)) {
+		/* The channel is closed. */
+		rc = -ENOENT;
+		goto exit;
+	}
+
+	rc = __ipc_send(kmb_ipc_dev, node_id, chan_id, paddr, size);
+exit:
+	/* End sleepable RCU critical section. */
+	srcu_read_unlock(&link->srcu_sp[chan_id], idx);
+	return rc;
+}
+EXPORT_SYMBOL(intel_keembay_ipc_send);
+
+/**
+ * intel_keembay_ipc_recv() - Read data via IPC
+ * @node_id:	The node ID of the remote node (used to identify the link we
+ *		want to receive from).
+ * @chan_id:	The IPC channel to read from.
+ * @paddr:	[out] The physical address of the received data.
+ * @size:	[out] Where to store the size of the received data.
+ * @timeout:	How long (in ms) the function will block waiting for an IPC
+ *		message; if UINT32_MAX it will block indefinitely; if 0 it
+ *		will not block.
+ *
+ * Return:	0 on success, negative error code otherwise
+ */
+int intel_keembay_ipc_recv(u8 node_id, u16 chan_id, uint32_t *paddr,
+			   size_t *size, u32 timeout)
+{
+	struct ipc_link *link = &kmb_ipc_dev->leon_mss_link;
+	struct device *dev = &kmb_ipc_dev->plat_dev->dev;
+	struct ipc_chan *chan;
+	struct rx_data *rx_entry;
+	unsigned long flags;
+	int idx, rc;
+
+	rc = validate_link_chan(dev, node_id, chan_id);
+	if (rc)
+		return rc;
+	/*
+	 * Start Sleepable RCU critical section (this prevents close() from
+	 * destroying the channels struct while we are using it)
+	 */
+	idx = srcu_read_lock(&link->srcu_sp[chan_id]);
+	/* Get channel. */
+	chan = rcu_dereference(link->channels[chan_id]);
+	if (unlikely(!chan)) {
+		rc = -ENOENT;
+		goto err;
+	}
+	do {
+		/*
+		 * Wait for RX data.
+		 *
+		 * Note: wait_event_interruptible_timeout() has different
+		 * return values than wait_event_interruptible().
+		 *
+		 * The following if/then branch ensures that return values are
+		 * consistent for the both cases, that is:
+		 * - rc == 0 only if the wait was successfully (i.e., we were
+		 *   notified of a message or of a channel closure)
+		 * - rc < 0 if an error occurred (we got interrupted or the
+		 *   timeout expired).
+		 */
+		if (timeout == U32_MAX) {
+			rc = wait_event_interruptible(
+					chan->rx_wait_queue,
+					!list_empty(&chan->rx_data_list) ||
+					chan->closing);
+		} else {
+			rc = wait_event_interruptible_timeout(
+					chan->rx_wait_queue,
+					!list_empty(&chan->rx_data_list) ||
+					chan->closing,
+					msecs_to_jiffies(timeout));
+			if (!rc)
+				rc = -ETIME;
+			if (rc > 0)
+				rc = 0;
+		}
+		if (rc)
+			goto err;
+		/* Check if we the channel was closed while waiting. */
+		if (chan->closing) {
+			rc = -EPIPE;
+			goto err;
+		}
+
+		/* Extract RX entry. */
+		spin_lock_irqsave(&chan->rx_lock, flags);
+		rx_entry = list_first_entry_or_null(&chan->rx_data_list,
+						    struct rx_data, list);
+		/*
+		 * The list_empty() test above can be done by two threads
+		 * concurrently; if that happens, one of the two may get a NULL
+		 * rx_entry. Therefore we must check if rx_entry is null before
+		 * calling list_del(). Also, if rx_entry is NULL, we must go
+		 * back to the start of this while loop.
+		 */
+		if (rx_entry)
+			list_del(&rx_entry->list);
+		spin_unlock_irqrestore(&chan->rx_lock, flags);
+	} while (!rx_entry);
+	/* Set output parameters. */
+	*paddr =  rx_entry->data_paddr;
+	*size = rx_entry->data_size;
+	/* Free RX entry. */
+	kfree(rx_entry);
+err:
+	/* End sleepable RCU critical section. */
+	srcu_read_unlock(&link->srcu_sp[chan_id], idx);
+	return rc;
+}
+EXPORT_SYMBOL(intel_keembay_ipc_recv);
+
+/* Device tree driver match. */
+static const struct of_device_id kmb_ipc_of_match[] = {
+	{
+		.compatible = "intel,keembay-ipc",
+	},
+	{}
+};
+
+/* The IPC driver is a platform device. */
+static struct platform_driver kmb_ipc_driver = {
+	.probe = kmb_ipc_probe,
+	.remove = kmb_ipc_remove,
+	.driver = {
+			.name = DRV_NAME,
+			.of_match_table = kmb_ipc_of_match,
+		},
+};
+
+module_platform_driver(kmb_ipc_driver);
+
+MODULE_DESCRIPTION("KeemBay IPC Driver");
+MODULE_AUTHOR("Daniele Alessandrelli <daniele.alessandrelli@intel.com>");
+MODULE_AUTHOR("Paul Murphy <paul.j.murphy@intel.com>");
+MODULE_LICENSE("GPL v2");
diff --git a/include/linux/keembay-ipc.h b/include/linux/keembay-ipc.h
new file mode 100644
index 000000000000..a5c1d2f7280c
--- /dev/null
+++ b/include/linux/keembay-ipc.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * keembay-ipc.h - KeemBay IPC Linux Kernel API
+ *
+ * Copyright (C) 2018-2019 Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; version 2.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __KEEMBAY_IPC_H
+#define __KEEMBAY_IPC_H
+
+/* The possible node IDs. */
+enum {
+	KMB_IPC_NODE_ARM_CSS = 0,
+	KMB_IPC_NODE_LEON_MSS,
+};
+
+int intel_keembay_ipc_open_channel(u8 node_id, u16 chan_id);
+int intel_keembay_ipc_close_channel(u8 node_id, u16 chan_id);
+int intel_keembay_ipc_send(u8 node_id, u16 chan_id, uint32_t paddr,
+			   size_t size);
+int intel_keembay_ipc_recv(u8 node_id, u16 chan_id, uint32_t *paddr,
+			   size_t *size, u32 timeout);
+
+#endif /* __KEEMBAY_IPC_H */
-- 
2.27.0

