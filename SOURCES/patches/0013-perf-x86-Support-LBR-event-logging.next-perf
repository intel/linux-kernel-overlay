From 672df9cb33571bc2cf3a12daa0d37afcdb6d9262 Mon Sep 17 00:00:00 2001
From: Kan Liang <kan.liang@linux.intel.com>
Date: Mon, 26 Apr 2021 12:43:58 -0700
Subject: [PATCH 13/49] perf/x86: Support LBR event logging

The LBR event logging introduces a per-counter indication of precise
event occurrences in LBRs. It can provides a means to attribute exposed
retirement latency to combinations of events across a block of
instructions. It also provides a means of attributing Timed LBR
latencies to events.

Add a new event/group modifier, branch_events, to indicate the event for
LBR event logging.

The feature is only supported on the first 4 GP counters. Force the
event constraint to the first 4 GP counters.

Extend the struct perf_branch_entry to save the event logging.
For a group with branch_events, the four saturate values will be
returned in events order.
For a event with branch_events, only return the event's saturate value.

(TODO: Need to make it more generic.)
(Only allow one branch event group or one grep event simultaneously.)

Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
---
 arch/x86/events/core.c            | 44 +++++++++++++++++++-----------
 arch/x86/events/intel/core.c      | 45 ++++++++++++++++++++++++++++---
 arch/x86/events/intel/lbr.c       | 44 ++++++++++++++++++++++++++++++
 arch/x86/events/perf_event.h      |  1 +
 arch/x86/include/asm/msr-index.h  |  2 ++
 arch/x86/include/asm/perf_event.h |  4 +++
 include/uapi/linux/perf_event.h   |  7 +++--
 7 files changed, 126 insertions(+), 21 deletions(-)

diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 8500ce01afbf..a19c5732afc1 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1158,7 +1158,7 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 	int num_counters = hybrid(cpuc->pmu, num_counters);
 	int num_counters_fixed = hybrid(cpuc->pmu, num_counters_fixed);
 	struct perf_event *event;
-	int n, max_count;
+	int i, n, max_count;
 
 	max_count = num_counters + num_counters_fixed;
 
@@ -1167,23 +1167,35 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 	if (!cpuc->n_events)
 		cpuc->pebs_output = 0;
 
-	if (!cpuc->is_fake && leader->attr.precise_ip) {
-		/*
-		 * For PEBS->PT, if !aux_event, the group leader (PT) went
-		 * away, the group was broken down and this singleton event
-		 * can't schedule any more.
-		 */
-		if (is_pebs_pt(leader) && !leader->aux_event)
-			return -EINVAL;
+	if (!cpuc->is_fake) {
+		if (leader->attr.precise_ip) {
+			/*
+			 * For PEBS->PT, if !aux_event, the group leader (PT) went
+			 * away, the group was broken down and this singleton event
+			 * can't schedule any more.
+			 */
+			if (is_pebs_pt(leader) && !leader->aux_event)
+				return -EINVAL;
 
-		/*
-		 * pebs_output: 0: no PEBS so far, 1: PT, 2: DS
-		 */
-		if (cpuc->pebs_output &&
-		    cpuc->pebs_output != is_pebs_pt(leader) + 1)
-			return -EINVAL;
+			/*
+			 * pebs_output: 0: no PEBS so far, 1: PT, 2: DS
+			 */
+			if (cpuc->pebs_output &&
+			    cpuc->pebs_output != is_pebs_pt(leader) + 1)
+				return -EINVAL;
+
+			cpuc->pebs_output = is_pebs_pt(leader) + 1;
+		}
 
-		cpuc->pebs_output = is_pebs_pt(leader) + 1;
+		/* TODO
+		 * Only support one group or one event for branch events.
+		 */
+		if (leader->attr.branch_events) {
+			for (i = 0; i < cpuc->n_events; i++) {
+				if (cpuc->event_list[i]->group_leader != leader->group_leader)
+					return -EINVAL;
+			}
+		}
 	}
 
 	if (is_x86_event(leader)) {
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index e880b72d728a..8a03d59718c3 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -2679,15 +2679,21 @@ static void intel_pmu_enable_fixed(struct perf_event *event)
 static void intel_pmu_enable_event(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
+	u64 enable_mask = ARCH_PERFMON_EVENTSEL_ENABLE;
 	int idx = hwc->idx;
 
 	if (unlikely(event->attr.precise_ip))
 		intel_pmu_pebs_enable(event);
 
 	switch (idx) {
-	case 0 ... INTEL_PMC_IDX_FIXED - 1:
+	case 0 ... 3:
+		if (event->attr.branch_events &&
+		    this_cpu_ptr(&cpu_hw_events)->lbr_users)
+			enable_mask = ARCH_PERFMON_EVENTSEL_LBR_LOG;
+		fallthrough;
+	case 4 ... INTEL_PMC_IDX_FIXED - 1:
 		intel_set_masks(event, idx);
-		__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+		__x86_pmu_enable_event(hwc, enable_mask);
 		break;
 	case INTEL_PMC_IDX_FIXED ... INTEL_PMC_IDX_FIXED_BTS - 1:
 	case INTEL_PMC_IDX_METRIC_BASE ... INTEL_PMC_IDX_METRIC_END:
@@ -3441,6 +3447,14 @@ intel_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
 		c2 = c1;
 	}
 
+	if (event->attr.branch_events) {
+		c1 = &cpuc->constraint_list[idx];
+		*c1 = *c2;
+		c1->idxmsk64 &= x86_pmu.lbr_events;
+		c1->weight = hweight64(c1->idxmsk64);
+		return c1;
+	}
+
 	if (cpuc->excl_cntrs)
 		return intel_get_excl_constraints(cpuc, event, idx, c2);
 
@@ -3732,6 +3746,31 @@ static int intel_pmu_hw_config(struct perf_event *event)
 			event->attr.sample_type |= __PERF_SAMPLE_CALLCHAIN_EARLY;
 	}
 
+	if (event->attr.branch_events) {
+		struct perf_event *leader, *sibling;
+		int num_branch_events = 0;
+
+		if (!static_cpu_has(X86_FEATURE_ARCH_LBR) ||
+		    !event->attr.precise_ip ||
+		    !x86_pmu.lbr_events ||
+		    (event->attr.config & ~INTEL_ARCH_EVENT_MASK))
+			return -EINVAL;
+
+		/*
+		 * The number of group members with branch_events attribute
+		 * cannot exceed the number of the supported counters.
+		 */
+		leader = event->group_leader;
+		if (leader->attr.branch_events)
+			num_branch_events++;
+		for_each_sibling_event(sibling, leader) {
+			if (sibling->attr.branch_events)
+				num_branch_events++;
+		}
+		if (num_branch_events > PERF_MAX_BRANCH_EVENTS)
+			return -EINVAL;
+	}
+
 	if (needs_branch_stack(event)) {
 		ret = intel_pmu_setup_lbr_filter(event);
 		if (ret)
@@ -4241,7 +4280,7 @@ int intel_cpuc_prepare(struct cpu_hw_events *cpuc, int cpu)
 			goto err;
 	}
 
-	if (x86_pmu.flags & (PMU_FL_EXCL_CNTRS | PMU_FL_TFA)) {
+	if (x86_pmu.flags & (PMU_FL_EXCL_CNTRS | PMU_FL_TFA) || !!x86_pmu.lbr_events) {
 		size_t sz = X86_PMC_IDX_MAX * sizeof(struct event_constraint);
 
 		cpuc->constraint_list = kzalloc_node(sz, GFP_KERNEL, cpu_to_node(cpu));
diff --git a/arch/x86/events/intel/lbr.c b/arch/x86/events/intel/lbr.c
index 4409d2cccfda..198be9fb9b01 100644
--- a/arch/x86/events/intel/lbr.c
+++ b/arch/x86/events/intel/lbr.c
@@ -943,6 +943,48 @@ static __always_inline u16 get_lbr_cycles(u64 info)
 	return info & LBR_INFO_CYCLES;
 }
 
+static inline u8 update_lbr_event(struct perf_event *event, u64 info, int pos)
+{
+	int idx = event->hw.idx;
+	u8 lbr_event;
+
+	lbr_event = (info & LBR_INFO_EVENTS) >> (LBR_INFO_EVENTS_OFFSET + idx * 2) & 0x3;
+	lbr_event <<= pos;
+
+	return lbr_event;
+}
+
+static u8 get_lbr_events(struct cpu_hw_events *cpuc, u64 info)
+{
+	struct perf_event *event, *leader;
+	unsigned long mask;
+	u8 lbr_events = 0;
+	int idx, pos = 0;
+
+	if (!static_cpu_has(X86_FEATURE_ARCH_LBR) ||
+	    !x86_pmu.lbr_events ||
+	    !(info & LBR_INFO_EVENTS))
+		return 0;
+
+	mask = *cpuc->active_mask & ((1ULL << PERF_MAX_BRANCH_EVENTS) - 1);
+	if (!mask)
+		return 0;
+
+	idx = find_first_bit(&mask, PERF_MAX_BRANCH_EVENTS);
+	event = cpuc->events[idx];
+	leader = event->group_leader;
+	if (leader->attr.branch_events) {
+		lbr_events |= update_lbr_event(leader, info, pos++);
+	}
+	for_each_sibling_event(event, leader) {
+		if (!event->attr.branch_events)
+			continue;
+		lbr_events |= update_lbr_event(event, info, pos++);
+	}
+
+	return lbr_events;
+}
+
 static void intel_pmu_store_lbr(struct cpu_hw_events *cpuc,
 				struct lbr_entry *entries)
 {
@@ -973,6 +1015,7 @@ static void intel_pmu_store_lbr(struct cpu_hw_events *cpuc,
 		e->abort	= !!(info & LBR_INFO_ABORT);
 		e->cycles	= get_lbr_cycles(info);
 		e->type		= get_lbr_br_type(info);
+		e->events	= get_lbr_events(cpuc, info);
 		e->reserved	= 0;
 	}
 
@@ -1770,6 +1813,7 @@ void __init intel_pmu_arch_lbr_init(void)
 	x86_pmu.lbr_mispred = ecx.split.lbr_mispred;
 	x86_pmu.lbr_timed_lbr = ecx.split.lbr_timed_lbr;
 	x86_pmu.lbr_br_type = ecx.split.lbr_br_type;
+	x86_pmu.lbr_events = ecx.split.lbr_events;
 	x86_pmu.lbr_nr = lbr_nr;
 
 
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index df64fb8a8e37..d2bae1d63f37 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -834,6 +834,7 @@ struct x86_pmu {
 	unsigned int	lbr_mispred:1;
 	unsigned int	lbr_timed_lbr:1;
 	unsigned int	lbr_br_type:1;
+	unsigned int	lbr_events:4;
 
 	void		(*lbr_reset)(void);
 	void		(*lbr_read)(struct cpu_hw_events *cpuc);
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 211ba3375ee9..fcaef5ed7cdc 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -167,6 +167,8 @@
 #define LBR_INFO_CYCLES			0xffff
 #define LBR_INFO_BR_TYPE_OFFSET		56
 #define LBR_INFO_BR_TYPE		(0xfull << LBR_INFO_BR_TYPE_OFFSET)
+#define LBR_INFO_EVENTS_OFFSET		32
+#define LBR_INFO_EVENTS			(0xffull << LBR_INFO_EVENTS_OFFSET)
 
 #define MSR_ARCH_LBR_CTL		0x000014ce
 #define ARCH_LBR_CTL_LBREN		BIT(0)
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 544f41a179fb..ebf4ebd63a07 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -29,6 +29,7 @@
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
+#define ARCH_PERFMON_EVENTSEL_LBR_LOG			(1ULL << 35)
 
 #define HSW_IN_TX					(1ULL << 32)
 #define HSW_IN_TX_CHECKPOINTED				(1ULL << 33)
@@ -180,6 +181,9 @@ union cpuid28_ecx {
 		unsigned int    lbr_timed_lbr:1;
 		/* Branch Type Field Supported */
 		unsigned int    lbr_br_type:1;
+		unsigned int	reserved:13;
+		/* Event Logging Supported */
+		unsigned int	lbr_events:4;
 	} split;
 	unsigned int            full;
 };
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index f92880a15645..2f19aa16b843 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -409,7 +409,8 @@ struct perf_event_attr {
 				inherit_thread :  1, /* children only inherit if cloned with CLONE_THREAD */
 				remove_on_exec :  1, /* event is removed from task on exec */
 				sigtrap        :  1, /* send synchronous SIGTRAP on event */
-				__reserved_1   : 26;
+				branch_events  :  1, /* include branch events */
+				__reserved_1   : 25;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -1310,6 +1311,7 @@ union perf_mem_data_src {
 #define PERF_MEM_S(a, s) \
 	(((__u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
 
+#define PERF_MAX_BRANCH_EVENTS	4
 /*
  * single taken branch record layout:
  *
@@ -1335,7 +1337,8 @@ struct perf_branch_entry {
 		abort:1,    /* transaction abort */
 		cycles:16,  /* cycle count to last branch */
 		type:4,     /* branch type */
-		reserved:40;
+		events:8,   /* TODO: */
+		reserved:32;
 };
 
 union perf_sample_weight {
-- 
2.27.0

