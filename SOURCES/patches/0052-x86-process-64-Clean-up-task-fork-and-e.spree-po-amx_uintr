From f729772280332cd918ff052108c0023675a1bed2 Mon Sep 17 00:00:00 2001
From: Sohil Mehta <sohil.mehta@intel.com>
Date: Wed, 22 Jul 2020 18:02:25 -0700
Subject: [PATCH 52/85] x86/process/64: Clean up task fork and exit paths

The user interrupt MSRs and the user interrupt state is task specific.
During task fork and exit clear the task state, clear the MSRs and
dereference the shared resources.

Some of the memory resources like the UPID and UITT are referenced in
the file descriptor and could be in use while the uintr_fd is still
valid. Instead of freeing up  the UPID and UITT just dereference them.
Eventually when every user releases their reference the memory resource
will be freed up.

Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
Signed-off-by: Sohil Mehta <sohil.mehta@intel.com>
---
 arch/x86/include/asm/uintr.h |  3 ++
 arch/x86/kernel/fpu/core.c   |  8 +++++
 arch/x86/kernel/process.c    | 10 ++++++
 arch/x86/kernel/uintr_core.c | 67 ++++++++++++++++++++++++++++++++++++
 4 files changed, 88 insertions(+)

diff --git a/arch/x86/include/asm/uintr.h b/arch/x86/include/asm/uintr.h
index 103c28978569..c480e86661bb 100644
--- a/arch/x86/include/asm/uintr.h
+++ b/arch/x86/include/asm/uintr.h
@@ -37,12 +37,15 @@ int do_uintr_register_sender(struct uintr_receiver_info *r_info,
 void do_uintr_unregister_sender(struct uintr_receiver_info *r_info,
 				struct uintr_sender_info *s_info);
 
+void uintr_free(struct task_struct *task);
+
 /* TODO: To inline the switch functions move the definition to the header file */
 void switch_uintr_prepare(struct task_struct *prev);
 void switch_uintr_return(void);
 
 #else /* !CONFIG_X86_USER_INTERRUPTS */
 
+static inline void uintr_free(struct task_struct *task) {}
 static inline void switch_uintr_prepare(struct task_struct *prev) {}
 static inline void switch_uintr_return(void) {}
 
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index cfeba87b997f..a824ba735bbd 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -224,6 +224,7 @@ int fpu__copy(struct task_struct *dst, struct task_struct *src)
 {
 	struct fpu *dst_fpu = &dst->thread.fpu;
 	struct fpu *src_fpu = &src->thread.fpu;
+	struct uintr_state *uintr_state;
 
 	dst_fpu->last_cpu = -1;
 
@@ -253,6 +254,13 @@ int fpu__copy(struct task_struct *dst, struct task_struct *src)
 	else if (!copy_fpregs_to_fpstate(dst_fpu))
 		copy_kernel_to_fpregs(&dst_fpu->state);
 
+	/* UINTR state should never be inherited */
+	if (static_cpu_has(X86_FEATURE_UINTR)) {
+		uintr_state = get_xsave_addr(&dst_fpu->state.xsave, XFEATURE_UINTR);
+		if (uintr_state)
+			memset(uintr_state, 0, sizeof(*uintr_state));
+	}
+
 	fpregs_unlock();
 
 	set_tsk_thread_flag(dst, TIF_NEED_FPU_LOAD);
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 8071b5d770fb..2c3ce9b68ae3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -26,6 +26,7 @@
 #include <linux/elf-randomize.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
+#include <asm/uintr.h>
 #include <asm/cpu.h>
 #include <asm/apic.h>
 #include <linux/uaccess.h>
@@ -94,6 +95,12 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	dst->thread.vm86 = NULL;
 #endif
 
+#ifdef CONFIG_X86_USER_INTERRUPTS
+	/* User Interrupt state is unique for each task */
+	dst->thread.ui_recv = NULL;
+	dst->thread.ui_send = NULL;
+#endif
+
 	return fpu__copy(dst, src);
 }
 
@@ -111,6 +118,9 @@ void exit_thread(struct task_struct *tsk)
 	free_vm86(t);
 
 	cet_free_shstk(tsk);
+	
+	uintr_free(tsk);
+
 	fpu__drop(fpu);
 }
 
diff --git a/arch/x86/kernel/uintr_core.c b/arch/x86/kernel/uintr_core.c
index f7cc80905236..6721f494c911 100644
--- a/arch/x86/kernel/uintr_core.c
+++ b/arch/x86/kernel/uintr_core.c
@@ -87,6 +87,11 @@ static inline bool is_uintr_sender(struct task_struct *t)
 	return !!t->thread.ui_send;
 }
 
+static inline bool is_uintr_task(struct task_struct *t)
+{
+	return(is_uintr_receiver(t) || is_uintr_sender(t));
+}
+
 static inline bool is_uitt_empty(struct task_struct *t)
 {
 	return !!bitmap_empty((unsigned long *)t->thread.ui_send->uitt_mask,
@@ -730,3 +735,65 @@ void switch_uintr_return(void)
 			apic->send_IPI_self(UINTR_NOTIFICATION_VECTOR);
 	}
 }
+
+/*
+ * This should only be called from exit_thread().
+ * exit_thread() can happen in current context when the current thread is
+ * exiting or it can happen for a new thread that is being created.
+ * For new threads is_uintr_task() should fail.
+ */
+void uintr_free(struct task_struct *t)
+{
+	struct fpu *fpu;
+
+	if (!static_cpu_has(X86_FEATURE_UINTR) || !is_uintr_task(t))
+		return;
+
+	if (WARN_ON_ONCE(t != current))
+		return;
+
+	fpu = &t->thread.fpu;
+
+	fpregs_lock();
+
+	if (fpregs_state_valid(fpu, smp_processor_id())) {
+		wrmsrl(MSR_IA32_UINTR_MISC, 0ULL);
+		wrmsrl(MSR_IA32_UINTR_TT, 0ULL);
+		wrmsrl(MSR_IA32_UINTR_PD, 0ULL);
+		wrmsrl(MSR_IA32_UINTR_RR, 0ULL);
+		wrmsrl(MSR_IA32_UINTR_HANDLER, 0ULL);
+	} else {
+		struct uintr_state *p;
+
+		p = get_xsave_addr(&fpu->state.xsave, XFEATURE_UINTR);
+		if (p) {
+			p->handler = 0;
+			p->uirr = 0;
+			p->upid_addr = 0;
+			p->uinv = 0;
+			p->uitt_addr = 0;
+			p->uitt_size = 0;
+		}
+	}
+
+	/* Check: Can a thread be context switched while it is exiting? */
+	if (is_uintr_receiver(t)) {
+		/*
+		 * Suppress notifications so that no further interrupts are
+		 * generated based on this UPID.
+		 */
+		t->thread.ui_recv->upid_ctx->upid->sc.sn = 1;
+		t->thread.ui_recv->upid_ctx->receiver_active = false;
+		put_upid_ref(t->thread.ui_recv->upid_ctx);
+		kfree(t->thread.ui_recv);
+		t->thread.ui_recv = NULL;
+	}
+
+	fpregs_unlock();
+
+	if (is_uintr_sender(t)) {
+		put_uitt_ref(t->thread.ui_send->uitt_ctx);
+		kfree(t->thread.ui_send);
+		t->thread.ui_send = NULL;
+	}
+}
-- 
2.27.0

