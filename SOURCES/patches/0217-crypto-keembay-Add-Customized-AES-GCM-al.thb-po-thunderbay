From 6e87d6faeb2719a32433ff91cd9257f824d16da7 Mon Sep 17 00:00:00 2001
From: hemanthk <hemanthkumar.sm@intel.com>
Date: Fri, 12 Mar 2021 07:51:30 +0530
Subject: [PATCH 217/223] crypto: keembay - Add Customized AES GCM algortihm
 for SecureXLink Driver

    Add support for Customized AES GCM algorithm optimized for securexlink driver.

Signed-off-by: hemanthk <hemanthkumar.sm@intel.com>
---
 drivers/crypto/keembay/Kconfig                |  15 +
 drivers/crypto/keembay/keembay-ocs-aes-core.c | 314 ++++++++++++++++++
 drivers/crypto/keembay/ocs-aes.c              | 159 +++++++++
 drivers/crypto/keembay/ocs-aes.h              |  14 +
 4 files changed, 502 insertions(+)

diff --git a/drivers/crypto/keembay/Kconfig b/drivers/crypto/keembay/Kconfig
index 8815050c0a8d..dcb7976341c6 100644
--- a/drivers/crypto/keembay/Kconfig
+++ b/drivers/crypto/keembay/Kconfig
@@ -216,3 +216,18 @@ config CRYPTO_DEV_THUNDERBAY_OCS_ECDH_GEN_PRIV_KEY_SUPPORT
 
 	  Say Y if you need the driver to pass crypto self-tests. If unsure,
 	  say N.
+
+config CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK
+	bool "Support for Thunderbay OCS AES HW acceleration customized for SECUREXLINK"
+	depends on CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4
+
+	help
+	  Support for Thunderbay Offload and Crypto Subsystem (OCS)
+	  AES hardware acceleration for use with Crypto API.
+
+	  Provides OCS version of gcm(aes) customized for SecureXLink.
+
+	  Say Y if you are compiling for the Intel Thunder Bay SoC with
+	  SecureXLink.
+
+	  If unsure, say N.
diff --git a/drivers/crypto/keembay/keembay-ocs-aes-core.c b/drivers/crypto/keembay/keembay-ocs-aes-core.c
index 758202354110..e2aa49418cd9 100644
--- a/drivers/crypto/keembay/keembay-ocs-aes-core.c
+++ b/drivers/crypto/keembay/keembay-ocs-aes-core.c
@@ -72,6 +72,8 @@
  * @cipher:		OCS cipher to use (either AES or SM4).
  * @sw_cipher:		The cipher to use as fallback.
  * @use_fallback:	Whether or not fallback cipher should be used.
+ * @sks_prepare_aes:	Indicates whether key should be loaded from sks slot
+ * @sks_slot_id:	OCS SKS Slot id to load key
  */
 struct ocs_aes_tctx {
 	struct crypto_engine_ctx engine_ctx;
@@ -84,6 +86,8 @@ struct ocs_aes_tctx {
 		struct crypto_aead *aead;
 	} sw_cipher;
 	bool use_fallback;
+	bool sks_prepare_aes;
+	u32 sks_slot_id;
 };
 
 /**
@@ -1606,6 +1610,298 @@ static int register_aes_algs(struct ocs_aes_dev *aes_dev)
 	return ret;
 }
 
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK
+#include <linux/tee_drv.h>
+
+#define TA_XLINK_SECURE_CMD_SKS_PREPARE_AES             9
+
+static int test_optee_match(struct tee_ioctl_version_data *data,
+			    const void *vers)
+{
+	return !!1;
+}
+
+static const uuid_t secure_xlink_ta_uuid =
+	UUID_INIT(0xe80bfbf9, 0x103f, 0x41f4,
+		  0xb5, 0xf4, 0x62, 0xe6, 0x03, 0xe3, 0x46, 0xec);
+
+const char *ocs_slice_name_list[] = {
+	"thunderbay-ocs_cpu",
+	"thunderbay-ocs_0",
+	"thunderbay-ocs_1",
+	"thunderbay-ocs_2",
+	"thunderbay-ocs_3"
+};
+
+static int ocs_slice_name_to_id(const char *name)
+{
+	int i;
+
+	for (i = 0;  i < 5;  i++)
+		if (!strcmp(name, ocs_slice_name_list[i]))
+			return i;
+	pr_info("ocs slice name no match found\n");
+	return -1;
+}
+
+static inline int save_key_slot(struct ocs_aes_tctx *tctx, const u8 *in_key, unsigned int key_len)
+{
+	if (key_len == 1) {
+		tctx->sks_slot_id = *in_key;
+		tctx->sks_prepare_aes = true;
+	}
+
+	return 0;
+}
+
+static int xlink_set_key_slot(struct crypto_aead *tfm, const u8 *in_key, unsigned int key_len)
+{
+	struct ocs_aes_tctx *tctx = crypto_aead_ctx(tfm);
+	int ret;
+
+	ret = save_key_slot(tctx, in_key, key_len);
+
+	return ret;
+}
+
+static int tee_sks_prepare_aes(u32 slot_id, u32 slice_ocs)
+{
+	struct tee_context *ctx = NULL;
+	int ret;
+	struct tee_ioctl_open_session_arg session_arg;
+	struct tee_ioctl_invoke_arg transceive_args;
+	struct tee_param command_params[4];
+
+	ctx = tee_client_open_context(NULL, test_optee_match, NULL, NULL);
+	memset(&session_arg, 0, sizeof(session_arg));
+	memcpy(&session_arg.uuid, &secure_xlink_ta_uuid.b, TEE_IOCTL_UUID_LEN);
+	session_arg.clnt_login = TEE_IOCTL_LOGIN_PUBLIC;
+	session_arg.num_params = 0;
+
+	ret = tee_client_open_session(ctx, &session_arg, NULL);
+	if (ret < 0) {
+		pr_info("tee open session error ret=%d\n", ret);
+		tee_client_close_context(ctx);
+		return -1;
+	}
+
+	if (session_arg.ret < 0) {
+		pr_info("tee open session error session_arg.ret=%d\n", session_arg.ret);
+		tee_client_close_context(ctx);
+		return -1;
+	}
+
+	memset(&transceive_args, 0, sizeof(transceive_args));
+	memset(command_params, 0, sizeof(command_params));
+
+	transceive_args = (struct tee_ioctl_invoke_arg) {
+		.func = TA_XLINK_SECURE_CMD_SKS_PREPARE_AES,
+		.session = session_arg.session,
+		.num_params = 1,
+	};
+
+	command_params[0] = (struct tee_param) {
+		.attr = TEE_IOCTL_PARAM_ATTR_TYPE_VALUE_INPUT,
+		.u.value = {
+			.a = slot_id,
+			.b = slice_ocs,
+			.c = 0,
+		},
+	};
+
+	ret = tee_client_invoke_func(ctx, &transceive_args, command_params);
+	tee_client_close_session(ctx, session_arg.session);
+	tee_client_close_context(ctx);
+	if (ret < 0) {
+		pr_info("tee invoke func error ret=%d\n", ret);
+		return -1;
+	}
+	if (transceive_args.ret < 0) {
+		pr_info("tee invoke func error transceive_args.ret=%d\n", transceive_args.ret);
+		return -1;
+	}
+
+	return 0;
+}
+
+static int xlink_ocs_aes_gcm_encrypt(struct aead_request *req)
+{
+	return kmb_ocs_aead_common(req, OCS_AES, OCS_ENCRYPT, OCS_MODE_GCM);
+}
+
+static int xlink_ocs_aes_gcm_decrypt(struct aead_request *req)
+{
+	return kmb_ocs_aead_common(req, OCS_AES, OCS_DECRYPT, OCS_MODE_GCM);
+}
+
+static int xlink_ocs_aead_run(struct aead_request *req)
+{
+	struct ocs_aes_tctx *tctx = crypto_aead_ctx(crypto_aead_reqtfm(req));
+	const int tag_size = crypto_aead_authsize(crypto_aead_reqtfm(req));
+	struct ocs_aes_rctx *rctx = aead_request_ctx(req);
+	u32 in_size;	/* The length of the data mapped by src_dll. */
+	int rc;
+
+	rctx->src_nents = sg_nents_for_len(req->src,
+					   req->assoclen + req->cryptlen);
+	if (rctx->src_nents < 0) {
+		rc = -EBADMSG;
+		goto exit;
+	}
+
+	in_size = req->cryptlen;
+	if (rctx->instruction == OCS_DECRYPT) {
+		/*
+		 * For decrypt:
+		 * - src sg list is:		AAD|CT|tag
+		 * - dst sg list expects:	AAD|PT
+		 *
+		 * in_size == len(CT); out_size == len(PT)
+		 */
+
+		/* req->cryptlen includes both CT and tag. */
+		in_size -= tag_size;
+
+		/*
+		 * Copy tag from source SG list to 'in_tag' buffer.
+		 *
+		 * Note: this needs to be done here, before DMA mapping src_sg.
+		 */
+		sg_pcopy_to_buffer(&req->src[2], 1, rctx->in_tag, tag_size, 0);
+	}
+
+	/* GCM case; invoke OCS processing. */
+	rc = xlink_ocs_aes_gcm_op(tctx->aes_dev, tctx->cipher,
+				  rctx->instruction,
+			    sg_dma_address(&req->dst[1]),
+			    sg_dma_address(&req->src[1]), in_size,
+			    req->iv,
+			    sg_dma_address(req->src), req->assoclen,
+			    rctx->out_tag, tag_size);
+	if (rc)
+		goto exit;
+
+	/* For GCM decrypt, we have to compare in_tag with out_tag. */
+	if (rctx->instruction == OCS_DECRYPT) {
+		rc = memcmp(rctx->in_tag, rctx->out_tag, tag_size) ?
+		     -EBADMSG : 0;
+		goto exit;
+	}
+
+	/* For GCM encrypt, we must manually copy out_tag to DST sg. */
+	sg_pcopy_from_buffer(&req->dst[2], 1, rctx->out_tag, tag_size, 0);
+	rc = 0;
+
+exit:
+	return rc;
+}
+
+static int xlink_ocs_aes_aead_do_one_request(struct crypto_engine *engine, void *areq)
+{
+	struct aead_request *req = container_of(areq,
+						struct aead_request, base);
+	struct ocs_aes_tctx *tctx = crypto_aead_ctx(crypto_aead_reqtfm(req));
+	int err;
+	int ocs_slice_id;
+
+	if (!tctx->aes_dev)
+		return -ENODEV;
+
+	ocs_slice_id = ocs_slice_name_to_id(tctx->aes_dev->driver_name);
+	if (ocs_slice_id < 0)
+		return -ENODEV;
+
+	if (tctx->sks_prepare_aes) {
+		err = tee_sks_prepare_aes(tctx->sks_slot_id, (u32)ocs_slice_id);
+		if (err)
+			goto exit;
+	}
+
+	err = xlink_ocs_aead_run(req);
+
+exit:
+	crypto_finalize_aead_request(tctx->aes_dev->engine, req, err);
+
+	return 0;
+}
+
+static inline int xlink_ocs_common_aead_init(struct ocs_aes_tctx *tctx)
+{
+	tctx->engine_ctx.op.prepare_request = NULL;
+	tctx->engine_ctx.op.do_one_request = xlink_ocs_aes_aead_do_one_request;
+	tctx->engine_ctx.op.unprepare_request = NULL;
+
+	return 0;
+}
+
+static int xlink_ocs_aead_cra_init(struct crypto_aead *tfm)
+{
+	struct ocs_aes_tctx *tctx = crypto_aead_ctx(tfm);
+
+	crypto_aead_set_reqsize(tfm, sizeof(struct ocs_aes_rctx));
+
+	return xlink_ocs_common_aead_init(tctx);
+}
+
+static void xlink_ocs_aead_cra_exit(struct crypto_aead *tfm)
+{
+	struct ocs_aes_tctx *tctx = crypto_aead_ctx(tfm);
+
+	if (tctx->sw_cipher.aead) {
+		crypto_free_aead(tctx->sw_cipher.aead);
+		tctx->sw_cipher.aead = NULL;
+	}
+}
+
+static struct aead_alg algs_xlink[] = {
+	{
+		.base = {
+			.cra_name = "xlink_gcm(aes)",
+			.cra_driver_name = "xlink-gcm-aes-",
+			.cra_priority = KMB_OCS_PRIORITY,
+			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+			.cra_blocksize = 1,
+			.cra_ctxsize = sizeof(struct ocs_aes_tctx),
+			.cra_alignmask = 0,
+			.cra_module = THIS_MODULE,
+		},
+		.init = xlink_ocs_aead_cra_init,
+		.exit = xlink_ocs_aead_cra_exit,
+		.ivsize = GCM_AES_IV_SIZE,
+		.maxauthsize = AES_BLOCK_SIZE,
+		.setauthsize = kmb_ocs_aead_gcm_setauthsize,
+		.setkey = xlink_set_key_slot,
+		.encrypt = xlink_ocs_aes_gcm_encrypt,
+		.decrypt = xlink_ocs_aes_gcm_decrypt,
+	}
+};
+
+static void unregister_xlink_algs(struct ocs_aes_dev *aes_dev)
+{
+	crypto_unregister_aeads(aes_dev->algs_xlink, ARRAY_SIZE(algs_xlink));
+	dev_info(aes_dev->dev, "OCS SecureXLink crypto algorithms unregistered\n");
+}
+
+static int register_xlink_algs(struct ocs_aes_dev *aes_dev)
+{
+	int i, ret;
+
+	for (i = 0; i < ARRAY_SIZE(algs_xlink); i++) {
+		strcat(aes_dev->algs_xlink[i].base.cra_driver_name, aes_dev->driver_name);
+		aes_dev->algs_xlink[i].base.cra_priority = aes_dev->priority;
+		ret = crypto_register_aead(&aes_dev->algs_xlink[i]);
+		if (ret)
+			break;
+	}
+
+	if (!ret)
+		dev_info(aes_dev->dev, "OCS AES Customized algorithm for SecureXlink registered\n");
+
+	return ret;
+}
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK */
+
 /* Device tree driver match. */
 static const struct of_device_id kmb_ocs_aes_of_match[] = {
 	{
@@ -1623,6 +1919,9 @@ static int kmb_ocs_aes_remove(struct platform_device *pdev)
 		return -ENODEV;
 
 	unregister_aes_algs(aes_dev);
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK
+	unregister_xlink_algs(aes_dev);
+#endif
 
 	spin_lock(&ocs_aes.lock);
 	list_del(&aes_dev->list);
@@ -1751,6 +2050,21 @@ static int kmb_ocs_aes_probe(struct platform_device *pdev)
 		goto cleanup;
 	}
 
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK
+	aes_dev->algs_xlink = devm_kzalloc(dev, sizeof(algs_xlink), GFP_KERNEL);
+	if (!aes_dev->algs_xlink)
+		return -ENOMEM;
+	memcpy(aes_dev->algs_xlink, algs_xlink, sizeof(algs_xlink));
+
+	if (aes_dev->algs_xlink) {
+		rc = register_xlink_algs(aes_dev);
+		if (rc) {
+			dev_err(dev, "Could not register OCS SecureXLink algorithm with Crypto API\n");
+			goto cleanup;
+		}
+	}
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK */
+
 	return 0;
 
 cleanup:
diff --git a/drivers/crypto/keembay/ocs-aes.c b/drivers/crypto/keembay/ocs-aes.c
index 29620532432c..9cba824cc882 100644
--- a/drivers/crypto/keembay/ocs-aes.c
+++ b/drivers/crypto/keembay/ocs-aes.c
@@ -1542,3 +1542,162 @@ int ocs_create_linked_list_from_sg(const struct ocs_aes_dev *aes_dev,
 
 	return 0;
 }
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK
+/**
+ * xlink_ocs_aes_gcm_op() - Perform GCM operation.
+ * @aes_dev:		The OCS AES device to use.
+ * @cipher:		The Cipher to use (AES or SM4).
+ * @instruction:	The instruction to perform (encrypt or decrypt).
+ * @dst_dma_addr:	The DMA Address for output memory.
+ * @src_dma_addr:	The DMA Address for input payload data.
+ * @src_size:		The amount of data mapped by @src_dma_list.
+ * @iv:			The input IV vector.
+ * @aad_dma_addr:	The DMA Address for input AAD data.
+ * @aad_size:		The amount of data mapped by @aad_dma_list.
+ * @out_tag:		Where to store computed tag.
+ * @tag_size:		The size (in bytes) of @out_tag.
+ *
+ * Return: 0 on success, negative error code otherwise.
+ */
+int xlink_ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
+			 enum ocs_cipher cipher,
+			 enum ocs_instruction instruction,
+			 dma_addr_t dst_dma_addr,
+			 dma_addr_t src_dma_addr,
+			 u32 src_size,
+			 const u8 *iv,
+			 dma_addr_t aad_dma_addr,
+			 u32 aad_size,
+			 u8 *out_tag,
+			 u32 tag_size)
+{
+	u64 bit_len;
+	u32 val;
+	int rc;
+
+	ocs_aes_init(aes_dev, OCS_MODE_GCM, cipher, instruction);
+
+	/* Compute and write J0 to OCS HW. */
+	ocs_aes_gcm_write_j0(aes_dev, iv);
+
+	/* Write out_tag byte length */
+	iowrite32(tag_size, aes_dev->base_reg + AES_TLEN_OFFSET);
+
+	/* Write the byte length of the last plaintext / ciphertext block. */
+	ocs_aes_write_last_data_blk_len(aes_dev, src_size);
+
+	/* Write ciphertext bit length */
+	bit_len = (u64)src_size * 8;
+	val = bit_len & 0xFFFFFFFF;
+	iowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_0_OFFSET);
+	val = bit_len >> 32;
+	iowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_1_OFFSET);
+
+	/* Write aad bit length */
+	bit_len = (u64)aad_size * 8;
+	val = bit_len & 0xFFFFFFFF;
+	iowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_2_OFFSET);
+	val = bit_len >> 32;
+	iowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_3_OFFSET);
+
+	/* Set AES_ACTIVE.TRIGGER to start the operation. */
+	aes_a_op_trigger(aes_dev);
+
+	/* Process AAD. */
+	if (aad_size) {
+		/* Clear ISR*/
+		val = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		if (val)
+			iowrite32(val, aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		val = ioread32(aes_dev->base_reg + AES_ISR_OFFSET);
+		if (val)
+			iowrite32(val, aes_dev->base_reg + AES_ISR_OFFSET);
+
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ocs_wrapper_setconfig(aes_dev->wrapper_dev, (aad_dma_addr >> 32), 0);
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
+		/* Fetch AAD */
+		iowrite32(aad_size, aes_dev->base_reg + AES_A_DMA_SRC_SIZE_OFFSET);
+		iowrite32(aad_dma_addr, aes_dev->base_reg + AES_A_DMA_SRC_ADDR_OFFSET);
+		aes_a_dma_active(aes_dev);
+
+		/* Instructs engine to pad last block of aad, if needed. */
+		aes_a_set_last_gcx_and_adata(aes_dev);
+
+		/* Poll DMA_MSI_ISR SRC_DONE_INT_STATUS */
+		do {
+			val = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		} while ((val & 0x00000001) == 0);
+
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
+	} else {
+		aes_a_set_last_gcx_and_adata(aes_dev);
+	}
+
+	/* Wait until adata (if present) has been processed. */
+	aes_a_wait_last_gcx(aes_dev);
+	aes_a_dma_wait_input_buffer_occupancy(aes_dev);
+
+	/* Now process payload. */
+	if (src_size) {
+		/* Clear ISR*/
+		val = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		if (val)
+			iowrite32(val, aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		val = ioread32(aes_dev->base_reg + AES_ISR_OFFSET);
+		if (val)
+			iowrite32(val, aes_dev->base_reg + AES_ISR_OFFSET);
+
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ocs_wrapper_setconfig(aes_dev->wrapper_dev, (src_dma_addr >> 32),
+				      (dst_dma_addr >> 32));
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
+		iowrite32(src_size, aes_dev->base_reg + AES_A_DMA_SRC_SIZE_OFFSET);
+		val = src_dma_addr & 0xFFFFFFFF;
+		iowrite32(val, aes_dev->base_reg + AES_A_DMA_SRC_ADDR_OFFSET);
+		iowrite32(src_size, aes_dev->base_reg + AES_A_DMA_DST_SIZE_OFFSET);
+		val = dst_dma_addr & 0xFFFFFFFF;
+		iowrite32(val, aes_dev->base_reg + AES_A_DMA_DST_ADDR_OFFSET);
+	} else {
+		aes_a_dma_set_xfer_size_zero(aes_dev);
+	}
+
+	aes_a_dma_active(aes_dev);
+
+	/* Instruct AES/SMA4 engine payload processing is over. */
+	aes_a_set_last_gcx(aes_dev);
+
+	if (src_size < 4096) {
+		/* Poll AES_COMPLETE */
+		do {
+			val = ioread32(aes_dev->base_reg + AES_ISR_OFFSET);
+		} while ((val & 0x00000002) == 0);
+
+		/* Clear ISR*/
+		val = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		if (val)
+			iowrite32(val, aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);
+		val = ioread32(aes_dev->base_reg + AES_ISR_OFFSET);
+		if (val)
+			iowrite32(val, aes_dev->base_reg + AES_ISR_OFFSET);
+	} else {
+		/* Wait for OCS AES engine to complete processing. */
+		rc = ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);
+		if (rc)
+			return rc;
+	}
+
+#ifdef CONFIG_ARCH_THUNDERBAY
+	if (src_size)
+		ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
+	ocs_aes_gcm_read_tag(aes_dev, out_tag, tag_size);
+
+	return 0;
+}
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK */
diff --git a/drivers/crypto/keembay/ocs-aes.h b/drivers/crypto/keembay/ocs-aes.h
index 4ef4ea1ca392..c24cd560071f 100644
--- a/drivers/crypto/keembay/ocs-aes.h
+++ b/drivers/crypto/keembay/ocs-aes.h
@@ -117,6 +117,20 @@ int ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
 		   u8 *out_tag,
 		   u32 tag_size);
 
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK
+int xlink_ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
+			 enum ocs_cipher cipher,
+			 enum ocs_instruction instruction,
+			 dma_addr_t dst_dma_addr,
+			 dma_addr_t src_dma_addr,
+			 u32 src_size,
+			 const u8 *iv,
+			 dma_addr_t aad_dma_addr,
+			 u32 aad_size,
+			 u8 *out_tag,
+			 u32 tag_size);
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SECUREXLINK */
+
 int ocs_aes_ccm_op(struct ocs_aes_dev *aes_dev,
 		   enum ocs_cipher cipher,
 		   enum ocs_instruction instruction,
-- 
2.27.0

