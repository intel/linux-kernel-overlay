From 0b0cc61ff128f4a42a6528905bded845f1ca3e63 Mon Sep 17 00:00:00 2001
From: Kan Liang <kan.liang@linux.intel.com>
Date: Tue, 9 Mar 2021 06:44:56 -0800
Subject: [PATCH 09/49] perf/x86/intel: Read call chain from CET shadow stack

(Remove the dependency of __cet_get_shstk_addr().
 The original one can be found in tgl_release branch)

Current call chain in PERF_SAMPLE_CALLCHAIN data format is read from
frame pointer. However, the frame pointers are off by default on 64bit
code (and on modern 32bit gccs), so there are many binaries around that
do not use frame pointers.
There could be other ways to retrieve the call chain as well, e.g
dwarf unwinding. But it also only works when the applications are
correctly annotated everywhere, including any hand written assembler
code, and it is expensive because it needs to copy the whole stack.

With Control-flow Enforcement Technology, shadow stack can be used as
another option for accurate call chain.

To apply shadow stack from CET, applications have to be compiled for
CET, e.g. with gcc option -fcf-protection -mshstk.
Currently, the Shadow Stack is only enabled for user-mode applications.

Add a knob, cet_shadow_stack_call_chain in sysfs, to enable CET perf
support. The default value is true. Using the shadow stack to replace
frame pointer for user-mode applications, if CET is enabled. Otherwise,
fall back to frame pointer mode.

Optimize for 64 bit process by copying the whole shadow stack in one
__copy_from_user_nmi() operation.

Current implement is on top of full CET support in binaries for now.
In theory it might be able to only enable shadow stack. That can be
done later separately.

Here is an example.

 #gcc -o test_cet test_cet.c -fcf-protection -mshstk
  -fomit-frame-pointer

Without CET,
 #echo 0 > /sys/devices/cpu/cet_shadow_stack_call_chain
 #perf record -e cycles:up -g ./test_cet
 #perf report --stdio
 # Samples: 19K of event 'cycles:up'
 # Event count (approx.): 19093119326
 #
 # Children      Self  Command          Shared Object      Symbol
 # ........  ........  ...............  .................
 # ......................
 #
   100.00%     0.00%  test_cet         [unknown]          [.]
 0x49564100002be33d
            |
            ---0x49564100002be33d
               |
                --99.84%--f3

    99.84%    99.84%  test_cet         test_cet           [.] f3
            |
            ---0x49564100002be33d
               f3

With CET,
 #echo 1 > /sys/devices/cpu/cet_shadow_stack_call_chain
 #perf record -e cycles:up -g ./test_cet
 #perf report --stdio
 # Children      Self  Command          Shared Object     Symbol
 # ........  ........  ...............  ................
 # ......................
 #
   100.00%     0.00%  test_cet          test_cet      [.] _start
            |
            ---_start
               __libc_start_main
               main
               f1
               f2
               |
                --99.81%--f3

Here is the performance result for 64 bit process.

  #gcc -o test_cet_67 test_cet_67.c -fcf-protection -mshstk -m64
  #perf record -e instructions:u --call-graph fp -c10000 ./test_cet_67.c
  (The depth of call chain is 67 for almost all samples.)

FP:               echo 0 > /sys/devices/cpu/cet_shadow_stack_call_chain
CET shadow stack: echo 1 > /sys/devices/cpu/cet_shadow_stack_call_chain

The latency of perf_callchain_user()(Calculated by ftrace)
                               Latency (us)
FP:                            1.84
CET shadow stack:              0.17

The elapsed time of test_cet_67.
                               Elapsed time (us)    Overhead
Baseline (without perf):       26380272
FP:                            27790702             5.53%
CET shadow stack:              26967926             2.23%

For 32 bit process, there are no big differences. The performance of CET
shadow stack is a little bit better than FP.

Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
---
 arch/x86/events/core.c       |  10 +++
 arch/x86/events/intel/core.c | 125 +++++++++++++++++++++++++++++++++++
 arch/x86/events/perf_event.h |   5 ++
 3 files changed, 140 insertions(+)

diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 8f71dd72ef95..8500ce01afbf 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -2812,6 +2812,10 @@ perf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry_ctx *ent
 
 	fp = compat_ptr(ss_base + regs->bp);
 	pagefault_disable();
+
+	if (x86_pmu.store_shadow_stack_user && x86_pmu.store_shadow_stack_user(entry))
+		goto out;
+
 	while (entry->nr < entry->max_stack) {
 		if (!valid_user_frame(fp, sizeof(frame)))
 			break;
@@ -2824,6 +2828,7 @@ perf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry_ctx *ent
 		perf_callchain_store(entry, cs_base + frame.return_address);
 		fp = compat_ptr(ss_base + frame.next_frame);
 	}
+out:
 	pagefault_enable();
 	return 1;
 }
@@ -2863,6 +2868,10 @@ perf_callchain_user(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs
 		return;
 
 	pagefault_disable();
+
+	if (x86_pmu.store_shadow_stack_user && x86_pmu.store_shadow_stack_user(entry))
+		goto out;
+
 	while (entry->nr < entry->max_stack) {
 		if (!valid_user_frame(fp, sizeof(frame)))
 			break;
@@ -2875,6 +2884,7 @@ perf_callchain_user(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs
 		perf_callchain_store(entry, frame.return_address);
 		fp = (void __user *)frame.next_frame;
 	}
+out:
 	pagefault_enable();
 }
 
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index e28892270c58..e880b72d728a 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -4500,6 +4500,118 @@ static int intel_pmu_filter_match(struct perf_event *event)
 	return cpumask_test_cpu(cpu, &pmu->supported_cpus);
 }
 
+#ifdef CONFIG_X86_CET
+
+static int
+intel_pmu_store_shadow_stack_user(struct perf_callchain_entry_ctx *ctx_entry)
+{
+	unsigned long left, stack_base, stack_top, return_addr = 0;
+	struct cet_status *cet = &current->thread.cet;
+	int step = sizeof(unsigned long);
+	int nr = 0;
+
+	/* The callchain space is full */
+	if (ctx_entry->contexts_maxed)
+		return 0;
+
+	/* The shadow stack is not available. */
+	if (!cet->shstk_base || !cet->shstk_size)
+		return 0;
+
+	stack_base = cet->shstk_base + cet->shstk_size;
+	/* TODO: special edition */
+//	stack_top = __cet_get_shstk_addr();
+	rdmsrl(MSR_IA32_PL3_SSP, stack_top);
+
+	if ((stack_base <= stack_top) || ((stack_base - stack_top) > cet->shstk_size))
+		return 0;
+
+#ifdef CONFIG_X86_64
+	if (!any_64bit_mode(current_pt_regs()))
+		step = sizeof(u32);
+#endif
+
+	/*
+	 * Optimization for 64 bit process.
+	 * Copy the whole stack in one operation
+	 */
+	if (step == sizeof(u64)) {
+		struct perf_callchain_entry *entry = ctx_entry->entry;
+		u64 max_nr = (stack_base - stack_top) / sizeof(u64);
+		u64 max_size;
+
+		max_nr = min(max_nr, (u64)(ctx_entry->max_stack - ctx_entry->nr));
+		max_size = max_nr * sizeof(u64);
+
+		if (!access_ok((void __user *)stack_top, max_size))
+			return 0;
+
+		left = __copy_from_user_inatomic(&entry->ip[entry->nr],
+						 (void __user *)stack_top, max_size);
+		max_size -= left;
+		nr = max_size / sizeof(u64);
+		entry->nr += nr;
+		ctx_entry->nr += nr;
+	} else {
+		while ((ctx_entry->nr < ctx_entry->max_stack) && (stack_top < stack_base)) {
+
+			if (!access_ok((void __user *)stack_top, step))
+				break;
+
+			left = __copy_from_user_inatomic(&return_addr, (void __user *)stack_top, step);
+			stack_top += step;
+			if (left != 0)
+				break;
+
+			if (perf_callchain_store(ctx_entry, return_addr))
+				break;
+			nr++;
+		}
+	}
+
+	return nr;
+}
+
+#else
+
+static int
+intel_pmu_store_shadow_stack_user(struct perf_callchain_entry_ctx *ctx_entry)
+{
+	return 0;
+}
+
+#endif
+
+static ssize_t cet_shadow_stack_call_chain_show(struct device *cdev,
+						struct device_attribute *attr,
+						char *buf)
+{
+	return sprintf(buf, "%d\n", x86_pmu.store_shadow_stack_user ? 1 : 0);
+}
+
+static ssize_t cet_shadow_stack_call_chain_store(struct device *cdev,
+						 struct device_attribute *attr,
+						 const char *buf, size_t count)
+{
+	void *fun = NULL;
+	ssize_t ret;
+	bool val;
+
+
+	ret = kstrtobool(buf, &val);
+	if (ret)
+		return ret;
+
+	if (val)
+		fun = intel_pmu_store_shadow_stack_user;
+
+	xchg(&x86_pmu.store_shadow_stack_user, fun);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(cet_shadow_stack_call_chain);
+
 PMU_FORMAT_ATTR(offcore_rsp, "config1:0-63");
 
 PMU_FORMAT_ATTR(ldlat, "config1:0-15");
@@ -5114,6 +5226,7 @@ static DEVICE_ATTR(allow_tsx_force_abort, 0644,
 static struct attribute *intel_pmu_attrs[] = {
 	&dev_attr_freeze_on_smi.attr,
 	&dev_attr_allow_tsx_force_abort.attr,
+	&dev_attr_cet_shadow_stack_call_chain.attr,
 	NULL,
 };
 
@@ -5147,6 +5260,14 @@ default_is_visible(struct kobject *kobj, struct attribute *attr, int i)
 	if (attr == &dev_attr_allow_tsx_force_abort.attr)
 		return x86_pmu.flags & PMU_FL_TFA ? attr->mode : 0;
 
+#ifdef CONFIG_X86_CET
+	if (attr == &dev_attr_cet_shadow_stack_call_chain.attr)
+		return boot_cpu_has(X86_FEATURE_SHSTK) ? attr->mode : 0;
+#else
+	if (attr == &dev_attr_cet_shadow_stack_call_chain.attr)
+		return 0;
+#endif
+
 	return attr->mode;
 }
 
@@ -6063,6 +6184,10 @@ __init int intel_pmu_init(void)
 		x86_pmu.num_topdown_events = 4;
 		x86_pmu.update_topdown_event = icl_update_topdown_event;
 		x86_pmu.set_topdown_event_period = icl_set_topdown_event_period;
+#ifdef CONFIG_X86_CET
+		if (boot_cpu_has(X86_FEATURE_SHSTK))
+			x86_pmu.store_shadow_stack_user = intel_pmu_store_shadow_stack_user;
+#endif
 		pr_cont("Icelake events, ");
 		name = "icelake";
 		break;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index ad87cb36f7c8..df64fb8a8e37 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -885,6 +885,11 @@ struct x86_pmu {
 	int (*aux_output_match) (struct perf_event *event);
 
 	int (*filter_match)(struct perf_event *event);
+	/*
+	 * Intel CET Shadow Stack
+	 */
+	 int (*store_shadow_stack_user) (struct perf_callchain_entry_ctx *entry);
+
 	/*
 	 * Hybrid support
 	 *
-- 
2.27.0

