From 7f867523ad98cbd31d859f73024bbb3f86ce3e25 Mon Sep 17 00:00:00 2001
From: hemanthk <hemanthkumar.sm@intel.com>
Date: Fri, 12 Mar 2021 07:42:25 +0530
Subject: [PATCH 214/223] crypto: keembay - Add support for Thunder Bay OCS
 AES/SM4

    Extends the Keem Bay OCS AES/SM4 driver to support Thunder Bay Soc.

Signed-off-by: hemanthk <hemanthkumar.sm@intel.com>
---
 drivers/crypto/keembay/Kconfig                |  42 ++++
 drivers/crypto/keembay/Makefile               |   3 +
 drivers/crypto/keembay/keembay-ocs-aes-core.c | 204 ++++++++++++++----
 drivers/crypto/keembay/ocs-aes.c              |  65 +++++-
 drivers/crypto/keembay/ocs-aes.h              |  12 ++
 5 files changed, 276 insertions(+), 50 deletions(-)

diff --git a/drivers/crypto/keembay/Kconfig b/drivers/crypto/keembay/Kconfig
index d32329c11d8c..269b7cbf10c9 100644
--- a/drivers/crypto/keembay/Kconfig
+++ b/drivers/crypto/keembay/Kconfig
@@ -110,3 +110,45 @@ config CRYPTO_DEV_OCS_WRAPPER
 	  WRAPPER for use by OCS AES, HCU HW accelerators.
 
 	  Extends the Addressing capability of OCS Engines from 32 to 38 bits.
+
+config CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4
+	tristate "Support for Intel Thunder Bay OCS AES/SM4 HW acceleration"
+	depends on HAS_IOMEM
+	depends on ARCH_THUNDERBAY || COMPILE_TEST
+	depends on CRYPTO_DEV_OCS_WRAPPER
+	select CRYPTO_SKCIPHER
+	select CRYPTO_AEAD
+	select CRYPTO_ENGINE
+	help
+	  Support for Intel Thunder Bay Offload and Crypto Subsystem (OCS) AES and
+	  SM4 cipher hardware acceleration for use with Crypto API.
+
+	  Provides HW acceleration for the following transformations:
+	  cbc(aes), ctr(aes), ccm(aes), gcm(aes), cbc(sm4), ctr(sm4), ccm(sm4)
+	  and gcm(sm4).
+
+	  Optionally, support for the following transformations can also be
+	  enabled: ecb(aes), cts(cbc(aes)), ecb(sm4) and cts(cbc(sm4)).
+
+config CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_ECB
+	bool "Support for Intel Thunder Bay OCS AES/SM4 ECB HW acceleration"
+	depends on CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4
+	help
+	  Support for Intel Thunder Bay Offload and Crypto Subsystem (OCS)
+	  AES/SM4 ECB mode hardware acceleration for use with Crypto API.
+
+	  Provides OCS version of ecb(aes) and ecb(sm4)
+
+	  Intel does not recommend use of ECB mode with AES/SM4.
+
+config CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_CTS
+	bool "Support for Intel Thunder Bay OCS AES/SM4 CTS HW acceleration"
+	depends on CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4
+	help
+	  Support for Intel Thunder Bay Offload and Crypto Subsystem (OCS)
+	  AES/SM4 CBC with CTS mode hardware acceleration for use with
+	  Crypto API.
+
+	  Provides OCS version of cts(cbc(aes)) and cts(cbc(sm4)).
+
+	  Intel does not recommend use of CTS mode with AES/SM4.
diff --git a/drivers/crypto/keembay/Makefile b/drivers/crypto/keembay/Makefile
index 8f91cb908e14..7b32f24e1212 100644
--- a/drivers/crypto/keembay/Makefile
+++ b/drivers/crypto/keembay/Makefile
@@ -10,3 +10,6 @@ keembay-ocs-hcu-objs := keembay-ocs-hcu-core.o ocs-hcu.o
 obj-$(CONFIG_CRYPTO_DEV_KEEMBAY_OCS_ECC) += keembay-ocs-ecc.o
 
 obj-$(CONFIG_CRYPTO_DEV_OCS_WRAPPER) +=ocs-wrapper.o
+
+obj-$(CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4) += thunderbay-ocs-aes.o
+thunderbay-ocs-aes-objs := keembay-ocs-aes-core.o ocs-aes.o
diff --git a/drivers/crypto/keembay/keembay-ocs-aes-core.c b/drivers/crypto/keembay/keembay-ocs-aes-core.c
index b6b25d994af3..758202354110 100644
--- a/drivers/crypto/keembay/keembay-ocs-aes-core.c
+++ b/drivers/crypto/keembay/keembay-ocs-aes-core.c
@@ -25,6 +25,9 @@
 #include <crypto/internal/skcipher.h>
 
 #include "ocs-aes.h"
+#ifdef CONFIG_ARCH_THUNDERBAY
+#include "ocs-wrapper.h"
+#endif
 
 #define KMB_OCS_PRIORITY	350
 #define DRV_NAME		"keembay-ocs-aes"
@@ -36,6 +39,30 @@
 #define OCS_AES_KEYSIZE_256	32
 #define OCS_SM4_KEY_SIZE	16
 
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4
+#define ENABLE_OCS_AES_SM4
+#endif
+
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB
+#define ENABLE_OCS_AES_SM4_ECB
+#endif
+
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS
+#define ENABLE_OCS_AES_SM4_CTS
+#endif
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4
+#define ENABLE_OCS_AES_SM4
+#endif
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_ECB
+#define ENABLE_OCS_AES_SM4_ECB
+#endif
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_CTS
+#define ENABLE_OCS_AES_SM4_CTS
+#endif
+
 /**
  * struct ocs_aes_tctx - OCS AES Transform context
  * @engine_ctx:		Engine context.
@@ -119,9 +146,16 @@ static struct ocs_aes_drv ocs_aes = {
 	.lock = __SPIN_LOCK_UNLOCKED(ocs_aes.lock),
 };
 
-static struct ocs_aes_dev *kmb_ocs_aes_find_dev(struct ocs_aes_tctx *tctx)
+static struct ocs_aes_dev *kmb_ocs_aes_find_dev(struct ocs_aes_tctx *tctx, struct crypto_tfm *tfm)
 {
 	struct ocs_aes_dev *aes_dev;
+	struct list_head *p = NULL;
+	const char *driver_name = NULL;
+
+	if (!tfm)
+		return 0;
+
+	driver_name = crypto_tfm_alg_driver_name(tfm);
 
 	spin_lock(&ocs_aes.lock);
 
@@ -130,14 +164,18 @@ static struct ocs_aes_dev *kmb_ocs_aes_find_dev(struct ocs_aes_tctx *tctx)
 		goto exit;
 	}
 
-	/* Only a single OCS device available */
-	aes_dev = list_first_entry(&ocs_aes.dev_list, struct ocs_aes_dev, list);
-	tctx->aes_dev = aes_dev;
+	list_for_each(p, &ocs_aes.dev_list) {
+		aes_dev = list_entry(p, struct ocs_aes_dev, list);
+		if (strstr(driver_name, aes_dev->driver_name)) {
+			tctx->aes_dev = aes_dev;
+			goto exit;
+		}
+	}
 
 exit:
 	spin_unlock(&ocs_aes.lock);
 
-	return aes_dev;
+	return tctx->aes_dev;
 }
 
 /*
@@ -313,9 +351,9 @@ static int kmb_ocs_sk_common(struct skcipher_request *req,
 			     enum ocs_instruction instruction,
 			     enum ocs_mode mode)
 {
-	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_tfm *tfm = crypto_skcipher_tfm(crypto_skcipher_reqtfm(req));
 	struct ocs_aes_rctx *rctx = skcipher_request_ctx(req);
-	struct ocs_aes_tctx *tctx = crypto_skcipher_ctx(tfm);
+	struct ocs_aes_tctx *tctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	struct ocs_aes_dev *aes_dev;
 	int rc;
 
@@ -350,7 +388,7 @@ static int kmb_ocs_sk_common(struct skcipher_request *req,
 	if (rc)
 		return rc;
 
-	aes_dev = kmb_ocs_aes_find_dev(tctx);
+	aes_dev = kmb_ocs_aes_find_dev(tctx, tfm);
 	if (!aes_dev)
 		return -ENODEV;
 
@@ -640,6 +678,7 @@ static int kmb_ocs_aead_common(struct aead_request *req,
 			       enum ocs_instruction instruction,
 			       enum ocs_mode mode)
 {
+	struct crypto_tfm *tfm = crypto_aead_tfm(crypto_aead_reqtfm(req));
 	struct ocs_aes_tctx *tctx = crypto_aead_ctx(crypto_aead_reqtfm(req));
 	struct ocs_aes_rctx *rctx = aead_request_ctx(req);
 	struct ocs_aes_dev *dd;
@@ -668,7 +707,7 @@ static int kmb_ocs_aead_common(struct aead_request *req,
 	if (rc)
 		return rc;
 
-	dd = kmb_ocs_aes_find_dev(tctx);
+	dd = kmb_ocs_aes_find_dev(tctx, tfm);
 	if (!dd)
 		return -ENODEV;
 
@@ -1008,7 +1047,7 @@ static int kmb_ocs_aes_aead_set_key(struct crypto_aead *tfm, const u8 *in_key,
 	return kmb_ocs_aead_set_key(tfm, in_key, key_len, OCS_AES);
 }
 
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB
+#if defined(ENABLE_OCS_AES_SM4_ECB)
 static int kmb_ocs_aes_ecb_encrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_AES, OCS_ENCRYPT, OCS_MODE_ECB);
@@ -1018,7 +1057,7 @@ static int kmb_ocs_aes_ecb_decrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_AES, OCS_DECRYPT, OCS_MODE_ECB);
 }
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB */
+#endif /* ENABLE_OCS_AES_SM4_ECB */
 
 static int kmb_ocs_aes_cbc_encrypt(struct skcipher_request *req)
 {
@@ -1040,7 +1079,7 @@ static int kmb_ocs_aes_ctr_decrypt(struct skcipher_request *req)
 	return kmb_ocs_sk_common(req, OCS_AES, OCS_DECRYPT, OCS_MODE_CTR);
 }
 
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS
+#ifdef ENABLE_OCS_AES_SM4_CTS
 static int kmb_ocs_aes_cts_encrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_AES, OCS_ENCRYPT, OCS_MODE_CTS);
@@ -1050,7 +1089,7 @@ static int kmb_ocs_aes_cts_decrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_AES, OCS_DECRYPT, OCS_MODE_CTS);
 }
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS */
+#endif /* ENABLE_OCS_AES_SM4_CTS */
 
 static int kmb_ocs_aes_gcm_encrypt(struct aead_request *req)
 {
@@ -1084,7 +1123,7 @@ static int kmb_ocs_sm4_aead_set_key(struct crypto_aead *tfm, const u8 *in_key,
 	return kmb_ocs_aead_set_key(tfm, in_key, key_len, OCS_SM4);
 }
 
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB
+#ifdef ENABLE_OCS_AES_SM4_ECB
 static int kmb_ocs_sm4_ecb_encrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_SM4, OCS_ENCRYPT, OCS_MODE_ECB);
@@ -1094,7 +1133,7 @@ static int kmb_ocs_sm4_ecb_decrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_SM4, OCS_DECRYPT, OCS_MODE_ECB);
 }
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB */
+#endif /* ENABLE_OCS_AES_SM4_ECB */
 
 static int kmb_ocs_sm4_cbc_encrypt(struct skcipher_request *req)
 {
@@ -1116,7 +1155,7 @@ static int kmb_ocs_sm4_ctr_decrypt(struct skcipher_request *req)
 	return kmb_ocs_sk_common(req, OCS_SM4, OCS_DECRYPT, OCS_MODE_CTR);
 }
 
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS
+#ifdef ENABLE_OCS_AES_SM4_CTS
 static int kmb_ocs_sm4_cts_encrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_SM4, OCS_ENCRYPT, OCS_MODE_CTS);
@@ -1126,7 +1165,7 @@ static int kmb_ocs_sm4_cts_decrypt(struct skcipher_request *req)
 {
 	return kmb_ocs_sk_common(req, OCS_SM4, OCS_DECRYPT, OCS_MODE_CTS);
 }
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS */
+#endif /* ENABLE_OCS_AES_SM4_CTS */
 
 static int kmb_ocs_sm4_gcm_encrypt(struct aead_request *req)
 {
@@ -1281,10 +1320,10 @@ static void ocs_aead_cra_exit(struct crypto_aead *tfm)
 }
 
 static struct skcipher_alg algs[] = {
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB
+#ifdef ENABLE_OCS_AES_SM4_ECB
 	{
 		.base.cra_name = "ecb(aes)",
-		.base.cra_driver_name = "ecb-aes-keembay-ocs",
+		.base.cra_driver_name = "ecb-aes-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
@@ -1302,10 +1341,10 @@ static struct skcipher_alg algs[] = {
 		.init = ocs_aes_init_tfm,
 		.exit = ocs_exit_tfm,
 	},
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB */
+#endif /* ENABLE_OCS_AES_SM4_ECB */
 	{
 		.base.cra_name = "cbc(aes)",
-		.base.cra_driver_name = "cbc-aes-keembay-ocs",
+		.base.cra_driver_name = "cbc-aes-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
@@ -1326,7 +1365,7 @@ static struct skcipher_alg algs[] = {
 	},
 	{
 		.base.cra_name = "ctr(aes)",
-		.base.cra_driver_name = "ctr-aes-keembay-ocs",
+		.base.cra_driver_name = "ctr-aes-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
@@ -1345,10 +1384,10 @@ static struct skcipher_alg algs[] = {
 		.init = ocs_aes_init_tfm,
 		.exit = ocs_exit_tfm,
 	},
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS
+#ifdef ENABLE_OCS_AES_SM4_CTS
 	{
 		.base.cra_name = "cts(cbc(aes))",
-		.base.cra_driver_name = "cts-aes-keembay-ocs",
+		.base.cra_driver_name = "cts-aes-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
@@ -1367,11 +1406,11 @@ static struct skcipher_alg algs[] = {
 		.init = ocs_aes_init_tfm,
 		.exit = ocs_exit_tfm,
 	},
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS */
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB
+#endif /* ENABLE_OCS_AES_SM4_CTS */
+#ifdef ENABLE_OCS_AES_SM4_ECB
 	{
 		.base.cra_name = "ecb(sm4)",
-		.base.cra_driver_name = "ecb-sm4-keembay-ocs",
+		.base.cra_driver_name = "ecb-sm4-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY,
@@ -1388,10 +1427,10 @@ static struct skcipher_alg algs[] = {
 		.init = ocs_sm4_init_tfm,
 		.exit = ocs_exit_tfm,
 	},
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB */
+#endif /* ENABLE_OCS_AES_SM4_ECB */
 	{
 		.base.cra_name = "cbc(sm4)",
-		.base.cra_driver_name = "cbc-sm4-keembay-ocs",
+		.base.cra_driver_name = "cbc-sm4-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY,
@@ -1411,7 +1450,7 @@ static struct skcipher_alg algs[] = {
 	},
 	{
 		.base.cra_name = "ctr(sm4)",
-		.base.cra_driver_name = "ctr-sm4-keembay-ocs",
+		.base.cra_driver_name = "ctr-sm4-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY,
@@ -1429,10 +1468,10 @@ static struct skcipher_alg algs[] = {
 		.init = ocs_sm4_init_tfm,
 		.exit = ocs_exit_tfm,
 	},
-#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS
+#ifdef ENABLE_OCS_AES_SM4_CTS
 	{
 		.base.cra_name = "cts(cbc(sm4))",
-		.base.cra_driver_name = "cts-sm4-keembay-ocs",
+		.base.cra_driver_name = "cts-sm4-",
 		.base.cra_priority = KMB_OCS_PRIORITY,
 		.base.cra_flags = CRYPTO_ALG_ASYNC |
 				  CRYPTO_ALG_KERN_DRIVER_ONLY,
@@ -1450,14 +1489,14 @@ static struct skcipher_alg algs[] = {
 		.init = ocs_sm4_init_tfm,
 		.exit = ocs_exit_tfm,
 	}
-#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS */
+#endif /* ENABLE_OCS_AES_SM4_CTS */
 };
 
 static struct aead_alg algs_aead[] = {
 	{
 		.base = {
 			.cra_name = "gcm(aes)",
-			.cra_driver_name = "gcm-aes-keembay-ocs",
+			.cra_driver_name = "gcm-aes-",
 			.cra_priority = KMB_OCS_PRIORITY,
 			.cra_flags = CRYPTO_ALG_ASYNC |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
@@ -1479,7 +1518,7 @@ static struct aead_alg algs_aead[] = {
 	{
 		.base = {
 			.cra_name = "ccm(aes)",
-			.cra_driver_name = "ccm-aes-keembay-ocs",
+			.cra_driver_name = "ccm-aes-",
 			.cra_priority = KMB_OCS_PRIORITY,
 			.cra_flags = CRYPTO_ALG_ASYNC |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
@@ -1501,7 +1540,7 @@ static struct aead_alg algs_aead[] = {
 	{
 		.base = {
 			.cra_name = "gcm(sm4)",
-			.cra_driver_name = "gcm-sm4-keembay-ocs",
+			.cra_driver_name = "gcm-sm4-",
 			.cra_priority = KMB_OCS_PRIORITY,
 			.cra_flags = CRYPTO_ALG_ASYNC |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
@@ -1522,7 +1561,7 @@ static struct aead_alg algs_aead[] = {
 	{
 		.base = {
 			.cra_name = "ccm(sm4)",
-			.cra_driver_name = "ccm-sm4-keembay-ocs",
+			.cra_driver_name = "ccm-sm4-",
 			.cra_priority = KMB_OCS_PRIORITY,
 			.cra_flags = CRYPTO_ALG_ASYNC |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
@@ -1544,8 +1583,8 @@ static struct aead_alg algs_aead[] = {
 
 static void unregister_aes_algs(struct ocs_aes_dev *aes_dev)
 {
-	crypto_unregister_aeads(algs_aead, ARRAY_SIZE(algs_aead));
-	crypto_unregister_skciphers(algs, ARRAY_SIZE(algs));
+	crypto_unregister_aeads(aes_dev->algs_aead, ARRAY_SIZE(algs_aead));
+	crypto_unregister_skciphers(aes_dev->algs, ARRAY_SIZE(algs));
 }
 
 static int register_aes_algs(struct ocs_aes_dev *aes_dev)
@@ -1556,13 +1595,13 @@ static int register_aes_algs(struct ocs_aes_dev *aes_dev)
 	 * If any algorithm fails to register, all preceding algorithms that
 	 * were successfully registered will be automatically unregistered.
 	 */
-	ret = crypto_register_aeads(algs_aead, ARRAY_SIZE(algs_aead));
+	ret = crypto_register_aeads(aes_dev->algs_aead, ARRAY_SIZE(algs_aead));
 	if (ret)
 		return ret;
 
-	ret = crypto_register_skciphers(algs, ARRAY_SIZE(algs));
+	ret = crypto_register_skciphers(aes_dev->algs, ARRAY_SIZE(algs));
 	if (ret)
-		crypto_unregister_aeads(algs_aead, ARRAY_SIZE(algs));
+		crypto_unregister_aeads(aes_dev->algs_aead, ARRAY_SIZE(algs));
 
 	return ret;
 }
@@ -1599,7 +1638,9 @@ static int kmb_ocs_aes_probe(struct platform_device *pdev)
 	struct device *dev = &pdev->dev;
 	struct ocs_aes_dev *aes_dev;
 	struct resource *aes_mem;
-	int rc;
+	u64 mask;
+	int rc, i;
+	const char *driver_name = NULL;
 
 	aes_dev = devm_kzalloc(dev, sizeof(*aes_dev), GFP_KERNEL);
 	if (!aes_dev)
@@ -1609,9 +1650,14 @@ static int kmb_ocs_aes_probe(struct platform_device *pdev)
 
 	platform_set_drvdata(pdev, aes_dev);
 
-	rc = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));
+#ifdef CONFIG_ARCH_THUNDERBAY
+	mask = DMA_BIT_MASK(64);
+#else
+	mask = DMA_BIT_MASK(32);
+#endif
+	rc = dma_set_mask_and_coherent(dev, mask);
 	if (rc) {
-		dev_err(dev, "Failed to set 32 bit dma mask %d\n", rc);
+		dev_err(dev, "Failed to set 0x%llx dma mask %d\n", mask, rc);
 		return rc;
 	}
 
@@ -1640,6 +1686,46 @@ static int kmb_ocs_aes_probe(struct platform_device *pdev)
 		return rc;
 	}
 
+	rc = of_property_read_string(pdev->dev.of_node, "driver-name", &driver_name);
+	if (rc < 0) {
+		dev_err(dev, "Could not get driver-name\n");
+		return rc;
+	}
+	strcpy(aes_dev->driver_name, driver_name);
+
+#ifdef CONFIG_ARCH_THUNDERBAY
+	aes_dev->wrapper_dev = ocs_wrapper_find_dev(aes_dev->driver_name);
+	if (!aes_dev->wrapper_dev) {
+		dev_err(dev, "Could not find ocs wrapper device\n");
+		return -ENODEV;
+	}
+#endif
+
+	rc = of_property_read_u32(pdev->dev.of_node, "priority",
+				  (u32 *)&aes_dev->priority);
+	if (rc < 0) {
+		dev_err(dev, "Could not get priority\n");
+		return rc;
+	}
+
+	aes_dev->algs = devm_kzalloc(dev, sizeof(algs), GFP_KERNEL);
+	if (!aes_dev->algs)
+		return -ENOMEM;
+	memcpy(aes_dev->algs, algs, sizeof(algs));
+	for (i = 0; i < ARRAY_SIZE(algs); i++) {
+		strcat(aes_dev->algs[i].base.cra_driver_name, aes_dev->driver_name);
+		aes_dev->algs[i].base.cra_priority = aes_dev->priority;
+	}
+
+	aes_dev->algs_aead = devm_kzalloc(dev, sizeof(algs_aead), GFP_KERNEL);
+	if (!aes_dev->algs_aead)
+		return -ENOMEM;
+	memcpy(aes_dev->algs_aead, algs_aead, sizeof(algs_aead));
+	for (i = 0; i < ARRAY_SIZE(algs_aead); i++) {
+		strcat(aes_dev->algs_aead[i].base.cra_driver_name, aes_dev->driver_name);
+		aes_dev->algs_aead[i].base.cra_priority = aes_dev->priority;
+	}
+
 	INIT_LIST_HEAD(&aes_dev->list);
 	spin_lock(&ocs_aes.lock);
 	list_add_tail(&aes_dev->list, &ocs_aes.dev_list);
@@ -1689,9 +1775,14 @@ static struct platform_driver kmb_ocs_aes_driver = {
 
 module_platform_driver(kmb_ocs_aes_driver);
 
+#ifdef CONFIG_ARCH_THUNDERBAY
+MODULE_DESCRIPTION("Intel Thunder Bay Offload and Crypto Subsystem (OCS) AES/SM4 Driver");
+#else
 MODULE_DESCRIPTION("Intel Keem Bay Offload and Crypto Subsystem (OCS) AES/SM4 Driver");
+#endif
 MODULE_LICENSE("GPL");
 
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4
 MODULE_ALIAS_CRYPTO("cbc-aes-keembay-ocs");
 MODULE_ALIAS_CRYPTO("ctr-aes-keembay-ocs");
 MODULE_ALIAS_CRYPTO("gcm-aes-keembay-ocs");
@@ -1701,6 +1792,7 @@ MODULE_ALIAS_CRYPTO("cbc-sm4-keembay-ocs");
 MODULE_ALIAS_CRYPTO("ctr-sm4-keembay-ocs");
 MODULE_ALIAS_CRYPTO("gcm-sm4-keembay-ocs");
 MODULE_ALIAS_CRYPTO("ccm-sm4-keembay-ocs");
+#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4 */
 
 #ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_ECB
 MODULE_ALIAS_CRYPTO("ecb-aes-keembay-ocs");
@@ -1711,3 +1803,25 @@ MODULE_ALIAS_CRYPTO("ecb-sm4-keembay-ocs");
 MODULE_ALIAS_CRYPTO("cts-aes-keembay-ocs");
 MODULE_ALIAS_CRYPTO("cts-sm4-keembay-ocs");
 #endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_AES_SM4_CTS */
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4
+MODULE_ALIAS_CRYPTO("cbc-aes-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("ctr-aes-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("gcm-aes-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("ccm-aes-thunderbay-ocs");
+
+MODULE_ALIAS_CRYPTO("cbc-sm4-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("ctr-sm4-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("gcm-sm4-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("ccm-sm4-thunderbay-ocs");
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4 */
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_ECB
+MODULE_ALIAS_CRYPTO("ecb-aes-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("ecb-sm4-thunderbay-ocs");
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_ECB */
+
+#ifdef CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_CTS
+MODULE_ALIAS_CRYPTO("cts-aes-thunderbay-ocs");
+MODULE_ALIAS_CRYPTO("cts-sm4-thunderbay-ocs");
+#endif /* CONFIG_CRYPTO_DEV_THUNDERBAY_OCS_AES_SM4_CTS */
diff --git a/drivers/crypto/keembay/ocs-aes.c b/drivers/crypto/keembay/ocs-aes.c
index be9f32fc8f42..29620532432c 100644
--- a/drivers/crypto/keembay/ocs-aes.c
+++ b/drivers/crypto/keembay/ocs-aes.c
@@ -18,6 +18,9 @@
 #include <crypto/gcm.h>
 
 #include "ocs-aes.h"
+#ifdef CONFIG_ARCH_THUNDERBAY
+#include "ocs-wrapper.h"
+#endif
 
 #define AES_COMMAND_OFFSET			0x0000
 #define AES_KEY_0_OFFSET			0x0004
@@ -422,18 +425,22 @@ static int ocs_aes_irq_enable_and_wait(struct ocs_aes_dev *aes_dev, u8 irq)
 static inline void dma_to_ocs_aes_ll(struct ocs_aes_dev *aes_dev,
 				     dma_addr_t dma_list)
 {
+	u32 val;
+
 	iowrite32(0, aes_dev->base_reg + AES_A_DMA_SRC_SIZE_OFFSET);
-	iowrite32(dma_list,
-		  aes_dev->base_reg + AES_A_DMA_NEXT_SRC_DESCR_OFFSET);
+	val = dma_list & 0xFFFFFFFF;
+	iowrite32(val, aes_dev->base_reg + AES_A_DMA_NEXT_SRC_DESCR_OFFSET);
 }
 
 /* Configure DMA from OCS, linked list mode */
 static inline void dma_from_ocs_aes_ll(struct ocs_aes_dev *aes_dev,
 				       dma_addr_t dma_list)
 {
+	u32 val;
+
 	iowrite32(0, aes_dev->base_reg + AES_A_DMA_DST_SIZE_OFFSET);
-	iowrite32(dma_list,
-		  aes_dev->base_reg + AES_A_DMA_NEXT_DST_DESCR_OFFSET);
+	val = dma_list & 0xFFFFFFFF;
+	iowrite32(val, aes_dev->base_reg + AES_A_DMA_NEXT_DST_DESCR_OFFSET);
 }
 
 irqreturn_t ocs_aes_irq_handler(int irq, void *dev_id)
@@ -839,6 +846,10 @@ int ocs_aes_op(struct ocs_aes_dev *aes_dev,
 	/* Set AES_ACTIVE.TRIGGER to start the operation. */
 	aes_a_op_trigger(aes_dev);
 
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_setconfig(aes_dev->wrapper_dev, (src_dma_list >> 32), (dst_dma_list >> 32));
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
 	/* Configure and activate input / output DMA. */
 	dma_to_ocs_aes_ll(aes_dev, src_dma_list);
 	dma_from_ocs_aes_ll(aes_dev, dst_dma_list);
@@ -857,6 +868,9 @@ int ocs_aes_op(struct ocs_aes_dev *aes_dev,
 
 	/* Wait for engine to complete processing. */
 	rc = ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
 	if (rc)
 		return rc;
 
@@ -976,6 +990,10 @@ int ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
 
 	/* Process AAD. */
 	if (aad_size) {
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ocs_wrapper_setconfig(aes_dev->wrapper_dev, (aad_dma_list >> 32), 0);
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
 		/* If aad present, configure DMA to feed it to the engine. */
 		dma_to_ocs_aes_ll(aes_dev, aad_dma_list);
 		aes_a_dma_active_src_ll_en(aes_dev);
@@ -985,6 +1003,9 @@ int ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
 
 		/* Wait for DMA transfer to complete. */
 		rc = ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
 		if (rc)
 			return rc;
 	} else {
@@ -997,6 +1018,11 @@ int ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
 
 	/* Now process payload. */
 	if (src_size) {
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ocs_wrapper_setconfig(aes_dev->wrapper_dev, (src_dma_list >> 32),
+				      (dst_dma_list >> 32));
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
 		/* Configure and activate DMA for both input and output data. */
 		dma_to_ocs_aes_ll(aes_dev, src_dma_list);
 		dma_from_ocs_aes_ll(aes_dev, dst_dma_list);
@@ -1011,6 +1037,9 @@ int ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,
 
 	/* Wait for OCS AES engine to complete processing. */
 	rc = ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
 	if (rc)
 		return rc;
 
@@ -1175,6 +1204,10 @@ static int ocs_aes_ccm_do_adata(struct ocs_aes_dev *aes_dev,
 	 */
 	ocs_aes_ccm_write_adata_len(aes_dev, adata_size);
 
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_setconfig(aes_dev->wrapper_dev, (adata_dma_list >> 32), 0);
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
 	/* Configure the AES/SM4 DMA to fetch the Associated Data */
 	dma_to_ocs_aes_ll(aes_dev, adata_dma_list);
 
@@ -1186,6 +1219,9 @@ static int ocs_aes_ccm_do_adata(struct ocs_aes_dev *aes_dev,
 
 	/* Wait for DMA transfer to complete. */
 	rc = ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
 	if (rc)
 		return rc;
 
@@ -1231,6 +1267,8 @@ static int ocs_aes_ccm_decrypt_do_payload(struct ocs_aes_dev *aes_dev,
 					  dma_addr_t src_dma_list,
 					  u32 src_size)
 {
+	int rc;
+
 	if (!src_size) {
 		/* Let engine process 0-length input. */
 		aes_a_dma_set_xfer_size_zero(aes_dev);
@@ -1240,6 +1278,10 @@ static int ocs_aes_ccm_decrypt_do_payload(struct ocs_aes_dev *aes_dev,
 		return 0;
 	}
 
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_setconfig(aes_dev->wrapper_dev, (src_dma_list >> 32), (dst_dma_list >> 32));
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
 	/*
 	 * Configure and activate DMA for both input and output
 	 * data.
@@ -1257,7 +1299,12 @@ static int ocs_aes_ccm_decrypt_do_payload(struct ocs_aes_dev *aes_dev,
 	  * Enable DMA DONE interrupt; once DMA transfer is over,
 	  * interrupt handler will process the MAC/tag.
 	  */
-	return ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);
+	rc = ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);
+#ifdef CONFIG_ARCH_THUNDERBAY
+	ocs_wrapper_release(aes_dev->wrapper_dev);
+#endif /* CONFIG_ARCH_THUNDERBAY */
+
+	return rc;
 }
 
 /*
@@ -1473,6 +1520,14 @@ int ocs_create_linked_list_from_sg(const struct ocs_aes_dev *aes_dev,
 	ll = dll_desc->vaddr;
 	for (i = 0; i < dma_nents; i++, sg = sg_next(sg)) {
 		ll[i].src_addr = sg_dma_address(sg) + data_offset;
+#ifdef CONFIG_ARCH_THUNDERBAY
+		ll[i].src_addr = (sg_dma_address(sg) & 0xFFFFFFFF) + data_offset;
+		/* check descriptor and sg data are in same ddr slice */
+		if ((dll_desc->dma_addr >> 32) != (sg_dma_address(sg) >> 32))
+			return -EINVAL;
+#else
+		ll[i].src_addr = sg_dma_address(sg) + data_offset;
+#endif /* CONFIG_ARCH_THUNDERBAY */
 		ll[i].src_len = (sg_dma_len(sg) - data_offset) < data_size ?
 				(sg_dma_len(sg) - data_offset) : data_size;
 		data_offset = 0;
diff --git a/drivers/crypto/keembay/ocs-aes.h b/drivers/crypto/keembay/ocs-aes.h
index c035fc48b7ed..4ef4ea1ca392 100644
--- a/drivers/crypto/keembay/ocs-aes.h
+++ b/drivers/crypto/keembay/ocs-aes.h
@@ -41,6 +41,12 @@ enum ocs_instruction {
  * @irq_copy_completion:	Completion to indicate IRQ has been triggered.
  * @dma_err_mask:		Error reported by OCS DMA interrupts.
  * @engine:			Crypto engine for the device.
+ * @algs:			Address to skcipher algorithm configuration.
+ * @algs_aead:			Address to aead algorithm configuration.
+ * @algs_xlink:			Address to customized algorithm configuration for securexlink.
+ * @priority:			Crypto algorithm priority
+ * @wrapper_dev:		Address to OCS Wrapper device context
+ * @driver_name:		Identifier for OCS instance
  */
 struct ocs_aes_dev {
 	struct list_head list;
@@ -50,6 +56,12 @@ struct ocs_aes_dev {
 	struct completion irq_completion;
 	u32 dma_err_mask;
 	struct crypto_engine *engine;
+	struct skcipher_alg *algs;
+	struct aead_alg *algs_aead;
+	struct aead_alg *algs_xlink;
+	int priority;
+	struct ocs_wrapper_dev *wrapper_dev;
+	char driver_name[64];
 };
 
 /**
-- 
2.27.0

