From d8c5e323e42c8e9202863b4517e4315a955f3c8f Mon Sep 17 00:00:00 2001
From: vts <Vekatasundarakumar.T.S@intel.com>
Date: Wed, 17 Mar 2021 23:35:27 +0530
Subject: [PATCH 197/223] xlink-core:open close patch

open close patches ported from kmb

Signed-off-by: vts <Vekatasundarakumar.T.S@intel.com>
---
 drivers/misc/xlink-core/xlink-multiplexer.c | 378 ++++++++++++++------
 1 file changed, 263 insertions(+), 115 deletions(-)

diff --git a/drivers/misc/xlink-core/xlink-multiplexer.c b/drivers/misc/xlink-core/xlink-multiplexer.c
index 7f402d49786c..8d6f0e1193af 100644
--- a/drivers/misc/xlink-core/xlink-multiplexer.c
+++ b/drivers/misc/xlink-core/xlink-multiplexer.c
@@ -61,6 +61,8 @@ static const struct xlink_channel_table_entry default_channel_table[] = {
 
 struct channel {
 	struct open_channel *opchan;
+	struct open_channel *opchan_waiting_closure;
+	struct mutex lock;/*mutex lock*/
 	enum xlink_opmode mode;
 	enum xlink_channel_status status;
 	enum xlink_channel_status ipc_status;
@@ -92,6 +94,7 @@ struct open_channel {
 	s32 tx_packet_level;
 	s32 tx_up_limit;
 	struct completion opened;
+	struct completion closed;
 	struct completion pkt_available;
 	struct completion pkt_consumed;
 	struct completion pkt_released;
@@ -100,21 +103,26 @@ struct open_channel {
 	struct task_struct *consumed_calling_pid;
 	void *consumed_callback;
 	char callback_origin;
-	struct mutex lock;  // lock to protect channel config
 };
 
 struct xlink_multiplexer {
 	struct device *dev;
-	struct channel channels[XLINK_MAX_CONNECTIONS][NMB_CHANNELS];
+	struct channel *channels[XLINK_MAX_CONNECTIONS];
 };
 
 static struct xlink_multiplexer *xmux;
 
+struct remote_allocs {
+	void *virt;
+	dma_addr_t paddr;
+	struct list_head list;
+};
+
+static struct remote_allocs rmt_queue;
 /*
  * Multiplexer Internal Functions
  *
  */
-
 static enum xlink_error run_callback(struct open_channel *opchan,
 				     void *callback, struct task_struct *pid)
 {
@@ -238,16 +246,21 @@ static int is_control_channel(u16 chan)
 
 static struct open_channel *get_channel(u32 link_id, u16 chan)
 {
-	if (!xmux->channels[link_id][chan].opchan)
-		return NULL;
-	mutex_lock(&xmux->channels[link_id][chan].opchan->lock);
-	return xmux->channels[link_id][chan].opchan;
+	struct open_channel *opchan;
+
+	mutex_lock(&xmux->channels[link_id][chan].lock);
+	opchan = xmux->channels[link_id][chan].opchan;
+	if (!opchan)
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
+
+	return opchan;
 }
 
 static void release_channel(struct open_channel *opchan)
 {
-	if (opchan)
-		mutex_unlock(&opchan->lock);
+	if (opchan) {
+		mutex_unlock(&opchan->chan->lock);
+	}
 }
 
 static int add_packet_to_channel(struct open_channel *opchan,
@@ -352,10 +365,10 @@ static int multiplexer_open_channel(u32 link_id, u16 chan)
 	opchan->tx_packet_level = 0;
 	opchan->tx_up_limit = 0;
 	init_completion(&opchan->opened);
+	init_completion(&opchan->closed);
 	init_completion(&opchan->pkt_available);
 	init_completion(&opchan->pkt_consumed);
 	init_completion(&opchan->pkt_released);
-	mutex_init(&opchan->lock);
 	return X_LINK_SUCCESS;
 }
 
@@ -376,11 +389,8 @@ static int multiplexer_close_channel(struct open_channel *opchan)
 	}
 
 	// deallocate data structure and destroy
-	opchan->chan->opchan = NULL; // TODO: remove circular dependency
 	mutex_destroy(&opchan->rx_queue.lock);
 	mutex_destroy(&opchan->tx_queue.lock);
-	mutex_unlock(&opchan->lock);
-	mutex_destroy(&opchan->lock);
 	kfree(opchan);
 	return X_LINK_SUCCESS;
 }
@@ -393,14 +403,30 @@ static int multiplexer_close_channel(struct open_channel *opchan)
 enum xlink_error xlink_multiplexer_init(void *dev)
 {
 	struct platform_device *plat_dev = (struct platform_device *)dev;
-
+	int conni, chani;
 	// allocate multiplexer data structure
 	xmux = kzalloc(sizeof(*xmux), GFP_KERNEL);
 	if (!xmux)
 		return X_LINK_ERROR;
 
 	xmux->dev = &plat_dev->dev;
+
+	for (conni = 0; conni < XLINK_MAX_CONNECTIONS; conni++) {
+		xmux->channels[conni] = kcalloc(NMB_CHANNELS, sizeof(struct channel), GFP_KERNEL);
+		if (xmux->channels[conni]) {
+			for (chani = 0; chani < NMB_CHANNELS; chani++)
+				mutex_init(&xmux->channels[conni][chani].lock);
+		} else {
+			goto xlink_mx_init_error;
+		}
+	}
+
+	INIT_LIST_HEAD(&rmt_queue.list);
 	return X_LINK_SUCCESS;
+
+xlink_mx_init_error:
+	xlink_multiplexer_destroy();
+	return X_LINK_ERROR;
 }
 
 enum xlink_error xlink_multiplexer_connect(u32 link_id)
@@ -439,35 +465,54 @@ enum xlink_error xlink_multiplexer_connect(u32 link_id)
 
 enum xlink_error xlink_multiplexer_disconnect(u32 link_id)
 {
-	int i;
-	struct open_channel *opchan;
+	int i = 0;
 
 	if (!xmux)
 		return X_LINK_ERROR;
 
 	for (i = 0; i < NMB_CHANNELS; i++) {
+		mutex_lock(&xmux->channels[link_id][i].lock);
 		if (xmux->channels[link_id][i].opchan) {
-			opchan = get_channel(link_id, i);
-			if (opchan->chan->status == CHAN_OPEN_PEER) {
-				release_channel(opchan);
+			if (xmux->channels[link_id][i].status == CHAN_OPEN_PEER) {
+				pr_info("link 0x%x,chan %u in peer state & not closed", link_id, i);
+				mutex_unlock(&xmux->channels[link_id][i].lock);
 				continue;
 			}
-		multiplexer_close_channel(xmux->channels[link_id][i].opchan);
+			complete(&xmux->channels[link_id][i].opchan->pkt_available);
+			complete(&xmux->channels[link_id][i].opchan->pkt_consumed);
+			complete(&xmux->channels[link_id][i].opchan->pkt_released);
+			multiplexer_close_channel(xmux->channels[link_id][i].opchan);
 		}
+		xmux->channels[link_id][i].opchan = NULL;
+		xmux->channels[link_id][i].opchan_waiting_closure = NULL;
+		xmux->channels[link_id][i].status = CHAN_CLOSED;
+		xmux->channels[link_id][i].ipc_status = CHAN_CLOSED;
+		mutex_unlock(&xmux->channels[link_id][i].lock);
 	}
 	return X_LINK_SUCCESS;
 }
 
 enum xlink_error xlink_multiplexer_destroy(void)
 {
-	int i;
-
+	int i = 0;
+	int conni, chani;
 	if (!xmux)
 		return X_LINK_ERROR;
 
 	// close all open channels and deallocate remaining packets
-	for (i = 0; i < XLINK_MAX_CONNECTIONS; i++)
-		xlink_multiplexer_disconnect(i);
+	for (i = 0; i < XLINK_MAX_CONNECTIONS; i++) {
+		if (xmux->channels[i])
+			xlink_multiplexer_disconnect(i);
+		}
+
+	for (conni = 0; conni < XLINK_MAX_CONNECTIONS; conni++) {
+		if (xmux->channels[conni]) {
+			for (chani = 0; chani < NMB_CHANNELS; chani++)
+				mutex_destroy(&xmux->channels[conni][chani].lock);
+			kfree(xmux->channels[conni]);
+			xmux->channels[conni] = NULL;
+		}
+	}
 
 	// destroy multiplexer
 	kfree(xmux);
@@ -538,6 +583,8 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 	case XLINK_WRITE_CONTROL_REQ:
 		opchan = get_channel(link_id, chan);
 		if (!opchan || opchan->chan->status != CHAN_OPEN) {
+			pr_err("channel %u in invalid state: opchan=%p, status=%d\n",
+			       chan, opchan, opchan ? opchan->chan->status : -1);
 			rc = X_LINK_COMMUNICATION_FAIL;
 		} else {
 			event->header.timeout = opchan->chan->timeout;
@@ -547,7 +594,14 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 				    opchan->chan->mode == RXB_TXB) {
 					// channel is blocking,
 					// wait for packet to be released
+					release_channel(opchan);
 					rc = compl_wait(&opchan->pkt_released, opchan);
+					opchan = get_channel(link_id, chan);
+					if (!opchan) {
+						pr_err("link %u,chan %u invalid\n",
+						       link_id, chan);
+						return X_LINK_COMMUNICATION_FAIL;
+					}
 				} else {
 					rc = X_LINK_CHAN_FULL;
 					break;
@@ -566,9 +620,13 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 				    opchan->chan->mode == RXB_TXB) {
 					// channel is blocking,
 					// wait for packet to be consumed
-					mutex_unlock(&opchan->lock);
+					release_channel(opchan);
 					rc = compl_wait(&opchan->pkt_consumed, opchan);
-					mutex_lock(&opchan->lock);
+					opchan = get_channel(link_id, chan);
+					if (!opchan) {
+						pr_err("link %u,chan %u invalid\n", link_id, chan);
+						return X_LINK_COMMUNICATION_FAIL;
+					}
 				}
 			}
 		}
@@ -577,15 +635,21 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 	case XLINK_READ_REQ:
 		opchan = get_channel(link_id, chan);
 		if (!opchan || opchan->chan->status != CHAN_OPEN) {
+			pr_err("channel %u in invalid state: opchan=%p, status=%d\n",
+			       chan, opchan, opchan ? opchan->chan->status : -1);
 			rc = X_LINK_COMMUNICATION_FAIL;
 		} else {
 			event->header.timeout = opchan->chan->timeout;
 			if (opchan->chan->mode == RXB_TXN ||
 			    opchan->chan->mode == RXB_TXB) {
 				// channel is blocking, wait for packet to become available
-				mutex_unlock(&opchan->lock);
+				release_channel(opchan);
 				rc = compl_wait(&opchan->pkt_available, opchan);
-				mutex_lock(&opchan->lock);
+				opchan = get_channel(link_id, chan);
+					if (!opchan) {
+						pr_err("link %u,chan %u invalid\n", link_id, chan);
+						return X_LINK_COMMUNICATION_FAIL;
+					}
 			}
 			if (rc == X_LINK_SUCCESS) {
 				trace_xlink_read_pkt_tx_available(event->handle->sw_device_id,
@@ -612,15 +676,22 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 	case XLINK_READ_TO_BUFFER_REQ:
 		opchan = get_channel(link_id, chan);
 		if (!opchan || opchan->chan->status != CHAN_OPEN) {
+			pr_err("channel %u in invalid state: opchan=%p, status=%d\n",
+			       chan, opchan, opchan ? opchan->chan->status : -1);
+
 			rc = X_LINK_COMMUNICATION_FAIL;
 		} else {
 			event->header.timeout = opchan->chan->timeout;
 			if (opchan->chan->mode == RXB_TXN ||
 			    opchan->chan->mode == RXB_TXB) {
 				// channel is blocking, wait for packet to become available
-				mutex_unlock(&opchan->lock);
+				release_channel(opchan);
 				rc = compl_wait(&opchan->pkt_available, opchan);
-				mutex_lock(&opchan->lock);
+				opchan = get_channel(link_id, chan);
+				if (!opchan) {
+					pr_err("link %u,chan %u invalid\n", link_id, chan);
+					return X_LINK_COMMUNICATION_FAIL;
+				}
 			}
 			if (rc == X_LINK_SUCCESS) {
 				pkt = get_packet_from_channel(&opchan->rx_queue);
@@ -656,7 +727,30 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 		release_channel(opchan);
 		break;
 	case XLINK_OPEN_CHANNEL_REQ:
+		mutex_lock(&xmux->channels[link_id][chan].lock);
 		if (xmux->channels[link_id][chan].status == CHAN_CLOSED) {
+			if (xmux->channels[link_id][chan].opchan) {
+				//we need to wait for notification
+				////from peer of previous closure.
+				opchan = xmux->channels[link_id][chan].opchan;
+				xmux->channels[link_id][chan].opchan_waiting_closure = opchan;
+				xmux->channels[link_id][chan].opchan = NULL;
+
+				mutex_unlock(&xmux->channels[link_id][chan].lock);
+				rc = wait_for_completion_interruptible_timeout(&opchan->closed,
+									       msecs_to_jiffies
+							(OPEN_CHANNEL_TIMEOUT_MSEC / 5000));
+				if (rc <= 0)
+					pr_err("wait for prev close failed chan%u.rc=%d\n",
+					       chan, rc);
+				mutex_lock(&xmux->channels[link_id][chan].lock);
+
+				//whether our wait completed okay or not
+				//destroy the waiting for closure version.
+				multiplexer_close_channel(opchan);
+				xmux->channels[link_id][chan].opchan_waiting_closure = NULL;
+				opchan = NULL;
+			}
 			xmux->channels[link_id][chan].size = event->header.size;
 			xmux->channels[link_id][chan].timeout = event->header.timeout;
 			xmux->channels[link_id][chan].mode = (uintptr_t)event->data;
@@ -664,40 +758,44 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 			if (rc) {
 				rc = X_LINK_ERROR;
 			} else {
-				opchan = get_channel(link_id, chan);
+				opchan = xmux->channels[link_id][chan].opchan;
+
 				if (!opchan) {
 					rc = X_LINK_COMMUNICATION_FAIL;
 				} else {
 					xlink_dispatcher_event_add(EVENT_TX, event);
 					*event_queued = 1;
-					mutex_unlock(&opchan->lock);
+					mutex_unlock(&xmux->channels[link_id][chan].lock);
 					save_timeout = opchan->chan->timeout;
 					opchan->chan->timeout = OPEN_CHANNEL_TIMEOUT_MSEC;
 					rc = compl_wait(&opchan->opened, opchan);
 					opchan->chan->timeout = save_timeout;
+				if (opchan) {
 					if (rc == 0) {
 						xmux->channels[link_id][chan].status = CHAN_OPEN;
-						release_channel(opchan);
 					} else {
 						multiplexer_close_channel(opchan);
+						xmux->channels[link_id][chan].opchan = NULL;
 					}
+				} else {
+					pr_err("link %u, chan %u invalid state\n", link_id, chan);
+					rc = X_LINK_COMMUNICATION_FAIL;
+				}
 				}
 			}
 		} else if (xmux->channels[link_id][chan].status == CHAN_OPEN_PEER) {
 			/* channel already open */
+			xmux->channels[link_id][chan].status = CHAN_OPEN; // opened locally
+			xmux->channels[link_id][chan].size = event->header.size;
+			xmux->channels[link_id][chan].timeout = event->header.timeout;
+			xmux->channels[link_id][chan].mode = (uintptr_t)event->data;
 			rc = multiplexer_open_channel(link_id, chan);
-			if (rc == X_LINK_SUCCESS) {
-				struct channel *xchan = &xmux->channels[link_id][chan];
-
-				xchan->status	= CHAN_OPEN; // opened locally
-				xchan->size	= event->header.size;
-				xchan->timeout	= event->header.timeout;
-				xchan->mode	= (uintptr_t)event->data;
-			}
+			rc = X_LINK_SUCCESS;
 		} else {
 			/* channel already open */
 			rc = X_LINK_ALREADY_OPEN;
 		}
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
 		break;
 	case XLINK_DATA_READY_CALLBACK_REQ:
 		opchan = get_channel(link_id, chan);
@@ -726,19 +824,36 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 		}
 		break;
 	case XLINK_CLOSE_CHANNEL_REQ:
+		mutex_lock(&xmux->channels[link_id][chan].lock);
 		if (xmux->channels[link_id][chan].status == CHAN_OPEN) {
-			opchan = get_channel(link_id, chan);
-			if (!opchan)
-				return X_LINK_COMMUNICATION_FAIL;
-			rc = multiplexer_close_channel(opchan);
-			if (rc)
-				rc = X_LINK_ERROR;
-			else
+			opchan = xmux->channels[link_id][chan].opchan;
+			if (opchan) {
+				//if we've already received peer closure notification
+				if (try_wait_for_completion(&opchan->closed)) {
+					//we can go ahead and destroy it.
+					multiplexer_close_channel(opchan);
+					xmux->channels[link_id][chan].opchan = NULL;
+				}
 				xmux->channels[link_id][chan].status = CHAN_CLOSED;
+			} else {
+				/* can't close channel not open */
+				rc = X_LINK_ERROR;
+			}
 		} else {
 			/* can't close channel not open */
-			rc = X_LINK_ERROR;
+			rc = X_LINK_COMMUNICATION_NOT_OPEN;
 		}
+		if (!rc) {
+			//send an event that notifies of closure
+			rc = xlink_dispatcher_event_add(EVENT_TX, event);
+			if (!rc) {
+				*event_queued = 1;
+			} else {
+				pr_err("xlink_dispatcher_event_add chan %x failed.rc=%d\n",
+				       chan, rc);
+			}
+		}
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
 		break;
 	case XLINK_PING_REQ:
 		break;
@@ -754,7 +869,7 @@ enum xlink_error xlink_multiplexer_tx(struct xlink_event *event,
 	default:
 		rc = X_LINK_ERROR;
 	}
-return rc;
+	return rc;
 }
 
 enum xlink_error xlink_multiplexer_rx(struct xlink_event *event)
@@ -779,6 +894,7 @@ enum xlink_error xlink_multiplexer_rx(struct xlink_event *event)
 	case XLINK_WRITE_VOLATILE_REQ:
 		opchan = get_channel(link_id, chan);
 		if (!opchan) {
+			pr_warn("received data for channel %d in a closed state", chan);
 			// if we receive data on a closed channel - flush/read the data
 			buffer = xlink_platform_allocate(xmux->dev, &paddr,
 							 event->header.size,
@@ -859,6 +975,8 @@ enum xlink_error xlink_multiplexer_rx(struct xlink_event *event)
 					break;
 				}
 			} else {
+				pr_err("xlink_platform_allocate failed for size=%zu, on chan %u\n",
+				       event->header.size, chan);
 				// failed to allocate buffer
 				rc = X_LINK_ERROR;
 			}
@@ -929,44 +1047,52 @@ enum xlink_error xlink_multiplexer_rx(struct xlink_event *event)
 		break;
 	case XLINK_READ_REQ:
 	case XLINK_READ_TO_BUFFER_REQ:
-		opchan = get_channel(link_id, chan);
-		if (!opchan) {
-			rc = X_LINK_COMMUNICATION_FAIL;
-			break;
-		}
-		//complete regardless of mode/timeout
-		complete(&opchan->pkt_consumed);
-		trace_xlink_rx_read_consumed_arrived(event->handle->sw_device_id,
-						     event->header.chan,
-						     event->header.id,
-						     event->header.size);
-		// run callback
-		if (xmux->channels[link_id][chan].status == CHAN_OPEN &&
-		    chan_is_non_blocking_write(opchan) &&
-		    opchan->consumed_callback) {
-			rc = run_callback(opchan, opchan->consumed_callback,
-					  opchan->consumed_calling_pid);
+		mutex_lock(&xmux->channels[link_id][chan].lock);
+		if (xmux->channels[link_id][chan].status == CHAN_OPEN) {
+			opchan = xmux->channels[link_id][chan].opchan;
+			if (opchan) {
+				//complete regardless of mode/timeout
+				complete(&opchan->pkt_consumed);
+				trace_xlink_rx_read_consumed_arrived(event->handle->sw_device_id,
+								     event->header.chan,
+								     event->header.id,
+								     event->header.size);
+				// run callback
+				if (chan_is_non_blocking_write(opchan) &&
+				    opchan->consumed_callback) {
+					rc = run_callback(opchan, opchan->consumed_callback,
+							  opchan->consumed_calling_pid);
+				}
+			} else {
+				pr_err("channel %u in invalid state\n", chan);
+				rc = X_LINK_COMMUNICATION_FAIL;
+			}
 		}
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
 		if (rc == 0)
 			xlink_destroy_event(event);
-		release_channel(opchan);
 		break;
 	case XLINK_RELEASE_REQ:
-		opchan = get_channel(link_id, chan);
-		if (!opchan) {
-			rc = X_LINK_COMMUNICATION_FAIL;
-		} else {
-			event->header.timeout = opchan->chan->timeout;
-			opchan->tx_fill_level -= event->header.size;
-			opchan->tx_packet_level--;
-			//complete regardless of mode/timeout
-			complete(&opchan->pkt_released);
+		mutex_lock(&xmux->channels[link_id][chan].lock);
+		if (xmux->channels[link_id][chan].status == CHAN_OPEN) {
+			opchan = xmux->channels[link_id][chan].opchan;
+			if (opchan) {
+				event->header.timeout = opchan->chan->timeout;
+				opchan->tx_fill_level -= event->header.size;
+				opchan->tx_packet_level--;
+				//complete regardless of mode/timeout
+				complete(&opchan->pkt_released);
+			} else {
+				pr_err("channel %u in invalid state\n", chan);
+				rc = X_LINK_COMMUNICATION_FAIL;
+			}
 		}
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
 		if (rc == 0)
 			xlink_destroy_event(event);
-		release_channel(opchan);
 		break;
 	case XLINK_OPEN_CHANNEL_REQ:
+		mutex_lock(&xmux->channels[link_id][chan].lock);
 		if (xmux->channels[link_id][chan].status == CHAN_CLOSED) {
 			xmux->channels[link_id][chan].size = event->header.size;
 			xmux->channels[link_id][chan].timeout = event->header.timeout;
@@ -974,54 +1100,76 @@ enum xlink_error xlink_multiplexer_rx(struct xlink_event *event)
 			rc = multiplexer_open_channel(link_id, chan);
 			if (rc) {
 				rc = X_LINK_ERROR;
-				break;
-			}
-			opchan = get_channel(link_id, chan);
-			if (!opchan) {
-				rc = X_LINK_COMMUNICATION_FAIL;
-				break;
-			}
-			xmux->channels[link_id][chan].status = CHAN_OPEN_PEER;
-			complete(&opchan->opened);
-			passthru_event = xlink_create_event(link_id,
-							    XLINK_OPEN_CHANNEL_RESP,
-							    event->handle,
-							    chan,
-							    0,
-							    opchan->chan->timeout);
-			if (!passthru_event) {
-				rc = X_LINK_ERROR;
-				release_channel(opchan);
-				break;
+			} else {
+				opchan = xmux->channels[link_id][chan].opchan;
+				if (opchan) {
+					xmux->channels[link_id][chan].status = CHAN_OPEN_PEER;
+					complete(&opchan->opened);
+					passthru_event = xlink_create_event(link_id,
+									    XLINK_OPEN_CHANNEL_RESP,
+									    event->handle, chan, 0,
+									    opchan->chan->timeout);
+					rc = xlink_dispatcher_event_add(EVENT_RX, passthru_event);
+					if (rc)
+						pr_err("xlink_dispatch add failed chan%u\n", chan);
+				} else {
+					rc = X_LINK_COMMUNICATION_FAIL;
+				}
 			}
-			xlink_dispatcher_event_add(EVENT_RX,
-						   passthru_event);
-			release_channel(opchan);
 		} else {
 			/* channel already open */
-			opchan = get_channel(link_id, chan);
+			opchan = xmux->channels[link_id][chan].opchan;
 			if (!opchan) {
+				pr_err("channel %u in invalid state\n", chan);
 				rc = X_LINK_COMMUNICATION_FAIL;
-				break;
-			}
-			passthru_event = xlink_create_event(link_id,
-							    XLINK_OPEN_CHANNEL_RESP,
-							    event->handle,
-							    chan, 0, 0);
-			if (!passthru_event) {
-				release_channel(opchan);
-				rc = X_LINK_ERROR;
-				break;
+			} else {
+				passthru_event = xlink_create_event(link_id,
+								    XLINK_OPEN_CHANNEL_RESP,
+								    event->handle, chan, 0, 0);
+				rc = xlink_dispatcher_event_add(EVENT_RX, passthru_event);
+				if (rc)
+					pr_err("xlink_dispatcher_event_add failed chan%u\n", chan);
 			}
-			xlink_dispatcher_event_add(EVENT_RX,
-						   passthru_event);
-			release_channel(opchan);
 		}
-		rc = xlink_passthrough(event);
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
+		if (!rc)
+			rc = xlink_passthrough(event);
 		if (rc == 0)
 			xlink_destroy_event(event); // event is handled and can now be freed
 		break;
 	case XLINK_CLOSE_CHANNEL_REQ:
+		mutex_lock(&xmux->channels[link_id][chan].lock);
+		if (xmux->channels[link_id][chan].status == CHAN_CLOSED) {
+			//Locally, the channel was closed, but hasn't received
+			// notification from the peer yet (this notification).
+			if (xmux->channels[link_id][chan].opchan_waiting_closure) {
+				//in this case, we have a local thread that is in the open
+				// routine, but waiting on the closure completion. In this
+				// case, we signal completion, and just get outta here. The
+				// waiting thread will take care of destruction of opchan.
+				complete
+				(&xmux->channels[link_id][chan].opchan_waiting_closure->closed);
+			} else if (xmux->channels[link_id][chan].opchan) {
+				//in this case, locally the channel was closed, and they
+				// have yet to call xlink_open_channel for this channel again.
+				// So, we can directly destroy it.
+				multiplexer_close_channel(xmux->channels[link_id][chan].opchan);
+				xmux->channels[link_id][chan].opchan = NULL;
+			}
+		} else if (xmux->channels[link_id][chan].status == CHAN_OPEN &&
+				   xmux->channels[link_id][chan].opchan) {
+			//locally the channel hasn't been closed, so just
+			// set completion flag to indicate peer completion
+			complete(&xmux->channels[link_id][chan].opchan->closed);
+		} else {
+			pr_err("channel %u in invalid state: %d\n", chan,
+			       xmux->channels[link_id][chan].status);
+			rc = X_LINK_COMMUNICATION_FAIL;
+		}
+		mutex_unlock(&xmux->channels[link_id][chan].lock);
+		if (rc == 0)
+			xlink_destroy_event(event); // event is handled and can now be freed
+		break;
 	case XLINK_PING_REQ:
 		break;
 	case XLINK_WRITE_RESP:
-- 
2.27.0

