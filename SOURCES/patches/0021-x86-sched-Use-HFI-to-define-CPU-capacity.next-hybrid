From 28dd89cb2ef4d3c0911044efdcf1068aab229076 Mon Sep 17 00:00:00 2001
From: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date: Thu, 20 May 2021 12:43:55 -0700
Subject: [PATCH 21/76] x86/sched: Use HFI to define CPU capacity

The Intel Hardware Feedback Interface (HFI) table provides a performance
capability per logical processor. Use the performance capability of the
HFI table to set the per-CPU cpu_scale parameter. Update such capacities
whenever hardware signals (via a thermal interrupt) that the HFI table has
changed.

Set CPU capacities in two cases: when a CPU comes online or when there is
an HFI update. In the former case, retrieve the capacity from the local
copy of the HFI table. In the latter case, update the capacities of all
the CPUs in the package. Only rescale all capacities when the biggest
CPU is found in either case.

Since the HFI table defines different performance capabilities for each
logical processor, the flag SD_ASYM_CAPACITY must be set in the "MC"
scheduling domain. However, by the time we receive the first HFI update
the scheduling domains have been defined using identical capacities for
all CPUs. Thus, the mentioned flag was not set. Rebuild the scheduling
domains only at the first HFI update to reflect the fact that now
capacities are asymmetric. Subsequent capacity updates, if any, will
continue being asymmetric and the scheduling domains do not need to be
rebuilt.

Lastly, Intel has also published the specification of the Enhanced
Hardware Feedback Interface (EHFI). This interface introduces the concept
of classes of workloads. Classes of workloads have different performance
as CPUs are optimized to execute specific sets of instructions with higher
performance. Class 0 is a generic class and can be applied to the majority
of workloads. Also, Class 0 of the EHFI is simply the HFI.

Cc: Andi Kleen <ak@linux.intel.com>
Cc: Aubrey Li <aubrey.li@linux.intel.com>
Cc: Len Brown <len.brown@intel.com>
Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
Cc: Tim Chen <tim.c.chen@linux.intel.com>
Cc: "Ravi V. Shankar" <ravi.v.shankar@intel.com>
Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
---
Changes since v5:
  * Update capacities at CPU hotplug or when there is an HFI update.
  * Reduced the number of loops needed to update capacities. A full
    rescale is only done when the biggest CPU is found from either
    CPU hotplug or HFI interrupt. (Srinivas)
  * Removed dependency on topology_normalize_cpu_scale() and remove
    unneeded raw_data buffer.
  * Implemented generic functions to read the HFI table.
  * Triggered rebuild of scheduling domains from a workqueue.

Changes since v4:
  * Removed init_asymmetric_capacity() to set the SD_ASYM_CAPACITY flag.
    Instead, obtain the same result by rebuilding the scheduling domains
    only during the first HFI update. (Len)
  * Removed unnecessary pr_debug statements. (Srinivas)
  * Ignore HFI updates if performance capability is reported as zero for
    all CPUs. (Len)
  * Fix bug when setting the SD_ASYM_CAPACITY. It was previously being set
    on a single "SMT" domain, which broke find_energy_efficient_cpu().
  * Reworked update_capacity() to use EHFI class 0 when the EHFI is
    present.
  * Carve functionality to read the HFI performance data out of
    update_capacity() and put it in a separate function
    get_hfi_performance_cap().
  * Compare the HFI table timestamp against a single static global variable
    instead of per-CPU instances.

Changes since v3:
 * Removed custom capacity normalization code and instead use the existing
   topology_normalize_cpu_scale(). This requires the use of a raw_capacity
   array.
 * Removed unnecessary calls to rebuild_sched_domains(). (Srinivas)
 * Do not rebuild scheduling domains at every HFI update. Instead, enable
   asymmetric capacity at boot. (Srinivas)
 * Improved inline comments. (PeterZ)

Changes since v2:
 * Introduce this patch

Changes since v1:
 * N/A
---
 arch/x86/include/asm/hfi.h        |   1 +
 arch/x86/include/asm/topology.h   |   6 +
 drivers/thermal/intel/intel_hfi.c | 219 +++++++++++++++++++++++++++++-
 3 files changed, 225 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/hfi.h b/arch/x86/include/asm/hfi.h
index d7e4ea620a29..92983497bb55 100644
--- a/arch/x86/include/asm/hfi.h
+++ b/arch/x86/include/asm/hfi.h
@@ -29,6 +29,7 @@ void __init intel_hfi_init(void);
 int enable_hfi(unsigned int cpu);
 int disable_hfi(unsigned int cpu);
 void intel_hfi_check_event(__u64 pkg_therm_status_msr_val);
+unsigned long hfi_scale_cpu_capacity(int cpu);
 #else
 static inline void intel_hfi_init(void) { }
 static inline int enable_hfi(unsigned int cpu) { return 0; }
diff --git a/arch/x86/include/asm/topology.h b/arch/x86/include/asm/topology.h
index aa736b970119..fee816141d38 100644
--- a/arch/x86/include/asm/topology.h
+++ b/arch/x86/include/asm/topology.h
@@ -32,6 +32,8 @@
  */
 #include <linux/numa.h>
 
+#include <asm/hfi.h>
+
 #ifdef CONFIG_NUMA
 #include <linux/cpumask.h>
 
@@ -225,4 +227,8 @@ void init_freq_invariance_cppc(void);
 
 #define CPUTYPES_MAX_NR 2
 
+#ifdef CONFIG_INTEL_HFI
+#define arch_scale_cpu_capacity hfi_scale_cpu_capacity
+#endif
+
 #endif /* _ASM_X86_TOPOLOGY_H */
diff --git a/drivers/thermal/intel/intel_hfi.c b/drivers/thermal/intel/intel_hfi.c
index 1e6cce55d790..bf51089590da 100644
--- a/drivers/thermal/intel/intel_hfi.c
+++ b/drivers/thermal/intel/intel_hfi.c
@@ -18,6 +18,7 @@
 
 #define pr_fmt(fmt)  "intel-hfi: " fmt
 
+#include <linux/cpuset.h>
 #include <linux/io.h>
 #include <linux/slab.h>
 
@@ -117,6 +118,200 @@ static struct hfi_params *hfi_param_instances;
 
 static struct hfi_features hfi_features;
 static DEFINE_MUTEX(hfi_lock);
+static bool asym_capacity_initialized;
+
+/* Maximum and minimum HFI capabilities. Needed for scaling. */
+static struct hfi_cpu_data class0_max_caps;
+static struct hfi_cpu_data class0_min_caps = { U8_MAX, U8_MAX };
+
+unsigned long hfi_scale_cpu_capacity(int cpu)
+{
+	return per_cpu(cpu_scale, cpu);
+}
+
+static void get_one_hfi_cap(struct hfi_params *params, int cpu,
+			    struct hfi_cpu_data *hfi_caps)
+{
+	struct hfi_cpu_data *caps;
+	unsigned long flags;
+	s16 index;
+
+	index = per_cpu(hfi_cpu_info, cpu).index;
+	if (index < 0)
+		return;
+
+	/* Find the performance data of @cpu */
+	raw_spin_lock_irqsave(&params->hfi_event_lock, flags);
+	caps = params->data + index * hfi_features.cpu_stride;
+	memcpy(hfi_caps, caps, sizeof(*hfi_caps));
+	raw_spin_unlock_irqrestore(&params->hfi_event_lock, flags);
+}
+
+static void get_hfi_max_cap(struct hfi_params *params,
+			    struct hfi_cpu_data *max_caps)
+{
+	struct hfi_cpu_data caps;
+	int cpu;
+
+	memset(max_caps, 0, sizeof(*max_caps));
+
+	for_each_cpu(cpu, params->cpus) {
+		get_one_hfi_cap(params, cpu, &caps);
+		max_caps->perf_cap = max(max_caps->perf_cap, caps.perf_cap);
+		max_caps->ee_cap = max(max_caps->ee_cap, caps.ee_cap);
+	}
+}
+
+static void scale_capacity_cpu(u8 perf_cap, int cpu)
+{
+	unsigned long capacity;
+
+	/*
+	 * The CPU with highest performance has not been found. This can
+	 * happen if a CPU comes online before the first HFI interrupt
+	 * happens.
+	 */
+	if (!class0_max_caps.perf_cap)
+		return;
+
+	capacity = div64_u64(perf_cap << SCHED_CAPACITY_SHIFT,
+			     class0_max_caps.perf_cap);
+
+	topology_set_cpu_scale(cpu, capacity);
+}
+
+static void scale_capabilities(struct hfi_params *params)
+{
+	int cpu;
+
+	for_each_cpu(cpu, params->cpus) {
+		struct hfi_cpu_data caps;
+
+		get_one_hfi_cap(params, cpu, &caps);
+		scale_capacity_cpu(caps.perf_cap, cpu);
+	}
+}
+
+/* must  be called with hfi_lock */
+static void rescale_all_capabilities(struct hfi_cpu_data *new_max_caps)
+{
+	struct hfi_cpu_data caps;
+	int cpu;
+
+	if (new_max_caps->perf_cap > class0_max_caps.perf_cap)
+		class0_max_caps.perf_cap = new_max_caps->perf_cap;
+
+	if (new_max_caps->ee_cap > class0_max_caps.ee_cap)
+		class0_max_caps.ee_cap = new_max_caps->ee_cap;
+
+	for_each_possible_cpu(cpu) {
+		struct hfi_cpu_info *info;
+		struct hfi_params *params;
+
+		info = &per_cpu(hfi_cpu_info, cpu);
+		/*
+		 * HFI table of this CPU has not been initialized. This is fine
+		 * as this CPU is offline.
+		 */
+		if (!info->params)
+			continue;
+
+		params = info->params;
+		if (!params->initialized)
+			continue;
+
+		get_one_hfi_cap(params, cpu, &caps);
+		scale_capacity_cpu(caps.perf_cap, cpu);
+	}
+}
+
+static void init_asym_capacity(void)
+{
+	mutex_lock(&hfi_lock);
+	if (class0_max_caps.perf_cap == class0_min_caps.perf_cap)
+		goto out_unlock;
+	/*
+	 * Using the HFI means that not all CPUs will have the same capacity.
+	 * Thus,  the "MC" scheduling domain should have the SD_ASYM_CPUCAPACITY
+	 * flag set. Given that by the time we have the first HFI update the
+	 * scheduling domains have been built. Flags of the scheduling domains
+	 * are evaluated when rebuilding them. However, it is not necessary to
+	 * rebuild them every time there is a change in CPU capacities, as they
+	 * will continue to be asymmetric. Thus, rebuild capacity domains only
+	 * in the first HFI update.
+	 *
+	 * We are sure that SD_ASYM_CPUCAPACITY will be set only in the "MC"
+	 * domain and not in the "SMT" domain since all SMT siblings have the
+	 * same capacity.
+	 */
+
+	if (asym_capacity_initialized)
+		goto out_unlock;
+
+	asym_capacity_initialized = true;
+	x86_topology_update = true;
+	mutex_unlock(&hfi_lock);
+	rebuild_sched_domains();
+	return;
+
+out_unlock:
+	mutex_unlock(&hfi_lock);
+}
+
+/* The work item is needed to avoid CPU hotplug locking issues */
+static void hfi_asym_capacity_work_fn(struct work_struct *work)
+{
+	init_asym_capacity();
+}
+static DECLARE_WORK(hfi_asym_capacity_work, hfi_asym_capacity_work_fn);
+
+/*
+ * Call update_capabilities() when there are changes in the HFI table.
+ */
+static void update_capabilities(struct hfi_params *params)
+{
+	struct hfi_hdr *hdr = params->hdr;
+	struct hfi_cpu_data max_caps;
+
+	mutex_lock(&hfi_lock);
+	/*
+	 * Check if a new CPU with higher capacity came online
+	 * or became more performant after a thermal interrupt. If no more
+	 * performant CPU was found, simply keep the performance of the
+	 * most performant CPU. It may come online later in the future and
+	 * we can avoid expensive full capacity re-scaling.
+	 */
+	get_hfi_max_cap(params, &max_caps);
+
+	if (max_caps.perf_cap > class0_max_caps.perf_cap ||
+	    max_caps.ee_cap > class0_max_caps.ee_cap) {
+		/*
+		 * Acknowledge updates to all classes. We will do a full
+		 * capacity rescale.
+		 */
+		hdr->perf_updated = 0;
+		hdr->ee_updated = 0;
+
+		rescale_all_capabilities(&max_caps);
+		goto out;
+	}
+
+	if (!hdr->perf_updated && !hdr->ee_updated)
+		goto out;
+
+	hdr->perf_updated = 0;
+	hdr->ee_updated = 0;
+
+	/* If the HFI table has all zeros, ignore this update. */
+	if (!max_caps.perf_cap && !max_caps.ee_cap)
+		goto out;
+
+	scale_capabilities(params);
+
+out:
+	mutex_unlock(&hfi_lock);
+	init_asym_capacity();
+}
 
 static void hfi_update_work_fn(struct work_struct *work)
 {
@@ -143,7 +338,7 @@ static void hfi_update_work_fn(struct work_struct *work)
 
 	params->timestamp = *params->ts_counter;
 
-	/* TODO: Update capacity here. */
+	update_capabilities(params);
 
 	raw_spin_lock_irqsave(&params->hfi_event_lock, flags);
 	params->processing_update = false;
@@ -267,6 +462,8 @@ int enable_hfi(unsigned int cpu)
 	 * cpumask of this HFI instance.
 	 */
 	if (params->initialized) {
+		struct hfi_cpu_data caps;
+
 		mutex_lock(&hfi_lock);
 
 		/*
@@ -321,6 +518,26 @@ int enable_hfi(unsigned int cpu)
 			return 0;
 		}
 
+		/*
+		 * There was not a pending interrupt. Update capabilities of
+		 * @cpu.
+		 */
+		mutex_lock(&hfi_lock);
+		get_one_hfi_cap(params, cpu, &caps);
+		class0_min_caps.perf_cap = min(class0_min_caps.perf_cap,
+					       caps.perf_cap);
+
+		/* If we found the biggest CPU, rescale all capacities */
+		if (caps.perf_cap > class0_max_caps.perf_cap ||
+		    caps.ee_cap > class0_max_caps.ee_cap)
+			rescale_all_capabilities(&caps);
+		else
+			scale_capacity_cpu(caps.perf_cap, cpu);
+
+		schedule_work(&hfi_asym_capacity_work);
+		mutex_unlock(&hfi_lock);
+
+		return 0;
 	}
 
 	/* The HFI has not been initialized for @cpu. Initialize it. */
-- 
2.27.0

