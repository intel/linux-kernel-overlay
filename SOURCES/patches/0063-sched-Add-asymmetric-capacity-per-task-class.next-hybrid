From 4947e7ebac5a80dbeafa0a7628208278853e52ad Mon Sep 17 00:00:00 2001
From: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date: Sun, 1 Nov 2020 13:48:44 -0800
Subject: [PATCH 63/72] sched: Add asymmetric capacity per task class

On asymmetric computer topologies, performance differences are not
only among CPUs but also among types of tasks. That is, a task of
type A may have a bigger bost of performance by running on a the
same CPU as a task of type B.

Add infrastructure to detect and enable such asymmetric capacity.

Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
---
 include/linux/sched/topology.h |  8 ++++++++
 kernel/sched/sched.h           |  1 +
 kernel/sched/topology.c        | 10 ++++++++--
 3 files changed, 17 insertions(+), 2 deletions(-)

diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index c31ba803a278..aa47aa1c1bd9 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -275,6 +275,14 @@ bool arch_has_hw_sched_feedback(void)
 }
 #endif
 
+#ifndef arch_capacity_max_classes
+static __always_inline
+unsigned int arch_capacity_max_classes(void)
+{
+	return 1;
+}
+#endif
+
 static inline int task_node(const struct task_struct *p)
 {
 	return cpu_to_node(task_cpu(p));
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4f552bb44060..b427992546f4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1496,6 +1496,7 @@ DECLARE_PER_CPU(struct sched_domain __rcu *, sd_numa);
 DECLARE_PER_CPU(struct sched_domain __rcu *, sd_asym_packing);
 DECLARE_PER_CPU(struct sched_domain __rcu *, sd_asym_cpucapacity);
 extern struct static_key_false sched_asym_cpucapacity;
+DECLARE_STATIC_KEY_FALSE(sched_asym_classcpucapacity);
 
 struct sched_group_capacity {
 	atomic_t		ref;
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 5fd7449f33c3..1c95648dbe35 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -652,6 +652,7 @@ DEFINE_PER_CPU(struct sched_domain __rcu *, sd_numa);
 DEFINE_PER_CPU(struct sched_domain __rcu *, sd_asym_packing);
 DEFINE_PER_CPU(struct sched_domain __rcu *, sd_asym_cpucapacity);
 DEFINE_STATIC_KEY_FALSE(sched_asym_cpucapacity);
+DEFINE_STATIC_KEY_FALSE(sched_asym_classcpucapacity);
 
 static void update_top_cache_domain(int cpu)
 {
@@ -2146,8 +2147,11 @@ build_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *att
 	}
 	rcu_read_unlock();
 
-	if (has_asym)
+	if (has_asym) {
 		static_branch_inc_cpuslocked(&sched_asym_cpucapacity);
+		if (arch_capacity_max_classes() > 1)
+			static_branch_enable_cpuslocked(&sched_asym_classcpucapacity);
+	}
 
 	if (rq && sched_debug_verbose) {
 		pr_info("root domain span: %*pbl (max cpu_capacity = %lu)\n",
@@ -2244,8 +2248,10 @@ static void detach_destroy_domains(const struct cpumask *cpu_map)
 	unsigned int cpu = cpumask_any(cpu_map);
 	int i;
 
-	if (rcu_access_pointer(per_cpu(sd_asym_cpucapacity, cpu)))
+	if (rcu_access_pointer(per_cpu(sd_asym_cpucapacity, cpu))) {
 		static_branch_dec_cpuslocked(&sched_asym_cpucapacity);
+		static_branch_disable_cpuslocked(&sched_asym_classcpucapacity);
+	}
 
 	rcu_read_lock();
 	for_each_cpu(i, cpu_map)
-- 
2.27.0

