From 44dcafc04f7b732d694fc76e889fbb270bf45490 Mon Sep 17 00:00:00 2001
From: Srikanth Thokala <srikanth.thokala@intel.com>
Date: Wed, 2 Sep 2020 02:59:00 +0530
Subject: [PATCH 149/223] Change API names

Signed-off-by: Srikanth Thokala <srikanth.thokala@intel.com>
---
 drivers/misc/xlink-pcie/common/boot.h         |  56 +-
 drivers/misc/xlink-pcie/common/capabilities.h |  84 +--
 drivers/misc/xlink-pcie/common/common.h       | 108 ++--
 drivers/misc/xlink-pcie/common/core.h         |  24 +-
 drivers/misc/xlink-pcie/common/util.c         | 225 +++----
 drivers/misc/xlink-pcie/common/util.h         |  77 +--
 .../common/{xlink_pcie.h => xpcie.h}          |  68 +--
 drivers/misc/xlink-pcie/local_host/core.c     | 564 +++++++++---------
 drivers/misc/xlink-pcie/local_host/dma.c      | 183 +++---
 drivers/misc/xlink-pcie/local_host/dma.h      |  18 +-
 drivers/misc/xlink-pcie/local_host/epf.c      | 282 ++++-----
 drivers/misc/xlink-pcie/local_host/epf.h      |  22 +-
 drivers/misc/xlink-pcie/local_host/if.c       |  40 +-
 drivers/misc/xlink-pcie/local_host/struct.h   |  29 +-
 drivers/misc/xlink-pcie/remote_host/core.c    | 428 ++++++-------
 drivers/misc/xlink-pcie/remote_host/if.c      |  27 +-
 drivers/misc/xlink-pcie/remote_host/main.c    |  51 +-
 drivers/misc/xlink-pcie/remote_host/pci.c     | 462 +++++++-------
 drivers/misc/xlink-pcie/remote_host/pci.h     |  64 +-
 19 files changed, 1447 insertions(+), 1365 deletions(-)
 rename drivers/misc/xlink-pcie/common/{xlink_pcie.h => xpcie.h} (59%)

diff --git a/drivers/misc/xlink-pcie/common/boot.h b/drivers/misc/xlink-pcie/common/boot.h
index 5f396d7ea65a..6d81ec484fe9 100644
--- a/drivers/misc/xlink-pcie/common/boot.h
+++ b/drivers/misc/xlink-pcie/common/boot.h
@@ -7,19 +7,19 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_BOOT_HEADER_
-#define MXLK_BOOT_HEADER_
+#ifndef XPCIE_BOOT_HEADER_
+#define XPCIE_BOOT_HEADER_
 
 #include <linux/types.h>
 
-#define MXLK_BOOT_MAGIC_ROM "VPUROM"
-#define MXLK_BOOT_MAGIC_EMMC "VPUEMMC"
-#define MXLK_BOOT_MAGIC_BL2 "VPUBL2"
-#define MXLK_BOOT_MAGIC_UBOOT "VPUUBOOT"
-#define MXLK_BOOT_MAGIC_RECOV "VPURECOV"
-#define MXLK_BOOT_MAGIC_YOCTO "VPUYOCTO"
+#define XPCIE_BOOT_MAGIC_ROM "VPUROM"
+#define XPCIE_BOOT_MAGIC_EMMC "VPUEMMC"
+#define XPCIE_BOOT_MAGIC_BL2 "VPUBL2"
+#define XPCIE_BOOT_MAGIC_UBOOT "VPUUBOOT"
+#define XPCIE_BOOT_MAGIC_RECOV "VPURECOV"
+#define XPCIE_BOOT_MAGIC_YOCTO "VPUYOCTO"
 
-enum mxlk_stage {
+enum xpcie_stage {
 	STAGE_UNINIT,
 	STAGE_ROM,
 	STAGE_BL2,
@@ -28,27 +28,27 @@ enum mxlk_stage {
 	STAGE_OS
 };
 
-#define MXLK_BOOT_FIP_ID (0xFFFFFFFF)
-#define MXLK_BOOT_BOOT_ID (0xFFFFFF4F)
-#define MXLK_BOOT_SYSTEM_ID (0xFFFFFF46)
-#define MXLK_BOOT_RAW_ID (0xFFFFFF00)
-#define MXLK_BOOT_ERASE_ID (0xFFFFFF01)
-#define MXLK_BOOT_FLASH_ID (0xFFFFFF02)
+#define XPCIE_BOOT_FIP_ID (0xFFFFFFFF)
+#define XPCIE_BOOT_BOOT_ID (0xFFFFFF4F)
+#define XPCIE_BOOT_SYSTEM_ID (0xFFFFFF46)
+#define XPCIE_BOOT_RAW_ID (0xFFFFFF00)
+#define XPCIE_BOOT_ERASE_ID (0xFFFFFF01)
+#define XPCIE_BOOT_FLASH_ID (0xFFFFFF02)
 
-#define MXLK_BOOT_STATUS_START (0x55555555)
-#define MXLK_BOOT_STATUS_INVALID (0xDEADFFFF)
-#define MXLK_BOOT_STATUS_DOWNLOADED (0xDDDDDDDD)
-#define MXLK_BOOT_STATUS_ERROR (0xDEADAAAA)
-#define MXLK_BOOT_STATUS_DONE (0xBBBBBBBB)
+#define XPCIE_BOOT_STATUS_START (0x55555555)
+#define XPCIE_BOOT_STATUS_INVALID (0xDEADFFFF)
+#define XPCIE_BOOT_STATUS_DOWNLOADED (0xDDDDDDDD)
+#define XPCIE_BOOT_STATUS_ERROR (0xDEADAAAA)
+#define XPCIE_BOOT_STATUS_DONE (0xBBBBBBBB)
 
-#define MXLK_INT_ENABLE (0x1)
-#define MXLK_INT_MASK (0x1)
+#define XPCIE_INT_ENABLE (0x1)
+#define XPCIE_INT_MASK (0x1)
 
-#define MXLK_BOOT_MAGIC_STRLEN (16)
-#define MXLK_BOOT_DEST_STRLEN (128)
+#define XPCIE_BOOT_MAGIC_STRLEN (16)
+#define XPCIE_BOOT_DEST_STRLEN (128)
 
-struct mxlk_bootio {
-	u8 magic[MXLK_BOOT_MAGIC_STRLEN];
+struct xpcie_bootio {
+	u8 magic[XPCIE_BOOT_MAGIC_STRLEN];
 	u32 mf_ready;
 	u32 mf_len;
 	u64 reserved1;
@@ -58,8 +58,8 @@ struct mxlk_bootio {
 	u32 int_identity;
 	u32 reserved2;
 	u64 mf_offset;
-	u8 mf_dest[MXLK_BOOT_DEST_STRLEN];
+	u8 mf_dest[XPCIE_BOOT_DEST_STRLEN];
 	u64 dev_id;
 } __packed;
 
-#endif // MXLK_BOOT_HEADER_
+#endif // XPCIE_BOOT_HEADER_
diff --git a/drivers/misc/xlink-pcie/common/capabilities.h b/drivers/misc/xlink-pcie/common/capabilities.h
index 737e9e550af4..81e25bbc4f43 100644
--- a/drivers/misc/xlink-pcie/common/capabilities.h
+++ b/drivers/misc/xlink-pcie/common/capabilities.h
@@ -7,35 +7,35 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_CAPABILITIES_HEADER_
-#define MXLK_CAPABILITIES_HEADER_
+#ifndef XPCIE_CAPABILITIES_HEADER_
+#define XPCIE_CAPABILITIES_HEADER_
 
-#include "xlink_pcie.h"
+#include "xpcie.h"
 #include "common.h"
 
-#define MXLK_CAP_TTL (32)
+#define XPCIE_CAP_TTL (32)
 
 static inline
-void *mxlk_cap_find(struct mxlk *mxlk, u32 start, u16 id)
+void *intel_xpcie_cap_find(struct xpcie *xpcie, u32 start, u16 id)
 {
-	int ttl = MXLK_CAP_TTL;
-	struct mxlk_cap_hdr *hdr;
-	struct mxlk_cap_hdr cur_hdr;
+	int ttl = XPCIE_CAP_TTL;
+	struct xpcie_cap_hdr *hdr;
+	struct xpcie_cap_hdr cur_hdr;
 
 	// If user didn't specify start, assume start of mmio
 	if (!start)
-		start = mxlk_ioread32(&mxlk->mmio->cap_offset);
+		start = intel_xpcie_ioread32(&xpcie->mmio->cap_offset);
 
 	// Read header info
 #ifdef XLINK_PCIE_REMOTE
-	hdr = (struct mxlk_cap_hdr *)((void __iomem *)mxlk->mmio + start);
+	hdr = (struct xpcie_cap_hdr *)((void __iomem *)xpcie->mmio + start);
 #else
-	hdr = (struct mxlk_cap_hdr *)((void *)mxlk->mmio + start);
+	hdr = (struct xpcie_cap_hdr *)((void *)xpcie->mmio + start);
 #endif
 	// Check if we still have time to live
 	while (ttl--) {
 #ifdef XLINK_PCIE_REMOTE
-		memcpy_fromio(&cur_hdr, hdr, sizeof(struct mxlk_cap_hdr));
+		memcpy_fromio(&cur_hdr, hdr, sizeof(struct xpcie_cap_hdr));
 #else
 		cur_hdr = *hdr;
 #endif
@@ -43,16 +43,16 @@ void *mxlk_cap_find(struct mxlk *mxlk, u32 start, u16 id)
 		if (cur_hdr.id == id)
 			return hdr;
 		// If cap is NULL, we are at the end of the list
-		else if (cur_hdr.id == MXLK_CAP_NULL)
+		else if (cur_hdr.id == XPCIE_CAP_NULL)
 			return NULL;
 		// If no match and no end of list, traverse the linked list
 		else
 #ifdef XLINK_PCIE_REMOTE
-			hdr = (struct mxlk_cap_hdr *)
-				((void __iomem *)mxlk->mmio + cur_hdr.next);
+			hdr = (struct xpcie_cap_hdr *)
+				((void __iomem *)xpcie->mmio + cur_hdr.next);
 #else
-			hdr = (struct mxlk_cap_hdr *)
-				((void *)mxlk->mmio + cur_hdr.next);
+			hdr = (struct xpcie_cap_hdr *)
+				((void *)xpcie->mmio + cur_hdr.next);
 #endif
 	}
 
@@ -61,74 +61,74 @@ void *mxlk_cap_find(struct mxlk *mxlk, u32 start, u16 id)
 }
 
 static inline
-void mxlk_set_td_address(struct mxlk_transfer_desc *td, u64 address)
+void intel_xpcie_set_td_address(struct xpcie_transfer_desc *td, u64 address)
 {
-	mxlk_iowrite64(address, &td->address);
+	intel_xpcie_iowrite64(address, &td->address);
 }
 
 static inline
-u64 mxlk_get_td_address(struct mxlk_transfer_desc *td)
+u64 intel_xpcie_get_td_address(struct xpcie_transfer_desc *td)
 {
-	return mxlk_ioread64(&td->address);
+	return intel_xpcie_ioread64(&td->address);
 }
 
 static inline
-void mxlk_set_td_length(struct mxlk_transfer_desc *td, u32 length)
+void intel_xpcie_set_td_length(struct xpcie_transfer_desc *td, u32 length)
 {
-	mxlk_iowrite32(length, &td->length);
+	intel_xpcie_iowrite32(length, &td->length);
 }
 
 static inline
-u32 mxlk_get_td_length(struct mxlk_transfer_desc *td)
+u32 intel_xpcie_get_td_length(struct xpcie_transfer_desc *td)
 {
-	return mxlk_ioread32(&td->length);
+	return intel_xpcie_ioread32(&td->length);
 }
 
 static inline
-void mxlk_set_td_interface(struct mxlk_transfer_desc *td, u16 interface)
+void intel_xpcie_set_td_interface(struct xpcie_transfer_desc *td, u16 interface)
 {
-	mxlk_iowrite16(interface, &td->interface);
+	intel_xpcie_iowrite16(interface, &td->interface);
 }
 
 static inline
-u16 mxlk_get_td_interface(struct mxlk_transfer_desc *td)
+u16 intel_xpcie_get_td_interface(struct xpcie_transfer_desc *td)
 {
-	return mxlk_ioread16(&td->interface);
+	return intel_xpcie_ioread16(&td->interface);
 }
 
 static inline
-void mxlk_set_td_status(struct mxlk_transfer_desc *td, u16 status)
+void intel_xpcie_set_td_status(struct xpcie_transfer_desc *td, u16 status)
 {
-	mxlk_iowrite16(status, &td->status);
+	intel_xpcie_iowrite16(status, &td->status);
 }
 
 static inline
-u16 mxlk_get_td_status(struct mxlk_transfer_desc *td)
+u16 intel_xpcie_get_td_status(struct xpcie_transfer_desc *td)
 {
-	return mxlk_ioread16(&td->status);
+	return intel_xpcie_ioread16(&td->status);
 }
 
 static inline
-void mxlk_set_tdr_head(struct mxlk_pipe *p, u32 head)
+void intel_xpcie_set_tdr_head(struct xpcie_pipe *p, u32 head)
 {
-	mxlk_iowrite32(head, p->head);
+	intel_xpcie_iowrite32(head, p->head);
 }
 
 static inline
-u32 mxlk_get_tdr_head(struct mxlk_pipe *p)
+u32 intel_xpcie_get_tdr_head(struct xpcie_pipe *p)
 {
-	return mxlk_ioread32(p->head);
+	return intel_xpcie_ioread32(p->head);
 }
 
 static inline
-void mxlk_set_tdr_tail(struct mxlk_pipe *p, u32 tail)
+void intel_xpcie_set_tdr_tail(struct xpcie_pipe *p, u32 tail)
 {
-	mxlk_iowrite32(tail, p->tail);
+	intel_xpcie_iowrite32(tail, p->tail);
 }
 
 static inline
-u32 mxlk_get_tdr_tail(struct mxlk_pipe *p)
+u32 intel_xpcie_get_tdr_tail(struct xpcie_pipe *p)
 {
-	return mxlk_ioread32(p->tail);
+	return intel_xpcie_ioread32(p->tail);
 }
-#endif // MXLK_CAPABILITIES_HEADER_
+#endif // XPCIE_CAPABILITIES_HEADER_
diff --git a/drivers/misc/xlink-pcie/common/common.h b/drivers/misc/xlink-pcie/common/common.h
index eb068fe1a88a..774f4d391414 100644
--- a/drivers/misc/xlink-pcie/common/common.h
+++ b/drivers/misc/xlink-pcie/common/common.h
@@ -7,8 +7,8 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_COMMON_HEADER_
-#define MXLK_COMMON_HEADER_
+#ifndef XPCIE_COMMON_HEADER_
+#define XPCIE_COMMON_HEADER_
 
 #include <linux/io.h>
 #include <linux/types.h>
@@ -22,40 +22,40 @@
 /*
  * Number of interfaces to statically allocate resources for
  */
-#define MXLK_NUM_INTERFACES (1)
+#define XPCIE_NUM_INTERFACES (1)
 
-#define MXLK_FRAGMENT_SIZE SZ_128K
-#define MXLK_NUM_TX_DESCS (64)
-#define MXLK_NUM_RX_DESCS (64)
+#define XPCIE_FRAGMENT_SIZE SZ_128K
+#define XPCIE_NUM_TX_DESCS (64)
+#define XPCIE_NUM_RX_DESCS (64)
 
 /*
  * Status encoding of the transfer descriptors
  */
-#define MXLK_DESC_STATUS_SUCCESS (0)
-#define MXLK_DESC_STATUS_ERROR (0xFFFF)
+#define XPCIE_DESC_STATUS_SUCCESS (0)
+#define XPCIE_DESC_STATUS_ERROR (0xFFFF)
 
 /*
  * Layout transfer descriptors used by device and host
  */
-struct mxlk_transfer_desc {
+struct xpcie_transfer_desc {
 	uint64_t address;
 	uint32_t length;
 	uint16_t status;
 	uint16_t interface;
 } __packed;
 
-#define MXLK_IO_COMM_SIZE SZ_16K
-#define MXLK_MMIO_OFFSET SZ_4K
+#define XPCIE_IO_COMM_SIZE SZ_16K
+#define XPCIE_MMIO_OFFSET SZ_4K
 
-#define MXLK_VERSION_MAJOR 0
-#define MXLK_VERSION_MINOR 5
-#define MXLK_VERSION_BUILD 0
+#define XPCIE_VERSION_MAJOR 0
+#define XPCIE_VERSION_MINOR 5
+#define XPCIE_VERSION_BUILD 0
 #define _TOSTR(X) #X
 #define _VERSION(A, B, C) _TOSTR(A) "." _TOSTR(B) "." _TOSTR(C)
-#define MXLK_DRIVER_VERSION \
-	_VERSION(MXLK_VERSION_MAJOR, MXLK_VERSION_MINOR, MXLK_VERSION_BUILD)
+#define XPCIE_DRIVER_VERSION \
+	_VERSION(XPCIE_VERSION_MAJOR, XPCIE_VERSION_MINOR, XPCIE_VERSION_BUILD)
 
-struct mxlk_version {
+struct xpcie_version {
 	uint8_t major;
 	uint8_t minor;
 	uint16_t build;
@@ -64,21 +64,21 @@ struct mxlk_version {
 /*
  * Status encoding of both device and host
  */
-#define MXLK_STATUS_ERROR (0xFFFFFFFF)
-#define MXLK_STATUS_UNINIT (0)
-#define MXLK_STATUS_BOOT_FW (1)
-#define MXLK_STATUS_BOOT_OS (2)
-#define MXLK_STATUS_READY (3)
-#define MXLK_STATUS_RECOVERY (4)
-#define MXLK_STATUS_RUN (5)
-#define MXLK_STATUS_OFF (6)
-#define MXLK_STATUS_BOOT_PRE_OS (7)
+#define XPCIE_STATUS_ERROR (0xFFFFFFFF)
+#define XPCIE_STATUS_UNINIT (0)
+#define XPCIE_STATUS_BOOT_FW (1)
+#define XPCIE_STATUS_BOOT_OS (2)
+#define XPCIE_STATUS_READY (3)
+#define XPCIE_STATUS_RECOVERY (4)
+#define XPCIE_STATUS_RUN (5)
+#define XPCIE_STATUS_OFF (6)
+#define XPCIE_STATUS_BOOT_PRE_OS (7)
 
 /*
  * MMIO layout and offsets shared between device and host
  */
-struct mxlk_mmio {
-	struct mxlk_version version;
+struct xpcie_mmio {
+	struct xpcie_version version;
 	uint32_t device_status;
 	uint32_t host_status;
 	uint8_t legacy_a0;
@@ -95,21 +95,21 @@ struct mxlk_mmio {
 /*
  * Defined capabilities located in mmio space
  */
-#define MXLK_CAP_NULL (0)
-#define MXLK_CAP_TXRX (1)
+#define XPCIE_CAP_NULL (0)
+#define XPCIE_CAP_TXRX (1)
 
 /*
  * Header at the beginning of each capability to define and link to next
  */
-struct mxlk_cap_hdr {
+struct xpcie_cap_hdr {
 	uint16_t id;
 	uint16_t next;
 } __packed;
 
-#define MXLK_CAP_HDR_ID (offsetof(struct mxlk_cap_hdr, id))
-#define MXLK_CAP_HDR_NEXT (offsetof(struct mxlk_cap_hdr, next))
+#define XPCIE_CAP_HDR_ID (offsetof(struct xpcie_cap_hdr, id))
+#define XPCIE_CAP_HDR_NEXT (offsetof(struct xpcie_cap_hdr, next))
 
-struct mxlk_cap_pipe {
+struct xpcie_cap_pipe {
 	uint32_t ring;
 	uint32_t ndesc;
 	uint32_t head;
@@ -119,11 +119,11 @@ struct mxlk_cap_pipe {
 /*
  * Transmit and Receive capability
  */
-struct mxlk_cap_txrx {
-	struct mxlk_cap_hdr hdr;
+struct xpcie_cap_txrx {
+	struct xpcie_cap_hdr hdr;
 	u32 fragment_size;
-	struct mxlk_cap_pipe tx;
-	struct mxlk_cap_pipe rx;
+	struct xpcie_cap_pipe tx;
+	struct xpcie_cap_pipe rx;
 } __packed;
 
 static inline u64 _ioread64(void __iomem *addr)
@@ -144,25 +144,25 @@ static inline void _iowrite64(u64 value, void __iomem *addr)
 
 #ifdef XLINK_PCIE_REMOTE
 
-#define mxlk_iowrite64 _iowrite64
-#define mxlk_iowrite32 iowrite32
-#define mxlk_iowrite16 iowrite16
-#define mxlk_iowrite8 iowrite8
-#define mxlk_ioread64 _ioread64
-#define mxlk_ioread32 ioread32
-#define mxlk_ioread16 ioread16
-#define mxlk_ioread8 ioread8
+#define intel_xpcie_iowrite64 _iowrite64
+#define intel_xpcie_iowrite32 iowrite32
+#define intel_xpcie_iowrite16 iowrite16
+#define intel_xpcie_iowrite8 iowrite8
+#define intel_xpcie_ioread64 _ioread64
+#define intel_xpcie_ioread32 ioread32
+#define intel_xpcie_ioread16 ioread16
+#define intel_xpcie_ioread8 ioread8
 
 #else
 
-#define mxlk_iowrite64(value, addr)	{ *(addr) = value; }
-#define mxlk_iowrite32(value, addr)	{ *(addr) = value; }
-#define mxlk_iowrite16(value, addr)	{ *(addr) = value; }
-#define mxlk_iowrite8(value, addr)	{ *(addr) = value; }
-#define mxlk_ioread64(addr) (*(addr))
-#define mxlk_ioread32(addr) (*(addr))
-#define mxlk_ioread16(addr) (*(addr))
-#define mxlk_ioread8(addr)  (*(addr))
+#define intel_xpcie_iowrite64(value, addr)	{ *(addr) = value; }
+#define intel_xpcie_iowrite32(value, addr)	{ *(addr) = value; }
+#define intel_xpcie_iowrite16(value, addr)	{ *(addr) = value; }
+#define intel_xpcie_iowrite8(value, addr)	{ *(addr) = value; }
+#define intel_xpcie_ioread64(addr) (*(addr))
+#define intel_xpcie_ioread32(addr) (*(addr))
+#define intel_xpcie_ioread16(addr) (*(addr))
+#define intel_xpcie_ioread8(addr)  (*(addr))
 
 #endif // XLINK_PCIE_REMOTE
 
diff --git a/drivers/misc/xlink-pcie/common/core.h b/drivers/misc/xlink-pcie/common/core.h
index 19f1f0d192fd..2f375edde465 100644
--- a/drivers/misc/xlink-pcie/common/core.h
+++ b/drivers/misc/xlink-pcie/common/core.h
@@ -7,18 +7,18 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_CORE_HEADER_
-#define MXLK_CORE_HEADER_
+#ifndef XPCIE_CORE_HEADER_
+#define XPCIE_CORE_HEADER_
 
-#include "xlink_pcie.h"
+#include "xpcie.h"
 
-int mxlk_core_init(struct mxlk *mxlk);
-void mxlk_core_cleanup(struct mxlk *mxlk);
+int intel_xpcie_core_init(struct xpcie *xpcie);
+void intel_xpcie_core_cleanup(struct xpcie *xpcie);
 
 /*
- * @brief Read buffer from mxlk. Function will block when no data.
+ * @brief Read buffer from xpcie. Function will block when no data.
  *
- * @param[in] mxlk          - pointer to mxlk instance
+ * @param[in] xpcie          - pointer to xpcie instance
  * @param[in] buffer        - pointer to buffer
  * @param[in] length        - max bytes to copy into buffer
  * @param[in] timeout_ms    - timeout in ms for blocking when no data
@@ -29,13 +29,13 @@ void mxlk_core_cleanup(struct mxlk *mxlk);
  *              -ETIME - timeout
  *              -EINTR - interrupted
  */
-int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
+int intel_xpcie_core_read(struct xpcie *xpcie, void *buffer, size_t *length,
 		   uint32_t timeout_ms);
 
 /*
- * @brief Writes buffer to mxlk. Function will block when no buffer.
+ * @brief Writes buffer to xpcie. Function will block when no buffer.
  *
- * @param[in] mxlk          - pointer to mxlk instance
+ * @param[in] xpcie          - pointer to xpcie instance
  * @param[in] buffer        - pointer to buffer
  * @param[in] length        - length of buffer to copy from
  * @param[in] timeout_ms    - timeout in ms for blocking when no buffer
@@ -46,11 +46,11 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
  *              -ETIME - timeout
  *              -EINTR - interrupted
  */
-int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
+int intel_xpcie_core_write(struct xpcie *xpcie, void *buffer, size_t *length,
 		    uint32_t timeout_ms);
 
 #ifdef XLINK_PCIE_LOCAL
-struct mxlk *mxlk_core_get_by_id(uint32_t sw_device_id);
+struct xpcie *intel_xpcie_core_get_by_id(uint32_t sw_device_id);
 #endif
 
 #endif
diff --git a/drivers/misc/xlink-pcie/common/util.c b/drivers/misc/xlink-pcie/common/util.c
index e29ff0d326bd..a638c08cf94e 100644
--- a/drivers/misc/xlink-pcie/common/util.c
+++ b/drivers/misc/xlink-pcie/common/util.c
@@ -9,63 +9,67 @@
 
 #include "util.h"
 
-void mxlk_set_device_status(struct mxlk *mxlk, u32 status)
+void intel_xpcie_set_device_status(struct xpcie *xpcie, u32 status)
 {
-	mxlk->status = status;
-	mxlk_iowrite32(status, &mxlk->mmio->device_status);
+	xpcie->status = status;
+	intel_xpcie_iowrite32(status, &xpcie->mmio->device_status);
 }
 
-u32 mxlk_get_device_status(struct mxlk *mxlk)
+u32 intel_xpcie_get_device_status(struct xpcie *xpcie)
 {
-	return mxlk_ioread32(&mxlk->mmio->device_status);
+	return intel_xpcie_ioread32(&xpcie->mmio->device_status);
 }
 
-static u8 *mxlk_doorbell_offset(struct mxlk *mxlk,
-				enum mxlk_doorbell_direction dirt,
-				enum mxlk_doorbell_type type)
+static u8 *intel_xpcie_doorbell_offset(struct xpcie *xpcie,
+				enum xpcie_doorbell_direction dirt,
+				enum xpcie_doorbell_type type)
 {
 	if (dirt == TO_DEVICE && type == DATA_SENT)
-		return &mxlk->mmio->htod_tx_doorbell;
+		return &xpcie->mmio->htod_tx_doorbell;
 	if (dirt == TO_DEVICE && type == DATA_RECEIVED)
-		return &mxlk->mmio->htod_rx_doorbell;
+		return &xpcie->mmio->htod_rx_doorbell;
 	if (dirt == TO_DEVICE && type == DEV_EVENT)
-		return &mxlk->mmio->htod_event_doorbell;
+		return &xpcie->mmio->htod_event_doorbell;
 	if (dirt == FROM_DEVICE && type == DATA_SENT)
-		return &mxlk->mmio->dtoh_tx_doorbell;
+		return &xpcie->mmio->dtoh_tx_doorbell;
 	if (dirt == FROM_DEVICE && type == DATA_RECEIVED)
-		return &mxlk->mmio->dtoh_rx_doorbell;
+		return &xpcie->mmio->dtoh_rx_doorbell;
 	if (dirt == FROM_DEVICE && type == DEV_EVENT)
-		return &mxlk->mmio->dtoh_event_doorbell;
+		return &xpcie->mmio->dtoh_event_doorbell;
 
 	return NULL;
 }
 
-void mxlk_set_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
-		       enum mxlk_doorbell_type type, u8 value)
+void intel_xpcie_set_doorbell(struct xpcie *xpcie,
+			      enum xpcie_doorbell_direction dirt,
+			      enum xpcie_doorbell_type type, u8 value)
 {
-	mxlk_iowrite8(value, mxlk_doorbell_offset(mxlk, dirt, type));
+	intel_xpcie_iowrite8(value,
+			     intel_xpcie_doorbell_offset(xpcie, dirt, type));
 }
 
-u8 mxlk_get_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
-		       enum mxlk_doorbell_type type)
+u8 intel_xpcie_get_doorbell(struct xpcie *xpcie,
+			    enum xpcie_doorbell_direction dirt,
+			    enum xpcie_doorbell_type type)
 {
-	return mxlk_ioread8(mxlk_doorbell_offset(mxlk, dirt, type));
+	return intel_xpcie_ioread8(intel_xpcie_doorbell_offset(xpcie,
+							       dirt, type));
 }
 
-u32 mxlk_get_host_status(struct mxlk *mxlk)
+u32 intel_xpcie_get_host_status(struct xpcie *xpcie)
 {
-	return mxlk_ioread32(&mxlk->mmio->host_status);
+	return intel_xpcie_ioread32(&xpcie->mmio->host_status);
 }
 
-void mxlk_set_host_status(struct mxlk *mxlk, u32 status)
+void intel_xpcie_set_host_status(struct xpcie *xpcie, u32 status)
 {
-	mxlk->status = status;
-	mxlk_iowrite32(status, &mxlk->mmio->host_status);
+	xpcie->status = status;
+	intel_xpcie_iowrite32(status, &xpcie->mmio->host_status);
 }
 
-struct mxlk_buf_desc *mxlk_alloc_bd(size_t length)
+struct xpcie_buf_desc *intel_xpcie_alloc_bd(size_t length)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
 	bd = kzalloc(sizeof(*bd), GFP_KERNEL);
 	if (!bd)
@@ -85,10 +89,10 @@ struct mxlk_buf_desc *mxlk_alloc_bd(size_t length)
 	return bd;
 }
 
-struct mxlk_buf_desc *mxlk_alloc_bd_reuse(size_t length, void *virt,
+struct xpcie_buf_desc *intel_xpcie_alloc_bd_reuse(size_t length, void *virt,
 					  dma_addr_t phys)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
 	bd = kzalloc(sizeof(*bd), GFP_KERNEL);
 	if (!bd)
@@ -104,7 +108,7 @@ struct mxlk_buf_desc *mxlk_alloc_bd_reuse(size_t length, void *virt,
 	return bd;
 }
 
-void mxlk_free_bd(struct mxlk_buf_desc *bd)
+void intel_xpcie_free_bd(struct xpcie_buf_desc *bd)
 {
 	if (bd) {
 		if (bd->own_mem)
@@ -113,7 +117,7 @@ void mxlk_free_bd(struct mxlk_buf_desc *bd)
 	}
 }
 
-int mxlk_list_init(struct mxlk_list *list)
+int intel_xpcie_list_init(struct xpcie_list *list)
 {
 	spin_lock_init(&list->lock);
 	list->bytes = 0;
@@ -124,22 +128,22 @@ int mxlk_list_init(struct mxlk_list *list)
 	return 0;
 }
 
-void mxlk_list_cleanup(struct mxlk_list *list)
+void intel_xpcie_list_cleanup(struct xpcie_list *list)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
 	spin_lock(&list->lock);
 	while (list->head) {
 		bd = list->head;
 		list->head = bd->next;
-		mxlk_free_bd(bd);
+		intel_xpcie_free_bd(bd);
 	}
 
 	list->head = list->tail = NULL;
 	spin_unlock(&list->lock);
 }
 
-int mxlk_list_put(struct mxlk_list *list, struct mxlk_buf_desc *bd)
+int intel_xpcie_list_put(struct xpcie_list *list, struct xpcie_buf_desc *bd)
 {
 	if (!bd)
 		return -EINVAL;
@@ -160,9 +164,10 @@ int mxlk_list_put(struct mxlk_list *list, struct mxlk_buf_desc *bd)
 	return 0;
 }
 
-int mxlk_list_put_head(struct mxlk_list *list, struct mxlk_buf_desc *bd)
+int intel_xpcie_list_put_head(struct xpcie_list *list,
+			      struct xpcie_buf_desc *bd)
 {
-	struct mxlk_buf_desc *old_head;
+	struct xpcie_buf_desc *old_head;
 
 	if (!bd)
 		return -EINVAL;
@@ -185,9 +190,9 @@ int mxlk_list_put_head(struct mxlk_list *list, struct mxlk_buf_desc *bd)
 	return 0;
 }
 
-struct mxlk_buf_desc *mxlk_list_get(struct mxlk_list *list)
+struct xpcie_buf_desc *intel_xpcie_list_get(struct xpcie_list *list)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
 	spin_lock(&list->lock);
 	bd = list->head;
@@ -204,7 +209,8 @@ struct mxlk_buf_desc *mxlk_list_get(struct mxlk_list *list)
 	return bd;
 }
 
-void mxlk_list_info(struct mxlk_list *list, size_t *bytes, size_t *buffers)
+void intel_xpcie_list_info(struct xpcie_list *list,
+			   size_t *bytes, size_t *buffers)
 {
 	spin_lock(&list->lock);
 	*bytes = list->bytes;
@@ -212,11 +218,11 @@ void mxlk_list_info(struct mxlk_list *list, size_t *bytes, size_t *buffers)
 	spin_unlock(&list->lock);
 }
 
-struct mxlk_buf_desc *mxlk_alloc_rx_bd(struct mxlk *mxlk)
+struct xpcie_buf_desc *intel_xpcie_alloc_rx_bd(struct xpcie *xpcie)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
-	bd = mxlk_list_get(&mxlk->rx_pool);
+	bd = intel_xpcie_list_get(&xpcie->rx_pool);
 	if (bd) {
 		bd->data = bd->head;
 		bd->length = bd->true_len;
@@ -227,49 +233,49 @@ struct mxlk_buf_desc *mxlk_alloc_rx_bd(struct mxlk *mxlk)
 	return bd;
 }
 
-void mxlk_free_rx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd)
+void intel_xpcie_free_rx_bd(struct xpcie *xpcie, struct xpcie_buf_desc *bd)
 {
 	if (bd)
-		mxlk_list_put(&mxlk->rx_pool, bd);
+		intel_xpcie_list_put(&xpcie->rx_pool, bd);
 }
 
-struct mxlk_buf_desc *mxlk_alloc_tx_bd(struct mxlk *mxlk)
+struct xpcie_buf_desc *intel_xpcie_alloc_tx_bd(struct xpcie *xpcie)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
-	bd = mxlk_list_get(&mxlk->tx_pool);
+	bd = intel_xpcie_list_get(&xpcie->tx_pool);
 	if (bd) {
 		bd->data = bd->head;
 		bd->length = bd->true_len;
 		bd->next = NULL;
 		bd->interface = 0;
 	} else {
-		mxlk->no_tx_buffer = true;
+		xpcie->no_tx_buffer = true;
 	}
 
 	return bd;
 }
 
-void mxlk_free_tx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd)
+void intel_xpcie_free_tx_bd(struct xpcie *xpcie, struct xpcie_buf_desc *bd)
 {
 	if (!bd)
 		return;
 
-	mxlk_list_put(&mxlk->tx_pool, bd);
+	intel_xpcie_list_put(&xpcie->tx_pool, bd);
 
-	mxlk->no_tx_buffer = false;
-	wake_up_interruptible(&mxlk->tx_waitqueue);
+	xpcie->no_tx_buffer = false;
+	wake_up_interruptible(&xpcie->tx_waitqueue);
 }
 
-int mxlk_interface_init(struct mxlk *mxlk, int id)
+int intel_xpcie_interface_init(struct xpcie *xpcie, int id)
 {
-	struct mxlk_interface *inf = mxlk->interfaces + id;
+	struct xpcie_interface *inf = xpcie->interfaces + id;
 
 	inf->id = id;
-	inf->mxlk = mxlk;
+	inf->xpcie = xpcie;
 
 	inf->partial_read = NULL;
-	mxlk_list_init(&inf->read);
+	intel_xpcie_list_init(&inf->read);
 	mutex_init(&inf->rlock);
 	inf->data_available = false;
 	init_waitqueue_head(&inf->rx_waitqueue);
@@ -277,50 +283,51 @@ int mxlk_interface_init(struct mxlk *mxlk, int id)
 	return 0;
 }
 
-void mxlk_interface_cleanup(struct mxlk_interface *inf)
+void intel_xpcie_interface_cleanup(struct xpcie_interface *inf)
 {
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 
 	mutex_destroy(&inf->rlock);
 
-	mxlk_free_rx_bd(inf->mxlk, inf->partial_read);
-	while ((bd = mxlk_list_get(&inf->read)))
-		mxlk_free_rx_bd(inf->mxlk, bd);
+	intel_xpcie_free_rx_bd(inf->xpcie, inf->partial_read);
+	while ((bd = intel_xpcie_list_get(&inf->read)))
+		intel_xpcie_free_rx_bd(inf->xpcie, bd);
 }
 
-void mxlk_interfaces_cleanup(struct mxlk *mxlk)
+void intel_xpcie_interfaces_cleanup(struct xpcie *xpcie)
 {
 	int index;
 
-	for (index = 0; index < MXLK_NUM_INTERFACES; index++)
-		mxlk_interface_cleanup(mxlk->interfaces + index);
+	for (index = 0; index < XPCIE_NUM_INTERFACES; index++)
+		intel_xpcie_interface_cleanup(xpcie->interfaces + index);
 
-	mxlk_list_cleanup(&mxlk->write);
-	mutex_destroy(&mxlk->wlock);
+	intel_xpcie_list_cleanup(&xpcie->write);
+	mutex_destroy(&xpcie->wlock);
 }
 
-int mxlk_interfaces_init(struct mxlk *mxlk)
+int intel_xpcie_interfaces_init(struct xpcie *xpcie)
 {
 	int index;
 
-	mutex_init(&mxlk->wlock);
-	mxlk_list_init(&mxlk->write);
-	init_waitqueue_head(&mxlk->tx_waitqueue);
-	mxlk->no_tx_buffer = false;
+	mutex_init(&xpcie->wlock);
+	intel_xpcie_list_init(&xpcie->write);
+	init_waitqueue_head(&xpcie->tx_waitqueue);
+	xpcie->no_tx_buffer = false;
 
-	for (index = 0; index < MXLK_NUM_INTERFACES; index++)
-		mxlk_interface_init(mxlk, index);
+	for (index = 0; index < XPCIE_NUM_INTERFACES; index++)
+		intel_xpcie_interface_init(xpcie, index);
 
 	return 0;
 }
 
-void mxlk_add_bd_to_interface(struct mxlk *mxlk, struct mxlk_buf_desc *bd)
+void intel_xpcie_add_bd_to_interface(struct xpcie *xpcie,
+				     struct xpcie_buf_desc *bd)
 {
-	struct mxlk_interface *inf;
+	struct xpcie_interface *inf;
 
-	inf = mxlk->interfaces + bd->interface;
+	inf = xpcie->interfaces + bd->interface;
 
-	mxlk_list_put(&inf->read, bd);
+	intel_xpcie_list_put(&inf->read, bd);
 
 	mutex_lock(&inf->rlock);
 	inf->data_available = true;
@@ -339,22 +346,22 @@ static ssize_t debug_show(struct device *dev, struct device_attribute *attr,
 {
 #ifdef XLINK_PCIE_LOCAL
 	struct pci_epf *epf = container_of(dev, struct pci_epf, dev);
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
-	struct mxlk *mxlk = &mxlk_epf->mxlk;
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	struct xpcie *xpcie = &xpcie_epf->xpcie;
 #else
 	struct pci_dev *pdev = container_of(dev, struct pci_dev, dev);
-	struct mxlk_pcie *xdev = pci_get_drvdata(pdev);
-	struct mxlk *mxlk = &xdev->mxlk;
+	struct xpcie_dev *xdev = pci_get_drvdata(pdev);
+	struct xpcie *xpcie = &xdev->xpcie;
 #endif
 	size_t bytes, tx_list_num, rx_list_num, tx_pool_num, rx_pool_num;
 
-	if (!mxlk->debug_enable)
+	if (!xpcie->debug_enable)
 		return 0;
 
-	mxlk_list_info(&mxlk->write, &bytes, &tx_list_num);
-	mxlk_list_info(&mxlk->interfaces[0].read, &bytes, &rx_list_num);
-	mxlk_list_info(&mxlk->tx_pool, &bytes, &tx_pool_num);
-	mxlk_list_info(&mxlk->rx_pool, &bytes, &rx_pool_num);
+	intel_xpcie_list_info(&xpcie->write, &bytes, &tx_list_num);
+	intel_xpcie_list_info(&xpcie->interfaces[0].read, &bytes, &rx_list_num);
+	intel_xpcie_list_info(&xpcie->tx_pool, &bytes, &tx_pool_num);
+	intel_xpcie_list_info(&xpcie->rx_pool, &bytes, &rx_pool_num);
 
 	snprintf(buf, 4096,
 		 "tx_krn, cnts %zu bytes %zu\n"
@@ -365,13 +372,13 @@ static ssize_t debug_show(struct device *dev, struct device_attribute *attr,
 		 "rx_list %zu rx_pool %zu\n"
 		 "interrupts %zu, send_ints %zu\n"
 		 "rx runs %zu tx runs %zu\n",
-		 mxlk->stats.tx_krn.cnts, mxlk->stats.tx_krn.bytes,
-		 mxlk->stats.tx_usr.cnts, mxlk->stats.tx_usr.bytes,
-		 mxlk->stats.rx_krn.cnts, mxlk->stats.rx_krn.bytes,
-		 mxlk->stats.rx_usr.cnts, mxlk->stats.rx_usr.bytes,
+		 xpcie->stats.tx_krn.cnts, xpcie->stats.tx_krn.bytes,
+		 xpcie->stats.tx_usr.cnts, xpcie->stats.tx_usr.bytes,
+		 xpcie->stats.rx_krn.cnts, xpcie->stats.rx_krn.bytes,
+		 xpcie->stats.rx_usr.cnts, xpcie->stats.rx_usr.bytes,
 		 tx_list_num, tx_pool_num, rx_list_num, rx_pool_num,
-		 mxlk->stats.interrupts, mxlk->stats.send_ints,
-		 mxlk->stats.rx_event_runs, mxlk->stats.tx_event_runs
+		 xpcie->stats.interrupts, xpcie->stats.send_ints,
+		 xpcie->stats.rx_event_runs, xpcie->stats.tx_event_runs
 	 );
 
 	return strlen(buf);
@@ -385,42 +392,42 @@ static ssize_t debug_store(struct device *dev, struct device_attribute *attr,
 
 #ifdef XLINK_PCIE_LOCAL
 	struct pci_epf *epf = container_of(dev, struct pci_epf, dev);
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
-	struct mxlk *mxlk = &mxlk_epf->mxlk;
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	struct xpcie *xpcie = &xpcie_epf->xpcie;
 #else
 	struct pci_dev *pdev = container_of(dev, struct pci_dev, dev);
-	struct mxlk_pcie *xdev = pci_get_drvdata(pdev);
-	struct mxlk *mxlk = &xdev->mxlk;
+	struct xpcie_dev *xdev = pci_get_drvdata(pdev);
+	struct xpcie *xpcie = &xdev->xpcie;
 #endif
 
 	rc = kstrtol(buf, 10, &value);
 	if (rc)
 		return rc;
 
-	mxlk->debug_enable = value ? true : false;
+	xpcie->debug_enable = value ? true : false;
 
-	if (!mxlk->debug_enable)
-		memset(&mxlk->stats, 0, sizeof(struct mxlk_debug_stats));
+	if (!xpcie->debug_enable)
+		memset(&xpcie->stats, 0, sizeof(struct xpcie_debug_stats));
 
 	return count;
 }
 
-void mxlk_init_debug(struct mxlk *mxlk, struct device *dev)
+void intel_xpcie_init_debug(struct xpcie *xpcie, struct device *dev)
 {
 	DEVICE_ATTR_RW(debug);
-	memset(&mxlk->stats, 0, sizeof(struct mxlk_debug_stats));
-	mxlk->debug = dev_attr_debug;
-	device_create_file(dev, &mxlk->debug);
+	memset(&xpcie->stats, 0, sizeof(struct xpcie_debug_stats));
+	xpcie->debug = dev_attr_debug;
+	device_create_file(dev, &xpcie->debug);
 }
 
-void mxlk_uninit_debug(struct mxlk *mxlk, struct device *dev)
+void intel_xpcie_uninit_debug(struct xpcie *xpcie, struct device *dev)
 {
-	device_remove_file(dev, &mxlk->debug);
-	memset(&mxlk->stats, 0, sizeof(struct mxlk_debug_stats));
+	device_remove_file(dev, &xpcie->debug);
+	memset(&xpcie->stats, 0, sizeof(struct xpcie_debug_stats));
 }
 
-void mxlk_debug_incr(struct mxlk *mxlk, size_t *attr, size_t v)
+void intel_xpcie_debug_incr(struct xpcie *xpcie, size_t *attr, size_t v)
 {
-	if (unlikely(mxlk->debug_enable))
+	if (unlikely(xpcie->debug_enable))
 		*attr += v;
 }
diff --git a/drivers/misc/xlink-pcie/common/util.h b/drivers/misc/xlink-pcie/common/util.h
index 8b8389a5b895..4b766fe284d4 100644
--- a/drivers/misc/xlink-pcie/common/util.h
+++ b/drivers/misc/xlink-pcie/common/util.h
@@ -7,63 +7,68 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_UTIL_HEADER_
-#define MXLK_UTIL_HEADER_
+#ifndef XPCIE_UTIL_HEADER_
+#define XPCIE_UTIL_HEADER_
 
-#include "xlink_pcie.h"
+#include "xpcie.h"
 
-enum mxlk_doorbell_direction {
+enum xpcie_doorbell_direction {
 	TO_DEVICE,
 	FROM_DEVICE
 };
 
-enum mxlk_doorbell_type {
+enum xpcie_doorbell_type {
 	DATA_SENT,
 	DATA_RECEIVED,
 	DEV_EVENT
 };
 
-enum mxlk_event_type {
+enum xpcie_event_type {
 	NO_OP,
 	REQUEST_RESET,
 	DEV_SHUTDOWN
 };
 
-void mxlk_set_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
-		       enum mxlk_doorbell_type type, u8 value);
-u8 mxlk_get_doorbell(struct mxlk *mxlk, enum mxlk_doorbell_direction dirt,
-		     enum mxlk_doorbell_type type);
+void intel_xpcie_set_doorbell(struct xpcie *xpcie,
+			      enum xpcie_doorbell_direction dirt,
+			      enum xpcie_doorbell_type type, u8 value);
+u8 intel_xpcie_get_doorbell(struct xpcie *xpcie,
+			    enum xpcie_doorbell_direction dirt,
+			    enum xpcie_doorbell_type type);
 
-void mxlk_set_device_status(struct mxlk *mxlk, u32 status);
-u32 mxlk_get_device_status(struct mxlk *mxlk);
-u32 mxlk_get_host_status(struct mxlk *mxlk);
-void mxlk_set_host_status(struct mxlk *mxlk, u32 status);
+void intel_xpcie_set_device_status(struct xpcie *xpcie, u32 status);
+u32 intel_xpcie_get_device_status(struct xpcie *xpcie);
+u32 intel_xpcie_get_host_status(struct xpcie *xpcie);
+void intel_xpcie_set_host_status(struct xpcie *xpcie, u32 status);
 
-struct mxlk_buf_desc *mxlk_alloc_bd(size_t length);
-struct mxlk_buf_desc *mxlk_alloc_bd_reuse(size_t length, void *virt,
+struct xpcie_buf_desc *intel_xpcie_alloc_bd(size_t length);
+struct xpcie_buf_desc *intel_xpcie_alloc_bd_reuse(size_t length, void *virt,
 					  dma_addr_t phys);
-void mxlk_free_bd(struct mxlk_buf_desc *bd);
+void intel_xpcie_free_bd(struct xpcie_buf_desc *bd);
 
-int mxlk_list_init(struct mxlk_list *list);
-void mxlk_list_cleanup(struct mxlk_list *list);
-int mxlk_list_put(struct mxlk_list *list, struct mxlk_buf_desc *bd);
-int mxlk_list_put_head(struct mxlk_list *list, struct mxlk_buf_desc *bd);
-struct mxlk_buf_desc *mxlk_list_get(struct mxlk_list *list);
-void mxlk_list_info(struct mxlk_list *list, size_t *bytes, size_t *buffers);
+int intel_xpcie_list_init(struct xpcie_list *list);
+void intel_xpcie_list_cleanup(struct xpcie_list *list);
+int intel_xpcie_list_put(struct xpcie_list *list, struct xpcie_buf_desc *bd);
+int intel_xpcie_list_put_head(struct xpcie_list *list,
+			      struct xpcie_buf_desc *bd);
+struct xpcie_buf_desc *intel_xpcie_list_get(struct xpcie_list *list);
+void intel_xpcie_list_info(struct xpcie_list *list, size_t *bytes,
+			   size_t *buffers);
 
-struct mxlk_buf_desc *mxlk_alloc_rx_bd(struct mxlk *mxlk);
-void mxlk_free_rx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd);
-struct mxlk_buf_desc *mxlk_alloc_tx_bd(struct mxlk *mxlk);
-void mxlk_free_tx_bd(struct mxlk *mxlk, struct mxlk_buf_desc *bd);
+struct xpcie_buf_desc *intel_xpcie_alloc_rx_bd(struct xpcie *xpcie);
+void intel_xpcie_free_rx_bd(struct xpcie *xpcie, struct xpcie_buf_desc *bd);
+struct xpcie_buf_desc *intel_xpcie_alloc_tx_bd(struct xpcie *xpcie);
+void intel_xpcie_free_tx_bd(struct xpcie *xpcie, struct xpcie_buf_desc *bd);
 
-int mxlk_interface_init(struct mxlk *mxlk, int id);
-void mxlk_interface_cleanup(struct mxlk_interface *inf);
-void mxlk_interfaces_cleanup(struct mxlk *mxlk);
-int mxlk_interfaces_init(struct mxlk *mxlk);
-void mxlk_add_bd_to_interface(struct mxlk *mxlk, struct mxlk_buf_desc *bd);
+int intel_xpcie_interface_init(struct xpcie *xpcie, int id);
+void intel_xpcie_interface_cleanup(struct xpcie_interface *inf);
+void intel_xpcie_interfaces_cleanup(struct xpcie *xpcie);
+int intel_xpcie_interfaces_init(struct xpcie *xpcie);
+void intel_xpcie_add_bd_to_interface(struct xpcie *xpcie,
+				     struct xpcie_buf_desc *bd);
 
-void mxlk_init_debug(struct mxlk *mxlk, struct device *dev);
-void mxlk_uninit_debug(struct mxlk *mxlk, struct device *dev);
-void mxlk_debug_incr(struct mxlk *mxlk, size_t *attr, size_t v);
+void intel_xpcie_init_debug(struct xpcie *xpcie, struct device *dev);
+void intel_xpcie_uninit_debug(struct xpcie *xpcie, struct device *dev);
+void intel_xpcie_debug_incr(struct xpcie *xpcie, size_t *attr, size_t v);
 
-#endif // MXLK_UTIL_HEADER_
+#endif // XPCIE_UTIL_HEADER_
diff --git a/drivers/misc/xlink-pcie/common/xlink_pcie.h b/drivers/misc/xlink-pcie/common/xpcie.h
similarity index 59%
rename from drivers/misc/xlink-pcie/common/xlink_pcie.h
rename to drivers/misc/xlink-pcie/common/xpcie.h
index eb13f041ab37..45a62fa69231 100644
--- a/drivers/misc/xlink-pcie/common/xlink_pcie.h
+++ b/drivers/misc/xlink-pcie/common/xpcie.h
@@ -7,8 +7,8 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_HEADER_
-#define MXLK_HEADER_
+#ifndef XPCIE_HEADER_
+#define XPCIE_HEADER_
 
 #include <linux/kernel.h>
 #include <linux/module.h>
@@ -28,23 +28,23 @@
 #include "boot.h"
 
 #ifdef XLINK_PCIE_REMOTE
-#define MXLK_DRIVER_NAME "mxlk"
-#define MXLK_DRIVER_DESC "Intel(R) Keem Bay XLink PCIe driver"
+#define XPCIE_DRIVER_NAME "mxlk"
+#define XPCIE_DRIVER_DESC "Intel(R) Keem Bay XLink PCIe driver"
 #else
-#define MXLK_DRIVER_NAME "mxlk_pcie_epf"
-#define MXLK_DRIVER_DESC "Intel(R) xLink PCIe endpoint function driver"
+#define XPCIE_DRIVER_NAME "mxlk_pcie_epf"
+#define XPCIE_DRIVER_DESC "Intel(R) xLink PCIe endpoint function driver"
 #endif
 
-struct mxlk_pipe {
+struct xpcie_pipe {
 	u32 old;
 	u32 ndesc;
 	u32 *head;
 	u32 *tail;
-	struct mxlk_transfer_desc *tdr;
+	struct xpcie_transfer_desc *tdr;
 };
 
-struct mxlk_buf_desc {
-	struct mxlk_buf_desc *next;
+struct xpcie_buf_desc {
+	struct xpcie_buf_desc *next;
 	void *head;
 	dma_addr_t phys;
 	size_t true_len;
@@ -54,33 +54,33 @@ struct mxlk_buf_desc {
 	bool own_mem;
 };
 
-struct mxlk_stream {
+struct xpcie_stream {
 	size_t frag;
-	struct mxlk_pipe pipe;
+	struct xpcie_pipe pipe;
 #ifdef XLINK_PCIE_REMOTE
-	struct mxlk_buf_desc **ddr;
+	struct xpcie_buf_desc **ddr;
 #endif
 };
 
-struct mxlk_list {
+struct xpcie_list {
 	spinlock_t lock;
 	size_t bytes;
 	size_t buffers;
-	struct mxlk_buf_desc *head;
-	struct mxlk_buf_desc *tail;
+	struct xpcie_buf_desc *head;
+	struct xpcie_buf_desc *tail;
 };
 
-struct mxlk_interface {
+struct xpcie_interface {
 	int id;
-	struct mxlk *mxlk;
+	struct xpcie *xpcie;
 	struct mutex rlock;
-	struct mxlk_list read;
-	struct mxlk_buf_desc *partial_read;
+	struct xpcie_list read;
+	struct xpcie_buf_desc *partial_read;
 	bool data_available;
 	wait_queue_head_t rx_waitqueue;
 };
 
-struct mxlk_debug_stats {
+struct xpcie_debug_stats {
 	struct {
 		size_t cnts;
 		size_t bytes;
@@ -91,47 +91,47 @@ struct mxlk_debug_stats {
 	size_t tx_event_runs;
 };
 
-struct mxlk {
+struct xpcie {
 	u32 status;
 	bool legacy_a0;
 
 #ifdef XLINK_PCIE_REMOTE
 	void __iomem *bar0;
-	struct mxlk_bootio __iomem *io_comm; /* IO communication space */
-	struct mxlk_mmio __iomem *mmio; /* XLink memory space */
+	struct xpcie_bootio __iomem *io_comm; /* IO communication space */
+	struct xpcie_mmio __iomem *mmio; /* XLink memory space */
 	void __iomem *bar4;
 #else
-	struct mxlk_bootio *io_comm; /* IO communication space */
-	struct mxlk_mmio *mmio; /* XLink memory space */
+	struct xpcie_bootio *io_comm; /* IO communication space */
+	struct xpcie_mmio *mmio; /* XLink memory space */
 	void *bar4;
 #endif
 
 	struct workqueue_struct *rx_wq;
 	struct workqueue_struct *tx_wq;
 
-	struct mxlk_interface interfaces[MXLK_NUM_INTERFACES];
+	struct xpcie_interface interfaces[XPCIE_NUM_INTERFACES];
 
 	size_t fragment_size;
-	struct mxlk_cap_txrx *txrx;
-	struct mxlk_stream tx;
-	struct mxlk_stream rx;
+	struct xpcie_cap_txrx *txrx;
+	struct xpcie_stream tx;
+	struct xpcie_stream rx;
 
 	struct mutex wlock;
-	struct mxlk_list write;
+	struct xpcie_list write;
 	bool no_tx_buffer;
 	wait_queue_head_t tx_waitqueue;
 	bool tx_pending;
 	bool stop_flag;
 
-	struct mxlk_list rx_pool;
-	struct mxlk_list tx_pool;
+	struct xpcie_list rx_pool;
+	struct xpcie_list tx_pool;
 
 	struct delayed_work rx_event;
 	struct delayed_work tx_event;
 
 	struct device_attribute debug;
 	bool debug_enable;
-	struct mxlk_debug_stats stats;
+	struct xpcie_debug_stats stats;
 };
 
 #endif
diff --git a/drivers/misc/xlink-pcie/local_host/core.c b/drivers/misc/xlink-pcie/local_host/core.c
index 9e037408e488..85b127bc396c 100644
--- a/drivers/misc/xlink-pcie/local_host/core.c
+++ b/drivers/misc/xlink-pcie/local_host/core.c
@@ -17,9 +17,9 @@
 #include "epf.h"
 #include "struct.h"
 
-static struct mxlk *global_mxlk;
+static struct xpcie *global_xpcie;
 
-#define MXLK_CIRCULAR_INC(val, max) (((val) + 1) & (max - 1))
+#define XPCIE_CIRCULAR_INC(val, max) (((val) + 1) & (max - 1))
 
 static int rx_pool_size = SZ_32M;
 module_param(rx_pool_size, int, 0664);
@@ -29,23 +29,26 @@ static int tx_pool_size = SZ_32M;
 module_param(tx_pool_size, int, 0664);
 MODULE_PARM_DESC(tx_pool_size, "transmitting pool size (default 32 MiB)");
 
-static int fragment_size = MXLK_FRAGMENT_SIZE;
+static int fragment_size = XPCIE_FRAGMENT_SIZE;
 module_param(fragment_size, int, 0664);
 MODULE_PARM_DESC(fragment_size, "transfer descriptor size (default 128 KiB)");
 
 static bool tx_pool_coherent = true;
 module_param(tx_pool_coherent, bool, 0664);
-MODULE_PARM_DESC(tx_pool_coherent, "transmitting pool using coherent memory (default true)");
+MODULE_PARM_DESC(tx_pool_coherent,
+		 "transmitting pool using coherent memory (default true)");
 
 static bool rx_pool_coherent;
 module_param(rx_pool_coherent, bool, 0664);
-MODULE_PARM_DESC(rx_pool_coherent, "receiving pool using coherent memory (default false)");
+MODULE_PARM_DESC(rx_pool_coherent,
+		 "receiving pool using coherent memory (default false)");
 
-static int mxlk_map_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
+static int intel_xpcie_map_dma(struct xpcie *xpcie, struct xpcie_buf_desc *bd,
 			int direction)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct pci_epf *epf = mxlk_epf->epf;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct pci_epf *epf = xpcie_epf->epf;
 	struct device *dma_dev = epf->epc->dev.parent;
 
 	bd->phys = dma_map_single(dma_dev, bd->data, bd->length, direction);
@@ -53,183 +56,189 @@ static int mxlk_map_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
 	return dma_mapping_error(dma_dev, bd->phys);
 }
 
-static void mxlk_unmap_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
-			   int direction)
+static void intel_xpcie_unmap_dma(struct xpcie *xpcie,
+				  struct xpcie_buf_desc *bd, int direction)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct pci_epf *epf = mxlk_epf->epf;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct pci_epf *epf = xpcie_epf->epf;
 	struct device *dma_dev = epf->epc->dev.parent;
 
 	dma_unmap_single(dma_dev, bd->phys, bd->length, direction);
 }
 
-static void mxlk_set_cap_txrx(struct mxlk *mxlk)
+static void intel_xpcie_set_cap_txrx(struct xpcie *xpcie)
 {
-	struct mxlk_cap_txrx *cap;
-	struct mxlk_cap_hdr *hdr;
-	uint32_t start = sizeof(struct mxlk_mmio);
-	size_t hdr_len = sizeof(struct mxlk_cap_txrx);
-	size_t tx_len = sizeof(struct mxlk_transfer_desc) * MXLK_NUM_TX_DESCS;
-	size_t rx_len = sizeof(struct mxlk_transfer_desc) * MXLK_NUM_RX_DESCS;
+	struct xpcie_cap_txrx *cap;
+	struct xpcie_cap_hdr *hdr;
+	uint32_t start = sizeof(struct xpcie_mmio);
+	size_t hdr_len = sizeof(struct xpcie_cap_txrx);
+	size_t tx_len = sizeof(struct xpcie_transfer_desc) *
+				XPCIE_NUM_TX_DESCS;
+	size_t rx_len = sizeof(struct xpcie_transfer_desc) *
+				XPCIE_NUM_RX_DESCS;
 	uint16_t next = (uint16_t)(start + hdr_len + tx_len + rx_len);
 
-	mxlk->mmio->cap_offset = start;
-	cap = (void *)mxlk->mmio + start;
-	memset(cap, 0, sizeof(struct mxlk_cap_txrx));
-	cap->hdr.id = MXLK_CAP_TXRX;
+	xpcie->mmio->cap_offset = start;
+	cap = (void *)xpcie->mmio + start;
+	memset(cap, 0, sizeof(struct xpcie_cap_txrx));
+	cap->hdr.id = XPCIE_CAP_TXRX;
 	cap->hdr.next = next;
 	cap->fragment_size = fragment_size;
 	cap->tx.ring = start + hdr_len;
-	cap->tx.ndesc = MXLK_NUM_TX_DESCS;
+	cap->tx.ndesc = XPCIE_NUM_TX_DESCS;
 	cap->rx.ring = start + hdr_len + tx_len;
-	cap->rx.ndesc = MXLK_NUM_RX_DESCS;
+	cap->rx.ndesc = XPCIE_NUM_RX_DESCS;
 
-	hdr = (struct mxlk_cap_hdr *)((void *)mxlk->mmio + next);
-	hdr->id = MXLK_CAP_NULL;
+	hdr = (struct xpcie_cap_hdr *)((void *)xpcie->mmio + next);
+	hdr->id = XPCIE_CAP_NULL;
 }
 
-static int mxlk_set_version(struct mxlk *mxlk)
+static int intel_xpcie_set_version(struct xpcie *xpcie)
 {
-	struct mxlk_version version;
+	struct xpcie_version version;
 
-	version.major = MXLK_VERSION_MAJOR;
-	version.minor = MXLK_VERSION_MINOR;
-	version.build = MXLK_VERSION_BUILD;
+	version.major = XPCIE_VERSION_MAJOR;
+	version.minor = XPCIE_VERSION_MINOR;
+	version.build = XPCIE_VERSION_BUILD;
 
-	memcpy(&mxlk->mmio->version, &version, sizeof(version));
+	memcpy(&xpcie->mmio->version, &version, sizeof(version));
 
-	dev_info(mxlk_to_dev(mxlk), "ver: device %u.%u.%u\n",
+	dev_info(xpcie_to_dev(xpcie), "ver: device %u.%u.%u\n",
 		 version.major, version.minor, version.build);
 
 	return 0;
 }
 
-static void mxlk_txrx_cleanup(struct mxlk *mxlk)
+static void intel_xpcie_txrx_cleanup(struct xpcie *xpcie)
 {
 	int index;
-	struct mxlk_transfer_desc *td;
-	struct mxlk_stream *tx = &mxlk->tx;
-	struct mxlk_stream *rx = &mxlk->rx;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
-	struct mxlk_interface *inf = &mxlk->interfaces[0];
-
-	mxlk->stop_flag = true;
-	mxlk->no_tx_buffer = false;
+	struct xpcie_transfer_desc *td;
+	struct xpcie_stream *tx = &xpcie->tx;
+	struct xpcie_stream *rx = &xpcie->rx;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct device *dma_dev = xpcie_epf->epf->epc->dev.parent;
+	struct xpcie_interface *inf = &xpcie->interfaces[0];
+
+	xpcie->stop_flag = true;
+	xpcie->no_tx_buffer = false;
 	inf->data_available = true;
-	wake_up_interruptible(&mxlk->tx_waitqueue);
+	wake_up_interruptible(&xpcie->tx_waitqueue);
 	wake_up_interruptible(&inf->rx_waitqueue);
-	mutex_lock(&mxlk->wlock);
+	mutex_lock(&xpcie->wlock);
 	mutex_lock(&inf->rlock);
 
 	for (index = 0; index < rx->pipe.ndesc; index++) {
 		td = rx->pipe.tdr + index;
-		mxlk_set_td_address(td, 0);
-		mxlk_set_td_length(td, 0);
+		intel_xpcie_set_td_address(td, 0);
+		intel_xpcie_set_td_length(td, 0);
 	}
 	for (index = 0; index < tx->pipe.ndesc; index++) {
 		td = tx->pipe.tdr + index;
-		mxlk_set_td_address(td, 0);
-		mxlk_set_td_length(td, 0);
+		intel_xpcie_set_td_address(td, 0);
+		intel_xpcie_set_td_length(td, 0);
 	}
 
-	mxlk_list_cleanup(&mxlk->tx_pool);
-	mxlk_list_cleanup(&mxlk->rx_pool);
+	intel_xpcie_list_cleanup(&xpcie->tx_pool);
+	intel_xpcie_list_cleanup(&xpcie->rx_pool);
 
-	if (rx_pool_coherent && mxlk_epf->rx_virt) {
-		dma_free_coherent(dma_dev, mxlk_epf->rx_size,
-				  mxlk_epf->rx_virt, mxlk_epf->rx_phys);
+	if (rx_pool_coherent && xpcie_epf->rx_virt) {
+		dma_free_coherent(dma_dev, xpcie_epf->rx_size,
+				  xpcie_epf->rx_virt, xpcie_epf->rx_phys);
 	}
 
-	if (tx_pool_coherent && mxlk_epf->tx_virt) {
-		dma_free_coherent(dma_dev, mxlk_epf->tx_size,
-				  mxlk_epf->tx_virt, mxlk_epf->tx_phys);
+	if (tx_pool_coherent && xpcie_epf->tx_virt) {
+		dma_free_coherent(dma_dev, xpcie_epf->tx_size,
+				  xpcie_epf->tx_virt, xpcie_epf->tx_phys);
 	}
 
 	mutex_unlock(&inf->rlock);
-	mutex_unlock(&mxlk->wlock);
+	mutex_unlock(&xpcie->wlock);
 }
 
 /*
  * The RX/TX are named for Remote Host, in Local Host RX/TX is reversed.
  */
-static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
+static int intel_xpcie_txrx_init(struct xpcie *xpcie,
+				 struct xpcie_cap_txrx *cap)
 {
 	int index;
 	int ndesc;
-	struct mxlk_buf_desc *bd;
-	struct mxlk_stream *tx = &mxlk->tx;
-	struct mxlk_stream *rx = &mxlk->rx;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
+	struct xpcie_buf_desc *bd;
+	struct xpcie_stream *tx = &xpcie->tx;
+	struct xpcie_stream *rx = &xpcie->rx;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct device *dma_dev = xpcie_epf->epf->epc->dev.parent;
 
-	mxlk->txrx = cap;
-	mxlk->fragment_size = cap->fragment_size;
-	mxlk->stop_flag = false;
+	xpcie->txrx = cap;
+	xpcie->fragment_size = cap->fragment_size;
+	xpcie->stop_flag = false;
 
 	rx->pipe.ndesc = cap->tx.ndesc;
 	rx->pipe.head = &cap->tx.head;
 	rx->pipe.tail = &cap->tx.tail;
-	rx->pipe.tdr = (void *)mxlk->mmio + cap->tx.ring;
+	rx->pipe.tdr = (void *)xpcie->mmio + cap->tx.ring;
 
 	tx->pipe.ndesc = cap->rx.ndesc;
 	tx->pipe.head = &cap->rx.head;
 	tx->pipe.tail = &cap->rx.tail;
-	tx->pipe.tdr = (void *)mxlk->mmio + cap->rx.ring;
+	tx->pipe.tdr = (void *)xpcie->mmio + cap->rx.ring;
 
-	mxlk_list_init(&mxlk->rx_pool);
-	rx_pool_size = roundup(rx_pool_size, mxlk->fragment_size);
-	ndesc = rx_pool_size / mxlk->fragment_size;
+	intel_xpcie_list_init(&xpcie->rx_pool);
+	rx_pool_size = roundup(rx_pool_size, xpcie->fragment_size);
+	ndesc = rx_pool_size / xpcie->fragment_size;
 
 	if (rx_pool_coherent) {
-		mxlk_epf->rx_size = rx_pool_size;
-		mxlk_epf->rx_virt = dma_alloc_coherent(dma_dev,
-			mxlk_epf->rx_size, &mxlk_epf->rx_phys, GFP_KERNEL);
-		if (!mxlk_epf->rx_virt)
+		xpcie_epf->rx_size = rx_pool_size;
+		xpcie_epf->rx_virt = dma_alloc_coherent(dma_dev,
+			xpcie_epf->rx_size, &xpcie_epf->rx_phys, GFP_KERNEL);
+		if (!xpcie_epf->rx_virt)
 			goto error;
 	}
 
 	for (index = 0; index < ndesc; index++) {
 		if (rx_pool_coherent) {
-			bd = mxlk_alloc_bd_reuse(mxlk->fragment_size,
-			mxlk_epf->rx_virt + index * mxlk->fragment_size,
-			mxlk_epf->rx_phys + index * mxlk->fragment_size);
+			bd = intel_xpcie_alloc_bd_reuse(xpcie->fragment_size,
+			xpcie_epf->rx_virt + index * xpcie->fragment_size,
+			xpcie_epf->rx_phys + index * xpcie->fragment_size);
 		} else {
-			bd = mxlk_alloc_bd(mxlk->fragment_size);
+			bd = intel_xpcie_alloc_bd(xpcie->fragment_size);
 		}
 		if (bd) {
-			mxlk_list_put(&mxlk->rx_pool, bd);
+			intel_xpcie_list_put(&xpcie->rx_pool, bd);
 		} else {
-			dev_err(mxlk_to_dev(mxlk),
+			dev_err(xpcie_to_dev(xpcie),
 				"failed to alloc all rx pool descriptors\n");
 			goto error;
 		}
 	}
 
-	mxlk_list_init(&mxlk->tx_pool);
-	tx_pool_size = roundup(tx_pool_size, mxlk->fragment_size);
-	ndesc = tx_pool_size / mxlk->fragment_size;
+	intel_xpcie_list_init(&xpcie->tx_pool);
+	tx_pool_size = roundup(tx_pool_size, xpcie->fragment_size);
+	ndesc = tx_pool_size / xpcie->fragment_size;
 
 	if (tx_pool_coherent) {
-		mxlk_epf->tx_size = tx_pool_size;
-		mxlk_epf->tx_virt = dma_alloc_coherent(dma_dev,
-			mxlk_epf->tx_size, &mxlk_epf->tx_phys, GFP_KERNEL);
-		if (!mxlk_epf->tx_virt)
+		xpcie_epf->tx_size = tx_pool_size;
+		xpcie_epf->tx_virt = dma_alloc_coherent(dma_dev,
+			xpcie_epf->tx_size, &xpcie_epf->tx_phys, GFP_KERNEL);
+		if (!xpcie_epf->tx_virt)
 			goto error;
 	}
 
 	for (index = 0; index < ndesc; index++) {
 		if (tx_pool_coherent) {
-			bd = mxlk_alloc_bd_reuse(mxlk->fragment_size,
-			mxlk_epf->tx_virt + index * mxlk->fragment_size,
-			mxlk_epf->tx_phys + index * mxlk->fragment_size);
+			bd = intel_xpcie_alloc_bd_reuse(xpcie->fragment_size,
+			xpcie_epf->tx_virt + index * xpcie->fragment_size,
+			xpcie_epf->tx_phys + index * xpcie->fragment_size);
 		} else {
-			bd = mxlk_alloc_bd(mxlk->fragment_size);
+			bd = intel_xpcie_alloc_bd(xpcie->fragment_size);
 		}
 		if (bd) {
-			mxlk_list_put(&mxlk->tx_pool, bd);
+			intel_xpcie_list_put(&xpcie->tx_pool, bd);
 		} else {
-			dev_err(mxlk_to_dev(mxlk),
+			dev_err(xpcie_to_dev(xpcie),
 				"failed to alloc all tx pool descriptors\n");
 			goto error;
 		}
@@ -238,43 +247,43 @@ static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
 	return 0;
 
 error:
-	mxlk_txrx_cleanup(mxlk);
+	intel_xpcie_txrx_cleanup(xpcie);
 
 	return -ENOMEM;
 }
 
-static int mxlk_discover_txrx(struct mxlk *mxlk)
+static int intel_xpcie_discover_txrx(struct xpcie *xpcie)
 {
 	int error;
-	struct mxlk_cap_txrx *cap;
+	struct xpcie_cap_txrx *cap;
 
-	cap = mxlk_cap_find(mxlk, 0, MXLK_CAP_TXRX);
+	cap = intel_xpcie_cap_find(xpcie, 0, XPCIE_CAP_TXRX);
 	if (cap) {
-		error = mxlk_txrx_init(mxlk, cap);
+		error = intel_xpcie_txrx_init(xpcie, cap);
 	} else {
-		dev_err(mxlk_to_dev(mxlk), "mxlk txrx info not found\n");
+		dev_err(xpcie_to_dev(xpcie), "xpcie txrx info not found\n");
 		error = -EIO;
 	}
 
 	return error;
 }
 
-static void mxlk_start_tx(struct mxlk *mxlk, unsigned long delay)
+static void intel_xpcie_start_tx(struct xpcie *xpcie, unsigned long delay)
 {
-	if (mxlk->legacy_a0)
-		queue_delayed_work(mxlk->rx_wq, &mxlk->tx_event, delay);
+	if (xpcie->legacy_a0)
+		queue_delayed_work(xpcie->rx_wq, &xpcie->tx_event, delay);
 	else
-		queue_delayed_work(mxlk->tx_wq, &mxlk->tx_event, delay);
+		queue_delayed_work(xpcie->tx_wq, &xpcie->tx_event, delay);
 }
 
-static void mxlk_start_rx(struct mxlk *mxlk, unsigned long delay)
+static void intel_xpcie_start_rx(struct xpcie *xpcie, unsigned long delay)
 {
-	queue_delayed_work(mxlk->rx_wq, &mxlk->rx_event, delay);
+	queue_delayed_work(xpcie->rx_wq, &xpcie->rx_event, delay);
 }
 
-static void mxlk_rx_event_handler(struct work_struct *work)
+static void intel_xpcie_rx_event_handler(struct work_struct *work)
 {
-	struct mxlk *mxlk = container_of(work, struct mxlk, rx_event.work);
+	struct xpcie *xpcie = container_of(work, struct xpcie, rx_event.work);
 
 	int rc;
 	u16 interface;
@@ -283,28 +292,29 @@ static void mxlk_rx_event_handler(struct work_struct *work)
 	u32 initial_head;
 	int descs_num = 0;
 	int chan = 0;
-	struct mxlk_stream *rx = &mxlk->rx;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct mxlk_dma_ll_desc *desc;
-	struct mxlk_buf_desc *bd_head, *bd_tail, *bd;
-	struct mxlk_transfer_desc *td;
+	struct xpcie_stream *rx = &xpcie->rx;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct xpcie_dma_ll_desc *desc;
+	struct xpcie_buf_desc *bd_head, *bd_tail, *bd;
+	struct xpcie_transfer_desc *td;
 	unsigned long delay = msecs_to_jiffies(1);
 	bool reset_work = false;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.rx_event_runs, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.rx_event_runs, 1);
 
-	if (mxlk_get_host_status(mxlk) != MXLK_STATUS_RUN)
+	if (intel_xpcie_get_host_status(xpcie) != XPCIE_STATUS_RUN)
 		return;
 
 	bd_head = bd_tail = NULL;
 	ndesc = rx->pipe.ndesc;
-	tail = mxlk_get_tdr_tail(&rx->pipe);
-	initial_head = head = mxlk_get_tdr_head(&rx->pipe);
+	tail = intel_xpcie_get_tdr_tail(&rx->pipe);
+	initial_head = head = intel_xpcie_get_tdr_head(&rx->pipe);
 
 	while (head != tail) {
 		td = rx->pipe.tdr + head;
 
-		bd = mxlk_alloc_rx_bd(mxlk);
+		bd = intel_xpcie_alloc_rx_bd(xpcie);
 		if (!bd) {
 			reset_work = true;
 			if (descs_num == 0) {
@@ -314,23 +324,23 @@ static void mxlk_rx_event_handler(struct work_struct *work)
 			break;
 		}
 
-		interface = mxlk_get_td_interface(td);
-		length = mxlk_get_td_length(td);
-		address = mxlk_get_td_address(td);
+		interface = intel_xpcie_get_td_interface(td);
+		length = intel_xpcie_get_td_length(td);
+		address = intel_xpcie_get_td_address(td);
 
 		bd->length = length;
 		bd->interface = interface;
 		if (!rx_pool_coherent) {
-			rc = mxlk_map_dma(mxlk, bd, DMA_FROM_DEVICE);
+			rc = intel_xpcie_map_dma(xpcie, bd, DMA_FROM_DEVICE);
 			if (rc) {
-				dev_err(mxlk_to_dev(mxlk),
+				dev_err(xpcie_to_dev(xpcie),
 					"failed to map rx bd (%d)\n", rc);
-				mxlk_free_rx_bd(mxlk, bd);
+				intel_xpcie_free_rx_bd(xpcie, bd);
 				break;
 			}
 		}
 
-		desc = &mxlk_epf->rx_desc_buf[chan].virt[descs_num++];
+		desc = &xpcie_epf->rx_desc_buf[chan].virt[descs_num++];
 		desc->dma_transfer_size = length;
 		desc->dst_addr = bd->phys;
 		desc->src_addr = address;
@@ -341,24 +351,24 @@ static void mxlk_rx_event_handler(struct work_struct *work)
 			bd_head = bd;
 		bd_tail = bd;
 
-		head = MXLK_CIRCULAR_INC(head, ndesc);
+		head = XPCIE_CIRCULAR_INC(head, ndesc);
 	}
 
 	if (descs_num == 0)
 		goto task_exit;
 
-	rc = mxlk_copy_from_host_ll(mxlk, chan, descs_num);
+	rc = intel_xpcie_copy_from_host_ll(xpcie, chan, descs_num);
 
 	bd = bd_head;
 	while (bd && !rx_pool_coherent) {
-		mxlk_unmap_dma(mxlk, bd, DMA_FROM_DEVICE);
+		intel_xpcie_unmap_dma(xpcie, bd, DMA_FROM_DEVICE);
 		bd = bd->next;
 	}
 
 	if (rc) {
-		dev_err(mxlk_to_dev(mxlk),
+		dev_err(xpcie_to_dev(xpcie),
 			"failed to DMA from host (%d)\n", rc);
-		mxlk_free_rx_bd(mxlk, bd_head);
+		intel_xpcie_free_rx_bd(xpcie, bd_head);
 		delay = msecs_to_jiffies(5);
 		reset_work = true;
 		goto task_exit;
@@ -371,38 +381,41 @@ static void mxlk_rx_event_handler(struct work_struct *work)
 		bd_head = bd_head->next;
 		bd->next = NULL;
 
-		if (likely(bd->interface < MXLK_NUM_INTERFACES)) {
-			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.cnts, 1);
-			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.bytes,
-					bd->length);
+		if (likely(bd->interface < XPCIE_NUM_INTERFACES)) {
+			intel_xpcie_debug_incr(xpcie,
+					       &xpcie->stats.rx_krn.cnts, 1);
+			intel_xpcie_debug_incr(xpcie,
+					       &xpcie->stats.rx_krn.bytes,
+					       bd->length);
 
-			mxlk_set_td_status(td, MXLK_DESC_STATUS_SUCCESS);
-			mxlk_add_bd_to_interface(mxlk, bd);
+			intel_xpcie_set_td_status(td,
+						  XPCIE_DESC_STATUS_SUCCESS);
+			intel_xpcie_add_bd_to_interface(xpcie, bd);
 		} else {
-			dev_err(mxlk_to_dev(mxlk),
+			dev_err(xpcie_to_dev(xpcie),
 				"detected rx desc interface failure (%u)\n",
 				bd->interface);
-			mxlk_set_td_status(td, MXLK_DESC_STATUS_ERROR);
-			mxlk_free_rx_bd(mxlk, bd);
+			intel_xpcie_set_td_status(td, XPCIE_DESC_STATUS_ERROR);
+			intel_xpcie_free_rx_bd(xpcie, bd);
 		}
 
 		bd = bd_head;
-		head = MXLK_CIRCULAR_INC(head, ndesc);
+		head = XPCIE_CIRCULAR_INC(head, ndesc);
 	}
 
 	if (head != initial_head) {
-		mxlk_set_tdr_head(&rx->pipe, head);
-		mxlk_raise_irq(mxlk, DATA_RECEIVED);
+		intel_xpcie_set_tdr_head(&rx->pipe, head);
+		intel_xpcie_raise_irq(xpcie, DATA_RECEIVED);
 	}
 
 task_exit:
 	if (reset_work)
-		mxlk_start_rx(mxlk, delay);
+		intel_xpcie_start_rx(xpcie, delay);
 }
 
-static void mxlk_tx_event_handler(struct work_struct *work)
+static void intel_xpcie_tx_event_handler(struct work_struct *work)
 {
-	struct mxlk *mxlk = container_of(work, struct mxlk, tx_event.work);
+	struct xpcie *xpcie = container_of(work, struct xpcie, tx_event.work);
 
 	int rc;
 	u32 head, tail, ndesc;
@@ -410,43 +423,44 @@ static void mxlk_tx_event_handler(struct work_struct *work)
 	u32 initial_tail;
 	int descs_num = 0;
 	int chan = 0;
-	struct mxlk_stream *tx = &mxlk->tx;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct mxlk_dma_ll_desc *desc;
-	struct mxlk_buf_desc *bd_head, *bd_tail, *bd;
-	struct mxlk_transfer_desc *td;
+	struct xpcie_stream *tx = &xpcie->tx;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct xpcie_dma_ll_desc *desc;
+	struct xpcie_buf_desc *bd_head, *bd_tail, *bd;
+	struct xpcie_transfer_desc *td;
 	size_t bytes = 0, buffers = 0;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.tx_event_runs, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_event_runs, 1);
 
-	if (mxlk_get_host_status(mxlk) != MXLK_STATUS_RUN)
+	if (intel_xpcie_get_host_status(xpcie) != XPCIE_STATUS_RUN)
 		return;
 
 	bd_head = bd_tail = NULL;
 	ndesc = tx->pipe.ndesc;
-	initial_tail = tail = mxlk_get_tdr_tail(&tx->pipe);
-	head = mxlk_get_tdr_head(&tx->pipe);
+	initial_tail = tail = intel_xpcie_get_tdr_tail(&tx->pipe);
+	head = intel_xpcie_get_tdr_head(&tx->pipe);
 
 	// add new entries
-	while (MXLK_CIRCULAR_INC(tail, ndesc) != head) {
-		bd = mxlk_list_get(&mxlk->write);
+	while (XPCIE_CIRCULAR_INC(tail, ndesc) != head) {
+		bd = intel_xpcie_list_get(&xpcie->write);
 		if (!bd)
 			break;
 
 		if (!tx_pool_coherent) {
-			if (mxlk_map_dma(mxlk, bd, DMA_TO_DEVICE)) {
-				dev_err(mxlk_to_dev(mxlk),
+			if (intel_xpcie_map_dma(xpcie, bd, DMA_TO_DEVICE)) {
+				dev_err(xpcie_to_dev(xpcie),
 				"dma mapping error bd addr %p, size %zu\n",
 				bd->data, bd->length);
-				mxlk_list_put_head(&mxlk->write, bd);
+				intel_xpcie_list_put_head(&xpcie->write, bd);
 				break;
 			}
 		}
 
 		td = tx->pipe.tdr + tail;
-		address = mxlk_get_td_address(td);
+		address = intel_xpcie_get_td_address(td);
 
-		desc = &mxlk_epf->tx_desc_buf[chan].virt[descs_num++];
+		desc = &xpcie_epf->tx_desc_buf[chan].virt[descs_num++];
 		desc->dma_transfer_size = bd->length;
 		desc->src_addr = bd->phys;
 		desc->dst_addr = address;
@@ -457,177 +471,181 @@ static void mxlk_tx_event_handler(struct work_struct *work)
 			bd_head = bd;
 		bd_tail = bd;
 
-		tail = MXLK_CIRCULAR_INC(tail, ndesc);
+		tail = XPCIE_CIRCULAR_INC(tail, ndesc);
 	}
 
 	if (descs_num == 0)
 		goto task_exit;
 
-	rc = mxlk_copy_to_host_ll(mxlk, chan, descs_num);
+	rc = intel_xpcie_copy_to_host_ll(xpcie, chan, descs_num);
 
 	tail = initial_tail;
 	bd = bd_head;
 	while (bd) {
 		if (!tx_pool_coherent)
-			mxlk_unmap_dma(mxlk, bd, DMA_TO_DEVICE);
+			intel_xpcie_unmap_dma(xpcie, bd, DMA_TO_DEVICE);
 
 		if (rc) {
 			bd = bd->next;
 			continue;
 		}
 
-		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.cnts, 1);
-		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.bytes, bd->length);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_krn.cnts, 1);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_krn.bytes,
+				       bd->length);
 
 		td = tx->pipe.tdr + tail;
-		mxlk_set_td_status(td, MXLK_DESC_STATUS_SUCCESS);
-		mxlk_set_td_length(td, bd->length);
-		mxlk_set_td_interface(td, bd->interface);
+		intel_xpcie_set_td_status(td, XPCIE_DESC_STATUS_SUCCESS);
+		intel_xpcie_set_td_length(td, bd->length);
+		intel_xpcie_set_td_interface(td, bd->interface);
 
 		bd = bd->next;
-		tail = MXLK_CIRCULAR_INC(tail, ndesc);
+		tail = XPCIE_CIRCULAR_INC(tail, ndesc);
 	}
 
 	if (rc) {
-		dev_err(mxlk_to_dev(mxlk), "failed to DMA to host (%d)\n", rc);
-		mxlk_list_put_head(&mxlk->write, bd_head);
+		dev_err(xpcie_to_dev(xpcie),
+			"failed to DMA to host (%d)\n", rc);
+		intel_xpcie_list_put_head(&xpcie->write, bd_head);
 		return;
 	}
 
-	mxlk_free_tx_bd(mxlk, bd_head);
+	intel_xpcie_free_tx_bd(xpcie, bd_head);
 
-	if (mxlk_get_tdr_tail(&tx->pipe) != tail) {
-		mxlk_set_tdr_tail(&tx->pipe, tail);
-		mxlk_raise_irq(mxlk, DATA_SENT);
-		mxlk_debug_incr(mxlk, &mxlk->stats.send_ints, 1);
+	if (intel_xpcie_get_tdr_tail(&tx->pipe) != tail) {
+		intel_xpcie_set_tdr_tail(&tx->pipe, tail);
+		intel_xpcie_raise_irq(xpcie, DATA_SENT);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.send_ints, 1);
 	}
 
 task_exit:
-	mxlk_list_info(&mxlk->write, &bytes, &buffers);
+	intel_xpcie_list_info(&xpcie->write, &bytes, &buffers);
 	if (buffers) {
-		mxlk->tx_pending = true;
-		head = mxlk_get_tdr_head(&tx->pipe);
-		if (MXLK_CIRCULAR_INC(tail, ndesc) != head)
-			mxlk_start_tx(mxlk, 0);
+		xpcie->tx_pending = true;
+		head = intel_xpcie_get_tdr_head(&tx->pipe);
+		if (XPCIE_CIRCULAR_INC(tail, ndesc) != head)
+			intel_xpcie_start_tx(xpcie, 0);
 	} else {
-		mxlk->tx_pending = false;
+		xpcie->tx_pending = false;
 	}
 }
 
-static irqreturn_t mxlk_core_irq_cb(int irq, void *args)
+static irqreturn_t intel_xpcie_core_irq_cb(int irq, void *args)
 {
-	struct mxlk *mxlk = args;
+	struct xpcie *xpcie = args;
 
-	if (mxlk_get_doorbell(mxlk, TO_DEVICE, DATA_SENT)) {
-		mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_SENT, 0);
-		mxlk_debug_incr(mxlk, &mxlk->stats.interrupts, 1);
-		mxlk_start_rx(mxlk, 0);
+	if (intel_xpcie_get_doorbell(xpcie, TO_DEVICE, DATA_SENT)) {
+		intel_xpcie_set_doorbell(xpcie, TO_DEVICE, DATA_SENT, 0);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.interrupts, 1);
+		intel_xpcie_start_rx(xpcie, 0);
 	}
-	if (mxlk_get_doorbell(mxlk, TO_DEVICE, DATA_RECEIVED)) {
-		mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_RECEIVED, 0);
-		if (mxlk->tx_pending)
-			mxlk_start_tx(mxlk, 0);
+	if (intel_xpcie_get_doorbell(xpcie, TO_DEVICE, DATA_RECEIVED)) {
+		intel_xpcie_set_doorbell(xpcie, TO_DEVICE, DATA_RECEIVED, 0);
+		if (xpcie->tx_pending)
+			intel_xpcie_start_tx(xpcie, 0);
 	}
 
 	return IRQ_HANDLED;
 }
 
-static int mxlk_events_init(struct mxlk *mxlk)
+static int intel_xpcie_events_init(struct xpcie *xpcie)
 {
-	mxlk->rx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+	xpcie->rx_wq = alloc_ordered_workqueue(XPCIE_DRIVER_NAME,
 					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
-	if (!mxlk->rx_wq) {
-		dev_err(mxlk_to_dev(mxlk), "failed to allocate workqueue\n");
+	if (!xpcie->rx_wq) {
+		dev_err(xpcie_to_dev(xpcie), "failed to allocate workqueue\n");
 		return -ENOMEM;
 	}
 
-	if (!mxlk->legacy_a0) {
-		mxlk->tx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+	if (!xpcie->legacy_a0) {
+		xpcie->tx_wq = alloc_ordered_workqueue(XPCIE_DRIVER_NAME,
 					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
-		if (!mxlk->tx_wq) {
-			dev_err(mxlk_to_dev(mxlk),
+		if (!xpcie->tx_wq) {
+			dev_err(xpcie_to_dev(xpcie),
 				"failed to allocate workqueue\n");
-			destroy_workqueue(mxlk->rx_wq);
+			destroy_workqueue(xpcie->rx_wq);
 			return -ENOMEM;
 		}
 	}
 
-	INIT_DELAYED_WORK(&mxlk->rx_event, mxlk_rx_event_handler);
-	INIT_DELAYED_WORK(&mxlk->tx_event, mxlk_tx_event_handler);
+	INIT_DELAYED_WORK(&xpcie->rx_event, intel_xpcie_rx_event_handler);
+	INIT_DELAYED_WORK(&xpcie->tx_event, intel_xpcie_tx_event_handler);
 
 	return 0;
 }
 
-static void mxlk_events_cleanup(struct mxlk *mxlk)
+static void intel_xpcie_events_cleanup(struct xpcie *xpcie)
 {
-	cancel_delayed_work_sync(&mxlk->rx_event);
-	cancel_delayed_work_sync(&mxlk->tx_event);
+	cancel_delayed_work_sync(&xpcie->rx_event);
+	cancel_delayed_work_sync(&xpcie->tx_event);
 
-	destroy_workqueue(mxlk->rx_wq);
-	if (!mxlk->legacy_a0)
-		destroy_workqueue(mxlk->tx_wq);
+	destroy_workqueue(xpcie->rx_wq);
+	if (!xpcie->legacy_a0)
+		destroy_workqueue(xpcie->tx_wq);
 }
 
-int mxlk_core_init(struct mxlk *mxlk)
+int intel_xpcie_core_init(struct xpcie *xpcie)
 {
 	int error;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
 
-	mxlk_init_debug(mxlk, &mxlk_epf->epf->dev);
+	intel_xpcie_init_debug(xpcie, &xpcie_epf->epf->dev);
 
-	global_mxlk = mxlk;
+	global_xpcie = xpcie;
 
-	mxlk_set_version(mxlk);
-	mxlk_set_cap_txrx(mxlk);
+	intel_xpcie_set_version(xpcie);
+	intel_xpcie_set_cap_txrx(xpcie);
 
-	error = mxlk_events_init(mxlk);
+	error = intel_xpcie_events_init(xpcie);
 	if (error)
 		return error;
 
-	error = mxlk_discover_txrx(mxlk);
+	error = intel_xpcie_discover_txrx(xpcie);
 	if (error)
 		goto error_txrx;
 
-	mxlk_interfaces_init(mxlk);
+	intel_xpcie_interfaces_init(xpcie);
 
-	mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_SENT, 0);
-	mxlk_set_doorbell(mxlk, TO_DEVICE, DATA_RECEIVED, 0);
-	mxlk_set_doorbell(mxlk, TO_DEVICE, DEV_EVENT, NO_OP);
-	mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_SENT, 0);
-	mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_RECEIVED, 0);
-	mxlk_set_doorbell(mxlk, FROM_DEVICE, DEV_EVENT, NO_OP);
+	intel_xpcie_set_doorbell(xpcie, TO_DEVICE, DATA_SENT, 0);
+	intel_xpcie_set_doorbell(xpcie, TO_DEVICE, DATA_RECEIVED, 0);
+	intel_xpcie_set_doorbell(xpcie, TO_DEVICE, DEV_EVENT, NO_OP);
+	intel_xpcie_set_doorbell(xpcie, FROM_DEVICE, DATA_SENT, 0);
+	intel_xpcie_set_doorbell(xpcie, FROM_DEVICE, DATA_RECEIVED, 0);
+	intel_xpcie_set_doorbell(xpcie, FROM_DEVICE, DEV_EVENT, NO_OP);
 
-	mxlk_register_host_irq(mxlk, mxlk_core_irq_cb);
+	intel_xpcie_register_host_irq(xpcie, intel_xpcie_core_irq_cb);
 
 	return 0;
 
 error_txrx:
-	mxlk_events_cleanup(mxlk);
+	intel_xpcie_events_cleanup(xpcie);
 
 	return error;
 }
 
-void mxlk_core_cleanup(struct mxlk *mxlk)
+void intel_xpcie_core_cleanup(struct xpcie *xpcie)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
 
-	if (mxlk->status == MXLK_STATUS_RUN) {
-		mxlk_events_cleanup(mxlk);
-		mxlk_interfaces_cleanup(mxlk);
-		mxlk_txrx_cleanup(mxlk);
+	if (xpcie->status == XPCIE_STATUS_RUN) {
+		intel_xpcie_events_cleanup(xpcie);
+		intel_xpcie_interfaces_cleanup(xpcie);
+		intel_xpcie_txrx_cleanup(xpcie);
 	}
 
-	mxlk_uninit_debug(mxlk, &mxlk_epf->epf->dev);
+	intel_xpcie_uninit_debug(xpcie, &xpcie_epf->epf->dev);
 }
 
-int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
+int intel_xpcie_core_read(struct xpcie *xpcie, void *buffer, size_t *length,
 		   unsigned int timeout_ms)
 {
 	int ret = 0;
-	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	struct xpcie_interface *inf = &xpcie->interfaces[0];
 	size_t len = *length;
 	size_t remaining = len;
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 	unsigned long jiffies_start = jiffies;
 	long jiffies_passed = 0;
 	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
@@ -636,10 +654,10 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 	if (len == 0)
 		return -EINVAL;
 
-	if (mxlk->status != MXLK_STATUS_RUN)
+	if (xpcie->status != XPCIE_STATUS_RUN)
 		return -ENODEV;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.cnts, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.rx_usr.cnts, 1);
 
 	ret = mutex_lock_interruptible(&inf->rlock);
 	if (ret < 0)
@@ -650,7 +668,8 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 			mutex_unlock(&inf->rlock);
 			if (timeout_ms == 0) {
 				ret = wait_event_interruptible(
-					inf->rx_waitqueue, inf->data_available);
+					inf->rx_waitqueue,
+					inf->data_available);
 			} else {
 				ret = wait_event_interruptible_timeout(
 					inf->rx_waitqueue, inf->data_available,
@@ -658,7 +677,7 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 				if (ret == 0)
 					return -ETIME;
 			}
-			if (ret < 0 || mxlk->stop_flag)
+			if (ret < 0 || xpcie->stop_flag)
 				return -EINTR;
 
 			ret = mutex_lock_interruptible(&inf->rlock);
@@ -667,7 +686,7 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 		}
 
 		bd = (inf->partial_read) ? inf->partial_read :
-					   mxlk_list_get(&inf->read);
+					   intel_xpcie_list_get(&inf->read);
 
 		while (remaining && bd) {
 			size_t bcopy;
@@ -680,15 +699,17 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 			bd->data += bcopy;
 			bd->length -= bcopy;
 
-			mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.bytes, bcopy);
+			intel_xpcie_debug_incr(xpcie,
+					       &xpcie->stats.rx_usr.bytes,
+					       bcopy);
 
 			if (bd->length == 0) {
-				mxlk_free_rx_bd(mxlk, bd);
-				bd = mxlk_list_get(&inf->read);
+				intel_xpcie_free_rx_bd(xpcie, bd);
+				bd = intel_xpcie_list_get(&inf->read);
 			}
 		}
 
-		// save for next time
+		/* save for next time */
 		inf->partial_read = bd;
 
 		if (!bd)
@@ -705,14 +726,14 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 	return 0;
 }
 
-int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
+int intel_xpcie_core_write(struct xpcie *xpcie, void *buffer, size_t *length,
 		    unsigned int timeout_ms)
 {
 	int ret;
 	size_t len = *length;
 	size_t remaining = len;
-	struct mxlk_interface *inf = &mxlk->interfaces[0];
-	struct mxlk_buf_desc *bd, *head;
+	struct xpcie_interface *inf = &xpcie->interfaces[0];
+	struct xpcie_buf_desc *bd, *head;
 	unsigned long jiffies_start = jiffies;
 	long jiffies_passed = 0;
 	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
@@ -721,41 +742,42 @@ int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
 	if (len == 0)
 		return -EINVAL;
 
-	if (mxlk->status != MXLK_STATUS_RUN)
+	if (xpcie->status != XPCIE_STATUS_RUN)
 		return -ENODEV;
 
-	if (mxlk_get_host_status(mxlk) != MXLK_STATUS_RUN)
+	if (intel_xpcie_get_host_status(xpcie) != XPCIE_STATUS_RUN)
 		return -ENODEV;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.cnts, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_usr.cnts, 1);
 
-	ret = mutex_lock_interruptible(&mxlk->wlock);
+	ret = mutex_lock_interruptible(&xpcie->wlock);
 	if (ret < 0)
 		return -EINTR;
 
 	do {
-		bd = head = mxlk_alloc_tx_bd(mxlk);
+		bd = head = intel_xpcie_alloc_tx_bd(xpcie);
 		while (!head) {
-			mutex_unlock(&mxlk->wlock);
+			mutex_unlock(&xpcie->wlock);
 			if (timeout_ms == 0) {
 				ret = wait_event_interruptible(
-						mxlk->tx_waitqueue,
-						!mxlk->no_tx_buffer);
+						xpcie->tx_waitqueue,
+						!xpcie->no_tx_buffer);
 			} else {
 				ret = wait_event_interruptible_timeout(
-					mxlk->tx_waitqueue, !mxlk->no_tx_buffer,
+					xpcie->tx_waitqueue,
+					!xpcie->no_tx_buffer,
 					jiffies_timeout - jiffies_passed);
 				if (ret == 0)
 					return -ETIME;
 			}
-			if (ret < 0 || mxlk->stop_flag)
+			if (ret < 0 || xpcie->stop_flag)
 				return -EINTR;
 
-			ret = mutex_lock_interruptible(&mxlk->wlock);
+			ret = mutex_lock_interruptible(&xpcie->wlock);
 			if (ret < 0)
 				return -EINTR;
 
-			bd = head = mxlk_alloc_tx_bd(mxlk);
+			bd = head = intel_xpcie_alloc_tx_bd(xpcie);
 		}
 
 		while (remaining && bd) {
@@ -769,16 +791,18 @@ int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
 			bd->length = bcopy;
 			bd->interface = inf->id;
 
-			mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.bytes, bcopy);
+			intel_xpcie_debug_incr(xpcie,
+					       &xpcie->stats.tx_usr.bytes,
+					       bcopy);
 
 			if (remaining) {
-				bd->next = mxlk_alloc_tx_bd(mxlk);
+				bd->next = intel_xpcie_alloc_tx_bd(xpcie);
 				bd = bd->next;
 			}
 		}
 
-		mxlk_list_put(&inf->mxlk->write, head);
-		mxlk_start_tx(mxlk, 0);
+		intel_xpcie_list_put(&inf->xpcie->write, head);
+		intel_xpcie_start_tx(xpcie, 0);
 
 		*length = len - remaining;
 
@@ -786,12 +810,12 @@ int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
 	} while (remaining > 0 && (jiffies_passed < jiffies_timeout ||
 				   timeout_ms == 0));
 
-	mutex_unlock(&mxlk->wlock);
+	mutex_unlock(&xpcie->wlock);
 
 	return 0;
 }
 
-struct mxlk *mxlk_core_get_by_id(uint32_t sw_device_id)
+struct xpcie *intel_xpcie_core_get_by_id(uint32_t sw_device_id)
 {
-	return (sw_device_id == xlink_sw_id) ? global_mxlk : NULL;
+	return (sw_device_id == xlink_sw_id) ? global_xpcie : NULL;
 }
diff --git a/drivers/misc/xlink-pcie/local_host/dma.c b/drivers/misc/xlink-pcie/local_host/dma.c
index 302ac13056d5..86da4c5deb51 100644
--- a/drivers/misc/xlink-pcie/local_host/dma.c
+++ b/drivers/misc/xlink-pcie/local_host/dma.c
@@ -12,7 +12,7 @@
 
 #include "dma.h"
 #include "struct.h"
-#include "../common/xlink_pcie.h"
+#include "../common/xpcie.h"
 
 #define DMA_DBI_OFFSET (0x380000)
 
@@ -146,7 +146,7 @@ struct __packed pcie_dma_chan {
 	u32 dma_llp_high;
 };
 
-enum mxlk_ep_engine_type {
+enum xpcie_ep_engine_type {
 	WRITE_ENGINE,
 	READ_ENGINE
 };
@@ -158,7 +158,7 @@ static u32 dma_chan_offset[2][DMA_CHAN_NUM] = {
 	{ 0x300, 0x500, 0x700, 0x900 }
 };
 
-static void __iomem *mxlk_ep_get_dma_base(struct pci_epf *epf)
+static void __iomem *intel_xpcie_ep_get_dma_base(struct pci_epf *epf)
 {
 	struct pci_epc *epc = epf->epc;
 	struct dw_pcie_ep *ep = epc_get_drvdata(epc);
@@ -167,8 +167,8 @@ static void __iomem *mxlk_ep_get_dma_base(struct pci_epf *epf)
 	return pci->dbi_base + DMA_DBI_OFFSET;
 }
 
-static int mxlk_ep_dma_disable(void __iomem *dma_base,
-			       enum mxlk_ep_engine_type rw)
+static int intel_xpcie_ep_dma_disable(void __iomem *dma_base,
+			       enum xpcie_ep_engine_type rw)
 {
 	int i;
 	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)(dma_base);
@@ -206,8 +206,8 @@ static int mxlk_ep_dma_disable(void __iomem *dma_base,
 	return -EBUSY;
 }
 
-static void mxlk_ep_dma_enable(void __iomem *dma_base,
-			       enum mxlk_ep_engine_type rw)
+static void intel_xpcie_ep_dma_enable(void __iomem *dma_base,
+			       enum xpcie_ep_engine_type rw)
 {
 	int i;
 	u32 offset;
@@ -259,36 +259,36 @@ static void mxlk_ep_dma_enable(void __iomem *dma_base,
  * The DMA controller may start the wrong channel if doorbell occurs at the
  * same time as controller is transitioning to L1.
  */
-static int mxlk_ep_dma_doorbell(struct mxlk_epf *mxlk_epf, int chan,
+static int intel_xpcie_ep_dma_doorbell(struct xpcie_epf *xpcie_epf, int chan,
 				void __iomem *doorbell)
 {
 	int rc = 0;
 	int i = 20;
 	u32 val, pm_val;
 
-	val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
+	val = ioread32(xpcie_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
 	iowrite32(val | APP_XFER_PENDING,
-		  mxlk_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
-	pm_val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_SII_PM_STATE_1);
+		  xpcie_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
+	pm_val = ioread32(xpcie_epf->apb_base + PCIE_REGS_PCIE_SII_PM_STATE_1);
 	while (pm_val & PM_LINKST_IN_L1) {
 		if (i-- < 0) {
 			rc = -ETIME;
 			break;
 		}
 		udelay(5);
-		pm_val = ioread32(mxlk_epf->apb_base +
+		pm_val = ioread32(xpcie_epf->apb_base +
 				  PCIE_REGS_PCIE_SII_PM_STATE_1);
 	}
 
 	iowrite32((u32)chan, doorbell);
 
 	iowrite32(val & ~APP_XFER_PENDING,
-		  mxlk_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
+		  xpcie_epf->apb_base + PCIE_REGS_PCIE_APP_CNTRL);
 
 	return rc;
 }
 
-static int mxlk_ep_dma_err_status(void __iomem *err_status, int chan)
+static int intel_xpcie_ep_dma_err_status(void __iomem *err_status, int chan)
 {
 	if (ioread32(err_status) &
 	    (DMA_AR_ERROR_CH_MASK(chan) | DMA_LL_ERROR_CH_MASK(chan)))
@@ -297,7 +297,8 @@ static int mxlk_ep_dma_err_status(void __iomem *err_status, int chan)
 	return 0;
 }
 
-static int mxlk_ep_dma_rd_err_status_high(void __iomem *err_status, int chan)
+static int intel_xpcie_ep_dma_rd_err_status_high(void __iomem *err_status,
+						 int chan)
 {
 	if (ioread32(err_status) &
 	    (DMA_UNREQ_ERROR_CH_MASK(chan) |
@@ -309,12 +310,12 @@ static int mxlk_ep_dma_rd_err_status_high(void __iomem *err_status, int chan)
 	return 0;
 }
 
-static void mxlk_ep_dma_setup_ll_descs(struct pcie_dma_chan *dma_chan,
-				       struct mxlk_dma_ll_desc_buf *desc_buf,
+static void intel_xpcie_ep_dma_setup_ll_descs(struct pcie_dma_chan *dma_chan,
+				       struct xpcie_dma_ll_desc_buf *desc_buf,
 				       int descs_num)
 {
 	int i = 0;
-	struct mxlk_dma_ll_desc *descs = desc_buf->virt;
+	struct xpcie_dma_ll_desc *descs = desc_buf->virt;
 
 	/* Setup linked list descriptors */
 	for (i = 0; i < descs_num; i++)
@@ -332,16 +333,16 @@ static void mxlk_ep_dma_setup_ll_descs(struct pcie_dma_chan *dma_chan,
 
 }
 
-int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
+int intel_xpcie_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 {
 	int i, rc = 0;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
-	void __iomem *dma_base = mxlk_epf->dma_base;
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	void __iomem *dma_base = xpcie_epf->dma_base;
 	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
 	struct pcie_dma_chan *dma_chan;
-	struct mxlk_dma_ll_desc_buf *desc_buf;
+	struct xpcie_dma_ll_desc_buf *desc_buf;
 
-	if (descs_num <= 0 || descs_num > MXLK_NUM_TX_DESCS)
+	if (descs_num <= 0 || descs_num > XPCIE_NUM_TX_DESCS)
 		return -EINVAL;
 
 	if (chan < 0 || chan >= DMA_CHAN_NUM)
@@ -350,12 +351,13 @@ int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 	dma_chan = (struct pcie_dma_chan *)
 		(dma_base + dma_chan_offset[WRITE_ENGINE][chan]);
 
-	desc_buf = &mxlk_epf->tx_desc_buf[chan];
+	desc_buf = &xpcie_epf->tx_desc_buf[chan];
 
-	mxlk_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
+	intel_xpcie_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
 
 	/* Start DMA transfer. */
-	rc = mxlk_ep_dma_doorbell(mxlk_epf, chan, &dma_reg->dma_write_doorbell);
+	rc = intel_xpcie_ep_dma_doorbell(xpcie_epf, chan,
+					 &dma_reg->dma_write_doorbell);
 	if (rc)
 		return rc;
 
@@ -371,7 +373,8 @@ int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 		goto cleanup;
 	}
 
-	rc = mxlk_ep_dma_err_status(&dma_reg->dma_write_err_status, chan);
+	rc = intel_xpcie_ep_dma_err_status(&dma_reg->dma_write_err_status,
+					   chan);
 
 cleanup:
 	/* Clear the done/abort interrupt. */
@@ -379,23 +382,23 @@ int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 		  &dma_reg->dma_write_int_clear);
 
 	if (rc) {
-		mxlk_ep_dma_disable(dma_base, WRITE_ENGINE);
-		mxlk_ep_dma_enable(dma_base, WRITE_ENGINE);
+		intel_xpcie_ep_dma_disable(dma_base, WRITE_ENGINE);
+		intel_xpcie_ep_dma_enable(dma_base, WRITE_ENGINE);
 	}
 
 	return rc;
 }
 
-int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
+int intel_xpcie_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 {
 	int i, rc = 0;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
-	void __iomem *dma_base = mxlk_epf->dma_base;
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	void __iomem *dma_base = xpcie_epf->dma_base;
 	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
 	struct pcie_dma_chan *dma_chan;
-	struct mxlk_dma_ll_desc_buf *desc_buf;
+	struct xpcie_dma_ll_desc_buf *desc_buf;
 
-	if (descs_num <= 0 || descs_num > MXLK_NUM_RX_DESCS)
+	if (descs_num <= 0 || descs_num > XPCIE_NUM_RX_DESCS)
 		return -EINVAL;
 
 	if (chan < 0 || chan >= DMA_CHAN_NUM)
@@ -404,12 +407,13 @@ int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 	dma_chan = (struct pcie_dma_chan *)
 		(dma_base + dma_chan_offset[READ_ENGINE][chan]);
 
-	desc_buf = &mxlk_epf->rx_desc_buf[chan];
+	desc_buf = &xpcie_epf->rx_desc_buf[chan];
 
-	mxlk_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
+	intel_xpcie_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
 
 	/* Start DMA transfer. */
-	rc = mxlk_ep_dma_doorbell(mxlk_epf, chan, &dma_reg->dma_read_doorbell);
+	rc = intel_xpcie_ep_dma_doorbell(xpcie_epf, chan,
+					 &dma_reg->dma_read_doorbell);
 	if (rc)
 		return rc;
 
@@ -425,9 +429,10 @@ int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 		goto cleanup;
 	}
 
-	rc = mxlk_ep_dma_err_status(&dma_reg->dma_read_err_status_low, chan);
+	rc = intel_xpcie_ep_dma_err_status(&dma_reg->dma_read_err_status_low,
+					   chan);
 	if (!rc) {
-		rc = mxlk_ep_dma_rd_err_status_high(
+		rc = intel_xpcie_ep_dma_rd_err_status_high(
 			&dma_reg->dma_read_err_status_high, chan);
 	}
 cleanup:
@@ -436,75 +441,75 @@ int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 		  &dma_reg->dma_read_int_clear);
 
 	if (rc) {
-		mxlk_ep_dma_disable(dma_base, READ_ENGINE);
-		mxlk_ep_dma_enable(dma_base, READ_ENGINE);
+		intel_xpcie_ep_dma_disable(dma_base, READ_ENGINE);
+		intel_xpcie_ep_dma_enable(dma_base, READ_ENGINE);
 	}
 
 	return rc;
 }
 
-static void mxlk_ep_dma_free_ll_descs_mem(struct mxlk_epf *mxlk_epf)
+static void intel_xpcie_ep_dma_free_ll_descs_mem(struct xpcie_epf *xpcie_epf)
 {
 	int i;
-	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
+	struct device *dma_dev = xpcie_epf->epf->epc->dev.parent;
 
 	for (i = 0; i < DMA_CHAN_NUM; i++) {
-		if (mxlk_epf->tx_desc_buf[i].virt) {
+		if (xpcie_epf->tx_desc_buf[i].virt) {
 			dma_free_coherent(dma_dev,
-					  mxlk_epf->tx_desc_buf[i].size,
-					  mxlk_epf->tx_desc_buf[i].virt,
-					  mxlk_epf->tx_desc_buf[i].phys);
+					  xpcie_epf->tx_desc_buf[i].size,
+					  xpcie_epf->tx_desc_buf[i].virt,
+					  xpcie_epf->tx_desc_buf[i].phys);
 		}
-		if (mxlk_epf->rx_desc_buf[i].virt) {
+		if (xpcie_epf->rx_desc_buf[i].virt) {
 			dma_free_coherent(dma_dev,
-					  mxlk_epf->rx_desc_buf[i].size,
-					  mxlk_epf->rx_desc_buf[i].virt,
-					  mxlk_epf->rx_desc_buf[i].phys);
+					  xpcie_epf->rx_desc_buf[i].size,
+					  xpcie_epf->rx_desc_buf[i].virt,
+					  xpcie_epf->rx_desc_buf[i].phys);
 		}
 
-		memset(&mxlk_epf->tx_desc_buf[i], 0,
-		       sizeof(struct mxlk_dma_ll_desc_buf));
-		memset(&mxlk_epf->rx_desc_buf[i], 0,
-		       sizeof(struct mxlk_dma_ll_desc_buf));
+		memset(&xpcie_epf->tx_desc_buf[i], 0,
+		       sizeof(struct xpcie_dma_ll_desc_buf));
+		memset(&xpcie_epf->rx_desc_buf[i], 0,
+		       sizeof(struct xpcie_dma_ll_desc_buf));
 	}
 }
 
-static int mxlk_ep_dma_alloc_ll_descs_mem(struct mxlk_epf *mxlk_epf)
+static int intel_xpcie_ep_dma_alloc_ll_descs_mem(struct xpcie_epf *xpcie_epf)
 {
 	int i;
-	struct device *dma_dev = mxlk_epf->epf->epc->dev.parent;
-	int tx_num = MXLK_NUM_TX_DESCS + 1;
-	int rx_num = MXLK_NUM_RX_DESCS + 1;
-	size_t tx_size = tx_num * sizeof(struct mxlk_dma_ll_desc);
-	size_t rx_size = rx_num * sizeof(struct mxlk_dma_ll_desc);
+	struct device *dma_dev = xpcie_epf->epf->epc->dev.parent;
+	int tx_num = XPCIE_NUM_TX_DESCS + 1;
+	int rx_num = XPCIE_NUM_RX_DESCS + 1;
+	size_t tx_size = tx_num * sizeof(struct xpcie_dma_ll_desc);
+	size_t rx_size = rx_num * sizeof(struct xpcie_dma_ll_desc);
 
 	for (i = 0; i < DMA_CHAN_NUM; i++) {
-		mxlk_epf->tx_desc_buf[i].virt =
+		xpcie_epf->tx_desc_buf[i].virt =
 			dma_alloc_coherent(dma_dev, tx_size,
-					   &mxlk_epf->tx_desc_buf[i].phys,
+					   &xpcie_epf->tx_desc_buf[i].phys,
 					   GFP_KERNEL);
-		mxlk_epf->rx_desc_buf[i].virt =
+		xpcie_epf->rx_desc_buf[i].virt =
 			dma_alloc_coherent(dma_dev, rx_size,
-					   &mxlk_epf->rx_desc_buf[i].phys,
+					   &xpcie_epf->rx_desc_buf[i].phys,
 					   GFP_KERNEL);
 
-		if (!mxlk_epf->tx_desc_buf[i].virt ||
-		    !mxlk_epf->rx_desc_buf[i].virt) {
-			mxlk_ep_dma_free_ll_descs_mem(mxlk_epf);
+		if (!xpcie_epf->tx_desc_buf[i].virt ||
+		    !xpcie_epf->rx_desc_buf[i].virt) {
+			intel_xpcie_ep_dma_free_ll_descs_mem(xpcie_epf);
 			return -ENOMEM;
 		}
 
-		mxlk_epf->tx_desc_buf[i].size = tx_size;
-		mxlk_epf->rx_desc_buf[i].size = rx_size;
+		xpcie_epf->tx_desc_buf[i].size = tx_size;
+		xpcie_epf->rx_desc_buf[i].size = rx_size;
 	}
 	return 0;
 }
 
-bool mxlk_ep_dma_enabled(struct pci_epf *epf)
+bool intel_xpcie_ep_dma_enabled(struct pci_epf *epf)
 {
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)
-					mxlk_epf->dma_base;
+					xpcie_epf->dma_base;
 	void __iomem *w_engine_en = &dma_reg->dma_write_engine_en;
 	void __iomem *r_engine_en = &dma_reg->dma_read_engine_en;
 
@@ -512,45 +517,45 @@ bool mxlk_ep_dma_enabled(struct pci_epf *epf)
 		(ioread32(r_engine_en) & DMA_ENGINE_EN_MASK);
 }
 
-int mxlk_ep_dma_reset(struct pci_epf *epf)
+int intel_xpcie_ep_dma_reset(struct pci_epf *epf)
 {
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
 	/* Disable the DMA read/write engine. */
-	if (mxlk_ep_dma_disable(mxlk_epf->dma_base, WRITE_ENGINE) ||
-	    mxlk_ep_dma_disable(mxlk_epf->dma_base, READ_ENGINE))
+	if (intel_xpcie_ep_dma_disable(xpcie_epf->dma_base, WRITE_ENGINE) ||
+	    intel_xpcie_ep_dma_disable(xpcie_epf->dma_base, READ_ENGINE))
 		return -EBUSY;
 
-	mxlk_ep_dma_enable(mxlk_epf->dma_base, WRITE_ENGINE);
-	mxlk_ep_dma_enable(mxlk_epf->dma_base, READ_ENGINE);
+	intel_xpcie_ep_dma_enable(xpcie_epf->dma_base, WRITE_ENGINE);
+	intel_xpcie_ep_dma_enable(xpcie_epf->dma_base, READ_ENGINE);
 
 	return 0;
 }
 
-int mxlk_ep_dma_uninit(struct pci_epf *epf)
+int intel_xpcie_ep_dma_uninit(struct pci_epf *epf)
 {
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
-	if (mxlk_ep_dma_disable(mxlk_epf->dma_base, WRITE_ENGINE) ||
-	    mxlk_ep_dma_disable(mxlk_epf->dma_base, READ_ENGINE))
+	if (intel_xpcie_ep_dma_disable(xpcie_epf->dma_base, WRITE_ENGINE) ||
+	    intel_xpcie_ep_dma_disable(xpcie_epf->dma_base, READ_ENGINE))
 		return -EBUSY;
 
-	mxlk_ep_dma_free_ll_descs_mem(mxlk_epf);
+	intel_xpcie_ep_dma_free_ll_descs_mem(xpcie_epf);
 
 	return 0;
 }
 
-int mxlk_ep_dma_init(struct pci_epf *epf)
+int intel_xpcie_ep_dma_init(struct pci_epf *epf)
 {
 	int rc = 0;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
-	mxlk_epf->dma_base = mxlk_ep_get_dma_base(epf);
+	xpcie_epf->dma_base = intel_xpcie_ep_get_dma_base(epf);
 
-	rc = mxlk_ep_dma_alloc_ll_descs_mem(mxlk_epf);
+	rc = intel_xpcie_ep_dma_alloc_ll_descs_mem(xpcie_epf);
 	if (rc)
 		return rc;
 
-	return mxlk_ep_dma_reset(epf);
+	return intel_xpcie_ep_dma_reset(epf);
 }
 
diff --git a/drivers/misc/xlink-pcie/local_host/dma.h b/drivers/misc/xlink-pcie/local_host/dma.h
index cd75666d3f66..5ed65053fcd2 100644
--- a/drivers/misc/xlink-pcie/local_host/dma.h
+++ b/drivers/misc/xlink-pcie/local_host/dma.h
@@ -7,18 +7,18 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_DMA_HEADER_
-#define MXLK_DMA_HEADER_
+#ifndef XPCIE_DMA_HEADER_
+#define XPCIE_DMA_HEADER_
 
 #include <linux/types.h>
 #include <linux/pci-epc.h>
 #include <linux/pci-epf.h>
 
-int mxlk_ep_dma_init(struct pci_epf *epf);
-int mxlk_ep_dma_uninit(struct pci_epf *epf);
-int mxlk_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num);
-int mxlk_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num);
-bool mxlk_ep_dma_enabled(struct pci_epf *epf);
-int mxlk_ep_dma_reset(struct pci_epf *epf);
+int intel_xpcie_ep_dma_init(struct pci_epf *epf);
+int intel_xpcie_ep_dma_uninit(struct pci_epf *epf);
+int intel_xpcie_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num);
+int intel_xpcie_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num);
+bool intel_xpcie_ep_dma_enabled(struct pci_epf *epf);
+int intel_xpcie_ep_dma_reset(struct pci_epf *epf);
 
-#endif // MXLK_DMA_HEADER_
+#endif // XPCIE_DMA_HEADER_
diff --git a/drivers/misc/xlink-pcie/local_host/epf.c b/drivers/misc/xlink-pcie/local_host/epf.c
index 47117fc6eff0..04d9b08228ba 100644
--- a/drivers/misc/xlink-pcie/local_host/epf.c
+++ b/drivers/misc/xlink-pcie/local_host/epf.c
@@ -17,7 +17,7 @@
 #include <linux/pci_ids.h>
 #include <linux/reboot.h>
 #include <linux/xlink_drv_inf.h>
-#include "../common/xlink_pcie.h"
+#include "../common/xpcie.h"
 #include "../common/core.h"
 #include "../common/util.h"
 #include "../common/boot.h"
@@ -41,7 +41,7 @@
 #define PCIE_REGS_PCIE_ERR_INTR_FLAGS 0x24
 #define LINK_REQ_RST_FLG BIT(15)
 
-static struct pci_epf_header mxlk_pcie_header = {
+static struct pci_epf_header xpcie_header = {
 	.vendorid = PCI_VENDOR_ID_INTEL,
 	.deviceid = PCI_DEVICE_ID_INTEL_KEEMBAY,
 	.baseclass_code = PCI_BASE_CLASS_MULTIMEDIA,
@@ -50,7 +50,7 @@ static struct pci_epf_header mxlk_pcie_header = {
 	.subsys_id = 0x0,
 };
 
-static const struct pci_epf_device_id mxlk_pcie_epf_ids[] = {
+static const struct pci_epf_device_id xpcie_epf_ids[] = {
 	{
 		.name = "mxlk_pcie_epf",
 	},
@@ -59,66 +59,71 @@ static const struct pci_epf_device_id mxlk_pcie_epf_ids[] = {
 
 u32 xlink_sw_id;
 
-static irqreturn_t mxlk_err_interrupt(int irq, void *args)
+static irqreturn_t intel_xpcie_err_interrupt(int irq, void *args)
 {
-	struct mxlk *mxlk = args;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct xpcie *xpcie = args;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
 	u32 val;
 
-	val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_ERR_INTR_FLAGS);
+	val = ioread32(xpcie_epf->apb_base + PCIE_REGS_PCIE_ERR_INTR_FLAGS);
 	if (val & LINK_REQ_RST_FLG)
-		mxlk_ep_dma_reset(mxlk_epf->epf);
+		intel_xpcie_ep_dma_reset(xpcie_epf->epf);
 
-	iowrite32(val, mxlk_epf->apb_base + PCIE_REGS_PCIE_ERR_INTR_FLAGS);
+	iowrite32(val, xpcie_epf->apb_base + PCIE_REGS_PCIE_ERR_INTR_FLAGS);
 
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t mxlk_host_interrupt(int irq, void *args)
+static irqreturn_t intel_xpcie_host_interrupt(int irq, void *args)
 {
-	struct mxlk *mxlk = args;
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct xpcie *xpcie = args;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
 	u32 val;
 	u8 event;
 
-	val = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_INTR_FLAGS);
+	val = ioread32(xpcie_epf->apb_base + PCIE_REGS_PCIE_INTR_FLAGS);
 	if (val & LBC_CII_EVENT_FLAG) {
 		iowrite32(LBC_CII_EVENT_FLAG,
-			  mxlk_epf->apb_base + PCIE_REGS_PCIE_INTR_FLAGS);
+			  xpcie_epf->apb_base + PCIE_REGS_PCIE_INTR_FLAGS);
 
-		event = mxlk_get_doorbell(mxlk, TO_DEVICE, DEV_EVENT);
+		event = intel_xpcie_get_doorbell(xpcie, TO_DEVICE, DEV_EVENT);
 		if (unlikely(event != NO_OP)) {
-			mxlk_set_doorbell(mxlk, TO_DEVICE, DEV_EVENT, NO_OP);
+			intel_xpcie_set_doorbell(xpcie, TO_DEVICE,
+						 DEV_EVENT, NO_OP);
 			if (event == REQUEST_RESET)
 				orderly_reboot();
 			return IRQ_HANDLED;
 		}
 
-		if (likely(mxlk_epf->core_irq_callback))
-			mxlk_epf->core_irq_callback(irq, mxlk);
+		if (likely(xpcie_epf->core_irq_callback))
+			xpcie_epf->core_irq_callback(irq, xpcie);
 	}
 
 	return IRQ_HANDLED;
 }
 
-void mxlk_register_host_irq(struct mxlk *mxlk, irq_handler_t func)
+void intel_xpcie_register_host_irq(struct xpcie *xpcie, irq_handler_t func)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
 
-	mxlk_epf->core_irq_callback = func;
+	xpcie_epf->core_irq_callback = func;
 }
 
-int mxlk_raise_irq(struct mxlk *mxlk, enum mxlk_doorbell_type type)
+int intel_xpcie_raise_irq(struct xpcie *xpcie, enum xpcie_doorbell_type type)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct pci_epf *epf = mxlk_epf->epf;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct pci_epf *epf = xpcie_epf->epf;
 
-	mxlk_set_doorbell(mxlk, FROM_DEVICE, type, 1);
+	intel_xpcie_set_doorbell(xpcie, FROM_DEVICE, type, 1);
 
 	return pci_epc_raise_irq(epf->epc, epf->func_no, PCI_EPC_IRQ_MSI, 1);
 }
 
-static void __iomem *mxlk_epc_alloc_addr(struct pci_epc *epc,
+static void __iomem *intel_xpcie_epc_alloc_addr(struct pci_epc *epc,
 					 phys_addr_t *phys_addr, size_t size)
 {
 	void __iomem *virt_addr;
@@ -131,8 +136,9 @@ static void __iomem *mxlk_epc_alloc_addr(struct pci_epc *epc,
 	return virt_addr;
 }
 
-static void mxlk_epc_free_addr(struct pci_epc *epc, phys_addr_t phys_addr,
-			       void __iomem *virt_addr, size_t size)
+static void intel_xpcie_epc_free_addr(struct pci_epc *epc,
+				      phys_addr_t phys_addr,
+				      void __iomem *virt_addr, size_t size)
 {
 	unsigned long flags;
 
@@ -141,24 +147,28 @@ static void mxlk_epc_free_addr(struct pci_epc *epc, phys_addr_t phys_addr,
 	spin_unlock_irqrestore(&epc->lock, flags);
 }
 
-int mxlk_copy_from_host_ll(struct mxlk *mxlk, int chan, int descs_num)
+int intel_xpcie_copy_from_host_ll(struct xpcie *xpcie, int chan, int descs_num)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct pci_epf *epf = mxlk_epf->epf;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct pci_epf *epf = xpcie_epf->epf;
 
-	return mxlk_ep_dma_read_ll(epf, chan, descs_num);
+	return intel_xpcie_ep_dma_read_ll(epf, chan, descs_num);
 }
 
-int mxlk_copy_to_host_ll(struct mxlk *mxlk, int chan, int descs_num)
+int intel_xpcie_copy_to_host_ll(struct xpcie *xpcie, int chan, int descs_num)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
-	struct pci_epf *epf = mxlk_epf->epf;
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
+	struct pci_epf *epf = xpcie_epf->epf;
 
-	return mxlk_ep_dma_write_ll(epf, chan, descs_num);
+	return intel_xpcie_ep_dma_write_ll(epf, chan, descs_num);
 }
 
-static int mxlk_check_bar(struct pci_epf *epf, struct pci_epf_bar *epf_bar,
-			  enum pci_barno barno, size_t size, u8 reserved_bar)
+static int intel_xpcie_check_bar(struct pci_epf *epf,
+				 struct pci_epf_bar *epf_bar,
+				 enum pci_barno barno,
+				 size_t size, u8 reserved_bar)
 {
 	if (reserved_bar & (1 << barno)) {
 		dev_err(&epf->dev, "BAR%d is already reserved\n", barno);
@@ -173,7 +183,7 @@ static int mxlk_check_bar(struct pci_epf *epf, struct pci_epf_bar *epf_bar,
 	return 0;
 }
 
-static int mxlk_configure_bar(struct pci_epf *epf,
+static int intel_xpcie_configure_bar(struct pci_epf *epf,
 			      const struct pci_epc_features *epc_features)
 {
 	struct pci_epf_bar *epf_bar;
@@ -190,7 +200,7 @@ static int mxlk_configure_bar(struct pci_epf *epf,
 			epf_bar->size = epc_features->bar_fixed_size[i];
 
 		if (i == BAR_2) {
-			ret = mxlk_check_bar(epf, epf_bar, BAR_2,
+			ret = intel_xpcie_check_bar(epf, epf_bar, BAR_2,
 					     BAR2_MIN_SIZE,
 					     epc_features->reserved_bar);
 			if (ret)
@@ -198,7 +208,7 @@ static int mxlk_configure_bar(struct pci_epf *epf,
 		}
 
 		if (i == BAR_4) {
-			ret = mxlk_check_bar(epf, epf_bar, BAR_4,
+			ret = intel_xpcie_check_bar(epf, epf_bar, BAR_4,
 					     BAR4_MIN_SIZE,
 					     epc_features->reserved_bar);
 			if (ret)
@@ -209,37 +219,37 @@ static int mxlk_configure_bar(struct pci_epf *epf,
 	return 0;
 }
 
-static void mxlk_cleanup_bar(struct pci_epf *epf, enum pci_barno barno)
+static void intel_xpcie_cleanup_bar(struct pci_epf *epf, enum pci_barno barno)
 {
 	struct pci_epc *epc = epf->epc;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
-	if (mxlk_epf->vaddr[barno]) {
+	if (xpcie_epf->vaddr[barno]) {
 		pci_epc_clear_bar(epc, epf->func_no, &epf->bar[barno]);
-		pci_epf_free_space(epf, mxlk_epf->vaddr[barno], barno);
+		pci_epf_free_space(epf, xpcie_epf->vaddr[barno], barno);
 	}
 
-	mxlk_epf->vaddr[barno] = NULL;
+	xpcie_epf->vaddr[barno] = NULL;
 }
 
-static void mxlk_cleanup_bars(struct pci_epf *epf)
+static void intel_xpcie_cleanup_bars(struct pci_epf *epf)
 {
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
-	mxlk_cleanup_bar(epf, BAR_2);
-	mxlk_cleanup_bar(epf, BAR_4);
-	mxlk_epf->mxlk.io_comm = NULL;
-	mxlk_epf->mxlk.mmio = NULL;
-	mxlk_epf->mxlk.bar4 = NULL;
+	intel_xpcie_cleanup_bar(epf, BAR_2);
+	intel_xpcie_cleanup_bar(epf, BAR_4);
+	xpcie_epf->xpcie.io_comm = NULL;
+	xpcie_epf->xpcie.mmio = NULL;
+	xpcie_epf->xpcie.bar4 = NULL;
 }
 
-static int mxlk_setup_bar(struct pci_epf *epf, enum pci_barno barno,
+static int intel_xpcie_setup_bar(struct pci_epf *epf, enum pci_barno barno,
 			  size_t min_size, size_t align)
 {
 	int ret;
 	void *vaddr = NULL;
 	struct pci_epc *epc = epf->epc;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 	struct pci_epf_bar *bar = &epf->bar[barno];
 
 	bar->flags |= PCI_BASE_ADDRESS_MEM_TYPE_64;
@@ -264,41 +274,42 @@ static int mxlk_setup_bar(struct pci_epf *epf, enum pci_barno barno,
 		return ret;
 	}
 
-	mxlk_epf->vaddr[barno] = vaddr;
+	xpcie_epf->vaddr[barno] = vaddr;
 
 	return 0;
 }
 
-static int mxlk_setup_bars(struct pci_epf *epf, size_t align)
+static int intel_xpcie_setup_bars(struct pci_epf *epf, size_t align)
 {
 	int ret;
 
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
-	ret = mxlk_setup_bar(epf, BAR_2, BAR2_MIN_SIZE, align);
+	ret = intel_xpcie_setup_bar(epf, BAR_2, BAR2_MIN_SIZE, align);
 	if (ret)
 		return ret;
 
-	ret = mxlk_setup_bar(epf, BAR_4, BAR4_MIN_SIZE, align);
+	ret = intel_xpcie_setup_bar(epf, BAR_4, BAR4_MIN_SIZE, align);
 	if (ret) {
-		mxlk_cleanup_bar(epf, BAR_2);
+		intel_xpcie_cleanup_bar(epf, BAR_2);
 		return ret;
 	}
 
-	mxlk_epf->comm_bar = BAR_2;
-	mxlk_epf->mxlk.io_comm = mxlk_epf->vaddr[BAR_2];
-	mxlk_epf->mxlk.mmio = (void *)mxlk_epf->mxlk.io_comm + MXLK_MMIO_OFFSET;
+	xpcie_epf->comm_bar = BAR_2;
+	xpcie_epf->xpcie.io_comm = xpcie_epf->vaddr[BAR_2];
+	xpcie_epf->xpcie.mmio = (void *)xpcie_epf->xpcie.io_comm +
+				XPCIE_MMIO_OFFSET;
 
-	mxlk_epf->bar4 = BAR_4;
-	mxlk_epf->mxlk.bar4 = mxlk_epf->vaddr[BAR_4];
+	xpcie_epf->bar4 = BAR_4;
+	xpcie_epf->xpcie.bar4 = xpcie_epf->vaddr[BAR_4];
 
 	return 0;
 }
 
-static int epf_bind(struct pci_epf *epf)
+static int intel_xpcie_epf_bind(struct pci_epf *epf)
 {
 	struct pci_epc *epc = epf->epc;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 	struct dw_pcie_ep *ep = epc_get_drvdata(epc);
 	struct dw_pcie *pci = to_dw_pcie_from_ep(ep);
 	struct keembay_pcie *keembay = to_keembay_pcie(pci);
@@ -313,42 +324,42 @@ static int epf_bind(struct pci_epf *epf)
 		return -EINVAL;
 
 	features = pci_epc_get_features(epc, epf->func_no);
-	mxlk_epf->epc_features = features;
+	xpcie_epf->epc_features = features;
 	if (features) {
 		msi_capable = features->msi_capable;
 		align = features->align;
-		ret = mxlk_configure_bar(epf, features);
+		ret = intel_xpcie_configure_bar(epf, features);
 		if (ret)
 			return ret;
 	}
 
-	ret = mxlk_setup_bars(epf, align);
+	ret = intel_xpcie_setup_bars(epf, align);
 	if (ret) {
 		dev_err(&epf->dev, "BAR initialization failed\n");
 		return ret;
 	}
 
-	mxlk_epf->irq = keembay->ev_irq;
-	mxlk_epf->irq_dma = keembay->irq;
-	mxlk_epf->irq_err = keembay->err_irq;
-	mxlk_epf->apb_base = keembay->base;
+	xpcie_epf->irq = keembay->ev_irq;
+	xpcie_epf->irq_dma = keembay->irq;
+	xpcie_epf->irq_err = keembay->err_irq;
+	xpcie_epf->apb_base = keembay->base;
 	if (!strcmp(keembay->stepping, "A0")) {
-		mxlk_epf->mxlk.legacy_a0 = true;
-		mxlk_epf->mxlk.mmio->legacy_a0 = 1;
+		xpcie_epf->xpcie.legacy_a0 = true;
+		xpcie_epf->xpcie.mmio->legacy_a0 = 1;
 	} else {
-		mxlk_epf->mxlk.legacy_a0 = false;
-		mxlk_epf->mxlk.mmio->legacy_a0 = 0;
+		xpcie_epf->xpcie.legacy_a0 = false;
+		xpcie_epf->xpcie.mmio->legacy_a0 = 0;
 	}
 
-	ret = mxlk_ep_dma_init(epf);
+	ret = intel_xpcie_ep_dma_init(epf);
 	if (ret) {
 		dev_err(&epf->dev, "DMA initialization failed\n");
 		goto bind_error;
 	}
 
-	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_READY);
+	intel_xpcie_set_device_status(&xpcie_epf->xpcie, XPCIE_STATUS_READY);
 
-	ret = ioread32(mxlk_epf->apb_base + PCIE_REGS_PCIE_SYS_CFG_CORE);
+	ret = ioread32(xpcie_epf->apb_base + PCIE_REGS_PCIE_SYS_CFG_CORE);
 	bus_num = (ret >> PCIE_CFG_PBUS_NUM_OFFSET) & PCIE_CFG_PBUS_NUM_MASK;
 	dev_num = (ret >> PCIE_CFG_PBUS_DEV_NUM_OFFSET) &
 			PCIE_CFG_PBUS_DEV_NUM_MASK;
@@ -359,7 +370,7 @@ static int epf_bind(struct pci_epf *epf)
 		   (XLINK_DEV_SLICE_0 << XLINK_DEV_SLICE_ID_SHIFT) |
 		   (XLINK_DEV_FUNC_VPU << XLINK_DEV_FUNC_SHIFT);
 
-	ret = mxlk_core_init(&mxlk_epf->mxlk);
+	ret = intel_xpcie_core_init(&xpcie_epf->xpcie);
 	if (ret) {
 		dev_err(&epf->dev, "Core component configuration failed\n");
 		goto bind_error;
@@ -367,115 +378,116 @@ static int epf_bind(struct pci_epf *epf)
 
 	/* Enable interrupt */
 	writel(LBC_CII_EVENT_FLAG,
-	       mxlk_epf->apb_base + PCIE_REGS_PCIE_INTR_ENABLE);
-	ret = request_irq(mxlk_epf->irq, &mxlk_host_interrupt,
-			  0, MXLK_DRIVER_NAME, &mxlk_epf->mxlk);
+	       xpcie_epf->apb_base + PCIE_REGS_PCIE_INTR_ENABLE);
+	ret = request_irq(xpcie_epf->irq, &intel_xpcie_host_interrupt,
+			  0, XPCIE_DRIVER_NAME, &xpcie_epf->xpcie);
 	if (ret) {
 		dev_err(&epf->dev, "failed to request irq\n");
 		goto bind_error;
 	}
 
-	ret = request_irq(mxlk_epf->irq_err, &mxlk_err_interrupt, 0,
-			  MXLK_DRIVER_NAME, &mxlk_epf->mxlk);
+	ret = request_irq(xpcie_epf->irq_err, &intel_xpcie_err_interrupt, 0,
+			  XPCIE_DRIVER_NAME, &xpcie_epf->xpcie);
 	if (ret) {
 		dev_err(&epf->dev, "failed to request error irq\n");
-		free_irq(mxlk_epf->irq, &mxlk_epf->mxlk);
+		free_irq(xpcie_epf->irq, &xpcie_epf->xpcie);
 		goto bind_error;
 	}
 
-	if (!mxlk_ep_dma_enabled(mxlk_epf->epf))
-		mxlk_ep_dma_reset(mxlk_epf->epf);
+	if (!intel_xpcie_ep_dma_enabled(xpcie_epf->epf))
+		intel_xpcie_ep_dma_reset(xpcie_epf->epf);
 
-	mxlk_epf->mxlk.mmio->host_status = MXLK_STATUS_UNINIT;
-	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_RUN);
-	mxlk_set_doorbell(&mxlk_epf->mxlk, FROM_DEVICE, DEV_EVENT, NO_OP);
-	strncpy(mxlk_epf->mxlk.io_comm->magic, MXLK_BOOT_MAGIC_YOCTO,
-		strlen(MXLK_BOOT_MAGIC_YOCTO));
+	xpcie_epf->xpcie.mmio->host_status = XPCIE_STATUS_UNINIT;
+	intel_xpcie_set_device_status(&xpcie_epf->xpcie, XPCIE_STATUS_RUN);
+	intel_xpcie_set_doorbell(&xpcie_epf->xpcie, FROM_DEVICE,
+				 DEV_EVENT, NO_OP);
+	strncpy(xpcie_epf->xpcie.io_comm->magic, XPCIE_BOOT_MAGIC_YOCTO,
+		strlen(XPCIE_BOOT_MAGIC_YOCTO));
 
 	return 0;
 
 bind_error:
-	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_ERROR);
-	strncpy(mxlk_epf->mxlk.io_comm->magic, MXLK_BOOT_MAGIC_YOCTO,
-		strlen(MXLK_BOOT_MAGIC_YOCTO));
+	intel_xpcie_set_device_status(&xpcie_epf->xpcie, XPCIE_STATUS_ERROR);
+	strncpy(xpcie_epf->xpcie.io_comm->magic, XPCIE_BOOT_MAGIC_YOCTO,
+		strlen(XPCIE_BOOT_MAGIC_YOCTO));
 
 	return ret;
 }
 
-static void epf_unbind(struct pci_epf *epf)
+static void intel_xpcie_epf_unbind(struct pci_epf *epf)
 {
 	struct pci_epc *epc = epf->epc;
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
-	free_irq(mxlk_epf->irq, &mxlk_epf->mxlk);
-	free_irq(mxlk_epf->irq_err, &mxlk_epf->mxlk);
+	free_irq(xpcie_epf->irq, &xpcie_epf->xpcie);
+	free_irq(xpcie_epf->irq_err, &xpcie_epf->xpcie);
 
-	mxlk_core_cleanup(&mxlk_epf->mxlk);
-	mxlk_set_device_status(&mxlk_epf->mxlk, MXLK_STATUS_READY);
+	intel_xpcie_core_cleanup(&xpcie_epf->xpcie);
+	intel_xpcie_set_device_status(&xpcie_epf->xpcie, XPCIE_STATUS_READY);
 
-	mxlk_ep_dma_uninit(epf);
+	intel_xpcie_ep_dma_uninit(epf);
 
 	pci_epc_stop(epc);
 
-	mxlk_cleanup_bars(epf);
+	intel_xpcie_cleanup_bars(epf);
 }
 
-static void epf_linkup(struct pci_epf *epf)
+static void intel_xpcie_epf_linkup(struct pci_epf *epf)
 {
 }
 
-static int epf_probe(struct pci_epf *epf)
+static int intel_xpcie_epf_probe(struct pci_epf *epf)
 {
-	struct mxlk_epf *mxlk_epf;
+	struct xpcie_epf *xpcie_epf;
 	struct device *dev = &epf->dev;
 
-	mxlk_epf = devm_kzalloc(dev, sizeof(*mxlk_epf), GFP_KERNEL);
-	if (!mxlk_epf)
+	xpcie_epf = devm_kzalloc(dev, sizeof(*xpcie_epf), GFP_KERNEL);
+	if (!xpcie_epf)
 		return -ENOMEM;
 
-	epf->header = &mxlk_pcie_header;
-	mxlk_epf->epf = epf;
+	epf->header = &xpcie_header;
+	xpcie_epf->epf = epf;
 
-	epf_set_drvdata(epf, mxlk_epf);
+	epf_set_drvdata(epf, xpcie_epf);
 
 	return 0;
 }
 
-static void epf_shutdown(struct device *dev)
+static void intel_xpcie_epf_shutdown(struct device *dev)
 {
 	struct pci_epf *epf = to_pci_epf(dev);
-	struct mxlk_epf *mxlk_epf = epf_get_drvdata(epf);
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
 
 	/*
 	 * Notify host in case PCIe hot plug not supported
 	 */
-	if (mxlk_epf && mxlk_epf->mxlk.status == MXLK_STATUS_RUN) {
-		mxlk_set_doorbell(&mxlk_epf->mxlk, FROM_DEVICE, DEV_EVENT,
-				  DEV_SHUTDOWN);
+	if (xpcie_epf && xpcie_epf->xpcie.status == XPCIE_STATUS_RUN) {
+		intel_xpcie_set_doorbell(&xpcie_epf->xpcie, FROM_DEVICE,
+					 DEV_EVENT, DEV_SHUTDOWN);
 		pci_epc_raise_irq(epf->epc, epf->func_no, PCI_EPC_IRQ_MSI, 1);
 	}
 }
 
 static struct pci_epf_ops ops = {
-	.bind = epf_bind,
-	.unbind = epf_unbind,
-	.linkup = epf_linkup,
+	.bind = intel_xpcie_epf_bind,
+	.unbind = intel_xpcie_epf_unbind,
+	.linkup = intel_xpcie_epf_linkup,
 };
 
-static struct pci_epf_driver mxlk_pcie_epf_driver = {
+static struct pci_epf_driver xpcie_epf_driver = {
 	.driver.name = "mxlk_pcie_epf",
-	.driver.shutdown = epf_shutdown,
-	.probe = epf_probe,
-	.id_table = mxlk_pcie_epf_ids,
+	.driver.shutdown = intel_xpcie_epf_shutdown,
+	.probe = intel_xpcie_epf_probe,
+	.id_table = xpcie_epf_ids,
 	.ops = &ops,
 	.owner = THIS_MODULE,
 };
 
-static int __init mxlk_epf_init(void)
+static int __init intel_xpcie_epf_init(void)
 {
 	int ret = -EBUSY;
 
-	ret = pci_epf_register_driver(&mxlk_pcie_epf_driver);
+	ret = pci_epf_register_driver(&xpcie_epf_driver);
 	if (ret) {
 		pr_err("Failed to register xlink pcie epf driver: %d\n", ret);
 		return ret;
@@ -483,15 +495,15 @@ static int __init mxlk_epf_init(void)
 
 	return 0;
 }
-module_init(mxlk_epf_init);
+module_init(intel_xpcie_epf_init);
 
-static void __exit mxlk_epf_exit(void)
+static void __exit intel_xpcie_epf_exit(void)
 {
-	pci_epf_unregister_driver(&mxlk_pcie_epf_driver);
+	pci_epf_unregister_driver(&xpcie_epf_driver);
 }
-module_exit(mxlk_epf_exit);
+module_exit(intel_xpcie_epf_exit);
 
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Intel");
-MODULE_DESCRIPTION(MXLK_DRIVER_DESC);
-MODULE_VERSION(MXLK_DRIVER_VERSION);
+MODULE_DESCRIPTION(XPCIE_DRIVER_DESC);
+MODULE_VERSION(XPCIE_DRIVER_VERSION);
diff --git a/drivers/misc/xlink-pcie/local_host/epf.h b/drivers/misc/xlink-pcie/local_host/epf.h
index b17a19737a5e..84ea60d4bc63 100644
--- a/drivers/misc/xlink-pcie/local_host/epf.h
+++ b/drivers/misc/xlink-pcie/local_host/epf.h
@@ -7,25 +7,29 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_EPF_HEADER_
-#define MXLK_EPF_HEADER_
+#ifndef XPCIE_EPF_HEADER_
+#define XPCIE_EPF_HEADER_
 
-#include "../common/xlink_pcie.h"
+#include "../common/xpcie.h"
 #include "../common/util.h"
 
 extern u32 xlink_sw_id;
 
-extern void mxlk_register_host_irq(struct mxlk *mxlk, irq_handler_t func);
+extern void intel_xpcie_register_host_irq(struct xpcie *xpcie,
+					  irq_handler_t func);
 
-extern int mxlk_raise_irq(struct mxlk *mxlk, enum mxlk_doorbell_type type);
+extern int intel_xpcie_raise_irq(struct xpcie *xpcie,
+				 enum xpcie_doorbell_type type);
 
 /*
  * These two functions are for DMA linked list mode.
  *
  * Caller should set the dst/src addresses and length for DMA descriptors in
- * mxlk_epf.dma_ll_tx_descs/dma_ll_rx_descs.
+ * xpcie_epf.dma_ll_tx_descs/dma_ll_rx_descs.
  */
-extern int mxlk_copy_from_host_ll(struct mxlk *mxlk, int chan, int descs_num);
-extern int mxlk_copy_to_host_ll(struct mxlk *mxlk, int chan, int descs_num);
+extern int intel_xpcie_copy_from_host_ll(struct xpcie *xpcie,
+					 int chan, int descs_num);
+extern int intel_xpcie_copy_to_host_ll(struct xpcie *xpcie,
+				       int chan, int descs_num);
 
-#endif // MXLK_EPF_HEADER_
+#endif // XPCIE_EPF_HEADER_
diff --git a/drivers/misc/xlink-pcie/local_host/if.c b/drivers/misc/xlink-pcie/local_host/if.c
index edca679bcb2d..d8cae7f4af01 100644
--- a/drivers/misc/xlink-pcie/local_host/if.c
+++ b/drivers/misc/xlink-pcie/local_host/if.c
@@ -30,15 +30,15 @@ EXPORT_SYMBOL(xlink_pcie_get_device_list);
 int xlink_pcie_get_device_name(uint32_t sw_device_id, char *device_name,
 			       size_t name_size)
 {
-	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+	struct xpcie *xpcie = intel_xpcie_core_get_by_id(sw_device_id);
 
-	if (!mxlk)
+	if (!xpcie)
 		return -ENODEV;
 
 	memset(device_name, 0, name_size);
-	if (name_size > strlen(MXLK_DRIVER_NAME))
-		name_size = strlen(MXLK_DRIVER_NAME);
-	strncpy(device_name, MXLK_DRIVER_NAME, name_size);
+	if (name_size > strlen(XPCIE_DRIVER_NAME))
+		name_size = strlen(XPCIE_DRIVER_NAME);
+	strncpy(device_name, XPCIE_DRIVER_NAME, name_size);
 
 	return 0;
 }
@@ -46,17 +46,17 @@ EXPORT_SYMBOL(xlink_pcie_get_device_name);
 
 int xlink_pcie_get_device_status(uint32_t sw_device_id, uint32_t *device_status)
 {
-	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+	struct xpcie *xpcie = intel_xpcie_core_get_by_id(sw_device_id);
 
-	if (!mxlk)
+	if (!xpcie)
 		return -ENODEV;
 
-	switch (mxlk->status) {
-	case MXLK_STATUS_READY:
-	case MXLK_STATUS_RUN:
+	switch (xpcie->status) {
+	case XPCIE_STATUS_READY:
+	case XPCIE_STATUS_RUN:
 		*device_status = _XLINK_DEV_READY;
 		break;
-	case MXLK_STATUS_ERROR:
+	case XPCIE_STATUS_ERROR:
 		*device_status = _XLINK_DEV_ERROR;
 		break;
 	default:
@@ -76,12 +76,12 @@ EXPORT_SYMBOL(xlink_pcie_boot_device);
 
 int xlink_pcie_connect(uint32_t sw_device_id)
 {
-	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+	struct xpcie *xpcie = intel_xpcie_core_get_by_id(sw_device_id);
 
-	if (!mxlk)
+	if (!xpcie)
 		return -ENODEV;
 
-	if (mxlk->status != MXLK_STATUS_RUN)
+	if (xpcie->status != XPCIE_STATUS_RUN)
 		return -EIO;
 
 	return 0;
@@ -91,24 +91,24 @@ EXPORT_SYMBOL(xlink_pcie_connect);
 int xlink_pcie_read(uint32_t sw_device_id, void *data, size_t *const size,
 		    uint32_t timeout)
 {
-	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+	struct xpcie *xpcie = intel_xpcie_core_get_by_id(sw_device_id);
 
-	if (!mxlk)
+	if (!xpcie)
 		return -ENODEV;
 
-	return mxlk_core_read(mxlk, data, size, timeout);
+	return intel_xpcie_core_read(xpcie, data, size, timeout);
 }
 EXPORT_SYMBOL(xlink_pcie_read);
 
 int xlink_pcie_write(uint32_t sw_device_id, void *data, size_t *const size,
 		     uint32_t timeout)
 {
-	struct mxlk *mxlk = mxlk_core_get_by_id(sw_device_id);
+	struct xpcie *xpcie = intel_xpcie_core_get_by_id(sw_device_id);
 
-	if (!mxlk)
+	if (!xpcie)
 		return -ENODEV;
 
-	return mxlk_core_write(mxlk, data, size, timeout);
+	return intel_xpcie_core_write(xpcie, data, size, timeout);
 }
 EXPORT_SYMBOL(xlink_pcie_write);
 
diff --git a/drivers/misc/xlink-pcie/local_host/struct.h b/drivers/misc/xlink-pcie/local_host/struct.h
index 41ad8ad38100..7a45d76199a7 100644
--- a/drivers/misc/xlink-pcie/local_host/struct.h
+++ b/drivers/misc/xlink-pcie/local_host/struct.h
@@ -7,17 +7,17 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_STRUCT_HEADER_
-#define MXLK_STRUCT_HEADER_
+#ifndef XPCIE_STRUCT_HEADER_
+#define XPCIE_STRUCT_HEADER_
 
 #include <linux/pci-epc.h>
 #include <linux/pci-epf.h>
 #include <pcie-keembay.h>
-#include "../common/xlink_pcie.h"
+#include "../common/xpcie.h"
 
 extern bool dma_ll_mode;
 
-struct mxlk_dma_ll_desc {
+struct xpcie_dma_ll_desc {
 	u32 dma_ch_control1;
 	u32 dma_transfer_size;
 	union {
@@ -36,19 +36,19 @@ struct mxlk_dma_ll_desc {
 	};
 } __packed;
 
-struct mxlk_dma_ll_desc_buf {
-	struct mxlk_dma_ll_desc *virt;
+struct xpcie_dma_ll_desc_buf {
+	struct xpcie_dma_ll_desc *virt;
 	dma_addr_t phys;
 	size_t size;
 };
 
-struct mxlk_epf {
+struct xpcie_epf {
 	struct pci_epf			*epf;
 	void				*vaddr[BAR_5 + 1];
 	enum pci_barno			comm_bar;
 	enum pci_barno			bar4;
 	const struct pci_epc_features	*epc_features;
-	struct mxlk			mxlk;
+	struct xpcie			xpcie;
 	int				irq;
 	int				irq_dma;
 	int				irq_err;
@@ -63,15 +63,16 @@ struct mxlk_epf {
 	void				*rx_virt;
 	size_t				rx_size;
 
-	struct mxlk_dma_ll_desc_buf	tx_desc_buf[4];
-	struct mxlk_dma_ll_desc_buf	rx_desc_buf[4];
+	struct xpcie_dma_ll_desc_buf	tx_desc_buf[4];
+	struct xpcie_dma_ll_desc_buf	rx_desc_buf[4];
 };
 
-static inline struct device *mxlk_to_dev(struct mxlk *mxlk)
+static inline struct device *xpcie_to_dev(struct xpcie *xpcie)
 {
-	struct mxlk_epf *mxlk_epf = container_of(mxlk, struct mxlk_epf, mxlk);
+	struct xpcie_epf *xpcie_epf = container_of(xpcie,
+						   struct xpcie_epf, xpcie);
 
-	return &mxlk_epf->epf->dev;
+	return &xpcie_epf->epf->dev;
 }
 
-#endif // MXLK_STRUCT_HEADER_
+#endif // XPCIE_STRUCT_HEADER_
diff --git a/drivers/misc/xlink-pcie/remote_host/core.c b/drivers/misc/xlink-pcie/remote_host/core.c
index 591e66180e03..2c405400b17a 100644
--- a/drivers/misc/xlink-pcie/remote_host/core.c
+++ b/drivers/misc/xlink-pcie/remote_host/core.c
@@ -15,7 +15,7 @@
 #include "../common/util.h"
 #include "../common/capabilities.h"
 
-#define MXLK_CIRCULAR_INC(val, max) (((val) + 1) & (max - 1))
+#define XPCIE_CIRCULAR_INC(val, max) (((val) + 1) & (max - 1))
 
 static int rx_pool_size = SZ_32M;
 module_param(rx_pool_size, int, 0664);
@@ -25,26 +25,26 @@ static int tx_pool_size = SZ_32M;
 module_param(tx_pool_size, int, 0664);
 MODULE_PARM_DESC(tx_pool_size, "transmit pool size (default 32 MiB)");
 
-static int mxlk_version_check(struct mxlk *mxlk)
+static int intel_xpcie_version_check(struct xpcie *xpcie)
 {
-	struct mxlk_version version;
+	struct xpcie_version version;
 
-	memcpy_fromio(&version, &mxlk->mmio->version, sizeof(version));
+	memcpy_fromio(&version, &xpcie->mmio->version, sizeof(version));
 
-	dev_info(mxlk_to_dev(mxlk), "ver: device %u.%u.%u, host %u.%u.%u\n",
+	dev_info(xpcie_to_dev(xpcie), "ver: device %u.%u.%u, host %u.%u.%u\n",
 		 version.major, version.minor, version.build,
-		 MXLK_VERSION_MAJOR, MXLK_VERSION_MINOR, MXLK_VERSION_BUILD);
+		 XPCIE_VERSION_MAJOR, XPCIE_VERSION_MINOR, XPCIE_VERSION_BUILD);
 
-	if (ioread8(&mxlk->mmio->legacy_a0))
-		mxlk->legacy_a0 = true;
+	if (ioread8(&xpcie->mmio->legacy_a0))
+		xpcie->legacy_a0 = true;
 
 	return 0;
 }
 
-static int mxlk_map_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
+static int intel_xpcie_map_dma(struct xpcie *xpcie, struct xpcie_buf_desc *bd,
 			int direction)
 {
-	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct xpcie_dev *xdev = container_of(xpcie, struct xpcie_dev, xpcie);
 	struct device *dev = &xdev->pci->dev;
 
 	bd->phys = dma_map_single(dev, bd->data, bd->length, direction);
@@ -52,41 +52,42 @@ static int mxlk_map_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
 	return dma_mapping_error(dev, bd->phys);
 }
 
-static void mxlk_unmap_dma(struct mxlk *mxlk, struct mxlk_buf_desc *bd,
-			   int direction)
+static void intel_xpcie_unmap_dma(struct xpcie *xpcie,
+				  struct xpcie_buf_desc *bd,
+				  int direction)
 {
-	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct xpcie_dev *xdev = container_of(xpcie, struct xpcie_dev, xpcie);
 	struct device *dev = &xdev->pci->dev;
 
 	dma_unmap_single(dev, bd->phys, bd->length, direction);
 }
 
-static void mxlk_txrx_cleanup(struct mxlk *mxlk)
+static void intel_xpcie_txrx_cleanup(struct xpcie *xpcie)
 {
 	int index;
-	struct mxlk_buf_desc *bd;
-	struct mxlk_stream *tx = &mxlk->tx;
-	struct mxlk_stream *rx = &mxlk->rx;
-	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	struct xpcie_buf_desc *bd;
+	struct xpcie_stream *tx = &xpcie->tx;
+	struct xpcie_stream *rx = &xpcie->rx;
+	struct xpcie_interface *inf = &xpcie->interfaces[0];
 
-	mxlk->stop_flag = true;
-	mxlk->no_tx_buffer = false;
+	xpcie->stop_flag = true;
+	xpcie->no_tx_buffer = false;
 	inf->data_available = true;
-	wake_up_interruptible(&mxlk->tx_waitqueue);
+	wake_up_interruptible(&xpcie->tx_waitqueue);
 	wake_up_interruptible(&inf->rx_waitqueue);
-	mutex_lock(&mxlk->wlock);
+	mutex_lock(&xpcie->wlock);
 	mutex_lock(&inf->rlock);
 
 	if (tx->ddr) {
 		for (index = 0; index < tx->pipe.ndesc; index++) {
-			struct mxlk_transfer_desc *td = tx->pipe.tdr + index;
+			struct xpcie_transfer_desc *td = tx->pipe.tdr + index;
 
 			bd = tx->ddr[index];
 			if (bd) {
-				mxlk_unmap_dma(mxlk, bd, DMA_TO_DEVICE);
-				mxlk_free_tx_bd(mxlk, bd);
-				mxlk_set_td_address(td, 0);
-				mxlk_set_td_length(td, 0);
+				intel_xpcie_unmap_dma(xpcie, bd, DMA_TO_DEVICE);
+				intel_xpcie_free_tx_bd(xpcie, bd);
+				intel_xpcie_set_td_address(td, 0);
+				intel_xpcie_set_td_length(td, 0);
 			}
 		}
 		kfree(tx->ddr);
@@ -94,46 +95,48 @@ static void mxlk_txrx_cleanup(struct mxlk *mxlk)
 
 	if (rx->ddr) {
 		for (index = 0; index < rx->pipe.ndesc; index++) {
-			struct mxlk_transfer_desc *td = rx->pipe.tdr + index;
+			struct xpcie_transfer_desc *td = rx->pipe.tdr + index;
 
 			bd = rx->ddr[index];
 			if (bd) {
-				mxlk_unmap_dma(mxlk, bd, DMA_FROM_DEVICE);
-				mxlk_free_rx_bd(mxlk, bd);
-				mxlk_set_td_address(td, 0);
-				mxlk_set_td_length(td, 0);
+				intel_xpcie_unmap_dma(xpcie,
+						      bd, DMA_FROM_DEVICE);
+				intel_xpcie_free_rx_bd(xpcie, bd);
+				intel_xpcie_set_td_address(td, 0);
+				intel_xpcie_set_td_length(td, 0);
 			}
 		}
 		kfree(rx->ddr);
 	}
 
-	mxlk_list_cleanup(&mxlk->tx_pool);
-	mxlk_list_cleanup(&mxlk->rx_pool);
+	intel_xpcie_list_cleanup(&xpcie->tx_pool);
+	intel_xpcie_list_cleanup(&xpcie->rx_pool);
 
 	mutex_unlock(&inf->rlock);
-	mutex_unlock(&mxlk->wlock);
+	mutex_unlock(&xpcie->wlock);
 }
 
-static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
+static int intel_xpcie_txrx_init(struct xpcie *xpcie,
+				 struct xpcie_cap_txrx *cap)
 {
 	int rc;
 	int index;
 	int ndesc;
-	struct mxlk_buf_desc *bd;
-	struct mxlk_stream *tx = &mxlk->tx;
-	struct mxlk_stream *rx = &mxlk->rx;
+	struct xpcie_buf_desc *bd;
+	struct xpcie_stream *tx = &xpcie->tx;
+	struct xpcie_stream *rx = &xpcie->rx;
 
-	mxlk->txrx = cap;
-	mxlk->fragment_size = ioread32(&cap->fragment_size);
-	mxlk->stop_flag = false;
+	xpcie->txrx = cap;
+	xpcie->fragment_size = ioread32(&cap->fragment_size);
+	xpcie->stop_flag = false;
 
 	tx->pipe.ndesc = ioread32(&cap->tx.ndesc);
 	tx->pipe.head = &cap->tx.head;
 	tx->pipe.tail = &cap->tx.tail;
 	tx->pipe.old = ioread32(&cap->tx.tail);
-	tx->pipe.tdr = (void __iomem *)mxlk->mmio + ioread32(&cap->tx.ring);
+	tx->pipe.tdr = (void __iomem *)xpcie->mmio + ioread32(&cap->tx.ring);
 
-	tx->ddr = kcalloc(tx->pipe.ndesc, sizeof(struct mxlk_buf_desc *),
+	tx->ddr = kcalloc(tx->pipe.ndesc, sizeof(struct xpcie_buf_desc *),
 			  GFP_KERNEL);
 	if (!tx->ddr) {
 		rc = -ENOMEM;
@@ -144,37 +147,37 @@ static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
 	rx->pipe.head = &cap->rx.head;
 	rx->pipe.tail = &cap->rx.tail;
 	rx->pipe.old = ioread32(&cap->rx.head);
-	rx->pipe.tdr = (void __iomem *)mxlk->mmio + ioread32(&cap->rx.ring);
+	rx->pipe.tdr = (void __iomem *)xpcie->mmio + ioread32(&cap->rx.ring);
 
-	rx->ddr = kcalloc(rx->pipe.ndesc, sizeof(struct mxlk_buf_desc *),
+	rx->ddr = kcalloc(rx->pipe.ndesc, sizeof(struct xpcie_buf_desc *),
 			  GFP_KERNEL);
 	if (!rx->ddr) {
 		rc = -ENOMEM;
 		goto error;
 	}
 
-	mxlk_list_init(&mxlk->rx_pool);
-	rx_pool_size = roundup(rx_pool_size, mxlk->fragment_size);
-	ndesc = rx_pool_size / mxlk->fragment_size;
+	intel_xpcie_list_init(&xpcie->rx_pool);
+	rx_pool_size = roundup(rx_pool_size, xpcie->fragment_size);
+	ndesc = rx_pool_size / xpcie->fragment_size;
 
 	for (index = 0; index < ndesc; index++) {
-		bd = mxlk_alloc_bd(mxlk->fragment_size);
+		bd = intel_xpcie_alloc_bd(xpcie->fragment_size);
 		if (bd) {
-			mxlk_list_put(&mxlk->rx_pool, bd);
+			intel_xpcie_list_put(&xpcie->rx_pool, bd);
 		} else {
 			rc = -ENOMEM;
 			goto error;
 		}
 	}
 
-	mxlk_list_init(&mxlk->tx_pool);
-	tx_pool_size = roundup(tx_pool_size, mxlk->fragment_size);
-	ndesc = tx_pool_size / mxlk->fragment_size;
+	intel_xpcie_list_init(&xpcie->tx_pool);
+	tx_pool_size = roundup(tx_pool_size, xpcie->fragment_size);
+	ndesc = tx_pool_size / xpcie->fragment_size;
 
 	for (index = 0; index < ndesc; index++) {
-		bd = mxlk_alloc_bd(mxlk->fragment_size);
+		bd = intel_xpcie_alloc_bd(xpcie->fragment_size);
 		if (bd) {
-			mxlk_list_put(&mxlk->tx_pool, bd);
+			intel_xpcie_list_put(&xpcie->tx_pool, bd);
 		} else {
 			rc = -ENOMEM;
 			goto error;
@@ -182,324 +185,327 @@ static int mxlk_txrx_init(struct mxlk *mxlk, struct mxlk_cap_txrx *cap)
 	}
 
 	for (index = 0; index < rx->pipe.ndesc; index++) {
-		struct mxlk_transfer_desc *td = rx->pipe.tdr + index;
+		struct xpcie_transfer_desc *td = rx->pipe.tdr + index;
 
-		bd = mxlk_alloc_rx_bd(mxlk);
+		bd = intel_xpcie_alloc_rx_bd(xpcie);
 		if (!bd) {
 			rc = -ENOMEM;
 			goto error;
 		}
 
-		if (mxlk_map_dma(mxlk, bd, DMA_FROM_DEVICE)) {
-			dev_err(mxlk_to_dev(mxlk), "failed to map rx bd\n");
+		if (intel_xpcie_map_dma(xpcie, bd, DMA_FROM_DEVICE)) {
+			dev_err(xpcie_to_dev(xpcie), "failed to map rx bd\n");
 			rc = -ENOMEM;
 			goto error;
 		}
 
 		rx->ddr[index] = bd;
-		mxlk_set_td_address(td, bd->phys);
-		mxlk_set_td_length(td, bd->length);
+		intel_xpcie_set_td_address(td, bd->phys);
+		intel_xpcie_set_td_length(td, bd->length);
 	}
 
 	return 0;
 
 error:
-	mxlk_txrx_cleanup(mxlk);
+	intel_xpcie_txrx_cleanup(xpcie);
 
 	return rc;
 }
 
-static int mxlk_discover_txrx(struct mxlk *mxlk)
+static int intel_xpcie_discover_txrx(struct xpcie *xpcie)
 {
 	int error;
-	struct mxlk_cap_txrx *cap;
+	struct xpcie_cap_txrx *cap;
 
-	cap = mxlk_cap_find(mxlk, 0, MXLK_CAP_TXRX);
+	cap = intel_xpcie_cap_find(xpcie, 0, XPCIE_CAP_TXRX);
 	if (cap)
-		error = mxlk_txrx_init(mxlk, cap);
+		error = intel_xpcie_txrx_init(xpcie, cap);
 	else
 		error = -EIO;
 
 	return error;
 }
 
-static void mxlk_start_tx(struct mxlk *mxlk, unsigned long delay)
+static void intel_xpcie_start_tx(struct xpcie *xpcie, unsigned long delay)
 {
-	queue_delayed_work(mxlk->tx_wq, &mxlk->tx_event, delay);
+	queue_delayed_work(xpcie->tx_wq, &xpcie->tx_event, delay);
 }
 
-static void mxlk_start_rx(struct mxlk *mxlk, unsigned long delay)
+static void intel_xpcie_start_rx(struct xpcie *xpcie, unsigned long delay)
 {
-	queue_delayed_work(mxlk->rx_wq, &mxlk->rx_event, delay);
+	queue_delayed_work(xpcie->rx_wq, &xpcie->rx_event, delay);
 }
 
-static void mxlk_rx_event_handler(struct work_struct *work)
+static void intel_xpcie_rx_event_handler(struct work_struct *work)
 {
-	struct mxlk *mxlk = container_of(work, struct mxlk, rx_event.work);
+	struct xpcie *xpcie = container_of(work, struct xpcie, rx_event.work);
 
 	int rc;
-	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct xpcie_dev *xdev = container_of(xpcie, struct xpcie_dev, xpcie);
 	u16 status, interface;
 	u32 head, tail, ndesc, length;
-	struct mxlk_stream *rx = &mxlk->rx;
-	struct mxlk_buf_desc *bd, *replacement = NULL;
-	struct mxlk_transfer_desc *td;
+	struct xpcie_stream *rx = &xpcie->rx;
+	struct xpcie_buf_desc *bd, *replacement = NULL;
+	struct xpcie_transfer_desc *td;
 	unsigned long delay = msecs_to_jiffies(1);
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.rx_event_runs, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.rx_event_runs, 1);
 
-	if (mxlk_get_device_status(mxlk) != MXLK_STATUS_RUN)
+	if (intel_xpcie_get_device_status(xpcie) != XPCIE_STATUS_RUN)
 		return;
 
 	ndesc = rx->pipe.ndesc;
-	tail = mxlk_get_tdr_tail(&rx->pipe);
-	head = mxlk_get_tdr_head(&rx->pipe);
+	tail = intel_xpcie_get_tdr_tail(&rx->pipe);
+	head = intel_xpcie_get_tdr_head(&rx->pipe);
 
 	while (head != tail) {
 		td = rx->pipe.tdr + head;
 		bd = rx->ddr[head];
 
-		replacement = mxlk_alloc_rx_bd(mxlk);
+		replacement = intel_xpcie_alloc_rx_bd(xpcie);
 		if (!replacement) {
 			delay = msecs_to_jiffies(20);
 			break;
 		}
 
-		rc = mxlk_map_dma(mxlk, replacement, DMA_FROM_DEVICE);
+		rc = intel_xpcie_map_dma(xpcie, replacement, DMA_FROM_DEVICE);
 		if (rc) {
-			dev_err(mxlk_to_dev(mxlk),
+			dev_err(xpcie_to_dev(xpcie),
 				"failed to map rx bd (%d)\n", rc);
-			mxlk_free_rx_bd(mxlk, replacement);
+			intel_xpcie_free_rx_bd(xpcie, replacement);
 			break;
 		}
 
-		status = mxlk_get_td_status(td);
-		interface = mxlk_get_td_interface(td);
-		length = mxlk_get_td_length(td);
-		mxlk_unmap_dma(mxlk, bd, DMA_FROM_DEVICE);
+		status = intel_xpcie_get_td_status(td);
+		interface = intel_xpcie_get_td_interface(td);
+		length = intel_xpcie_get_td_length(td);
+		intel_xpcie_unmap_dma(xpcie, bd, DMA_FROM_DEVICE);
 
-		if (unlikely(status != MXLK_DESC_STATUS_SUCCESS) ||
-		    unlikely(interface >= MXLK_NUM_INTERFACES)) {
-			dev_err(mxlk_to_dev(mxlk),
+		if (unlikely(status != XPCIE_DESC_STATUS_SUCCESS) ||
+		    unlikely(interface >= XPCIE_NUM_INTERFACES)) {
+			dev_err(xpcie_to_dev(xpcie),
 			"detected rx desc failure, status(%u), interface(%u)\n",
 			status, interface);
-			mxlk_free_rx_bd(mxlk, bd);
+			intel_xpcie_free_rx_bd(xpcie, bd);
 		} else {
 			bd->interface = interface;
 			bd->length = length;
 			bd->next = NULL;
 
-			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.cnts, 1);
-			mxlk_debug_incr(mxlk, &mxlk->stats.rx_krn.bytes,
-					bd->length);
+			intel_xpcie_debug_incr(xpcie,
+					       &xpcie->stats.rx_krn.cnts, 1);
+			intel_xpcie_debug_incr(xpcie,
+					       &xpcie->stats.rx_krn.bytes,
+					       bd->length);
 
-			mxlk_add_bd_to_interface(mxlk, bd);
+			intel_xpcie_add_bd_to_interface(xpcie, bd);
 		}
 
 		rx->ddr[head] = replacement;
-		mxlk_set_td_address(td, replacement->phys);
-		mxlk_set_td_length(td, replacement->length);
-		head = MXLK_CIRCULAR_INC(head, ndesc);
+		intel_xpcie_set_td_address(td, replacement->phys);
+		intel_xpcie_set_td_length(td, replacement->length);
+		head = XPCIE_CIRCULAR_INC(head, ndesc);
 	}
 
-	if (mxlk_get_tdr_head(&rx->pipe) != head) {
-		mxlk_set_tdr_head(&rx->pipe, head);
-		mxlk_pci_raise_irq(xdev, DATA_RECEIVED, 1);
+	if (intel_xpcie_get_tdr_head(&rx->pipe) != head) {
+		intel_xpcie_set_tdr_head(&rx->pipe, head);
+		intel_xpcie_pci_raise_irq(xdev, DATA_RECEIVED, 1);
 	}
 
 	if (!replacement)
-		mxlk_start_rx(mxlk, delay);
+		intel_xpcie_start_rx(xpcie, delay);
 }
 
-static void mxlk_tx_event_handler(struct work_struct *work)
+static void intel_xpcie_tx_event_handler(struct work_struct *work)
 {
-	struct mxlk *mxlk = container_of(work, struct mxlk, tx_event.work);
+	struct xpcie *xpcie = container_of(work, struct xpcie, tx_event.work);
 
 	u16 status;
-	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct xpcie_dev *xdev = container_of(xpcie, struct xpcie_dev, xpcie);
 	u32 head, tail, old, ndesc;
-	struct mxlk_stream *tx = &mxlk->tx;
-	struct mxlk_buf_desc *bd;
-	struct mxlk_transfer_desc *td;
+	struct xpcie_stream *tx = &xpcie->tx;
+	struct xpcie_buf_desc *bd;
+	struct xpcie_transfer_desc *td;
 	size_t bytes, buffers;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.tx_event_runs, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_event_runs, 1);
 
-	if (mxlk_get_device_status(mxlk) != MXLK_STATUS_RUN)
+	if (intel_xpcie_get_device_status(xpcie) != XPCIE_STATUS_RUN)
 		return;
 
 	ndesc = tx->pipe.ndesc;
 	old = tx->pipe.old;
-	tail = mxlk_get_tdr_tail(&tx->pipe);
-	head = mxlk_get_tdr_head(&tx->pipe);
+	tail = intel_xpcie_get_tdr_tail(&tx->pipe);
+	head = intel_xpcie_get_tdr_head(&tx->pipe);
 
 	// clean old entries first
 	while (old != head) {
 		bd = tx->ddr[old];
 		td = tx->pipe.tdr + old;
-		status = mxlk_get_td_status(td);
-		if (status != MXLK_DESC_STATUS_SUCCESS)
-			dev_err(mxlk_to_dev(mxlk),
+		status = intel_xpcie_get_td_status(td);
+		if (status != XPCIE_DESC_STATUS_SUCCESS)
+			dev_err(xpcie_to_dev(xpcie),
 				"detected tx desc failure (%u)\n", status);
 
-		mxlk_unmap_dma(mxlk, bd, DMA_TO_DEVICE);
-		mxlk_free_tx_bd(mxlk, bd);
+		intel_xpcie_unmap_dma(xpcie, bd, DMA_TO_DEVICE);
+		intel_xpcie_free_tx_bd(xpcie, bd);
 		tx->ddr[old] = NULL;
-		old = MXLK_CIRCULAR_INC(old, ndesc);
+		old = XPCIE_CIRCULAR_INC(old, ndesc);
 	}
 	tx->pipe.old = old;
 
 	// add new entries
-	while (MXLK_CIRCULAR_INC(tail, ndesc) != head) {
-		bd = mxlk_list_get(&mxlk->write);
+	while (XPCIE_CIRCULAR_INC(tail, ndesc) != head) {
+		bd = intel_xpcie_list_get(&xpcie->write);
 		if (!bd)
 			break;
 
 		td = tx->pipe.tdr + tail;
 
-		if (mxlk_map_dma(mxlk, bd, DMA_TO_DEVICE)) {
-			dev_err(mxlk_to_dev(mxlk),
+		if (intel_xpcie_map_dma(xpcie, bd, DMA_TO_DEVICE)) {
+			dev_err(xpcie_to_dev(xpcie),
 				"dma mapping error bd addr %p, size %zu\n",
 				bd->data, bd->length);
 			break;
 		}
 
 		tx->ddr[tail] = bd;
-		mxlk_set_td_address(td, bd->phys);
-		mxlk_set_td_length(td, bd->length);
-		mxlk_set_td_interface(td, bd->interface);
-		mxlk_set_td_status(td, MXLK_DESC_STATUS_ERROR);
+		intel_xpcie_set_td_address(td, bd->phys);
+		intel_xpcie_set_td_length(td, bd->length);
+		intel_xpcie_set_td_interface(td, bd->interface);
+		intel_xpcie_set_td_status(td, XPCIE_DESC_STATUS_ERROR);
 
-		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.cnts, 1);
-		mxlk_debug_incr(mxlk, &mxlk->stats.tx_krn.bytes, bd->length);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_krn.cnts, 1);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_krn.bytes,
+				       bd->length);
 
-		tail = MXLK_CIRCULAR_INC(tail, ndesc);
+		tail = XPCIE_CIRCULAR_INC(tail, ndesc);
 	}
 
-	if (mxlk_get_tdr_tail(&tx->pipe) != tail) {
-		mxlk_set_tdr_tail(&tx->pipe, tail);
-		mxlk_pci_raise_irq(xdev, DATA_SENT, 1);
-		mxlk_debug_incr(mxlk, &mxlk->stats.send_ints, 1);
+	if (intel_xpcie_get_tdr_tail(&tx->pipe) != tail) {
+		intel_xpcie_set_tdr_tail(&tx->pipe, tail);
+		intel_xpcie_pci_raise_irq(xdev, DATA_SENT, 1);
+		intel_xpcie_debug_incr(xpcie, &xpcie->stats.send_ints, 1);
 	}
 
-	mxlk_list_info(&mxlk->write, &bytes, &buffers);
+	intel_xpcie_list_info(&xpcie->write, &bytes, &buffers);
 	if (buffers)
-		mxlk->tx_pending = true;
+		xpcie->tx_pending = true;
 	else
-		mxlk->tx_pending = false;
+		xpcie->tx_pending = false;
 }
 
-static irqreturn_t mxlk_interrupt(int irq, void *args)
+static irqreturn_t intel_xpcie_interrupt(int irq, void *args)
 {
-	struct mxlk_pcie *xdev = args;
-	struct mxlk *mxlk = &xdev->mxlk;
+	struct xpcie_dev *xdev = args;
+	struct xpcie *xpcie = &xdev->xpcie;
 
-	if (mxlk_get_doorbell(mxlk, FROM_DEVICE, DATA_SENT)) {
-		mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_SENT, 0);
-		mxlk_start_rx(mxlk, 0);
+	if (intel_xpcie_get_doorbell(xpcie, FROM_DEVICE, DATA_SENT)) {
+		intel_xpcie_set_doorbell(xpcie, FROM_DEVICE, DATA_SENT, 0);
+		intel_xpcie_start_rx(xpcie, 0);
 
-		mxlk_debug_incr(mxlk, &xdev->mxlk.stats.interrupts, 1);
+		intel_xpcie_debug_incr(xpcie, &xdev->xpcie.stats.interrupts, 1);
 	}
-	if (mxlk_get_doorbell(mxlk, FROM_DEVICE, DATA_RECEIVED)) {
-		mxlk_set_doorbell(mxlk, FROM_DEVICE, DATA_RECEIVED, 0);
-		if (mxlk->tx_pending)
-			mxlk_start_tx(mxlk, 0);
+	if (intel_xpcie_get_doorbell(xpcie, FROM_DEVICE, DATA_RECEIVED)) {
+		intel_xpcie_set_doorbell(xpcie, FROM_DEVICE, DATA_RECEIVED, 0);
+		if (xpcie->tx_pending)
+			intel_xpcie_start_tx(xpcie, 0);
 	}
 
 	return IRQ_HANDLED;
 }
 
-static int mxlk_events_init(struct mxlk *mxlk)
+static int intel_xpcie_events_init(struct xpcie *xpcie)
 {
-	mxlk->rx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+	xpcie->rx_wq = alloc_ordered_workqueue(XPCIE_DRIVER_NAME,
 					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
-	if (!mxlk->rx_wq) {
-		dev_err(mxlk_to_dev(mxlk), "failed to allocate workqueue\n");
+	if (!xpcie->rx_wq) {
+		dev_err(xpcie_to_dev(xpcie), "failed to allocate workqueue\n");
 		return -ENOMEM;
 	}
 
-	mxlk->tx_wq = alloc_ordered_workqueue(MXLK_DRIVER_NAME,
+	xpcie->tx_wq = alloc_ordered_workqueue(XPCIE_DRIVER_NAME,
 					      WQ_MEM_RECLAIM | WQ_HIGHPRI);
-	if (!mxlk->tx_wq) {
-		dev_err(mxlk_to_dev(mxlk), "failed to allocate workqueue\n");
-		destroy_workqueue(mxlk->rx_wq);
+	if (!xpcie->tx_wq) {
+		dev_err(xpcie_to_dev(xpcie), "failed to allocate workqueue\n");
+		destroy_workqueue(xpcie->rx_wq);
 		return -ENOMEM;
 	}
 
-	INIT_DELAYED_WORK(&mxlk->rx_event, mxlk_rx_event_handler);
-	INIT_DELAYED_WORK(&mxlk->tx_event, mxlk_tx_event_handler);
+	INIT_DELAYED_WORK(&xpcie->rx_event, intel_xpcie_rx_event_handler);
+	INIT_DELAYED_WORK(&xpcie->tx_event, intel_xpcie_tx_event_handler);
 
 	return 0;
 }
 
-static void mxlk_events_cleanup(struct mxlk *mxlk)
+static void intel_xpcie_events_cleanup(struct xpcie *xpcie)
 {
-	cancel_delayed_work_sync(&mxlk->rx_event);
-	cancel_delayed_work_sync(&mxlk->tx_event);
+	cancel_delayed_work_sync(&xpcie->rx_event);
+	cancel_delayed_work_sync(&xpcie->tx_event);
 
-	destroy_workqueue(mxlk->rx_wq);
-	destroy_workqueue(mxlk->tx_wq);
+	destroy_workqueue(xpcie->rx_wq);
+	destroy_workqueue(xpcie->tx_wq);
 }
 
-int mxlk_core_init(struct mxlk *mxlk)
+int intel_xpcie_core_init(struct xpcie *xpcie)
 {
 	int rc;
 	int status;
-	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct xpcie_dev *xdev = container_of(xpcie, struct xpcie_dev, xpcie);
 
-	status = mxlk_get_device_status(mxlk);
-	if (status != MXLK_STATUS_RUN) {
+	status = intel_xpcie_get_device_status(xpcie);
+	if (status != XPCIE_STATUS_RUN) {
 		dev_err(&xdev->pci->dev,
 			"device status not RUNNING (%d)\n", status);
 		rc = -EBUSY;
 		return rc;
 	}
 
-	mxlk_version_check(mxlk);
+	intel_xpcie_version_check(xpcie);
 
-	rc = mxlk_events_init(mxlk);
+	rc = intel_xpcie_events_init(xpcie);
 	if (rc)
 		return rc;
 
-	rc = mxlk_discover_txrx(mxlk);
+	rc = intel_xpcie_discover_txrx(xpcie);
 	if (rc)
 		goto error_txrx;
 
-	mxlk_interfaces_init(mxlk);
+	intel_xpcie_interfaces_init(xpcie);
 
-	rc = mxlk_pci_register_irq(xdev, &mxlk_interrupt);
+	rc = intel_xpcie_pci_register_irq(xdev, &intel_xpcie_interrupt);
 	if (rc)
 		goto error_txrx;
 
-	mxlk_set_host_status(mxlk, MXLK_STATUS_RUN);
+	intel_xpcie_set_host_status(xpcie, XPCIE_STATUS_RUN);
 
 	return 0;
 
 error_txrx:
-	mxlk_events_cleanup(mxlk);
-	mxlk_set_host_status(mxlk, MXLK_STATUS_ERROR);
+	intel_xpcie_events_cleanup(xpcie);
+	intel_xpcie_set_host_status(xpcie, XPCIE_STATUS_ERROR);
 
 	return rc;
 }
 
-void mxlk_core_cleanup(struct mxlk *mxlk)
+void intel_xpcie_core_cleanup(struct xpcie *xpcie)
 {
-	if (mxlk->status == MXLK_STATUS_RUN) {
-		mxlk_set_host_status(mxlk, MXLK_STATUS_UNINIT);
-		mxlk_events_cleanup(mxlk);
-		mxlk_interfaces_cleanup(mxlk);
-		mxlk_txrx_cleanup(mxlk);
+	if (xpcie->status == XPCIE_STATUS_RUN) {
+		intel_xpcie_set_host_status(xpcie, XPCIE_STATUS_UNINIT);
+		intel_xpcie_events_cleanup(xpcie);
+		intel_xpcie_interfaces_cleanup(xpcie);
+		intel_xpcie_txrx_cleanup(xpcie);
 	}
 }
 
-int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
+int intel_xpcie_core_read(struct xpcie *xpcie, void *buffer, size_t *length,
 		   uint32_t timeout_ms)
 {
 	int ret = 0;
-	struct mxlk_interface *inf = &mxlk->interfaces[0];
+	struct xpcie_interface *inf = &xpcie->interfaces[0];
 	size_t len = *length;
 	size_t remaining = len;
-	struct mxlk_buf_desc *bd;
+	struct xpcie_buf_desc *bd;
 	unsigned long jiffies_start = jiffies;
 	long jiffies_passed = 0;
 	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
@@ -508,10 +514,10 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 	if (len == 0)
 		return -EINVAL;
 
-	if (mxlk->status != MXLK_STATUS_RUN)
+	if (xpcie->status != XPCIE_STATUS_RUN)
 		return -ENODEV;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.cnts, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.rx_usr.cnts, 1);
 
 	ret = mutex_lock_interruptible(&inf->rlock);
 	if (ret < 0)
@@ -530,7 +536,7 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 				if (ret == 0)
 					return -ETIME;
 			}
-			if (ret < 0 || mxlk->stop_flag)
+			if (ret < 0 || xpcie->stop_flag)
 				return -EINTR;
 
 			ret = mutex_lock_interruptible(&inf->rlock);
@@ -539,7 +545,7 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 		}
 
 		bd = (inf->partial_read) ? inf->partial_read :
-					   mxlk_list_get(&inf->read);
+					   intel_xpcie_list_get(&inf->read);
 		while (remaining && bd) {
 			size_t bcopy;
 
@@ -551,11 +557,11 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 			bd->data += bcopy;
 			bd->length -= bcopy;
 
-			mxlk_debug_incr(mxlk, &mxlk->stats.rx_usr.bytes, bcopy);
+			intel_xpcie_debug_incr(xpcie, &xpcie->stats.rx_usr.bytes, bcopy);
 
 			if (bd->length == 0) {
-				mxlk_free_rx_bd(mxlk, bd);
-				bd = mxlk_list_get(&inf->read);
+				intel_xpcie_free_rx_bd(xpcie, bd);
+				bd = intel_xpcie_list_get(&inf->read);
 			}
 		}
 
@@ -576,14 +582,14 @@ int mxlk_core_read(struct mxlk *mxlk, void *buffer, size_t *length,
 	return 0;
 }
 
-int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
+int intel_xpcie_core_write(struct xpcie *xpcie, void *buffer, size_t *length,
 		    uint32_t timeout_ms)
 {
 	int ret;
 	size_t len = *length;
 	size_t remaining = len;
-	struct mxlk_interface *inf = &mxlk->interfaces[0];
-	struct mxlk_buf_desc *bd, *head;
+	struct xpcie_interface *inf = &xpcie->interfaces[0];
+	struct xpcie_buf_desc *bd, *head;
 	unsigned long jiffies_start = jiffies;
 	long jiffies_passed = 0;
 	long jiffies_timeout = (long)msecs_to_jiffies(timeout_ms);
@@ -592,38 +598,38 @@ int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
 	if (len == 0)
 		return -EINVAL;
 
-	if (mxlk->status != MXLK_STATUS_RUN)
+	if (xpcie->status != XPCIE_STATUS_RUN)
 		return -ENODEV;
 
-	mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.cnts, 1);
+	intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_usr.cnts, 1);
 
-	ret = mutex_lock_interruptible(&mxlk->wlock);
+	ret = mutex_lock_interruptible(&xpcie->wlock);
 	if (ret < 0)
 		return -EINTR;
 
 	do {
-		bd = head = mxlk_alloc_tx_bd(mxlk);
+		bd = head = intel_xpcie_alloc_tx_bd(xpcie);
 		while (!head) {
-			mutex_unlock(&mxlk->wlock);
+			mutex_unlock(&xpcie->wlock);
 			if (timeout_ms == 0) {
 				ret = wait_event_interruptible(
-					mxlk->tx_waitqueue,
-					!mxlk->no_tx_buffer);
+					xpcie->tx_waitqueue,
+					!xpcie->no_tx_buffer);
 			} else {
 				ret = wait_event_interruptible_timeout(
-					mxlk->tx_waitqueue, !mxlk->no_tx_buffer,
+					xpcie->tx_waitqueue, !xpcie->no_tx_buffer,
 					jiffies_timeout - jiffies_passed);
 				if (ret == 0)
 					return -ETIME;
 			}
-			if (ret < 0 || mxlk->stop_flag)
+			if (ret < 0 || xpcie->stop_flag)
 				return -EINTR;
 
-			ret = mutex_lock_interruptible(&mxlk->wlock);
+			ret = mutex_lock_interruptible(&xpcie->wlock);
 			if (ret < 0)
 				return -EINTR;
 
-			bd = head = mxlk_alloc_tx_bd(mxlk);
+			bd = head = intel_xpcie_alloc_tx_bd(xpcie);
 		}
 
 		while (remaining && bd) {
@@ -637,16 +643,16 @@ int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
 			bd->length = bcopy;
 			bd->interface = inf->id;
 
-			mxlk_debug_incr(mxlk, &mxlk->stats.tx_usr.bytes, bcopy);
+			intel_xpcie_debug_incr(xpcie, &xpcie->stats.tx_usr.bytes, bcopy);
 
 			if (remaining) {
-				bd->next = mxlk_alloc_tx_bd(mxlk);
+				bd->next = intel_xpcie_alloc_tx_bd(xpcie);
 				bd = bd->next;
 			}
 		}
 
-		mxlk_list_put(&mxlk->write, head);
-		mxlk_start_tx(mxlk, 0);
+		intel_xpcie_list_put(&xpcie->write, head);
+		intel_xpcie_start_tx(xpcie, 0);
 
 		*length = len - remaining;
 
@@ -654,7 +660,7 @@ int mxlk_core_write(struct mxlk *mxlk, void *buffer, size_t *length,
 	} while (remaining > 0 && (jiffies_passed < jiffies_timeout ||
 				   timeout_ms == 0));
 
-	mutex_unlock(&mxlk->wlock);
+	mutex_unlock(&xpcie->wlock);
 
 	return 0;
 }
diff --git a/drivers/misc/xlink-pcie/remote_host/if.c b/drivers/misc/xlink-pcie/remote_host/if.c
index 2db3ec60c0df..f14b914ebcb2 100644
--- a/drivers/misc/xlink-pcie/remote_host/if.c
+++ b/drivers/misc/xlink-pcie/remote_host/if.c
@@ -15,7 +15,7 @@
 int xlink_pcie_get_device_list(uint32_t *sw_device_id_list,
 			       uint32_t *num_devices)
 {
-	*num_devices = mxlk_get_device_num(sw_device_id_list);
+	*num_devices = intel_xpcie_get_device_num(sw_device_id_list);
 
 	return 0;
 }
@@ -24,7 +24,8 @@ EXPORT_SYMBOL(xlink_pcie_get_device_list);
 int xlink_pcie_get_device_name(uint32_t sw_device_id, char *device_name,
 			       size_t name_size)
 {
-	return mxlk_get_device_name_by_id(sw_device_id, device_name, name_size);
+	return intel_xpcie_get_device_name_by_id(sw_device_id,
+						 device_name, name_size);
 }
 EXPORT_SYMBOL(xlink_pcie_get_device_name);
 
@@ -33,22 +34,22 @@ int xlink_pcie_get_device_status(uint32_t sw_device_id, uint32_t *device_status)
 	int rc;
 	u32 status;
 
-	rc = mxlk_get_device_status_by_id(sw_device_id, &status);
+	rc = intel_xpcie_get_device_status_by_id(sw_device_id, &status);
 	if (rc)
 		return rc;
 
 	switch (status) {
-	case MXLK_STATUS_READY:
-	case MXLK_STATUS_RUN:
+	case XPCIE_STATUS_READY:
+	case XPCIE_STATUS_RUN:
 		*device_status = _XLINK_DEV_READY;
 		break;
-	case MXLK_STATUS_ERROR:
+	case XPCIE_STATUS_ERROR:
 		*device_status = _XLINK_DEV_ERROR;
 		break;
-	case MXLK_STATUS_RECOVERY:
+	case XPCIE_STATUS_RECOVERY:
 		*device_status = _XLINK_DEV_RECOVERY;
 		break;
-	case MXLK_STATUS_OFF:
+	case XPCIE_STATUS_OFF:
 		*device_status = _XLINK_DEV_OFF;
 		break;
 	default:
@@ -62,32 +63,32 @@ EXPORT_SYMBOL(xlink_pcie_get_device_status);
 
 int xlink_pcie_boot_device(uint32_t sw_device_id, const char *binary_name)
 {
-	return mxlk_pci_boot_device(sw_device_id, binary_name);
+	return intel_xpcie_pci_boot_device(sw_device_id, binary_name);
 }
 EXPORT_SYMBOL(xlink_pcie_boot_device);
 
 int xlink_pcie_connect(uint32_t sw_device_id)
 {
-	return mxlk_pci_connect_device(sw_device_id);
+	return intel_xpcie_pci_connect_device(sw_device_id);
 }
 EXPORT_SYMBOL(xlink_pcie_connect);
 
 int xlink_pcie_read(uint32_t sw_device_id, void *data, size_t *const size,
 		    uint32_t timeout)
 {
-	return mxlk_pci_read(sw_device_id, data, size, timeout);
+	return intel_xpcie_pci_read(sw_device_id, data, size, timeout);
 }
 EXPORT_SYMBOL(xlink_pcie_read);
 
 int xlink_pcie_write(uint32_t sw_device_id, void *data, size_t *const size,
 		     uint32_t timeout)
 {
-	return mxlk_pci_write(sw_device_id, data, size, timeout);
+	return intel_xpcie_pci_write(sw_device_id, data, size, timeout);
 }
 EXPORT_SYMBOL(xlink_pcie_write);
 
 int xlink_pcie_reset_device(uint32_t sw_device_id)
 {
-	return mxlk_pci_reset_device(sw_device_id);
+	return intel_xpcie_pci_reset_device(sw_device_id);
 }
 EXPORT_SYMBOL(xlink_pcie_reset_device);
diff --git a/drivers/misc/xlink-pcie/remote_host/main.c b/drivers/misc/xlink-pcie/remote_host/main.c
index e462bebfc82e..67e2e90313a6 100644
--- a/drivers/misc/xlink-pcie/remote_host/main.c
+++ b/drivers/misc/xlink-pcie/remote_host/main.c
@@ -9,20 +9,21 @@
 
 #include "pci.h"
 
-static const struct pci_device_id mxlk_pci_table[] = {
+static const struct pci_device_id xpcie_pci_table[] = {
 	{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_KEEMBAY), 0 },
 	{ 0 }
 };
 
 static bool driver_unload;
 
-static int mxlk_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+static int intel_xpcie_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *ent)
 {
 	int ret = 0;
 	u32 sw_devid = 0;
 	u32 hw_id = 0;
 	bool new_device = false;
-	struct mxlk_pcie *xdev;
+	struct xpcie_dev *xdev;
 
 	hw_id = ((u16)pdev->bus->number << 8) | PCI_SLOT(pdev->devfn);
 
@@ -32,59 +33,59 @@ static int mxlk_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		   (XLINK_DEV_SLICE_0 << XLINK_DEV_SLICE_ID_SHIFT) |
 		   (XLINK_DEV_FUNC_VPU << XLINK_DEV_FUNC_SHIFT);
 
-	xdev = mxlk_get_device_by_id(sw_devid);
+	xdev = intel_xpcie_get_device_by_id(sw_devid);
 	if (!xdev) {
-		xdev = mxlk_create_device(sw_devid, pdev);
+		xdev = intel_xpcie_create_device(sw_devid, pdev);
 		if (!xdev)
 			return -ENOMEM;
 
 		new_device = true;
 	}
 
-	ret = mxlk_pci_init(xdev, pdev);
+	ret = intel_xpcie_pci_init(xdev, pdev);
 	if (ret) {
-		mxlk_remove_device(xdev);
+		intel_xpcie_remove_device(xdev);
 		return ret;
 	}
 
 	if (new_device)
-		mxlk_list_add_device(xdev);
+		intel_xpcie_list_add_device(xdev);
 
 	return ret;
 }
 
-static void mxlk_remove(struct pci_dev *pdev)
+static void intel_xpcie_remove(struct pci_dev *pdev)
 {
-	struct mxlk_pcie *xdev = pci_get_drvdata(pdev);
+	struct xpcie_dev *xdev = pci_get_drvdata(pdev);
 
 	if (xdev) {
-		mxlk_pci_cleanup(xdev);
+		intel_xpcie_pci_cleanup(xdev);
 		if (driver_unload)
-			mxlk_remove_device(xdev);
+			intel_xpcie_remove_device(xdev);
 	}
 }
 
-static struct pci_driver mxlk_driver = {
-	.name = MXLK_DRIVER_NAME,
-	.id_table = mxlk_pci_table,
-	.probe = mxlk_probe,
-	.remove = mxlk_remove
+static struct pci_driver xpcie_driver = {
+	.name = XPCIE_DRIVER_NAME,
+	.id_table = xpcie_pci_table,
+	.probe = intel_xpcie_probe,
+	.remove = intel_xpcie_remove
 };
 
-static int __init mxlk_init_module(void)
+static int __init intel_xpcie_init_module(void)
 {
-	return pci_register_driver(&mxlk_driver);
+	return pci_register_driver(&xpcie_driver);
 }
 
-static void __exit mxlk_exit_module(void)
+static void __exit intel_xpcie_exit_module(void)
 {
 	driver_unload = true;
-	pci_unregister_driver(&mxlk_driver);
+	pci_unregister_driver(&xpcie_driver);
 }
 
-module_init(mxlk_init_module);
-module_exit(mxlk_exit_module);
+module_init(intel_xpcie_init_module);
+module_exit(intel_xpcie_exit_module);
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Intel");
-MODULE_DESCRIPTION(MXLK_DRIVER_DESC);
-MODULE_VERSION(MXLK_DRIVER_VERSION);
+MODULE_DESCRIPTION(XPCIE_DRIVER_DESC);
+MODULE_VERSION(XPCIE_DRIVER_VERSION);
diff --git a/drivers/misc/xlink-pcie/remote_host/pci.c b/drivers/misc/xlink-pcie/remote_host/pci.c
index 5138ad0355e7..f6ba994a55f5 100644
--- a/drivers/misc/xlink-pcie/remote_host/pci.c
+++ b/drivers/misc/xlink-pcie/remote_host/pci.c
@@ -30,15 +30,15 @@ static int aspm_enable;
 module_param(aspm_enable, int, 0664);
 MODULE_PARM_DESC(aspm_enable, "enable ASPM");
 
-static int mxlk_pci_setup_recovery_sysfs(struct mxlk_pcie *xdev);
-static void mxlk_pci_cleanup_recovery_sysfs(struct mxlk_pcie *xdev);
+static int intel_xpcie_pci_setup_recovery_sysfs(struct xpcie_dev *xdev);
+static void intel_xpcie_pci_cleanup_recovery_sysfs(struct xpcie_dev *xdev);
 
 static LIST_HEAD(dev_list);
 static DEFINE_MUTEX(dev_list_mutex);
 
-struct mxlk_pcie *mxlk_get_device_by_id(u32 id)
+struct xpcie_dev *intel_xpcie_get_device_by_id(u32 id)
 {
-	struct mxlk_pcie *xdev;
+	struct xpcie_dev *xdev;
 
 	mutex_lock(&dev_list_mutex);
 
@@ -59,15 +59,16 @@ struct mxlk_pcie *mxlk_get_device_by_id(u32 id)
 	return NULL;
 }
 
-struct mxlk_pcie *mxlk_create_device(u32 sw_device_id, struct pci_dev *pdev)
+struct xpcie_dev *intel_xpcie_create_device(u32 sw_device_id,
+					    struct pci_dev *pdev)
 {
-	struct mxlk_pcie *xdev = kzalloc(sizeof(struct mxlk_pcie), GFP_KERNEL);
+	struct xpcie_dev *xdev = kzalloc(sizeof(struct xpcie_dev), GFP_KERNEL);
 
 	if (!xdev)
 		return NULL;
 
 	xdev->devid = sw_device_id;
-	snprintf(xdev->name, MXLK_MAX_NAME_LEN, "%02x:%02x.%x",
+	snprintf(xdev->name, XPCIE_MAX_NAME_LEN, "%02x:%02x.%x",
 		 pdev->bus->number,
 		 PCI_SLOT(pdev->devfn),
 		 PCI_FUNC(pdev->devfn));
@@ -77,13 +78,13 @@ struct mxlk_pcie *mxlk_create_device(u32 sw_device_id, struct pci_dev *pdev)
 	return xdev;
 }
 
-void mxlk_remove_device(struct mxlk_pcie *xdev)
+void intel_xpcie_remove_device(struct xpcie_dev *xdev)
 {
 	mutex_destroy(&xdev->lock);
 	kfree(xdev);
 }
 
-void mxlk_list_add_device(struct mxlk_pcie *xdev)
+void intel_xpcie_list_add_device(struct xpcie_dev *xdev)
 {
 	mutex_lock(&dev_list_mutex);
 
@@ -92,7 +93,7 @@ void mxlk_list_add_device(struct mxlk_pcie *xdev)
 	mutex_unlock(&dev_list_mutex);
 }
 
-void mxlk_list_del_device(struct mxlk_pcie *xdev)
+void intel_xpcie_list_del_device(struct xpcie_dev *xdev)
 {
 	mutex_lock(&dev_list_mutex);
 
@@ -101,7 +102,7 @@ void mxlk_list_del_device(struct mxlk_pcie *xdev)
 	mutex_unlock(&dev_list_mutex);
 }
 
-static void mxlk_pci_set_aspm(struct mxlk_pcie *xdev, int aspm)
+static void intel_xpcie_pci_set_aspm(struct xpcie_dev *xdev, int aspm)
 {
 	u8 cap_exp;
 	u16 link_control;
@@ -120,47 +121,48 @@ static void mxlk_pci_set_aspm(struct mxlk_pcie *xdev, int aspm)
 			      link_control);
 }
 
-static void mxlk_pci_unmap_bar(struct mxlk_pcie *xdev)
+static void intel_xpcie_pci_unmap_bar(struct xpcie_dev *xdev)
 {
-	if (xdev->mxlk.bar0) {
-		iounmap(xdev->mxlk.bar0);
-		xdev->mxlk.bar0 = NULL;
+	if (xdev->xpcie.bar0) {
+		iounmap(xdev->xpcie.bar0);
+		xdev->xpcie.bar0 = NULL;
 	}
 
-	if (xdev->mxlk.io_comm) {
-		iounmap(xdev->mxlk.io_comm);
-		xdev->mxlk.io_comm = NULL;
-		xdev->mxlk.mmio = NULL;
+	if (xdev->xpcie.io_comm) {
+		iounmap(xdev->xpcie.io_comm);
+		xdev->xpcie.io_comm = NULL;
+		xdev->xpcie.mmio = NULL;
 	}
 
-	if (xdev->mxlk.bar4) {
-		iounmap(xdev->mxlk.bar4);
-		xdev->mxlk.bar4 = NULL;
+	if (xdev->xpcie.bar4) {
+		iounmap(xdev->xpcie.bar4);
+		xdev->xpcie.bar4 = NULL;
 	}
 }
 
-static int mxlk_pci_map_bar(struct mxlk_pcie *xdev)
+static int intel_xpcie_pci_map_bar(struct xpcie_dev *xdev)
 {
-	if (pci_resource_len(xdev->pci, 2) < MXLK_IO_COMM_SIZE) {
+	if (pci_resource_len(xdev->pci, 2) < XPCIE_IO_COMM_SIZE) {
 		dev_err(&xdev->pci->dev, "device BAR region is too small\n");
 		return -EIO;
 	}
 
-	xdev->mxlk.bar0 = pci_ioremap_bar(xdev->pci, 0);
-	if (!xdev->mxlk.bar0) {
+	xdev->xpcie.bar0 = pci_ioremap_bar(xdev->pci, 0);
+	if (!xdev->xpcie.bar0) {
 		dev_err(&xdev->pci->dev, "failed to ioremap BAR0\n");
 		goto bar_error;
 	}
 
-	xdev->mxlk.io_comm = pci_ioremap_bar(xdev->pci, 2);
-	if (!xdev->mxlk.io_comm) {
+	xdev->xpcie.io_comm = pci_ioremap_bar(xdev->pci, 2);
+	if (!xdev->xpcie.io_comm) {
 		dev_err(&xdev->pci->dev, "failed to ioremap BAR2\n");
 		goto bar_error;
 	}
-	xdev->mxlk.mmio = (void __iomem *)xdev->mxlk.io_comm + MXLK_MMIO_OFFSET;
+	xdev->xpcie.mmio = (void __iomem *)xdev->xpcie.io_comm +
+			   XPCIE_MMIO_OFFSET;
 
-	xdev->mxlk.bar4 = pci_ioremap_wc_bar(xdev->pci, 4);
-	if (!xdev->mxlk.bar4) {
+	xdev->xpcie.bar4 = pci_ioremap_wc_bar(xdev->pci, 4);
+	if (!xdev->xpcie.bar4) {
 		dev_err(&xdev->pci->dev, "failed to ioremap BAR4\n");
 		goto bar_error;
 	}
@@ -168,50 +170,50 @@ static int mxlk_pci_map_bar(struct mxlk_pcie *xdev)
 	return 0;
 
 bar_error:
-	mxlk_pci_unmap_bar(xdev);
+	intel_xpcie_pci_unmap_bar(xdev);
 	return -EIO;
 }
 
 #define STR_EQUAL(a, b) !strncmp(a, b, strlen(b))
 
-static enum mxlk_stage mxlk_check_magic(struct mxlk_pcie *xdev)
+static enum xpcie_stage intel_xpcie_check_magic(struct xpcie_dev *xdev)
 {
-	char magic[MXLK_BOOT_MAGIC_STRLEN];
+	char magic[XPCIE_BOOT_MAGIC_STRLEN];
 
-	memcpy_fromio(magic, xdev->mxlk.io_comm->magic,
-		      MXLK_BOOT_MAGIC_STRLEN);
+	memcpy_fromio(magic, xdev->xpcie.io_comm->magic,
+		      XPCIE_BOOT_MAGIC_STRLEN);
 
 	if (strlen(magic) == 0)
 		return STAGE_UNINIT;
 
-	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_ROM))
+	if (STR_EQUAL(magic, XPCIE_BOOT_MAGIC_ROM))
 		return STAGE_ROM;
 
-	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_EMMC))
+	if (STR_EQUAL(magic, XPCIE_BOOT_MAGIC_EMMC))
 		return STAGE_ROM;
 
-	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_BL2))
+	if (STR_EQUAL(magic, XPCIE_BOOT_MAGIC_BL2))
 		return STAGE_BL2;
 
-	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_UBOOT))
+	if (STR_EQUAL(magic, XPCIE_BOOT_MAGIC_UBOOT))
 		return STAGE_UBOOT;
 
-	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_RECOV))
+	if (STR_EQUAL(magic, XPCIE_BOOT_MAGIC_RECOV))
 		return STAGE_RECOV;
 
-	if (STR_EQUAL(magic, MXLK_BOOT_MAGIC_YOCTO))
+	if (STR_EQUAL(magic, XPCIE_BOOT_MAGIC_YOCTO))
 		return STAGE_OS;
 
 	return STAGE_UNINIT;
 }
 
-static irqreturn_t mxlk_interrupt(int irq, void *args)
+static irqreturn_t intel_xpcie_interrupt(int irq, void *args)
 {
-	struct mxlk_pcie *xdev = args;
-	enum mxlk_stage stage;
+	struct xpcie_dev *xdev = args;
+	enum xpcie_stage stage;
 	u8 event;
 
-	event = mxlk_get_doorbell(&xdev->mxlk, FROM_DEVICE, DEV_EVENT);
+	event = intel_xpcie_get_doorbell(&xdev->xpcie, FROM_DEVICE, DEV_EVENT);
 	if (event == DEV_SHUTDOWN || event == 0xFF) {
 		schedule_delayed_work(&xdev->shutdown_event, 0);
 		return IRQ_HANDLED;
@@ -220,22 +222,22 @@ static irqreturn_t mxlk_interrupt(int irq, void *args)
 	if (likely(xdev->core_irq_callback))
 		return xdev->core_irq_callback(irq, args);
 
-	stage = mxlk_check_magic(xdev);
+	stage = intel_xpcie_check_magic(xdev);
 	if (stage == STAGE_ROM) {
-		xdev->mxlk.status = MXLK_STATUS_BOOT_FW;
+		xdev->xpcie.status = XPCIE_STATUS_BOOT_FW;
 		wake_up_interruptible(&xdev->waitqueue);
 	} else if (stage == STAGE_UBOOT) {
-		xdev->mxlk.status = MXLK_STATUS_BOOT_OS;
+		xdev->xpcie.status = XPCIE_STATUS_BOOT_OS;
 		wake_up_interruptible(&xdev->waitqueue);
 	} else if (stage == STAGE_RECOV) {
-		xdev->mxlk.status = MXLK_STATUS_RECOVERY;
+		xdev->xpcie.status = XPCIE_STATUS_RECOVERY;
 		wake_up_interruptible(&xdev->waitqueue);
 	}
 
 	return IRQ_HANDLED;
 }
 
-static void mxlk_pci_irq_cleanup(struct mxlk_pcie *xdev)
+static void intel_xpcie_pci_irq_cleanup(struct xpcie_dev *xdev)
 {
 #if KERNEL_VERSION(4, 8, 0) <= LINUX_VERSION_CODE
 	int irq = pci_irq_vector(xdev->pci, 0);
@@ -257,7 +259,7 @@ static void mxlk_pci_irq_cleanup(struct mxlk_pcie *xdev)
 }
 
 #if KERNEL_VERSION(4, 8, 0) <= LINUX_VERSION_CODE
-static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
+static int intel_xpcie_pci_irq_init(struct xpcie_dev *xdev)
 {
 	int irq;
 	int rc;
@@ -275,7 +277,8 @@ static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
 		rc = irq;
 		goto error_irq;
 	}
-	rc = request_irq(irq, &mxlk_interrupt, 0, MXLK_DRIVER_NAME, xdev);
+	rc = request_irq(irq, &intel_xpcie_interrupt, 0,
+			 XPCIE_DRIVER_NAME, xdev);
 	if (rc) {
 		dev_err(&xdev->pci->dev, "failed to request irqs\n");
 		goto error_irq;
@@ -288,7 +291,7 @@ static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
 	return rc;
 }
 #else
-static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
+static int intel_xpcie_pci_irq_init(struct xpcie_dev *xdev)
 {
 	int rc;
 
@@ -298,8 +301,8 @@ static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
 		return rc;
 	}
 
-	rc = request_irq(xdev->pci->irq, &mxlk_interrupt, 0,
-			    MXLK_DRIVER_NAME, xdev);
+	rc = request_irq(xdev->pci->irq, &intel_xpcie_interrupt, 0,
+			    XPCIE_DRIVER_NAME, xdev);
 	if (rc) {
 		dev_err(&xdev->pci->dev, "failed to request irqs\n");
 		goto error_irq;
@@ -313,32 +316,32 @@ static int mxlk_pci_irq_init(struct mxlk_pcie *xdev)
 }
 #endif
 
-static int mxlk_device_wait_status(struct mxlk_pcie *xdev, u32 image_id,
+static int xpcie_device_wait_status(struct xpcie_dev *xdev, u32 image_id,
 				   u32 timeout_ms)
 {
-	u32 status = MXLK_BOOT_STATUS_START;
+	u32 status = XPCIE_BOOT_STATUS_START;
 	int count = 0;
 
 	if (timeout_ms == 0)
 		timeout_ms = STATUS_TIMEOUT;
 
-	iowrite32(image_id, &xdev->mxlk.io_comm->mf_ready);
+	iowrite32(image_id, &xdev->xpcie.io_comm->mf_ready);
 
-	while (status != MXLK_BOOT_STATUS_DOWNLOADED) {
+	while (status != XPCIE_BOOT_STATUS_DOWNLOADED) {
 		mdelay(1);
 		if (++count > timeout_ms) {
 			dev_err(&xdev->pci->dev, "operation takes too long.\n");
 			return -ETIME;
 		}
 
-		status = ioread32(&xdev->mxlk.io_comm->mf_ready);
+		status = ioread32(&xdev->xpcie.io_comm->mf_ready);
 
 		switch (status) {
-		case MXLK_BOOT_STATUS_INVALID:
+		case XPCIE_BOOT_STATUS_INVALID:
 			dev_err(&xdev->pci->dev,
 				"the firmware image data is invalid.\n");
 			return -EINVAL;
-		case MXLK_BOOT_STATUS_ERROR:
+		case XPCIE_BOOT_STATUS_ERROR:
 			dev_err(&xdev->pci->dev,
 				"failed to download firmware image.\n");
 			return -EINVAL;
@@ -350,26 +353,26 @@ static int mxlk_device_wait_status(struct mxlk_pcie *xdev, u32 image_id,
 	return 0;
 }
 
-static int mxlk_device_transfer(struct mxlk_pcie *xdev, u32 image_id,
+static int xpcie_device_transfer(struct xpcie_dev *xdev, u32 image_id,
 				dma_addr_t addr, size_t size)
 {
 	int rc;
 
-	_iowrite64(addr, &xdev->mxlk.io_comm->mf_start);
-	iowrite32(size, &xdev->mxlk.io_comm->mf_len);
+	_iowrite64(addr, &xdev->xpcie.io_comm->mf_start);
+	iowrite32(size, &xdev->xpcie.io_comm->mf_len);
 
-	if (image_id == MXLK_BOOT_RAW_ID)
+	if (image_id == XPCIE_BOOT_RAW_ID)
 		_iowrite64(xdev->partition_offset,
-			   &xdev->mxlk.io_comm->mf_offset);
+			   &xdev->xpcie.io_comm->mf_offset);
 
-	rc = mxlk_device_wait_status(xdev, image_id, 0);
+	rc = xpcie_device_wait_status(xdev, image_id, 0);
 	if (!rc)
 		xdev->partition_offset += size;
 
 	return rc;
 }
 
-static int mxlk_device_download_common(struct mxlk_pcie *xdev, u32 image_id,
+static int xpcie_device_download_common(struct xpcie_dev *xdev, u32 image_id,
 				       const void *buf, size_t buf_size,
 				       bool no_copy)
 {
@@ -390,7 +393,7 @@ static int mxlk_device_download_common(struct mxlk_pcie *xdev, u32 image_id,
 		if (dma_mapping_error(dev, phys_addr))
 			return -ENOMEM;
 
-		rc = mxlk_device_transfer(xdev, image_id, phys_addr, size);
+		rc = xpcie_device_transfer(xdev, image_id, phys_addr, size);
 
 		dma_unmap_single(dev, phys_addr, size, DMA_TO_DEVICE);
 
@@ -404,7 +407,7 @@ static int mxlk_device_download_common(struct mxlk_pcie *xdev, u32 image_id,
 	return rc;
 }
 
-static int mxlk_device_download_firmware(struct mxlk_pcie *xdev, u32 image_id,
+static int xpcie_device_download_firmware(struct xpcie_dev *xdev, u32 image_id,
 					 const char *fw_image)
 {
 	const struct firmware *firmware;
@@ -412,9 +415,9 @@ static int mxlk_device_download_firmware(struct mxlk_pcie *xdev, u32 image_id,
 	int rc = 0;
 
 	switch (image_id) {
-	case MXLK_BOOT_FIP_ID:
-	case MXLK_BOOT_BOOT_ID:
-	case MXLK_BOOT_SYSTEM_ID:
+	case XPCIE_BOOT_FIP_ID:
+	case XPCIE_BOOT_BOOT_ID:
+	case XPCIE_BOOT_SYSTEM_ID:
 		break;
 	default:
 		dev_err(dev, "unknown firmware id\n");
@@ -435,7 +438,7 @@ static int mxlk_device_download_firmware(struct mxlk_pcie *xdev, u32 image_id,
 		goto firmware_cleanup;
 	}
 
-	rc = mxlk_device_download_common(xdev, image_id, firmware->data,
+	rc = xpcie_device_download_common(xdev, image_id, firmware->data,
 					 firmware->size, false);
 
 	kfree(xdev->dma_buf);
@@ -447,49 +450,49 @@ static int mxlk_device_download_firmware(struct mxlk_pcie *xdev, u32 image_id,
 	return rc;
 }
 
-static int mxlk_device_flashless_boot(struct mxlk_pcie *xdev)
+static int xpcie_device_flashless_boot(struct xpcie_dev *xdev)
 {
-	if (mxlk_device_download_firmware(xdev, MXLK_BOOT_BOOT_ID,
+	if (xpcie_device_download_firmware(xdev, XPCIE_BOOT_BOOT_ID,
 					  xdev->fw_name)) {
 		dev_err(&xdev->pci->dev, "failed to download boot image\n");
 		return -EIO;
 	}
 
-	iowrite32(MXLK_BOOT_STATUS_DONE, &xdev->mxlk.io_comm->mf_ready);
+	iowrite32(XPCIE_BOOT_STATUS_DONE, &xdev->xpcie.io_comm->mf_ready);
 
 	return 0;
 }
 
-static int mxlk_device_fip(struct mxlk_pcie *xdev)
+static int xpcie_device_fip(struct xpcie_dev *xdev)
 {
-	if (mxlk_device_download_firmware(xdev, MXLK_BOOT_FIP_ID,
+	if (xpcie_device_download_firmware(xdev, XPCIE_BOOT_FIP_ID,
 					  xdev->fw_name)) {
 		dev_err(&xdev->pci->dev, "failed to download FIP image\n");
 		return -EIO;
 	}
 
-	iowrite32(MXLK_BOOT_STATUS_DONE, &xdev->mxlk.io_comm->mf_ready);
+	iowrite32(XPCIE_BOOT_STATUS_DONE, &xdev->xpcie.io_comm->mf_ready);
 
 	return 0;
 }
 
-static void mxlk_device_enable_irq(struct mxlk_pcie *xdev)
+static void xpcie_device_enable_irq(struct xpcie_dev *xdev)
 {
-	iowrite32(MXLK_INT_ENABLE, &xdev->mxlk.io_comm->int_enable);
-	iowrite32(~MXLK_INT_MASK, &xdev->mxlk.io_comm->int_mask);
+	iowrite32(XPCIE_INT_ENABLE, &xdev->xpcie.io_comm->int_enable);
+	iowrite32(~XPCIE_INT_MASK, &xdev->xpcie.io_comm->int_mask);
 }
 
-static void mxlk_device_poll(struct work_struct *work)
+static void xpcie_device_poll(struct work_struct *work)
 {
-	struct mxlk_pcie *xdev = container_of(work, struct mxlk_pcie,
+	struct xpcie_dev *xdev = container_of(work, struct xpcie_dev,
 					      wait_event.work);
-	enum mxlk_stage stage = mxlk_check_magic(xdev);
+	enum xpcie_stage stage = intel_xpcie_check_magic(xdev);
 
 	if (stage == STAGE_RECOV) {
-		xdev->mxlk.status = MXLK_STATUS_RECOVERY;
+		xdev->xpcie.status = XPCIE_STATUS_RECOVERY;
 		wake_up_interruptible(&xdev->waitqueue);
 	} else if (stage == STAGE_OS) {
-		xdev->mxlk.status = MXLK_STATUS_READY;
+		xdev->xpcie.status = XPCIE_STATUS_READY;
 		wake_up_interruptible(&xdev->waitqueue);
 		return;
 	}
@@ -497,30 +500,31 @@ static void mxlk_device_poll(struct work_struct *work)
 	schedule_delayed_work(&xdev->wait_event, msecs_to_jiffies(100));
 }
 
-static int mxlk_pci_prepare_dev_reset(struct mxlk_pcie *xdev, bool notify);
+static int intel_xpcie_pci_prepare_dev_reset(struct xpcie_dev *xdev,
+					     bool notify);
 
-static void mxlk_device_shutdown(struct work_struct *work)
+static void xpcie_device_shutdown(struct work_struct *work)
 {
-	struct mxlk_pcie *xdev = container_of(work, struct mxlk_pcie,
+	struct xpcie_dev *xdev = container_of(work, struct xpcie_dev,
 					      shutdown_event.work);
 
-	mxlk_pci_prepare_dev_reset(xdev, false);
+	intel_xpcie_pci_prepare_dev_reset(xdev, false);
 }
 
-static int mxlk_device_init(struct mxlk_pcie *xdev)
+static int xpcie_device_init(struct xpcie_dev *xdev)
 {
 	int rc;
 
-	INIT_DELAYED_WORK(&xdev->wait_event, mxlk_device_poll);
-	INIT_DELAYED_WORK(&xdev->shutdown_event, mxlk_device_shutdown);
+	INIT_DELAYED_WORK(&xdev->wait_event, xpcie_device_poll);
+	INIT_DELAYED_WORK(&xdev->shutdown_event, xpcie_device_shutdown);
 
-	rc = mxlk_pci_irq_init(xdev);
+	rc = intel_xpcie_pci_irq_init(xdev);
 	if (rc)
 		return rc;
 
 	pci_set_master(xdev->pci);
 
-	xdev->mxlk.status = MXLK_STATUS_UNINIT;
+	xdev->xpcie.status = XPCIE_STATUS_UNINIT;
 
 	init_waitqueue_head(&xdev->waitqueue);
 	schedule_delayed_work(&xdev->wait_event, 0);
@@ -528,7 +532,7 @@ static int mxlk_device_init(struct mxlk_pcie *xdev)
 	return rc;
 }
 
-int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev)
+int intel_xpcie_pci_init(struct xpcie_dev *xdev, struct pci_dev *pdev)
 {
 	int rc;
 
@@ -544,13 +548,13 @@ int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev)
 		goto error_exit;
 	}
 
-	rc = pci_request_regions(xdev->pci, MXLK_DRIVER_NAME);
+	rc = pci_request_regions(xdev->pci, XPCIE_DRIVER_NAME);
 	if (rc) {
 		dev_err(&pdev->dev, "failed to request mmio regions\n");
 		goto error_req_mem;
 	}
 
-	rc = mxlk_pci_map_bar(xdev);
+	rc = intel_xpcie_pci_map_bar(xdev);
 	if (rc)
 		goto error_map;
 
@@ -560,23 +564,23 @@ int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev)
 		goto error_dma_mask;
 	}
 
-	mxlk_pci_set_aspm(xdev, aspm_enable);
+	intel_xpcie_pci_set_aspm(xdev, aspm_enable);
 
-	rc = mxlk_pci_setup_recovery_sysfs(xdev);
+	rc = intel_xpcie_pci_setup_recovery_sysfs(xdev);
 	if (rc) {
 		dev_err(&pdev->dev,
 			"failed to setup recovery sysfs facilities\n");
 		goto error_dma_mask;
 	}
 
-	mxlk_init_debug(&xdev->mxlk, &xdev->pci->dev);
+	intel_xpcie_init_debug(&xdev->xpcie, &xdev->pci->dev);
 
-	rc = mxlk_device_init(xdev);
+	rc = xpcie_device_init(xdev);
 	if (!rc)
 		goto init_exit;
 
 error_dma_mask:
-	mxlk_pci_unmap_bar(xdev);
+	intel_xpcie_pci_unmap_bar(xdev);
 
 error_map:
 	pci_release_regions(xdev->pci);
@@ -585,7 +589,7 @@ int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev)
 	pci_disable_device(xdev->pci);
 
 error_exit:
-	xdev->mxlk.status = MXLK_STATUS_ERROR;
+	xdev->xpcie.status = XPCIE_STATUS_ERROR;
 
 init_exit:
 	mutex_unlock(&xdev->lock);
@@ -594,30 +598,30 @@ int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev)
 	return rc;
 }
 
-int mxlk_pci_cleanup(struct mxlk_pcie *xdev)
+int intel_xpcie_pci_cleanup(struct xpcie_dev *xdev)
 {
 	if (mutex_lock_interruptible(&xdev->lock))
 		return -EINTR;
 
-	mxlk_pci_cleanup_recovery_sysfs(xdev);
-	mxlk_uninit_debug(&xdev->mxlk, &xdev->pci->dev);
+	intel_xpcie_pci_cleanup_recovery_sysfs(xdev);
+	intel_xpcie_uninit_debug(&xdev->xpcie, &xdev->pci->dev);
 
 	cancel_delayed_work_sync(&xdev->wait_event);
 	cancel_delayed_work_sync(&xdev->shutdown_event);
 	xdev->core_irq_callback = NULL;
-	mxlk_pci_irq_cleanup(xdev);
+	intel_xpcie_pci_irq_cleanup(xdev);
 
 	kfree(xdev->dma_buf);
 	xdev->dma_buf = NULL;
 	xdev->dma_buf_offset = 0;
 
-	mxlk_core_cleanup(&xdev->mxlk);
+	intel_xpcie_core_cleanup(&xdev->xpcie);
 
-	mxlk_pci_unmap_bar(xdev);
+	intel_xpcie_pci_unmap_bar(xdev);
 	pci_release_regions(xdev->pci);
 	pci_disable_device(xdev->pci);
 	pci_set_drvdata(xdev->pci, NULL);
-	xdev->mxlk.status = MXLK_STATUS_OFF;
+	xdev->xpcie.status = XPCIE_STATUS_OFF;
 	xdev->irq_enabled = false;
 
 	mutex_unlock(&xdev->lock);
@@ -625,9 +629,10 @@ int mxlk_pci_cleanup(struct mxlk_pcie *xdev)
 	return 0;
 }
 
-int mxlk_pci_register_irq(struct mxlk_pcie *xdev, irq_handler_t irq_handler)
+int intel_xpcie_pci_register_irq(struct xpcie_dev *xdev,
+				 irq_handler_t irq_handler)
 {
-	if (xdev->mxlk.status != MXLK_STATUS_READY)
+	if (xdev->xpcie.status != XPCIE_STATUS_READY)
 		return -EINVAL;
 
 	xdev->core_irq_callback = irq_handler;
@@ -635,21 +640,22 @@ int mxlk_pci_register_irq(struct mxlk_pcie *xdev, irq_handler_t irq_handler)
 	return 0;
 }
 
-int mxlk_pci_raise_irq(struct mxlk_pcie *xdev, enum mxlk_doorbell_type type,
-		       u8 value)
+int intel_xpcie_pci_raise_irq(struct xpcie_dev *xdev,
+			      enum xpcie_doorbell_type type,
+			      u8 value)
 {
 	u16 pci_status;
 
-	mxlk_set_doorbell(&xdev->mxlk, TO_DEVICE, type, value);
+	intel_xpcie_set_doorbell(&xdev->xpcie, TO_DEVICE, type, value);
 	pci_read_config_word(xdev->pci, PCI_STATUS, &pci_status);
 
 	return 0;
 }
 
-u32 mxlk_get_device_num(u32 *id_list)
+u32 intel_xpcie_get_device_num(u32 *id_list)
 {
 	u32 num = 0;
-	struct mxlk_pcie *p;
+	struct xpcie_dev *p;
 
 	mutex_lock(&dev_list_mutex);
 
@@ -667,18 +673,20 @@ u32 mxlk_get_device_num(u32 *id_list)
 	return num;
 }
 
-int mxlk_get_device_name_by_id(u32 id, char *device_name, size_t name_size)
+int intel_xpcie_get_device_name_by_id(u32 id,
+				      char *device_name, size_t name_size)
 {
-	struct mxlk_pcie *xdev;
+	struct xpcie_dev *xdev;
 	size_t size;
 
-	xdev = mxlk_get_device_by_id(id);
+	xdev = intel_xpcie_get_device_by_id(id);
 	if (!xdev)
 		return -ENODEV;
 
 	mutex_lock(&xdev->lock);
 
-	size = (name_size > MXLK_MAX_NAME_LEN) ? MXLK_MAX_NAME_LEN : name_size;
+	size = (name_size > XPCIE_MAX_NAME_LEN) ?
+		XPCIE_MAX_NAME_LEN : name_size;
 	strncpy(device_name, xdev->name, size);
 
 	mutex_unlock(&xdev->lock);
@@ -686,21 +694,21 @@ int mxlk_get_device_name_by_id(u32 id, char *device_name, size_t name_size)
 	return 0;
 }
 
-int mxlk_get_device_status_by_id(u32 id, u32 *status)
+int intel_xpcie_get_device_status_by_id(u32 id, u32 *status)
 {
-	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+	struct xpcie_dev *xdev = intel_xpcie_get_device_by_id(id);
 
 	if (!xdev)
 		return -ENODEV;
 
 	mutex_lock(&xdev->lock);
-	*status = xdev->mxlk.status;
+	*status = xdev->xpcie.status;
 	mutex_unlock(&xdev->lock);
 
 	return 0;
 }
 
-#define mxlk_wait_event(cond)						\
+#define intel_xpcie_wait_event(cond)					\
 ({									\
 	int rc = 0;							\
 	int error = wait_event_interruptible_timeout(xdev->waitqueue,	\
@@ -712,41 +720,42 @@ int mxlk_get_device_status_by_id(u32 id, u32 *status)
 	rc;								\
 })
 
-int mxlk_pci_boot_device(u32 id, const char *binary_name)
+int intel_xpcie_pci_boot_device(u32 id, const char *binary_name)
 {
 	int rc = 0;
-	u32 expected = MXLK_STATUS_ERROR;
-	struct mxlk_pcie *xdev;
+	u32 expected = XPCIE_STATUS_ERROR;
+	struct xpcie_dev *xdev;
 
-	xdev = mxlk_get_device_by_id(id);
+	xdev = intel_xpcie_get_device_by_id(id);
 	if (!xdev)
 		return -ENODEV;
 
 	if (mutex_lock_interruptible(&xdev->lock))
 		return -EINTR;
 
-	if (xdev->mxlk.status == MXLK_STATUS_OFF) {
+	if (xdev->xpcie.status == XPCIE_STATUS_OFF) {
 		rc = -ENODEV;
 		goto boot_cleanup;
 	}
 
-	strncpy(xdev->fw_name, binary_name, MXLK_MAX_NAME_LEN - 1);
+	strncpy(xdev->fw_name, binary_name, XPCIE_MAX_NAME_LEN - 1);
 
 	if (!xdev->irq_enabled) {
-		mxlk_device_enable_irq(xdev);
+		xpcie_device_enable_irq(xdev);
 		xdev->irq_enabled = true;
 
-		rc = mxlk_wait_event(xdev->mxlk.status != MXLK_STATUS_UNINIT);
+		rc = intel_xpcie_wait_event(xdev->xpcie.status !=
+					    XPCIE_STATUS_UNINIT);
 		if (rc)
 			goto boot_cleanup;
 	}
 
-	switch (xdev->mxlk.status) {
-	case MXLK_STATUS_BOOT_FW:
-		xdev->mxlk.status = MXLK_STATUS_BOOT_PRE_OS;
-		rc = mxlk_device_fip(xdev);
+	switch (xdev->xpcie.status) {
+	case XPCIE_STATUS_BOOT_FW:
+		xdev->xpcie.status = XPCIE_STATUS_BOOT_PRE_OS;
+		rc = xpcie_device_fip(xdev);
 		goto boot_cleanup;
-	case MXLK_STATUS_BOOT_PRE_OS:
+	case XPCIE_STATUS_BOOT_PRE_OS:
 		/*
 		 * This fake stage is to avoid boot timeout after flashing FIP
 		 * if the function only returns after entering BOOT_OS stage.
@@ -756,35 +765,36 @@ int mxlk_pci_boot_device(u32 id, const char *binary_name)
 		 * So let boot call on FIP return early and check stage in next
 		 * boot call for OS.
 		 */
-		rc = mxlk_wait_event(xdev->mxlk.status == MXLK_STATUS_BOOT_OS);
+		rc = intel_xpcie_wait_event(xdev->xpcie.status ==
+					    XPCIE_STATUS_BOOT_OS);
 		if (rc)
 			goto boot_cleanup;
-		rc = mxlk_device_flashless_boot(xdev);
+		rc = xpcie_device_flashless_boot(xdev);
 		if (rc)
 			goto boot_cleanup;
-		expected = MXLK_STATUS_READY;
+		expected = XPCIE_STATUS_READY;
 		break;
-	case MXLK_STATUS_BOOT_OS:
-		rc = mxlk_device_flashless_boot(xdev);
+	case XPCIE_STATUS_BOOT_OS:
+		rc = xpcie_device_flashless_boot(xdev);
 		if (rc)
 			goto boot_cleanup;
-		expected = MXLK_STATUS_READY;
+		expected = XPCIE_STATUS_READY;
 		break;
-	case MXLK_STATUS_READY:
-	case MXLK_STATUS_RUN:
+	case XPCIE_STATUS_READY:
+	case XPCIE_STATUS_RUN:
 		rc = 0;
 		goto boot_cleanup;
-	case MXLK_STATUS_RECOVERY:
-	case MXLK_STATUS_ERROR:
+	case XPCIE_STATUS_RECOVERY:
+	case XPCIE_STATUS_ERROR:
 	default:
 		rc = -EIO;
 		goto boot_cleanup;
 	}
 
-	rc = mxlk_wait_event((xdev->mxlk.status == expected) ||
-			     (xdev->mxlk.status == MXLK_STATUS_RECOVERY));
+	rc = intel_xpcie_wait_event((xdev->xpcie.status == expected) ||
+			     (xdev->xpcie.status == XPCIE_STATUS_RECOVERY));
 
-	if (xdev->mxlk.status == MXLK_STATUS_RECOVERY)
+	if (xdev->xpcie.status == XPCIE_STATUS_RECOVERY)
 		rc = -EIO;
 
 boot_cleanup:
@@ -793,32 +803,32 @@ int mxlk_pci_boot_device(u32 id, const char *binary_name)
 	return rc;
 }
 
-int mxlk_pci_connect_device(u32 id)
+int intel_xpcie_pci_connect_device(u32 id)
 {
 	int rc = 0;
-	struct mxlk_pcie *xdev;
+	struct xpcie_dev *xdev;
 
-	xdev = mxlk_get_device_by_id(id);
+	xdev = intel_xpcie_get_device_by_id(id);
 	if (!xdev)
 		return -ENODEV;
 
 	if (mutex_lock_interruptible(&xdev->lock))
 		return -EINTR;
 
-	if (xdev->mxlk.status == MXLK_STATUS_RUN)
+	if (xdev->xpcie.status == XPCIE_STATUS_RUN)
 		goto connect_cleanup;
 
-	if (xdev->mxlk.status == MXLK_STATUS_OFF) {
+	if (xdev->xpcie.status == XPCIE_STATUS_OFF) {
 		rc = -ENODEV;
 		goto connect_cleanup;
 	}
 
-	if (xdev->mxlk.status != MXLK_STATUS_READY) {
+	if (xdev->xpcie.status != XPCIE_STATUS_READY) {
 		rc = -EBUSY;
 		goto connect_cleanup;
 	}
 
-	rc = mxlk_core_init(&xdev->mxlk);
+	rc = intel_xpcie_core_init(&xdev->xpcie);
 	if (rc < 0) {
 		dev_err(&xdev->pci->dev, "failed to sync with device\n");
 		goto connect_cleanup;
@@ -829,80 +839,82 @@ int mxlk_pci_connect_device(u32 id)
 	return rc;
 }
 
-int mxlk_pci_read(u32 id, void *data, size_t *size, u32 timeout)
+int intel_xpcie_pci_read(u32 id, void *data, size_t *size, u32 timeout)
 {
-	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+	struct xpcie_dev *xdev = intel_xpcie_get_device_by_id(id);
 
 	if (!xdev)
 		return -ENODEV;
 
-	return mxlk_core_read(&xdev->mxlk, data, size, timeout);
+	return intel_xpcie_core_read(&xdev->xpcie, data, size, timeout);
 }
 
-int mxlk_pci_write(u32 id, void *data, size_t *size, u32 timeout)
+int intel_xpcie_pci_write(u32 id, void *data, size_t *size, u32 timeout)
 {
-	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+	struct xpcie_dev *xdev = intel_xpcie_get_device_by_id(id);
 
 	if (!xdev)
 		return -ENODEV;
 
-	return mxlk_core_write(&xdev->mxlk, data, size, timeout);
+	return intel_xpcie_core_write(&xdev->xpcie, data, size, timeout);
 }
 
-static int mxlk_pci_prepare_dev_reset(struct mxlk_pcie *xdev, bool notify)
+static int intel_xpcie_pci_prepare_dev_reset(struct xpcie_dev *xdev,
+					     bool notify)
 {
 	if (mutex_lock_interruptible(&xdev->lock))
 		return -EINTR;
 
 	if (xdev->core_irq_callback) {
 		xdev->core_irq_callback = NULL;
-		mxlk_core_cleanup(&xdev->mxlk);
+		intel_xpcie_core_cleanup(&xdev->xpcie);
 	}
-	xdev->mxlk.status = MXLK_STATUS_OFF;
+	xdev->xpcie.status = XPCIE_STATUS_OFF;
 	if (notify)
-		mxlk_pci_raise_irq(xdev, DEV_EVENT, REQUEST_RESET);
+		intel_xpcie_pci_raise_irq(xdev, DEV_EVENT, REQUEST_RESET);
 
 	mutex_unlock(&xdev->lock);
 
 	return 0;
 }
 
-int mxlk_pci_reset_device(u32 id)
+int intel_xpcie_pci_reset_device(u32 id)
 {
-	struct mxlk_pcie *xdev = mxlk_get_device_by_id(id);
+	struct xpcie_dev *xdev = intel_xpcie_get_device_by_id(id);
 
 	if (!xdev)
 		return -ENOMEM;
 
-	return mxlk_pci_prepare_dev_reset(xdev, true);
+	return intel_xpcie_pci_prepare_dev_reset(xdev, true);
 }
 
-u64 mxlk_pci_hw_dev_id(struct mxlk_pcie *xdev)
+u64 intel_xpcie_pci_hw_dev_id(struct xpcie_dev *xdev)
 {
-	return _ioread64(&xdev->mxlk.io_comm->dev_id);
+	return _ioread64(&xdev->xpcie.io_comm->dev_id);
 }
 
-static int mxlk_boot_access_enter(struct mxlk_pcie *xdev)
+static int intel_xpcie_boot_access_enter(struct xpcie_dev *xdev)
 {
 	if (mutex_lock_interruptible(&xdev->lock))
 		return -EINTR;
-	if (xdev->mxlk.status != MXLK_STATUS_RECOVERY) {
+	if (xdev->xpcie.status != XPCIE_STATUS_RECOVERY) {
 		mutex_unlock(&xdev->lock);
 		return -EROFS;
 	}
 	return 0;
 }
 
-static void mxlk_boot_access_exit(struct mxlk_pcie *xdev)
+static void intel_xpcie_boot_access_exit(struct xpcie_dev *xdev)
 {
 	mutex_unlock(&xdev->lock);
 }
 
-static int mxlk_recovery_send_left(struct mxlk_pcie *xdev, bool reset_offset)
+static int intel_xpcie_recovery_send_left(struct xpcie_dev *xdev,
+					  bool reset_offset)
 {
 	int rc;
 
-	rc = mxlk_device_download_common(xdev, MXLK_BOOT_RAW_ID, NULL,
+	rc = xpcie_device_download_common(xdev, XPCIE_BOOT_RAW_ID, NULL,
 			xdev->dma_buf_offset, true);
 	xdev->dma_buf_offset = 0;
 	if (reset_offset)
@@ -911,88 +923,88 @@ static int mxlk_recovery_send_left(struct mxlk_pcie *xdev, bool reset_offset)
 	return rc;
 }
 
-static int mxlk_pci_flash_gpt_table(struct mxlk_pcie *xdev)
+static int intel_xpcie_pci_flash_gpt_table(struct xpcie_dev *xdev)
 {
 	int rc = 0;
 
-	rc = mxlk_boot_access_enter(xdev);
+	rc = intel_xpcie_boot_access_enter(xdev);
 	if (rc)
 		return rc;
 
 	if (xdev->dma_buf_offset) {
-		rc = mxlk_recovery_send_left(xdev, true);
+		rc = intel_xpcie_recovery_send_left(xdev, true);
 		if (rc)
 			goto create_error;
 	}
 
-	rc = mxlk_device_wait_status(xdev, MXLK_BOOT_FLASH_ID, 0);
+	rc = xpcie_device_wait_status(xdev, XPCIE_BOOT_FLASH_ID, 0);
 
 create_error:
-	mxlk_boot_access_exit(xdev);
+	intel_xpcie_boot_access_exit(xdev);
 	return rc;
 }
 
-static int mxlk_pci_erase_partition(struct mxlk_pcie *xdev,
+static int intel_xpcie_pci_erase_partition(struct xpcie_dev *xdev,
 				    const char *partition, size_t len)
 {
 	int rc = 0;
 
-	rc = mxlk_boot_access_enter(xdev);
+	rc = intel_xpcie_boot_access_enter(xdev);
 	if (rc)
 		return rc;
 
 	if (xdev->dma_buf_offset) {
-		rc = mxlk_recovery_send_left(xdev, true);
+		rc = intel_xpcie_recovery_send_left(xdev, true);
 		if (rc)
 			goto erase_error;
 	}
 
-	memcpy_toio(xdev->mxlk.io_comm->mf_dest, partition, len);
+	memcpy_toio(xdev->xpcie.io_comm->mf_dest, partition, len);
 
-	rc = mxlk_device_wait_status(xdev, MXLK_BOOT_ERASE_ID, ERASE_TIMEOUT);
+	rc = xpcie_device_wait_status(xdev, XPCIE_BOOT_ERASE_ID, ERASE_TIMEOUT);
 
 erase_error:
-	mxlk_boot_access_exit(xdev);
+	intel_xpcie_boot_access_exit(xdev);
 	return rc;
 }
 
-static int mxlk_pci_flash_partition_start(struct mxlk_pcie *xdev,
+static int intel_xpcie_pci_flash_partition_start(struct xpcie_dev *xdev,
 					  const char *partition,
 					  size_t name_len)
 {
 	int rc = 0;
 
-	rc = mxlk_boot_access_enter(xdev);
+	rc = intel_xpcie_boot_access_enter(xdev);
 	if (rc)
 		return rc;
 
 	if (xdev->dma_buf_offset) {
-		rc = mxlk_recovery_send_left(xdev, true);
+		rc = intel_xpcie_recovery_send_left(xdev, true);
 		if (rc)
 			goto start_error;
 	}
 
-	memset(xdev->partition_name, 0, MXLK_BOOT_DEST_STRLEN);
+	memset(xdev->partition_name, 0, XPCIE_BOOT_DEST_STRLEN);
 	memcpy(xdev->partition_name, partition,
-		(name_len >= MXLK_BOOT_DEST_STRLEN) ?
-			(MXLK_BOOT_DEST_STRLEN - 1) : name_len);
+		(name_len >= XPCIE_BOOT_DEST_STRLEN) ?
+			(XPCIE_BOOT_DEST_STRLEN - 1) : name_len);
 	xdev->partition_offset = 0;
 
-	memcpy_toio(xdev->mxlk.io_comm->mf_dest, xdev->partition_name,
-		    MXLK_BOOT_DEST_STRLEN);
+	memcpy_toio(xdev->xpcie.io_comm->mf_dest, xdev->partition_name,
+		    XPCIE_BOOT_DEST_STRLEN);
 
 start_error:
-	mxlk_boot_access_exit(xdev);
+	intel_xpcie_boot_access_exit(xdev);
 	return rc;
 }
 
-static int mxlk_pci_flash_partition_send(struct mxlk_pcie *xdev,
+static int intel_xpcie_pci_flash_partition_send(struct xpcie_dev *xdev,
 					 const void *data, size_t size)
 {
 	int rc = 0;
 	int size_left = size;
 
-	rc = mxlk_boot_access_enter(xdev);
+	rc = intel_xpcie_boot_access_enter(xdev);
 	if (rc)
 		return rc;
 
@@ -1015,36 +1027,36 @@ static int mxlk_pci_flash_partition_send(struct mxlk_pcie *xdev,
 		data += size;
 
 		if (xdev->dma_buf_offset == SECTION_SIZE)
-			rc = mxlk_recovery_send_left(xdev, false);
+			rc = intel_xpcie_recovery_send_left(xdev, false);
 	}
 
 send_error:
-	mxlk_boot_access_exit(xdev);
+	intel_xpcie_boot_access_exit(xdev);
 	return rc;
 }
 
-static int mxlk_pci_flash_done(struct mxlk_pcie *xdev)
+static int intel_xpcie_pci_flash_done(struct xpcie_dev *xdev)
 {
 	int rc = 0;
 
-	rc = mxlk_boot_access_enter(xdev);
+	rc = intel_xpcie_boot_access_enter(xdev);
 	if (rc)
 		return rc;
 
 	if (xdev->dma_buf_offset) {
-		rc = mxlk_recovery_send_left(xdev, true);
+		rc = intel_xpcie_recovery_send_left(xdev, true);
 		if (rc)
 			goto done_error;
 	}
 
-	iowrite32(MXLK_BOOT_STATUS_DONE, &xdev->mxlk.io_comm->mf_ready);
+	iowrite32(XPCIE_BOOT_STATUS_DONE, &xdev->xpcie.io_comm->mf_ready);
 
 	kfree(xdev->dma_buf);
 	xdev->dma_buf = NULL;
 	xdev->dma_buf_offset = 0;
 
 done_error:
-	mxlk_boot_access_exit(xdev);
+	intel_xpcie_boot_access_exit(xdev);
 	return rc;
 }
 
@@ -1054,9 +1066,9 @@ static ssize_t partition_store(struct device *dev,
 {
 	int rc;
 
-	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+	struct xpcie_dev *xdev = dev_get_drvdata(dev);
 
-	rc = mxlk_pci_flash_partition_start(xdev, buf, count);
+	rc = intel_xpcie_pci_flash_partition_start(xdev, buf, count);
 	if (rc) {
 		dev_err(dev, "failed to flash partition\n");
 		return rc;
@@ -1072,14 +1084,14 @@ static ssize_t create_partitions_store(struct device *dev,
 {
 	int rc;
 	long value;
-	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+	struct xpcie_dev *xdev = dev_get_drvdata(dev);
 
 	rc = kstrtol(buf, 10, &value);
 	if (rc)
 		return rc;
 
 	if (value) {
-		rc = mxlk_pci_flash_gpt_table(xdev);
+		rc = intel_xpcie_pci_flash_gpt_table(xdev);
 		if (rc) {
 			dev_err(dev, "failed to flash gpt table\n");
 			return rc;
@@ -1096,15 +1108,15 @@ static ssize_t erase_partition_store(struct device *dev,
 {
 	int rc;
 	long value;
-	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+	struct xpcie_dev *xdev = dev_get_drvdata(dev);
 
 	rc = kstrtol(buf, 10, &value);
 	if (rc)
 		return rc;
 
 	if (value) {
-		rc = mxlk_pci_erase_partition(xdev, xdev->partition_name,
-					      MXLK_BOOT_DEST_STRLEN);
+		rc = intel_xpcie_pci_erase_partition(xdev, xdev->partition_name,
+					      XPCIE_BOOT_DEST_STRLEN);
 		if (rc) {
 			dev_err(dev, "failed to erase partition\n");
 			return rc;
@@ -1121,14 +1133,14 @@ static ssize_t recovery_done_store(struct device *dev,
 {
 	int rc;
 	long value;
-	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+	struct xpcie_dev *xdev = dev_get_drvdata(dev);
 
 	rc = kstrtol(buf, 10, &value);
 	if (rc)
 		return rc;
 
 	if (value) {
-		rc = mxlk_pci_flash_done(xdev);
+		rc = intel_xpcie_pci_flash_done(xdev);
 		if (rc) {
 			dev_err(dev, "failed to recover\n");
 			return rc;
@@ -1145,9 +1157,9 @@ static ssize_t recov_write_data(struct file *filp, struct kobject *kobj,
 {
 	int rc;
 	struct device *dev = kobj_to_dev(kobj);
-	struct mxlk_pcie *xdev = dev_get_drvdata(dev);
+	struct xpcie_dev *xdev = dev_get_drvdata(dev);
 
-	rc = mxlk_pci_flash_partition_send(xdev, buf, count);
+	rc = intel_xpcie_pci_flash_partition_send(xdev, buf, count);
 	if (rc) {
 		dev_err(dev, "failed to flash partition\n");
 		return rc;
@@ -1180,12 +1192,12 @@ static const struct attribute_group *recovery_groups[] = {
 	NULL,
 };
 
-static int mxlk_pci_setup_recovery_sysfs(struct mxlk_pcie *xdev)
+static int intel_xpcie_pci_setup_recovery_sysfs(struct xpcie_dev *xdev)
 {
 	return sysfs_create_groups(&xdev->pci->dev.kobj, recovery_groups);
 }
 
-static void mxlk_pci_cleanup_recovery_sysfs(struct mxlk_pcie *xdev)
+static void intel_xpcie_pci_cleanup_recovery_sysfs(struct xpcie_dev *xdev)
 {
 	sysfs_remove_groups(&xdev->pci->dev.kobj, recovery_groups);
 }
diff --git a/drivers/misc/xlink-pcie/remote_host/pci.h b/drivers/misc/xlink-pcie/remote_host/pci.h
index 060b0a6d38d3..4208141f34f6 100644
--- a/drivers/misc/xlink-pcie/remote_host/pci.h
+++ b/drivers/misc/xlink-pcie/remote_host/pci.h
@@ -7,26 +7,26 @@
  *
  ****************************************************************************/
 
-#ifndef MXLK_PCI_HEADER_
-#define MXLK_PCI_HEADER_
+#ifndef XPCIE_PCI_HEADER_
+#define XPCIE_PCI_HEADER_
 
 #include <linux/list.h>
 #include <linux/interrupt.h>
 #include <linux/xlink_drv_inf.h>
-#include "../common/xlink_pcie.h"
+#include "../common/xpcie.h"
 #include "../common/boot.h"
 #include "../common/util.h"
 
-#define MXLK_MAX_NAME_LEN (32)
+#define XPCIE_MAX_NAME_LEN (32)
 
-struct mxlk_pcie {
+struct xpcie_dev {
 	struct list_head list;
 	struct mutex lock;
 
 	struct pci_dev *pci;
-	char name[MXLK_MAX_NAME_LEN];
+	char name[XPCIE_MAX_NAME_LEN];
 	u32 devid;
-	char fw_name[MXLK_MAX_NAME_LEN];
+	char fw_name[XPCIE_MAX_NAME_LEN];
 
 	struct delayed_work wait_event;
 	struct delayed_work shutdown_event;
@@ -34,42 +34,46 @@ struct mxlk_pcie {
 	bool irq_enabled;
 	irq_handler_t core_irq_callback;
 
-	char partition_name[MXLK_BOOT_DEST_STRLEN];
+	char partition_name[XPCIE_BOOT_DEST_STRLEN];
 	unsigned long partition_offset;
 	void *dma_buf;
 	size_t dma_buf_offset;
 
-	struct mxlk mxlk;
+	struct xpcie xpcie;
 };
 
-static inline struct device *mxlk_to_dev(struct mxlk *mxlk)
+static inline struct device *xpcie_to_dev(struct xpcie *xpcie)
 {
-	struct mxlk_pcie *xdev = container_of(mxlk, struct mxlk_pcie, mxlk);
+	struct xpcie_dev *xdev = container_of(xpcie, struct xpcie_dev, xpcie);
 
 	return &xdev->pci->dev;
 }
 
-int mxlk_pci_init(struct mxlk_pcie *xdev, struct pci_dev *pdev);
-int mxlk_pci_cleanup(struct mxlk_pcie *xdev);
-int mxlk_pci_register_irq(struct mxlk_pcie *xdev, irq_handler_t irq_handler);
-int mxlk_pci_raise_irq(struct mxlk_pcie *xdev, enum mxlk_doorbell_type type,
-		       u8 value);
+int intel_xpcie_pci_init(struct xpcie_dev *xdev, struct pci_dev *pdev);
+int intel_xpcie_pci_cleanup(struct xpcie_dev *xdev);
+int intel_xpcie_pci_register_irq(struct xpcie_dev *xdev,
+				 irq_handler_t irq_handler);
+int intel_xpcie_pci_raise_irq(struct xpcie_dev *xdev,
+			      enum xpcie_doorbell_type type,
+			      u8 value);
 
-struct mxlk_pcie *mxlk_create_device(u32 sw_device_id, struct pci_dev *pdev);
-void mxlk_remove_device(struct mxlk_pcie *xdev);
-void mxlk_list_add_device(struct mxlk_pcie *xdev);
-void mxlk_list_del_device(struct mxlk_pcie *xdev);
-u32 mxlk_get_device_num(u32 *id_list);
-struct mxlk_pcie *mxlk_get_device_by_id(u32 id);
-int mxlk_get_device_name_by_id(u32 id, char *device_name, size_t name_size);
-int mxlk_get_device_status_by_id(u32 id, u32 *status);
+struct xpcie_dev *intel_xpcie_create_device(u32 sw_device_id,
+					    struct pci_dev *pdev);
+void intel_xpcie_remove_device(struct xpcie_dev *xdev);
+void intel_xpcie_list_add_device(struct xpcie_dev *xdev);
+void intel_xpcie_list_del_device(struct xpcie_dev *xdev);
+u32 intel_xpcie_get_device_num(u32 *id_list);
+struct xpcie_dev *intel_xpcie_get_device_by_id(u32 id);
+int intel_xpcie_get_device_name_by_id(u32 id, char *device_name,
+				      size_t name_size);
+int intel_xpcie_get_device_status_by_id(u32 id, u32 *status);
 
-int mxlk_pci_boot_device(u32 id, const char *binary_name);
-int mxlk_pci_connect_device(u32 id);
-int mxlk_pci_read(u32 id, void *data, size_t *size, u32 timeout);
-int mxlk_pci_write(u32 id, void *data, size_t *size, u32 timeout);
-int mxlk_pci_reset_device(u32 id);
+int intel_xpcie_pci_boot_device(u32 id, const char *binary_name);
+int intel_xpcie_pci_connect_device(u32 id);
+int intel_xpcie_pci_read(u32 id, void *data, size_t *size, u32 timeout);
+int intel_xpcie_pci_write(u32 id, void *data, size_t *size, u32 timeout);
+int intel_xpcie_pci_reset_device(u32 id);
 
-u64 mxlk_pci_hw_dev_id(struct mxlk_pcie *xdev);
+u64 intel_xpcie_pci_hw_dev_id(struct xpcie_dev *xdev);
 
 #endif
-- 
2.27.0

